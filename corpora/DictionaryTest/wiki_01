<doc id="32611510" url="https://en.wikipedia.org/wiki?curid=32611510" title="Mixlink">
Mixlink

Mixlink (Mixlink II) is a computer used with Agfa scales. It was developed to facilitate calculation of color mixes.

Mixlink contains the Intel 80386 processor with the clock rate which may be set to either 9.2 or 33 MHz. The RAM size is 640 kB. The HDD function is served by the built-in flash memory that has the size of approximately 800 kB. Mixlink has the monochrome display.

Mixlink is also staffed with CD drive and floppy drive.

Mixlink was intended to be used with the supplied floppy and CD disk, which provided the system environment ("operating system") and the application to be used for calculating color mixes. However, it is possible to install and run MS-DOS on Mixlink.

Currently, Mixlink computers are not used; their functions may solely be performed by personal computers (PCs).


</doc>
<doc id="35023598" url="https://en.wikipedia.org/wiki?curid=35023598" title="Outline of computers">
Outline of computers

The following outline is provided as an overview of and topical guide to computers:

Computers – programmable machines designed to automatically carry out sequences of arithmetic or logical operations. The sequences of operations can be changed readily, allowing computers to solve more than one kind of problem.

Computers can be described as all of the following:


Computer architecture –



History of computing hardware


Hp, Toshiba, Dell, Apple, Acer, Asus.


Software development –


Computer magazines –
"See List of computer magazines"
Online –



</doc>
<doc id="38500724" url="https://en.wikipedia.org/wiki?curid=38500724" title="Electronic media and sleep">
Electronic media and sleep

The use of computers (including devices such as smartphones, tablet computers and laptops) by children and adolescents before bed has been associated with a reduction in the hours of sleep experienced by frequent users, along with a decreased quality of sleep, in most cases. The results of computer use at night have been linked with tiredness.

A 2010 review concluded that "the use of electronic media by children and adolescents does have a negative impact on their sleep, although the precise effects and mechanisms remain unclear", with the most consistent results associating excessive media use with shorter sleep duration and delayed bed times. A 2016 meta-analysis found that "Bedtime access and use of media devices was significantly associated with inadequate sleep quantity; poor sleep quality; and excessive daytime sleepiness".

The American Academy of Pediatrics recommends screen time for children be limited for multiple reasons, among them that "Too much screen time can also harm the amount and quality of sleep".

Many apps promise to improve sleep by filtering out blue light produced by media devices; there have been no large studies to assess whether such apps work. Some users express dissatisfaction with the resultant orange tint of screens. Some people use blue-blocking glasses, for the purpose of attempting to block out blue light both from electronic media and from other artificial light sources.



</doc>
<doc id="7878457" url="https://en.wikipedia.org/wiki?curid=7878457" title="Computer">
Computer

A computer is a device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called "programs." These programs enable computers to perform an extremely wide range of tasks. A "complete" computer including the hardware, the operating system (main software), and peripheral equipment required and used for "full" operation can be referred to as a computer system. This term may as well be used for a group of computers that are connected and work together, in particular a computer network or computer cluster.

Computers are used as control systems for a wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer-aided design, and also general purpose devices like personal computers and mobile devices such as smartphones. The Internet is run on computers and it connects hundreds of millions of other computers and their users.

Early computers were only conceived as calculating devices. Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The speed, power, and versatility of computers have been increasing dramatically ever since then.

Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.

According to the "Oxford English Dictionary", the first known use of the word "computer" was in 1613 in a book called "The Yong Mans Gleanings" by English writer Richard Braithwait: "I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number." This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. During the latter part of this period women were often hired as computers because they could be paid less than their male counterparts. By 1943, most human computers were women. From the end of the 19th century the word slowly began to take on its more familiar meaning, a machine that carries out computations.

The "Online Etymology Dictionary" gives the first attested use of "computer" in the "1640s, [meaning] "one who calculates,"; this is an "... agent noun from compute (v.)". The "Online Etymology Dictionary" states that the use of the term to mean "calculating machine" (of any type) is from 1897." The "Online Etymology Dictionary" indicates that the "modern use" of the term, to mean "programmable digital electronic computer" dates from "... 1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine".

Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.

The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.
The Antikythera mechanism is believed to be the earliest mechanical analog "computer", according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to . Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.

Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, .

The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.

The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.
The slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.

In the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.

The tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.

Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.

The machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the "mill") in 1888. He gave a successful demonstration of its use in computing tables in 1906.

During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers. The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (control systems) and aircraft (slide rule).

By 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.
Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.

In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was Turing complete.

Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes. In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942, the first "automatic electronic digital computer". This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.
During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus. He spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.

Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both 5 times faster and simpler to operate than Mark I, greatly speeding the decoding process.
The U.S.-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the "ENIAC girls".

It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.

The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper, "On Computable Numbers". Turing proposed a simple device that he called "Universal Computing machine" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine. With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945, Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report "Proposed Electronic Calculator" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his "First Draft of a Report on the EDVAC" in 1945.

The Manchester Baby was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. It was designed as a testbed for the Williams tube, the first random-access digital storage device. Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. Grace Hopper was the first person to develop a compiler for programming language.

The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer. Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam. In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job.

The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers.
Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.

At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell.

The next great advance in computing power came with the advent of the integrated circuit.
The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.

The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as "a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated". Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.

This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.

The first mobile computers were heavy and ran from mains power. The 50lb IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter, but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s. The same developments allowed manufacturers to integrate computing resources into cellular phones.

These smartphones and tablets run on a variety of operating systems and soon became the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.

Computers are typically classified based on their uses:



The term "hardware" covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and "mice" input devices are all hardware.

A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.
Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a "1", and when off it represents a "0" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.

When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:

The means through which computer gives output are known as output devices. Some examples of output devices are:

The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer. Control systems in advanced computers may change the order of execution of some instructions to improve performance.

A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.

The control system's function is as follows—note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:


Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).

The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.

The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a "microprocessor".

The ALU is capable of performing two classes of operations: arithmetic and logic. The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65?"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.

Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously. Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.

A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595." The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.

In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (2 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.

The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.

Computer main memory comes in two principal varieties:
RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.

In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.

I/O is the means by which a computer exchanges information with the outside world. Devices that provide input or output to the computer are called peripherals. On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.
I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics. Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.

While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn. One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time-sharing" since each program is allocated a "slice" of time in turn.

Before the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.

Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.

Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers. They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called "embarrassingly parallel" tasks.

"Software" refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software]] Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called "firmware".

There are thousands of different programming languages—some intended to be general purpose, others useful only for highly specialized applications.

The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.

This section applies to most common RAM machine–based computers.

In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.

Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.

Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:
Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.

In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.

While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers, it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.

Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.

Machine languages and the assembly languages that represent them (collectively termed "low-level programming languages") tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.

Although considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler. High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.

Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies.
The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.

Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to "hang", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.
Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.

Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre. In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved.

In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.
A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern definition of a computer is literally: ""A device that computes", especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information." Any device which "processes information" qualifies as a computer, especially if the processing is purposeful.

There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.

There are many types of computer architectures:

Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing. Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.

A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Artificial intelligence based products generally fall into two major categories: rule based systems and pattern recognition systems. Rule based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern based systems use data about a problem to generate conclusions. Examples of pattern based systems include voice recognition, font recognition, translation and the emerging field of on-line marketing.

As the use of computers has spread throughout society, there are an increasing number of careers involving computers.
The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.




</doc>
<doc id="39177819" url="https://en.wikipedia.org/wiki?curid=39177819" title="Cognitive computer">
Cognitive computer

A cognitive computer combines artificial intelligence and machine-learning algorithms, in an approach which attempts to reproduce the behaviour of the human brain. It generally adopts a Neuromorphic engineering approach. 
An example of neural network implementations of cognitive convolution and deep learning is provided by the IBM company's Watson machine. A subsequent development by IBM is the TrueNorth microchip architecture, which is designed to be closer in structure to the human brain than the von Neumann architecture used in conventional computers. In 2017 Intel announced its own version of a cognitive chip in "Loihi", which will be available to university and research labs in 2018.

Intel's self-learning neuromorphic chip, named Loihi, perhaps named after the Hawaiian seamount Loihi, offers substantial power efficiency designed after the human brain. Intel claims Loihi is about 1000 times more energy efficient than the general-purpose computing power needed to train the neural networks that rival Loihi's performance. 
In theory, this would support both machine learning training and inference on the same silicon independently of a cloud connection, and more efficient than using convolutional neural networks (CNNs) or deep learning neural networks. Intel points to a system for monitoring a person's heartbeat, taking readings after events such as exercise or eating, and uses the cognitive computing chip to normalize the data and work out the ‘normal’ heartbeat. It can then spot abnormalities, but also deal with any new events or conditions.

The first iteration of the Loihi chip was made using Intel's 14 nm fabrication process, and houses 128 clusters of 1,024 artificial neurons each for a total of 131,072 simulated neurons. This offers around 130 million synapses, which is still a rather long way from the human brain's 800 trillion synapses, and behind IBM's TrueNorth, which has around 16 billion by using 64 by 4,096 cores.
Loihi is now available for research purposes among more than 40 academic research groups as a USB from factor.

The IBM cognitive computers implement learning using Hebbian theory. Instead of being programmable in a traditional sense within machine language or a higher level programming language such a device learns by inputting instances through an input device that are aggregated within a computational convolution or neural network architecture consisting of weights within a parallel memory system. An early instantiation of such a device has been developed in 2012 under the Darpa SyNAPSE program at IBM directed by Dharmendra Modha.
In 2017 this IBM 64-chip array will contain the processing equivalent of 64 million neurons and 16 billion synapses, yet each processor consumes just 10 watts of electricity.
Like other neural networks, this system will be put to use in pattern recognition and sensory processing roles. The Air Force wants to combine the TrueNorth ability to convert multiple data feeds — whether it's audio, video or text — into machine readable symbols with a conventional supercomputer's ability to crunch data.
This isn't the first time that IBM's neural chip system has been integrated into cutting-edge technology. In August, 2017 Samsung installed the chips in its Dynamic Vision Sensors enabling cameras to capture images at up to 2,000 fps while using just 300 milliwatts of power.

Google has created three generations of a similar device, Tensor processing unit using low resolution, 8 bit computing rather than a Spiking neural network.

There are many approaches and definitions for a cognitive computer, and other approaches may be more fruitful than the others.

Specifically, there are critics who argue that a room-sized computer - like the case of Watson - is not a viable alternative to a three-pound human brain. Some also cite the difficulty for a single system to bring so many elements together such as the disparate sources of information as well as computing resources. During the 2018 World Economic Forum, there are experts who claim that cognitive systems could adopt the biases of their developers and this was demonstrated in the case of the Google image-recognition or computer vision algorithm, which identified African Americans unfavorably.


http://www.foxnews.com/tech/2018/01/09/ces-2018-intel-gives-glimpse-into-mind-blowing-future-computing.html


</doc>
<doc id="48144" url="https://en.wikipedia.org/wiki?curid=48144" title="Microcomputer">
Microcomputer

A microcomputer is a small, relatively inexpensive computer with a microprocessor as its central processing unit (CPU). It includes a microprocessor, memory, and minimal input/output (I/O) circuitry mounted on a single printed circuit board. Microcomputers became popular in the 1970s and 1980s with the advent of increasingly powerful microprocessors. The predecessors to these computers, mainframes and minicomputers, were comparatively much larger and more expensive (though indeed present-day mainframes such as the IBM System z machines use one or more custom microprocessors as their CPUs). Many microcomputers (when equipped with a keyboard and screen for input and output) are also personal computers (in the generic sense).

The abbreviation "micro" was common during the 1970s and 1980s, but has now fallen out of common usage.

The term "microcomputer" came into popular use after the introduction of the minicomputer, although Isaac Asimov used the term in his short story "The Dying Night" as early as 1956 (published in "The Magazine of Fantasy and Science Fiction" in July that year). Most notably, the microcomputer replaced the many separate components that made up the minicomputer's CPU with one integrated microprocessor chip.

The French developers of the Micral N (1973) filed their patents with the term "Micro-ordinateur", a literal equivalent of "Microcomputer", to designate a solid state machine designed with a microprocessor.
In the USA, the earliest models such as the Altair 8800 were often sold as kits to be assembled by the user, and came with as little as 256 bytes of RAM, and no input/output devices other than indicator lights and switches, useful as a proof of concept to demonstrate what such a simple device could do. 
However, as microprocessors and semiconductor memory became less expensive, microcomputers in turn grew cheaper and easier to use:
All these improvements in cost and usability resulted in an explosion in their popularity during the late 1970s and early 1980s.
A large number of computer makers packaged microcomputers for use in small business applications. By 1979, many companies such as Cromemco, Processor Technology, IMSAI, North Star Computers, Southwest Technical Products Corporation, Ohio Scientific, Altos Computer Systems, Morrow Designs and others produced systems designed either for a resourceful end user or consulting firm to deliver business systems such as accounting, database management, and word processing to small businesses. This allowed businesses unable to afford leasing of a minicomputer or time-sharing service the opportunity to automate business functions, without (usually) hiring a full-time staff to operate the computers. A representative system of this era would have used an S100 bus, an 8-bit processor such as an Intel 8080 or Zilog Z80, and either CP/M or MP/M operating system.
The increasing availability and power of desktop computers for personal use attracted the attention of more software developers. In time, and as the industry matured, the market for personal computers standardized around IBM PC compatibles running DOS, and later Windows.
Modern desktop computers, video game consoles, laptops, tablet PCs, and many types of handheld devices, including mobile phones, pocket calculators, and industrial embedded systems, may all be considered examples of microcomputers according to the definition given above.

Everyday use of the expression "microcomputer" (and in particular the "micro" abbreviation) has declined significantly from the mid-1980s and has declined in commonplace usage since 2000. The term is most commonly associated with the first wave of all-in-one 8-bit home computers and small business microcomputers (such as the Apple II, Commodore 64, BBC Micro, and TRS 80). Although, or perhaps because, an increasingly diverse range of modern microprocessor-based devices fit the definition of "microcomputer", they are no longer referred to as such in everyday speech.

In common usage, "microcomputer" has been largely supplanted by the term "personal computer" or "PC", which specifies a computer that has been designed to be used by one individual at a time, a term first coined in 1959. IBM first promoted the term "personal computer" to differentiate themselves from other microcomputers, often called "home computers", and also IBM's own mainframes and minicomputers. However, following its release, the IBM PC itself was widely imitated, as well as the term. The component parts were commonly available to producers and the BIOS was reverse engineered through cleanroom design techniques. IBM PC compatible "clones" became commonplace, and the terms "personal computer", and especially "PC", stuck with the general public, often specifically for a DOS or (nowadays) Windows-compatible computer.

Since the advent of microcontrollers (monolithic integrated circuits containing RAM, ROM and CPU all onboard), the term "micro" is more commonly used to refer to that meaning.

Monitors, keyboards and other devices for input and output may be integrated or separate. Computer memory in the form of RAM, and at least one other less volatile, memory storage device are usually combined with the CPU on a system bus in one unit. Other devices that make up a complete microcomputer system include batteries, a power supply unit, a keyboard and various input/output devices used to convey information to and from a human operator (printers, monitors, human interface devices). Microcomputers are designed to serve only one user at a time, although they can often be modified with software or hardware to concurrently serve more than one user. Microcomputers fit well on or under desks or tables, so that they are within easy access of users. Bigger computers like minicomputers, mainframes, and supercomputers take up large cabinets or even dedicated rooms.

A microcomputer comes equipped with at least one type of data storage, usually RAM. Although some microcomputers (particularly early 8-bit home micros) perform tasks using RAM alone, some form of secondary storage is normally desirable. In the early days of home micros, this was often a data cassette deck (in many cases as an external unit). Later, secondary storage (particularly in the form of floppy disk and hard disk drives) were built into the microcomputer case.

Although they did not contain any microprocessors, but were built around transistor-transistor logic (TTL), Hewlett-Packard calculators as far back as 1968 had various levels of programmability comparable to microcomputers. The HP 9100B (1968) had rudimentary conditional (if) statements, statement line numbers, jump statements (go to), registers that could be used as variables, and primitive subroutines. The programming language resembled assembly language in many ways. Later models incrementally added more features, including the BASIC programming language (HP 9830A in 1971). Some models had tape storage and small printers. However, displays were limited to one line at a time. The HP 9100A was referred to as a personal computer in an advertisement in a 1968 Science magazine, but that advertisement was quickly dropped. HP was reluctant to sell them as "computers" because the perception at that time was that a computer had to be big in size to be powerful, and thus decided to market them as calculators. Additionally, at that time, people were more likely to buy calculators than computers, and, purchasing agents also preferred the term "calculator" because purchasing a "computer" required additional layers of purchasing authority approvals. HP virtual museum

The Datapoint 2200, made by CTC in 1970, was also comparable to microcomputers. While it contains no microprocessor, the instruction set of its custom TTL processor was the basis of the instruction set for the Intel 8008, and for practical purposes the system behaves approximately as if it contains an 8008. This is because Intel was the contractor in charge of developing the Datapoint's CPU, but ultimately CTC rejected the 8008 design because it needed 20 support chips.

Another early system, the Kenbak-1, was released in 1971. Like the Datapoint 2200, it used discrete transistor–transistor logic instead of a microprocessor, but it functioned like a microcomputer in some ways. It was marketed as an educational and hobbyist tool, but it was not a commercial success; production ceased shortly after introduction.

In late 1972, a French team headed by François Gernelle within a small company, Réalisations & Etudes Electroniqes (R2E), developed and patented a computer based on a microprocessor – the Intel 8008 8-bit microprocessor. This Micral-N was marketed in early 1973 as a "Micro-ordinateur" or "microcomputer", mainly for scientific and process-control applications. About a hundred Micral-N were installed in the next two years, followed by a new version based on the Intel 8080. Meanwhile, another French team developed the Alvan, a small computer for office automation which found clients in banks and other sectors. The first version was based on LSI chips with an Intel 8008 as peripheral controller (keyboard, monitor and printer), before adopting the Zilog Z80 as main processor.

In late 1972, a Sacramento State University team led by Bill Pentz built the Sac State 8008 computer, able to handle thousands of patients' medical records. The Sac State 8008 was designed with the Intel 8008. It had a full set of hardware and software components: a disk operating system included in a series of programmable read-only memory chips (PROMs); 8 Kilobytes of RAM; IBM's Basic Assembly Language (BAL); a hard drive; a color display; a printer output; a 150 bit/s serial interface for connecting to a mainframe; and even the world's first microcomputer front panel.

In early 1973, Sord Computer Corporation (now Toshiba Personal Computer System Corporation) completed the SMP80/08, which used the Intel 8008 microprocessor. The SMP80/08, however, did not have a commercial release. After the first general-purpose microprocessor, the Intel 8080, was announced in April 1974, Sord announced the SMP80/x, the first microcomputer to use the 8080, in May 1974.

Virtually all early microcomputers were essentially boxes with lights and switches; one had to read and understand binary numbers and machine language to program and use them (the Datapoint 2200 was a striking exception, bearing a modern design based on a monitor, keyboard, and tape and disk drives). Of the early "box of switches"-type microcomputers, the MITS Altair 8800 (1975) was arguably the most famous. Most of these simple, early microcomputers were sold as electronic kits—bags full of loose components which the buyer had to solder together before the system could be used.

The period from about 1971 to 1976 is sometimes called the of microcomputers. Many companies such as DEC, National Semiconductor, Texas Instruments offered their microcomputers for use in terminal control, peripheral device interface control and industrial machine control. There were also machines for engineering development and hobbyist personal use. In 1975, the Processor Technology SOL-20 was designed, which consisted of one board which included all the parts of the computer system. The SOL-20 had built-in EPROM software which eliminated the need for rows of switches and lights. The MITS Altair just mentioned played an instrumental role in sparking significant hobbyist interest, which itself eventually led to the founding and success of many well-known personal computer hardware and software companies, such as Microsoft and Apple Computer. Although the Altair itself was only a mild commercial success, it helped spark a huge industry.

By 1977, the introduction of the second generation, known as home computers, made microcomputers considerably easier to use than their predecessors because their predecessors' operation often demanded thorough familiarity with practical electronics. The ability to connect to a monitor (screen) or TV set allowed visual manipulation of text and numbers. The BASIC language, which was easier to learn and use than raw machine language, became a standard feature. These features were already common in minicomputers, with which many hobbyists and early produces were familiar.

In 1979, the launch of the VisiCalc spreadsheet (initially for the Apple II) first turned the microcomputer from a hobby for computer enthusiasts into a business tool. After the 1981 release by IBM of its IBM PC, the term personal computer became generally used for microcomputers compatible with the IBM PC architecture (PC compatible).



</doc>
<doc id="43155338" url="https://en.wikipedia.org/wiki?curid=43155338" title="Data (word)">
Data (word)

The word data has generated considerable controversy on whether it is an uncountable noun used with verbs conjugated in the singular, or should be treated as the plural of the now-rarely-used "datum".

In one sense, "data" is the plural form of "datum". "Datum" actually can also be a count noun with the plural "datums" (see usage in datum article) that can be used with cardinal numbers (e.g. "80 datums"); "data" (originally a Latin plural) is not used like a normal count noun with cardinal numbers and can be plural with such plural determiners as "these" and "many" or as an uncountable noun with a verb in the singular form. 
Even when a very small quantity of data is referenced (one number, for example), the phrase "piece of data" is often used, as opposed to "datum". The debate over appropriate usage continues, but "data" as a singular form is far more common.

In English, the word "datum" is still used in the general sense of "an item given". In cartography, geography, nuclear magnetic resonance and technical drawing, it is often used to refer to a single specific reference datum from which distances to all other data are measured. Any measurement or result is a "datum", though "data point" is now far more common.

"Data" is most often used as a singular mass noun in everyday usage. Some major newspapers, such as "The New York Times," use it either in the singular or plural. In the "New York Times" the phrases "the survey data are still being analyzed" and "the first year for which data is available" have appeared within one day. The "Wall Street Journal" explicitly allows this usage in its style guide.
The Associated Press style guide classifies "data" as a collective noun that takes the singular when treated as a unit but the plural when referring to individual items (e.g., "The data is sound" and "The data have been carefully collected").

In scientific writing "data" is often treated as a plural, as in "These data do not support the conclusions", but the word is also used as a singular mass entity like "information", for instance in computing and related disciplines. British usage now widely accepts treating "data" as singular in standard English, including everyday newspaper usage at least in non-scientific use. UK scientific publishing still prefers treating it as a plural. Some UK university style guides recommend using "data" for both singular and plural use, and others recommend treating it only as a singular in connection with computers. The IEEE Computer Society allows usage of "data" as either a mass noun or plural based on author preference, while IEEE in the editorial style manual indicates to always use the plural form. Some professional organizations and style guides require that authors treat "data" as a plural noun. For example, the Air Force Flight Test Center specifically states that the word "data" is always plural, never singular.


</doc>
<doc id="43517179" url="https://en.wikipedia.org/wiki?curid=43517179" title="Motion History Images">
Motion History Images

The motion history image (MHI) is a static image template helps in understanding the motion location and path as it progresses. In MHI, the temporal motion information is collapsed into a single image template where intensity is a function of recency of motion. Thus, the MHI pixel intensity is a function of the motion history at that location, where brighter values correspond to a more recent motion. Using MHI, moving parts of a video sequence can be engraved with a single image, from where one can predict the motion flow as well as the moving parts of the video action.

Some important features of the MHI representation are:

 for each time "t"


</doc>
<doc id="46551142" url="https://en.wikipedia.org/wiki?curid=46551142" title="List of countries by computer exports">
List of countries by computer exports

The following is a list of countries by computer exports. Data is for 2014, in millions of United States dollars, as reported by The Observatory of Economic Complexity. Currently the top fifteen countries are listed.

atlas.media.mit.edu - Observatory of Economic complexity - Countries that export Computers (2012)


</doc>
<doc id="29504901" url="https://en.wikipedia.org/wiki?curid=29504901" title="Computer says no">
Computer says no

"Computer Says No", or the "Computer says no attitude", is the popular name given to an attitude seen in some public-facing organisations where the default response to a customer’s request is to check with information stored on or generated by a computer, and then make decisions based on that, often in the face of common sense.

There may also be an element of deliberate unhelpfulness towards customers and service-users, whereby more "could" be done to reach a mutually satisfactory outcome, but is not.

The name gained popularity through the British sketch comedy "Little Britain".

In "Little Britain", "Computer Says No" is the catchphrase of the character Carol Beer (played by David Walliams), a bank worker and later holiday rep and hospital receptionist, who always responds to a customer's enquiry by typing it into her computer and responding with "Computer Says No" to even the most reasonable of requests. When asked to do something aside from asking the computer, she would shrug and remain obstinate in her unhelpfulness, and ultimately cough in the customer's face. The phrase was also used in the Australian soap opera "Neighbours" in 2006 as a reference to "Little Britain".

The "Computer Says No" attitude often comes from larger companies that rely on information stored electronically. When this information is not updated, it can often lead to refusals of financial products or incorrect information being sent out to customers. These situations can often be resolved by an employee updating the information; however, when this cannot be done easily, the "Computer Says No" attitude can be viewed as becoming prevalent when there is unhelpfulness as a result. This attitude can also occur when an employee fails to read human emotion in the customer and reacts according to his or her professional training or relies upon a script. This attitude also crops up when larger companies rely on computer credit scores and do not meet with a customer to discuss his or her individual needs, instead basing a decision upon information stored in computers. Some organisations attempt to offset this attitude by moving away from reliance on electronic information and using a human approach towards requests.

"Computer Says No" happens in a more literal sense when computer systems employ filters that prevent messages being passed along, as when these messages are perceived to include obscenities. When information is not passed through to the person operating the computer, decisions may be made without seeing the whole picture.



</doc>
<doc id="1840072" url="https://en.wikipedia.org/wiki?curid=1840072" title="Computer rage">
Computer rage

Computer rage refers to negative psychological responses towards a computer due to heightened anger or frustration. Examples of computer rage include cursing or yelling at a computer, slamming or throwing keyboards and mice, and assaulting the computer or monitor with an object or weapon.

In April 2015, a Colorado man was cited for firing a gun within a residential area when he took his computer into a back alley and shot it eight times with a 9mm pistol. When questioned, he told police that he had become so frustrated with his computer that he had "reached critical mass," and stated that after he had shot his computer, "the angels sung on high." In 2007, a German man threw his computer out the window in the middle of the night, startling his neighbors. German police were sympathetic and did not press charges, stating "Who hasn't felt like doing that?" In 2006, the staged surveillance video "Bad Day", showing a man assaulting his computer at work, became a viral hit on the Internet, reaching over two million views. Other instances of reported computer rage have ranged from a restaurant owner who threw his laptop into a deep fryer, to an individual who threw his computer out the window, but forgot that the window was closed.

In 1999, it was speculated that computer rage had become more common than road rage in traffic, but in a 2015 study, it was found that reported rates of anger when using a computer were lower than reported rates of anger while driving. However, reports of anger while driving or using computers were found to be far more common than anger in other situations.

In a 2013 survey of American adults, 36% of respondents who reported experiencing computer issues, also reported that they had screamed, yelled, cursed, or physically assaulted their computers within the last six months.

In 2009, a survey was conducted with British computer users about their experiences with computers. This survey found that 54% of respondents reported verbally abusing their computers, and 40% reported that they had become physically violent toward their computers. The survey also found that most users experienced computer rage three to four times a month.

Differences in types of computer rage have also been found between different geographical regions. For example, one survey found that individuals from London have been found to be five times more likely to physically assault their computers, while those from Yorkshire and Humberside were found to be more likely to yell at their computers. Differences have also been observed for age groups, as younger adults (18–24 years old) have reported more abusive behaviors in the face of computer frustration when compared to older adults (over 35 years old). Individuals with less computer experience in particular have also been reported to experience increased feelings of anger and helplessness when it comes to computers, but other research has argued that it is the self-efficacy beliefs about computers that are predictive of computer frustration, not the amount of computer experience or use.

In 1999 Professor Robert J. Edelmann, a Chartered Clinical, Forensic and Health Psychologist and a Fellow of the British Psychological Society, was offering a special helpline in the UK for those suffering from technology related anger.

Users can experience computer anger and frustration for a number of reasons. American adults surveyed in 2013 reported that almost half (46%) of their computer problems were due to malware or computer viruses, followed by software issues (10%) and not enough memory (8%). In another survey, users reported email, word processors, web browsing, operating system crashes, inability to locate features, and program crashes as frequent initiators of computer frustration. These technical issues, paired with tight timelines, poor work progress, and failure to complete a computer task can create heightened computer anger and frustration. When this anger and frustration exceeds a person's control, it can turn into rage.

Research on emotion has shown that anger is often caused by interruptions of plans and expectations, especially through the violation of social norms. This sense of anger can be magnified when the individual does not understand why they are unable to meet their goal or task at hand or why there was a violation of social norms. Psychologists have argued that this is particularly relevant to computer rage, as computer users interact with computers in a similar manner that they interact with other people (for more information, see The Media Equation). Thus, when computers fail to function in the face of incoming deadlines or an important task to accomplish, users can feel betrayed by the computer in the same way they can feel betrayed by other people. Specifically, when users fail to understand why their computer will not work properly, often in the times they need it to the most, it can invoke a sense of hostility as it is interpreted as a breach of social norms or a personal attack. Consistent with this finding, perceived betrayal by the computer can also elicit other negative emotions. One survey of US adults reported that 10% of users who experience computer issues experienced feeling helplessness, and 4% reported feeling victimized. In the same survey, 7% adults ages 18–34 reported that they had cried over their computer problems within the previous six months.

Computer rage can result in damaged property or physical injuries, as well as psychological harm. Some experts have suggested that venting frustrations on the computer may have some benefits, but other experts disagree. For example, yelling at the computer has been suggested as a way to moderate one's anger to avoid the ill effects of anger suppression, but new research has suggested that yelling can negatively affect health in itself. Alternatively, releasing anger on a computer has been viewed as advantageous as it directs this rage at an object as opposed to another person, and can make individuals feel better afterwards.

In response to computer issues that invoke frustration, some experts have suggested walking away from the computer for 15 minutes to "cool off". Other methods to prevent computer rage can be backing up computer data often, increasing memory of the computer, and even imagining pleasant images, such as petting an animal. Adopting a goal of improving computer knowledge may also be beneficial, as users are less likely to report computer rage when they view the issue as a challenge and not as a setback. If computer rage cannot be avoided, guidelines on how to rage with minimal consequences, such as wearing safety goggles and taking frustration out on older equipment, can be followed to reduce the likelihood of injury and significant property loss.

Employers of staff who work with computers, often in situations where time is crucial, can take steps to prevent computer rage, such as making sure there is adequate software, and providing employees with anger management strategies. Some computer technician companies have reported that, to reduce computer rage, their technicians are trained on how to work with customers in sensitive psychological states just as much as how to diagnose and fix technical issues.

Designing computer interfaces to display more emotional support when errors occur, or provide therapy strategies, has also been suggested as a way to mitigate computer anger and rage. The application of affective computing has been shown to effectively mitigate negative emotions connected to computer use. One study found that an interface that sought the user's feelings, provided empathy, and validated reported emotional states significantly reduced negative emotions associated with computer frustration for users. Another study found that when error messages contain positive wording ("Great that the computer will soon work again") compared to negative wording ("This is frustrating") or a neutral error message, users exhibited more signs of happiness.



</doc>
<doc id="50051314" url="https://en.wikipedia.org/wiki?curid=50051314" title="PCaaS">
PCaaS

PCaaS or, "Personal Computer as a Service", is a Personal Computer hardware and optionally software leasing, licensing and delivery model in which personal computer and optionally software (particularly installed on the PC) are leased and licensed on a subscription basis. The subscription often includes services such as staging, imaging, maintenance, fix, logistics services and may also be bundled with helpdesk services, data backup and recovery.

There are several vendors that have PCaaS offerings including, Bizbang, Dell, HP (they call theirs Device as a Service), and Lenovo (in Australia only for now).


</doc>
<doc id="41920098" url="https://en.wikipedia.org/wiki?curid=41920098" title="ENIAC Day">
ENIAC Day

ENIAC Day or the World’s First Computer Day is celebrated on 15 February.
On February 10, 2011, the City of Philadelphia officially declared that February 15, 2011 - the 65th anniversary of the unveiling of the Electronic Numerical Integrator and Computer (ENIAC), the world's first general-purpose electronic computer, developed at the University of Pennsylvania's Moore School of Electrical Engineering - would that year and henceforth be known as ENIAC Day.


</doc>
<doc id="174397" url="https://en.wikipedia.org/wiki?curid=174397" title="Computer addiction">
Computer addiction

Computer addiction can be described as the excessive or compulsive use of the computer which persists despite serious negative consequences for personal, social, or occupational function. Another clear conceptualization is made by Block, who stated that "Conceptually, the diagnosis is a compulsive-impulsive spectrum disorder that involves online and/or offline computer usage and consists of at least three subtypes: excessive gaming, sexual preoccupations, and e-mail/text messaging". While it was expected that this new type of addiction would find a place under the compulsive disorders in the DSM-5, the current edition of the "Diagnostic and Statistical Manual of Mental Disorders", it is still counted as an unofficial disorder. The concept of computer addiction is broadly divided into two types, namely offline computer addiction and online computer addiction. The term offline computer addiction is normally used when speaking about excessive gaming behavior, which can be practiced both offline and online. Online computer addiction, also known as Internet addiction, gets more attention in general from scientific research than offline computer addiction, mainly because most cases of computer addiction are related to the excessive use of the Internet.

Although addiction is usually used to describe dependence on substances, addiction can also be used to describe pathological Internet use. Experts on Internet addiction have described this syndrome as an individual being intensely working on the Internet, prolonged use of the Internet, uncontrollable use of the Internet, unable to use the Internet with efficient time, not being interested in the outside world, not spending time with people from the outside world, and an increase in their loneliness and dejection. However, simply working long hours on the computer does not necessarily mean someone is addicted.


Excessive computer use may result in, or occur with:


Kimberly Young indicates that previous research links internet/computer addiction with existing mental health issues, most notably depression. She states that computer addiction has significant effects socially such as low self-esteem, psychologically and occupationally which led many subjects to academic failure.

According to a Korean study on internet/computer addiction, pathological use of the internet results in negative life impacts such as job loss, marriage breakdown, financial debt, and academic failure. 70% of internet users in Korea are reported to play online games, 18% of which are diagnosed as game addicts which relates to internet/computer addiction. The authors of the article conducted a study using Kimberly Young's questionnaire. The study showed that the majority of those who met the requirements of internet/computer addiction suffered from interpersonal difficulties and stress and that those addicted to online games specifically responded that they hoped to avoid reality.

Computers nowadays rely almost entirely on the internet and thus relevant research articles relating to internet addiction may also be relevant to computer addiction.


Many studies and surveys are being conducted to measure the extent of this type of addiction. Kimberly Young has created a questionnaire based on other disorders to assess the level of addiction. It is called the Internet Addict Diagnostic Questionnaire or IADQ. The questionnaire asks users about their online usage habits as well as their feelings about their internet usage. According to the IADQ sample, Internet Addiction resembles that of a Gambling disorder. Answering positively to five out of the eight questions on the IADQ may be indicative of an online addiction. 

Observations about the addictiveness of computers and more specifically computer games date back at least to the mid 1970s. Addiction and addictive behavior was common among the users of the PLATO system at the University of Illinois. British e-learning academic Nicholas Rushby suggested in his 1979 book, "An Introduction to Educational Computing", that people can be addicted to computers and suffer withdrawal symptoms. The term was also used by M. Shotton in 1989 in her book "Computer Addiction". However, Shotton concludes that the 'addicts' are not truly addicted. Dependency on computers, she argues, is better understood as a challenging and exiting pastime that can also lead to a professional career in the field. Computers do not turn gregarious, extroverted people into recluses; instead they offer introverts a source of inspiration, excitement and intellectual stimulation. Shotton's work seriously questions the legitimacy of the claim that computers cause addiction.

The term became more widespread with the explosive growth of the Internet, as well the availability of the personal computer. Computers and the Internet both started to take shape as a personal and comfortable medium which could be used by anyone who wanted to make use of it. With that explosive growth of individuals making use of PCs and the Internet, the question started to arise whether or not misuse or excessive use of these new technologies could be possible as well. It was hypothesized that, like any technology aimed specifically at human consumption and use, that abuse could have severe consequences for the individual in the short term and for the society in the long term. In the late nineties people who made use of PCs and the internet where already referred to the term webaholics or cyberholics. Pratarelli et al. suggested at that point already to label "a cluster of behaviors potentially causing problems" as computer or Internet addiction.
There are other examples of computer overuse that date back to the earliest computer games. Press reports have furthermore noted that some Finnish Defence Forces conscripts were not mature enough to meet the demands of military life, and were required to interrupt or postpone military service for a year. One reported source of the lack of needed social skills is overuse of computer games or the Internet. Forbes termed this overuse "Web fixations", and stated that they were responsible for 12 such interruptions or deferrals over the 5 years from 2000–2005.


Works cited



</doc>
<doc id="54812230" url="https://en.wikipedia.org/wiki?curid=54812230" title="Laser 50">
Laser 50

The Laser 50 is an educational portable computer that ran the BASIC programming language released in 1984.

The Laser 50 used a Zilog Z80 central processing unit running at 3.5 MHz, 2 kB to 18 kB of RAM, a 12 kB ROM, and a 80x7 dots LCD screen.


</doc>
<doc id="24366615" url="https://en.wikipedia.org/wiki?curid=24366615" title="Computer maintenance">
Computer maintenance

Computer maintenance is the practice of keeping computers in a good state of repair. A computer containing accumulated dust and debris may not run properly.

Dust and debris will accumulate as a result of air cooling. Any filters used to mitigate this need regular service and changes. If the cooling system is not filtered then regular computer cleaning may prevent short circuits and overheating.

The crumbs, dust, and other particulate that fall between the keys and build up underneath are loosened by spraying pressurized air into the keyboard, then removed with a low-pressure vacuum cleaner. A plastic-cleaning agent applied to the surface of the keys with a cloth is used to remove the accumulation of oil and dirt from repeated contact with a user's fingertips. If this is not sufficient for a more severely dirty keyboard, keys are physically removed for more focused individual cleaning, or for better access to the area beneath. Finally, the surface is wiped with a disinfectant.

 A monitor displays information in visual form, using text and graphics. The portion of the monitor that displays the information is called the screen. Like a television screen, a computer screen can show still or moving pictures and It’s a part of Output Devices.

Fingerprints, water spots, and dust are removed from the screen with a cleaning wipe specialized for the screen type (CRT, LCD, etc.). A general plastic-cleaning agent is used on the outer casing, which requires a less gentle cleanser but may need more focused attention to unusual buildups of dust, grime, pen marks, etc. idiosyncratic to the user and environment.

The top surface of the mouse is wiped with a plastic cleanser to remove the dirt that accumulates from contact with the hand, as on the keyboard. The bottom surface is also cleaned to ensure that it can slide freely. If it is a mechanical mouse, the trackball is taken out, not only to clean the ball itself, but to scrape dirt from the runners that sense the ball's movement and can become jittery or stuck if impeded by grime.

Internal components accumulate dust brought in by the airflow maintained by fans to keep the PC from overheating. A soft brush may remove loose dirt; the remainder is dislodged with compressed air and removed with a low-pressure vacuum. The case is wiped down with a cleaning agent. A pressurized blower or gas duster can remove dust that cannot be reached with a brush.

Important data stored on computers may be copied and archived securely so that, in the event of failure, the data and systems may be reconstructed. When major maintenance such as patching is performed, a backup is recommended as the first step in case the update fails and reversion is required.

Disk cleanup may be performed as regular maintenance to remove these. Files may become fragmented and so slow the performance of the computer. "Disk defragmentation" may be performed to combine these fragments and so improve performance.

In the U.S.A, the Digital Millennium Copyright Act specifically exempts computer-maintenance activities, so copies of copyright files may be made in the course of maintenance provided that they are destroyed afterwards.

Operating-system files such as the Windows registry may require maintenance. A utility such as a registry cleaner may be used for this. Also inbuilt Disk defragmenter will also help.

Software packages and operating systems may require regular updates to correct software bugs and to address security weaknesses.

Maintaining security involves vulnerability management and installation and proper operation of antivirus softwares like Kaspersky, Avast Antivirus, McAfee and Many are available.


</doc>
<doc id="42249611" url="https://en.wikipedia.org/wiki?curid=42249611" title="Digital electronic computer">
Digital electronic computer

In computer science, a digital electronic computer is a computer machine which is both an electronic computer and a digital computer. Examples of a digital electronic computers include the IBM PC, the Apple Macintosh as well as modern smartphones. When computers that were both digital and electronic appeared, they displaced almost all other kinds of computers, but computation has historically been performed in various non-digital and non-electronic ways: the Lehmer sieve is an example of a digital non-electronic computer, while analog computers are examples of non-digital computers which can be electronic (with analog electronics), and mechanical computers are examples of non-electronic computers (which may be digital or not). An example of a computer which is both non-digital and non-electronic is the ancient Antikythera mechanism found in Greece. All kinds of computers, whether they are digital or analog, and electronic or non-electronic, can be Turing complete if they have sufficient memory. A digital electronic computer is not necessarily a programmable computer, a stored program computer, or a general purpose computer, since in essence a digital electronic computer can be built for one specific application and be non-reprogrammable. As of 2014, most personal computers and smartphones in people's homes that use multicore central processing units (such as AMD FX, Intel Core i7, or the multicore varieties of ARM-based chips) are also parallel computers using the MIMD (multiple instructions - multiple data) paradigm, a technology previously only used in digital electronic supercomputers. As of 2014, most digital electronic supercomputers are also cluster computers, a technology that can be used at home in the form of small Beowulf clusters. Parallel computation is also possible with non-digital or non-electronic computers. An example of a parallel computation system using the abacus would be a group of human computers using a number of abacus machines for computation and communicating using natural language.

A digital computer can perform its operations in the decimal system, in binary, in ternary or in other numeral systems. As of 2014, all digital electronic computers commonly used, whether personal computers or supercomputers, are working in the binary number system and also use binary logic. A few ternary computers using ternary logic were built mainly in the Soviet Union as research projects.

A digital electronic computer is not necessarily a transistorized computer: before the advent of the transistor, computers used vacuum tubes. The transistor enabled electronic computers to become much more powerful, and recent and future developments in digital electronics may enable humanity to build even more powerful electronic computers. One such possible development is the memristor.

People living in the beginning of the 21st century use digital electronic computers for storing data, such as photos, music, documents, and for performing complex mathematical computations or for communication, commonly over a worldwide computer network called the internet which connects many of the world's computers. All these activities made possible by digital electronic computers could, in essence, be performed with non-digital or non-electronic computers if they were sufficiently powerful, but it was only the combination of electronics technology with digital computation in binary that enabled humanity to reach the computation power necessary for today's computing. Advances in quantum computing, DNA computing, optical computing or other technologies could lead to the development of more powerful computers in the future.

Digital computers are inherently best described by discrete mathematics, while analog computers are most commonly associated with continuous mathematics.

The philosophy of digital physics views the universe as being digital. Konrad Zuse wrote a book known as "Rechnender Raum" in which he described the whole universe as one all-encompassing computer.



</doc>
<doc id="56283993" url="https://en.wikipedia.org/wiki?curid=56283993" title="Octagon Systems">
Octagon Systems

Octagon Systems Corporation is an industrial computer design and manufacturing company based in Westminster, Colorado. Octagon Systems designs, manufactures, sells, repairs and supports its line of industrial, mobile and rugged computer systems for industries including mining, military, transportation and others. The company has international representatives in Africa, Asia, Europe, North America and South America.
Octagon Systems was founded in 1981 and introduced an embedded computer with a high level language and software development system and operating systems on a solid state disk. Octagon’s services and systems grew with new industrial computer system solutions including use in the STD Bus market and development of single-board computers.

Octagon Systems co-authored the EPIC (form factor). EPIC™ embedded computing specification that became a world standard. Growing applicational use of Octagon’s products led to use in areas such as public transportation systems, rugged computing systems for mining operations as well as others.
Octagon Systems products expanded into new markets as the use of industrial, transportation and rugged computer systems became increasingly common for a wide array of applications. The U.S. Navy chose Octagon’ products for a major long term contract to support amphibious warfare computing needs and Octagon products have been deployed in more than 100 mines worldwide.

Octagon Systems’ XMB Mobile Servers won the Flagship Product award from COTS Journal - The Journal of Military Electronics & Computing in 2006. 
Octagon Systems has been ISO certified since 1993.
Octagon Systems was a founding member of the Small Form Factor Special Interest Group in 2007.
Octagon Systems co-authored the EPIC (form factor) or EPIC™ embedded computing specification.


</doc>
<doc id="57816166" url="https://en.wikipedia.org/wiki?curid=57816166" title="WDR paper computer">
WDR paper computer

The WDR paper computer or Know-How computer was a "computer" that could be easily assembled from a sheet of paper and individual matches. This would allow anyone interested to learn how to program without having an electronic computer at their disposal. Thus, this "computer" served as an educational aid in the field of computer science. The know-how computer was developed by Wolfgang Back and Ulrich Rohde and first presented in the television program WDR Computerclub in 1983.

He was also published in German magazines MC and .

The "computer" worked on paper; Matches were used as information units. Only five commands were enough to represent all mathematical functions. This exercise computer on paper was sent in over 400,000 copies and belonged at that time to the computers with the widest circulation. An implementation as a computer program is available on Wolfgang Back's website.

The method of operation is based on register machine, but is more to the approach of Shepherdson and Sturgis.

A derived version of the "Paper Computer" is used as a "know how computer" in Namibian school education.


</doc>
<doc id="56096428" url="https://en.wikipedia.org/wiki?curid=56096428" title="Wevolver">
Wevolver

Wevolver provides access to high quality engineering projects, to help people develop better technology. Their stated mission is to "enable anyone, anywhere to develop hardware that improves life." Wevolver 1.0 was a project repository for strictly Open Source projects, but in 2.0, it has been expanded to include a version control system, and is available for use by private teams in addition to open source projects.

Among the projects using their version control system are OpenROV, Plen, exiii hackberry, and InMoov.

In interviews, Wevolver team members have said the strongest part of their platform is their community, with over 300k followers on instagram, three news blogs, and myriad hosted projects, they've said the most important part of any Open-source platform is the people using it.



</doc>
<doc id="37953707" url="https://en.wikipedia.org/wiki?curid=37953707" title="WATIAC">
WATIAC

WATIAC was a virtual computer developed for teaching the principles of assembly language programming to undergraduates.
WATIAC, and the WATMAP assembly language that ran on it were developed in 1973 by the newly founded Computer Systems Group,
at the University of Waterloo, under the direction of Wes Graham.

In the 1970s most programming was conducted through batch stream processing, where the operating systems of the day, like IBM`s OS-360, would allow a single program to use all the resources of a large computer, for a limited period of time.
Since student programs were only run a few times, possibly only once, after they had been successfully written, and debugged, efficient running of those programs was of relatively little importance, compared with quick compilation and relatively good error messages.

Waterloo had been a leader in writing single pass, compile-and-go teaching compilers, with first its WATFOR FORTRAN compiler, and its WATBOL COBOL compiler.
WATMAP was developed to be a similar compile-and-go teaching compiler.


</doc>
<doc id="45569620" url="https://en.wikipedia.org/wiki?curid=45569620" title="Stanford DASH">
Stanford DASH

Stanford DASH was a cache coherent multiprocessor developed in the late 1980s by a group led by Anoop Gupta, John L. Hennessy, Mark Horowitz, and Monica S. Lam at Stanford University. It was based on adding a pair of directory boards designed at Stanford to up to 16 SGI IRIS 4D Power Series machines and then cabling the systems in a mesh topology using a Stanford-modified version of the Torus Routing Chip. The boards designed at Stanford implemented a directory-based cache coherence protocol allowing Stanford DASH to support distributed shared memory for up to 64 processors. Stanford DASH was also notable for both supporting and helping to formalize weak memory consistency models, including release consistency. Because Stanford DASH was the first operational machine to include scalable cache coherence, it influenced subsequent computer science research as well as the commercially available SGI Origin 2000. Stanford DASH is included in the 25th anniversary retrospective of selected papers from the International Symposium on Computer Architecture and several computer science books, has been simulated by the University of Edinburgh, and is used as a case study in contemporary computer science classes.


</doc>
<doc id="5311" url="https://en.wikipedia.org/wiki?curid=5311" title="Computer programming">
Computer programming

Computer programming is the process of designing and building an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

Related programming tasks include testing, debugging, maintaining a program's source code, implementation of build systems, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term "software development" is used for this larger process with the term "programming", "implementation", or "coding" reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.

Programmable devices have existed at least as far back as 1206 AD, when the automata of Al-Jazari were programmable, via pegs and cams, to play various rhythms and drum patterns; and the 1801 Jacquard loom could produce entirely different weaves by changing the "program" - a series of pasteboard cards with holes punched in them.

However, the first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine. Women would continue to dominate the field of computer programming until the mid 1960s.
In the 1880s Herman Hollerith invented the concept of storing "data" in machine-readable form. Later a control panel (plugboard) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way; as were the first electronic computers. However, with the concept of the stored-program computers introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.

Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format, (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages. Kathleen Booth created one of the first Assembly languages in 1950 for various computers at Birkbeck College.
High-level languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it targetable to varying machine instruction sets via compilation declarations and heuristics. The first compiler for a programming language was developed by Grace Hopper. When Hopper went to work on UNIVAC in 1949, she brought the idea of using compilers with her. Compilers harness the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation (e.g., ) for example. FORTRAN, the first widely used high-level language to have a functional implementation which permitted the abstraction of reusable blocks of code, came out in 1957. In 1951 Frances E. Holberton developed the first sort-merge generator which ran on the UNIVAC I. Another woman working at UNIVAC, Adele Mildred Koss, developed a program that was a precursor to report generators. In Russia, Kateryna Yushchenko developed the Address programming language for the MESM in 1955.

The idea for the creation of COBOL started in 1959 when Mary K. Hawes, who worked for Burroughs Corporation, set up a meeting to discuss creating a common business language. She invited six people, including Grace Hopper. Hopper was involved in developing COBOL as a business language and creating "self-documenting" programming. Hopper's contribution to COBOL was based on her programming language, called FLOW-MATIC. In 1961, Jean E. Sammet developed FORMAC and also published "Programming Languages: History and Fundamentals" which went on to be a standard work on programming languages.

Programs were mostly still entered using punched cards or paper tape. See computer programming in the punch card era. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Frances Holberton created a code to allow keyboard inputs while she worked at UNIVAC. Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards. Sister Mary Kenneth Keller worked on developing the programming language, BASIC which she was a graduate student at Dartmouth in the 1960s. One of the first object-oriented programming languages, Smalltalk, was developed by seven programmers, including Adele Goldberg, in the 1970s. In 1985, Radia Perlman developed the Spinning Tree Protocol in order to route packets of network information efficiently.

Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:

In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.

Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.

Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:

The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.

Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.

The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into "orders" using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.

"Programming a Computer for Playing Chess" was a 1950 paper that evaluated a "minimax" algorithm that is part of the history of algorithmic complexity; a course on IBM's Deep Blue (chess computer) is part of the computer science curriculum at Stanford University.

The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.

Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.

A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).

Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.

It is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).

Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).

Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.

After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it crash when parsing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be done manually, using a divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.

Debugging is often done with IDEs like Eclipse, Visual Studio, Xcode, Kdevelop, NetBeans and . Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.

Different programming languages support different styles of programming (called "programming paradigms"). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.

Allen Downey, in his book "How To Think Like A Computer Scientist", writes:

Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.

Computer programmers are those who write computer software. Their jobs usually involve:





</doc>
<doc id="36674345" url="https://en.wikipedia.org/wiki?curid=36674345" title="Information technology">
Information technology

Information technology (IT) is the use of computers to store, retrieve, transmit, and manipulate data, or information, often in the context of a business or other enterprise. IT is considered to be a subset of information and communications technology (ICT). An information technology system (IT system) is generally an information system, a communications system or, more specifically speaking, a computer system – including all hardware, software and peripheral equipment – operated by a limited group of users.

Humans have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC, but the term "information technology" in its modern sense first appeared in a 1958 article published in the "Harvard Business Review"; authors Harold J. Leavitt and Thomas L. Whisler commented that "the new technology does not yet have a single established name. We shall call it information technology (IT)." Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.

The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce.

Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present). This article focuses on the most recent period (electronic), which began in about 1940.

Devices have been used to aid computation for thousands of years, probably initially in the form of a tally stick. The Antikythera mechanism, dating from about the beginning of the first century BC, is generally considered to be the earliest known mechanical analog computer, and the earliest known geared mechanism. Comparable geared devices did not emerge in Europe until the 16th century, and it was not until 1645 that the first mechanical calculator capable of performing the four basic arithmetical operations was developed.

Electronic computers, using either relays or valves, began to appear in the early 1940s. The electromechanical Zuse Z3, completed in 1941, was the world's first programmable computer, and by modern standards one of the first machines that could be considered a complete computing machine. Colossus, developed during the Second World War to decrypt German messages, was the first electronic digital computer. Although it was programmable, it was not general-purpose, being designed to perform only a single task. It also lacked the ability to store its program in memory; programming was carried out using plugs and switches to alter the internal wiring. The first recognisably modern electronic digital stored-program computer was the Manchester Baby, which ran its first program on 21 June 1948.

The development of transistors in the late 1940s at Bell Laboratories allowed a new generation of computers to be designed with greatly reduced power consumption. The first commercially available stored-program computer, the Ferranti Mark I, contained 4050 valves and had a power consumption of 25 kilowatts. By comparison the first transistorised computer, developed at the University of Manchester and operational by November 1953, consumed only 150 watts in its final version.

Early electronic computers such as Colossus made use of punched tape, a long strip of paper on which data was represented by a series of holes, a technology now obsolete. Electronic data storage, which is used in modern computers, dates from World War II, when a form of delay line memory was developed to remove the clutter from radar signals, the first practical application of which was the mercury delay line. The first random-access digital storage device was the Williams tube, based on a standard cathode ray tube, but the information stored in it and delay line memory was volatile in that it had to be continuously refreshed, and thus was lost once power was removed. The earliest form of non-volatile computer storage was the magnetic drum, invented in 1932 and used in the Ferranti Mark 1, the world's first commercially available general-purpose electronic computer.

IBM introduced the first hard disk drive in 1956, as a component of their 305 RAMAC computer system. Most digital data today is still stored magnetically on hard disks, or optically on media such as CD-ROMs. Until 2002 most information was stored on analog devices, but that year digital storage capacity exceeded analog for the first time. As of 2007 almost 94% of the data stored worldwide was held digitally: 52% on hard disks, 28% on optical devices and 11% on digital magnetic tape. It has been estimated that the worldwide capacity to store information on electronic devices grew from less than 3 exabytes in 1986 to 295 exabytes in 2007, doubling roughly every 3 years.

Database management systems emerged in the 1960s to address the problem of storing and retrieving large amounts of data accurately and quickly. One of the earliest such systems was IBM's Information Management System (IMS), which is still widely deployed more than 50 years later. IMS stores data hierarchically, but in the 1970s Ted Codd proposed an alternative relational storage model based on set theory and predicate logic and the familiar concepts of tables, rows and columns. The first commercially available relational database management system (RDBMS) was available from Oracle in 1981.

All database management systems consist of a number of components that together allow the data they store to be accessed simultaneously by many users while maintaining its integrity. A characteristic of all databases is that the structure of the data they contain is defined and stored separately from the data itself, in a database schema.

The extensible markup language (XML) has become a popular format for data representation in recent years. Although XML data can be stored in normal file systems, it is commonly held in relational databases to take advantage of their "robust implementation verified by years of both theoretical and practical effort". As an evolution of the Standard Generalized Markup Language (SGML), XML's text-based structure offers the advantage of being both machine and human-readable.

The relational database model introduced a programming-language independent Structured Query Language (SQL), based on relational algebra.

The terms "data" and "information" are not synonymous. Anything stored is data, but it only becomes information when it is organized and presented meaningfully. Most of the world's digital data is unstructured, and stored in a variety of different physical formats even within a single organization. Data warehouses began to be developed in the 1980s to integrate these disparate stores. They typically contain data extracted from various sources, including external sources such as the Internet, organized in such a way as to facilitate decision support systems (DSS).

Data transmission has three aspects: transmission, propagation, and reception. It can be broadly categorized as broadcasting, in which information is transmitted unidirectionally downstream, or telecommunications, with bidirectional upstream and downstream channels.

XML has been increasingly employed as a means of data interchange since the early 2000s, particularly for machine-oriented interactions such as those involved in web-oriented protocols such as SOAP, describing "data-in-transit rather than ... data-at-rest". One of the challenges of such usage is converting data from relational databases into XML Document Object Model (DOM) structures.

Hilbert and Lopez identify the exponential pace of technological change (a kind of Moore's law): machines' application-specific capacity to compute information per capita roughly doubled every 14 months between 1986 and 2007; the per capita capacity of the world's general-purpose computers doubled every 18 months during the same two decades; the global telecommunication capacity per capita doubled every 34 months; the world's storage capacity per capita required roughly 40 months to double (every 3 years); and per capita broadcast information has doubled every 12.3 years.

Massive amounts of data are stored worldwide every day, but unless it can be analysed and presented effectively it essentially resides in what have been called data tombs: "data archives that are seldom visited". To address that issue, the field of data mining – "the process of discovering interesting patterns and knowledge from large amounts of data" – emerged in the late 1980s.

In an academic context, the Association for Computing Machinery defines IT as "undergraduate degree programs that prepare students to meet the computer technology needs of business, government, healthcare, schools, and other kinds of organizations ... IT specialists assume responsibility for selecting hardware and software products appropriate for an organization, integrating those products with organizational needs and infrastructure, and installing, customizing, and maintaining those applications for the organization’s computer users."

Companies in the information technology field are often discussed as a group as the "tech sector" or the "tech industry".

In a business context, the Information Technology Association of America has defined information technology as "the study, design, development, application, implementation, support or management of computer-based information systems". The responsibilities of those working in the field include network administration, software development and installation, and the planning and management of an organization's technology life cycle, by which hardware and software are maintained, upgraded and replaced.

The business value of information technology lies in the automation of business processes, provision of information for decision making, connecting businesses with their customers, and the provision of productivity tools to increase efficiency.

The field of information ethics was established by mathematician Norbert Wiener in the 1940s. Some of the ethical issues associated with the use of information technology include:





</doc>
<doc id="1588581" url="https://en.wikipedia.org/wiki?curid=1588581" title="Lithotope">
Lithotope

A lithotope is either an environment in which a sediment was deposited or an area of uniform sedimentation.

1. Surface or area of uniform precipitation.
2. Sediment having a relatively homogeneous sedimentation environment.


</doc>
<doc id="202898" url="https://en.wikipedia.org/wiki?curid=202898" title="Atmosphere of Earth">
Atmosphere of Earth

The atmosphere of Earth is the layer of gases, commonly known as air, that surrounds the planet Earth and is retained by Earth's gravity. The atmosphere of Earth protects life on Earth by creating pressure allowing for liquid water to exist on the Earth's surface, absorbing ultraviolet solar radiation, warming the surface through heat retention (greenhouse effect), and reducing temperature extremes between day and night (the diurnal temperature variation).

By volume, dry air contains 78.09% nitrogen, 20.95% oxygen, 0.93% argon, 0.04% carbon dioxide, and small amounts of other gases. Air also contains a variable amount of water vapor, on average around 1% at sea level, and 0.4% over the entire atmosphere. Air content and atmospheric pressure vary at different layers, and air suitable for use in photosynthesis by terrestrial plants and breathing of terrestrial animals is found only in Earth's troposphere and in artificial atmospheres.

The atmosphere has a mass of about 5.15 kg, three quarters of which is within about of the surface. The atmosphere becomes thinner and thinner with increasing altitude, with no definite boundary between the atmosphere and outer space. The Kármán line, at , or 1.57% of Earth's radius, is often used as the border between the atmosphere and outer space. Atmospheric effects become noticeable during atmospheric reentry of spacecraft at an altitude of around . Several layers can be distinguished in the atmosphere, based on characteristics such as temperature and composition.

The study of Earth's atmosphere and its processes is called atmospheric science (aerology). Early pioneers in the field include Léon Teisserenc de Bort and Richard Assmann.

The three major constituents of Earth's atmosphere are nitrogen, oxygen, and argon. Water vapor accounts for roughly 0.25% of the atmosphere by mass. The concentration of water vapor (a greenhouse gas) varies significantly from around 10 ppm by volume in the coldest portions of the atmosphere to as much as 5% by volume in hot, humid air masses, and concentrations of other atmospheric gases are typically quoted in terms of dry air (without water vapor). The remaining gases are often referred to as trace gases, among which are the greenhouse gases, principally carbon dioxide, methane, nitrous oxide, and ozone. Filtered air includes trace amounts of many other chemical compounds. Many substances of natural origin may be present in locally and seasonally variable small amounts as aerosols in an unfiltered air sample, including dust of mineral and organic composition, pollen and spores, sea spray, and volcanic ash. Various industrial pollutants also may be present as gases or aerosols, such as chlorine (elemental or in compounds), fluorine compounds and elemental mercury vapor. Sulfur compounds such as hydrogen sulfide and sulfur dioxide (SO) may be derived from natural sources or from industrial air pollution.

The relative concentration of gases remains constant until about .

In general, air pressure and density decrease with altitude in the atmosphere. However, temperature has a more complicated profile with altitude, and may remain relatively constant or even increase with altitude in some regions (see the temperature section, below). Because the general pattern of the temperature/altitude profile is constant and measurable by means of instrumented balloon soundings, the temperature behavior provides a useful metric to distinguish atmospheric layers. In this way, Earth's atmosphere can be divided (called atmospheric stratification) into five main layers. Excluding the exosphere, the atmosphere has four primary layers, which are the troposphere, stratosphere, mesosphere, and thermosphere. From highest to lowest, the five main layers are:

The exosphere is the outermost layer of Earth's atmosphere (i.e. the upper limit of the atmosphere). It extends from the exobase, which is located at the top of the thermosphere at an altitude of about 700 km above sea level, to about 10,000 km (6,200 mi; 33,000,000 ft) where it merges into the solar wind.

This layer is mainly composed of extremely low densities of hydrogen, helium and several heavier molecules including nitrogen, oxygen and carbon dioxide closer to the exobase. The atoms and molecules are so far apart that they can travel hundreds of kilometers without colliding with one another. Thus, the exosphere no longer behaves like a gas, and the particles constantly escape into space. These free-moving particles follow ballistic trajectories and may migrate in and out of the magnetosphere or the solar wind.

The exosphere is located too far above Earth for any meteorological phenomena to be possible. However, the aurora borealis and aurora australis sometimes occur in the lower part of the exosphere, where they overlap into the thermosphere. The exosphere contains most of the satellites orbiting Earth.

The thermosphere is the second-highest layer of Earth's atmosphere. It extends from the mesopause (which separates it from the mesosphere) at an altitude of about up to the thermopause at an altitude range of . The height of the thermopause varies considerably due to changes in solar activity. Because the thermopause lies at the lower boundary of the exosphere, it is also referred to as the exobase. The lower part of the thermosphere, from above Earth's surface, contains the ionosphere.

The temperature of the thermosphere gradually increases with height. Unlike the stratosphere beneath it, wherein a temperature inversion is due to the absorption of radiation by ozone, the inversion in the thermosphere occurs due to the extremely low density of its molecules. The temperature of this layer can rise as high as , though the gas molecules are so far apart that its temperature in the usual sense is not very meaningful. The air is so rarefied that an individual molecule (of oxygen, for example) travels an average of between collisions with other molecules. Although the thermosphere has a high proportion of molecules with high energy, it would not feel hot to a human in direct contact, because its density is too low to conduct a significant amount of energy to or from the skin.

This layer is completely cloudless and free of water vapor. However, non-hydrometeorological phenomena such as the aurora borealis and aurora australis are occasionally seen in the thermosphere. The International Space Station orbits in this layer, between .

The mesosphere is the third highest layer of Earth's atmosphere, occupying the region above the stratosphere and below the thermosphere. It extends from the stratopause at an altitude of about to the mesopause at above sea level.

Temperatures drop with increasing altitude to the mesopause that marks the top of this middle layer of the atmosphere. It is the coldest place on Earth and has an average temperature around .

Just below the mesopause, the air is so cold that even the very scarce water vapor at this altitude can be sublimated into polar-mesospheric noctilucent clouds. These are the highest clouds in the atmosphere and may be visible to the naked eye if sunlight reflects off them about an hour or two after sunset or a similar length of time before sunrise. They are most readily visible when the Sun is around 4 to 16 degrees below the horizon. Lightning-induced discharges known as transient luminous events (TLEs) occasionally form in the mesosphere above tropospheric thunderclouds. The mesosphere is also the layer where most meteors burn up upon atmospheric entrance. It is too high above Earth to be accessible to jet-powered aircraft and balloons, and too low to permit orbital spacecraft. The mesosphere is mainly accessed by sounding rockets and rocket-powered aircraft.

The stratosphere is the second-lowest layer of Earth's atmosphere. It lies above the troposphere and is separated from it by the tropopause. This layer extends from the top of the troposphere at roughly above Earth's surface to the stratopause at an altitude of about .

The atmospheric pressure at the top of the stratosphere is roughly 1/1000 the pressure at sea level. It contains the ozone layer, which is the part of Earth's atmosphere that contains relatively high concentrations of that gas. The stratosphere defines a layer in which temperatures rise with increasing altitude. This rise in temperature is caused by the absorption of ultraviolet radiation (UV) radiation from the Sun by the ozone layer, which restricts turbulence and mixing. Although the temperature may be at the tropopause, the top of the stratosphere is much warmer, and may be near 0 °C.

The stratospheric temperature profile creates very stable atmospheric conditions, so the stratosphere lacks the weather-producing air turbulence that is so prevalent in the troposphere. Consequently, the stratosphere is almost completely free of clouds and other forms of weather. However, polar stratospheric or nacreous clouds are occasionally seen in the lower part of this layer of the atmosphere where the air is coldest. The stratosphere is the highest layer that can be accessed by jet-powered aircraft.

The troposphere is the lowest layer of Earth's atmosphere. It extends from Earth's surface to an average height of about , although this altitude varies from about at the geographic poles to at the Equator, with some variation due to weather. The troposphere is bounded above by the tropopause, a boundary marked in most places by a temperature inversion (i.e. a layer of relatively warm air above a colder one), and in others by a zone which is isothermal with height.

Although variations do occur, the temperature usually declines with increasing altitude in the troposphere because the troposphere is mostly heated through energy transfer from the surface. Thus, the lowest part of the troposphere (i.e. Earth's surface) is typically the warmest section of the troposphere. This promotes vertical mixing (hence, the origin of its name in the Greek word τρόπος, "tropos", meaning "turn"). The troposphere contains roughly 80% of the mass of Earth's atmosphere. The troposphere is denser than all its overlying atmospheric layers because a larger atmospheric weight sits on top of the troposphere and causes it to be most severely compressed. Fifty percent of the total mass of the atmosphere is located in the lower of the troposphere.

Nearly all atmospheric water vapor or moisture is found in the troposphere, so it is the layer where most of Earth's weather takes place. It has basically all the weather-associated cloud genus types generated by active wind circulation, although very tall cumulonimbus thunder clouds can penetrate the tropopause from below and rise into the lower part of the stratosphere. Most conventional aviation activity takes place in the troposphere, and it is the only layer that can be accessed by propeller-driven aircraft.

Within the five principal layers above, that are largely determined by temperature, several secondary layers may be distinguished by other properties:

The average temperature of the atmosphere at Earth's surface is or , depending on the reference.

The average atmospheric pressure at sea level is defined by the International Standard Atmosphere as . This is sometimes referred to as a unit of standard atmospheres (atm). Total atmospheric mass is 5.1480×10 kg (1.135×10 lb), about 2.5% less than would be inferred from the average sea level pressure and Earth's area of 51007.2 megahectares, this portion being displaced by Earth's mountainous terrain. Atmospheric pressure is the total weight of the air above unit area at the point where the pressure is measured. Thus air pressure varies with location and weather.

If the entire mass of the atmosphere had a uniform density equal to sea level density (about 1.2 kg per m) from sea level upwards, it would terminate abruptly at an altitude of . It actually decreases exponentially with altitude, dropping by half every or by a factor of 1/e every , the average scale height of the atmosphere below . However, the atmosphere is more accurately modeled with a customized equation for each layer that takes gradients of temperature, molecular composition, solar radiation and gravity into account.

In summary, the mass of Earth's atmosphere is distributed approximately as follows:

By comparison, the summit of Mt. Everest is at ;
commercial airliners typically cruise between where the thinner air improves fuel economy; weather balloons reach and above; and the highest X-15 flight in 1963 reached .

Even above the Kármán line, significant atmospheric effects such as auroras still occur. Meteors begin to glow in this region, though the larger ones may not burn up until they penetrate more deeply. The various layers of Earth's ionosphere, important to HF radio propagation, begin below 100 km and extend beyond 500 km. By comparison, the International Space Station and Space Shuttle typically orbit at 350–400 km, within the F-layer of the ionosphere where they encounter enough atmospheric drag to require reboosts every few months. Depending on solar activity, satellites can experience noticeable atmospheric drag at altitudes as high as 700–800 km.

The division of the atmosphere into layers mostly by reference to temperature is discussed above. Temperature decreases with altitude starting at sea level, but variations in this trend begin above 11 km, where the temperature stabilizes through a large vertical distance through the rest of the troposphere. In the stratosphere, starting above about 20 km, the temperature increases with height, due to heating within the ozone layer caused by capture of significant ultraviolet radiation from the Sun by the dioxygen and ozone gas in this region. Still another region of increasing temperature with altitude occurs at very high altitudes, in the aptly-named thermosphere above 90 km.

Because in an ideal gas of constant composition the speed of sound depends only on temperature and not on the gas pressure or density, the speed of sound in the atmosphere with altitude takes on the form of the complicated temperature profile (see illustration to the right), and does not mirror altitudinal changes in density or pressure.

The density of air at sea level is about 1.2 kg/m (1.2 g/L, 0.0012 g/cm). Density is not measured directly but is calculated from measurements of temperature, pressure and humidity using the equation of state for air (a form of the ideal gas law). Atmospheric density decreases as the altitude increases. This variation can be approximately modeled using the barometric formula. More sophisticated models are used to predict orbital decay of satellites.

The average mass of the atmosphere is about 5 quadrillion (5) tonnes or 1/1,200,000 the mass of Earth. According to the American National Center for Atmospheric Research, "The total mean mass of the atmosphere is 5.1480 kg with an annual range due to water vapor of 1.2 or 1.5 kg, depending on whether surface pressure or water vapor data are used; somewhat smaller than the previous estimate. The mean mass of water vapor is estimated as 1.27 kg and the dry air mass as 5.1352 ±0.0003 kg."

Solar radiation (or sunlight) is the energy Earth receives from the Sun. Earth also emits radiation back into space, but at longer wavelengths that we cannot see. Part of the incoming and emitted radiation is absorbed or reflected by the atmosphere. In May 2017, glints of light, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.

When light passes through Earth's atmosphere, photons interact with it through "scattering". If the light does not interact with the atmosphere, it is called "direct radiation" and is what you see if you were to look directly at the Sun. "Indirect radiation" is light that has been scattered in the atmosphere. For example, on an overcast day when you cannot see your shadow there is no direct radiation reaching you, it has all been scattered. As another example, due to a phenomenon called Rayleigh scattering, shorter (blue) wavelengths scatter more easily than longer (red) wavelengths. This is why the sky looks blue; you are seeing scattered blue light. This is also why sunsets are red. Because the Sun is close to the horizon, the Sun's rays pass through more atmosphere than normal to reach your eye. Much of the blue light has been scattered out, leaving the red light in a sunset.

Different molecules absorb different wavelengths of radiation. For example, O and O absorb almost all wavelengths shorter than 300 nanometers. Water (HO) absorbs many wavelengths above 700 nm. When a molecule absorbs a photon, it increases the energy of the molecule. This heats the atmosphere, but the atmosphere also cools by emitting radiation, as discussed below.

The combined absorption spectra of the gases in the atmosphere leave "windows" of low opacity, allowing the transmission of only certain bands of light. The optical window runs from around 300 nm (ultraviolet-C) up into the range humans can see, the visible spectrum (commonly called light), at roughly 400–700 nm and continues to the infrared to around 1100 nm. There are also infrared and radio windows that transmit some infrared and radio waves at longer wavelengths. For example, the radio window runs from about one centimeter to about eleven-meter waves.

"Emission" is the opposite of absorption, it is when an object emits radiation. Objects tend to emit amounts and wavelengths of radiation depending on their "black body" emission curves, therefore hotter objects tend to emit more radiation, with shorter wavelengths. Colder objects emit less radiation, with longer wavelengths. For example, the Sun is approximately , its radiation peaks near 500 nm, and is visible to the human eye. Earth is approximately , so its radiation peaks near 10,000 nm, and is much too long to be visible to humans.

Because of its temperature, the atmosphere emits infrared radiation. For example, on clear nights Earth's surface cools down faster than on cloudy nights. This is because clouds (HO) are strong absorbers and emitters of infrared radiation. This is also why it becomes colder at night at higher elevations.

The greenhouse effect is directly related to this absorption and emission effect. Some gases in the atmosphere absorb and emit infrared radiation, but do not interact with sunlight in the visible spectrum. Common examples of these are and HO.

The refractive index of air is close to, but just greater than 1. Systematic variations in refractive index can lead to the bending of light rays over long optical paths. One example is that, under some circumstances, observers onboard ships can see other vessels just over the horizon because light is refracted in the same direction as the curvature of Earth's surface.

The refractive index of air depends on temperature, giving rise to refraction effects when the temperature gradient is large. An example of such effects is the mirage.

"Atmospheric circulation" is the large-scale movement of air through the troposphere, and the means (with ocean circulation) by which heat is distributed around Earth. The large-scale structure of the atmospheric circulation varies from year to year, but the basic structure remains fairly constant because it is determined by Earth's rotation rate and the difference in solar radiation between the equator and poles.

The first atmosphere consisted of gases in the solar nebula, primarily hydrogen. There were probably simple hydrides such as those now found in the gas giants (Jupiter and Saturn), notably water vapor, methane and ammonia.

Outgassing from volcanism, supplemented by gases produced during the late heavy bombardment of Earth by huge asteroids, produced the next atmosphere, consisting largely of nitrogen plus carbon dioxide and inert gases. A major part of carbon-dioxide emissions dissolved in water and reacted with metals such as calcium and magnesium during weathering of crustal rocks to form carbonates that were deposited as sediments. Water-related sediments have been found that date from as early as 3.8 billion years ago.

About 3.4 billion years ago, nitrogen formed the major part of the then stable "second atmosphere". The influence of life has to be taken into account rather soon in the history of the atmosphere, because hints of early life-forms appear as early as 3.5 billion years ago. How Earth at that time maintained a climate warm enough for liquid water and life, if the early Sun put out 30% lower solar radiance than today, is a puzzle known as the "faint young Sun paradox".

The geological record however shows a continuous relatively warm surface during the complete early temperature record of Earth – with the exception of one cold glacial phase about 2.4 billion years ago. In the late Archean Eon an oxygen-containing atmosphere began to develop, apparently produced by photosynthesizing cyanobacteria (see Great Oxygenation Event), which have been found as stromatolite fossils from 2.7 billion years ago. The early basic carbon isotopy (isotope ratio proportions) strongly suggests conditions similar to the current, and that the fundamental features of the carbon cycle became established as early as 4 billion years ago.

Ancient sediments in the Gabon dating from between about 2,150 and 2,080 million years ago provide a record of Earth's dynamic oxygenation evolution. These fluctuations in oxygenation were likely driven by the Lomagundi carbon isotope excursion.

The constant re-arrangement of continents by plate tectonics influences the long-term evolution of the atmosphere by transferring carbon dioxide to and from large continental carbonate stores. Free oxygen did not exist in the atmosphere until about 2.4 billion years ago during the Great Oxygenation Event and its appearance is indicated by the end of the banded iron formations.

Before this time, any oxygen produced by photosynthesis was consumed by oxidation of reduced materials, notably iron. Molecules of free oxygen did not start to accumulate in the atmosphere until the rate of production of oxygen began to exceed the availability of reducing materials that removed oxygen. This point signifies a shift from a reducing atmosphere to an oxidizing atmosphere. O showed major variations until reaching a steady state of more than 15% by the end of the Precambrian. The following time span from 541 million years ago to the present day is the Phanerozoic Eon, during the earliest period of which, the Cambrian, oxygen-requiring metazoan life forms began to appear.

The amount of oxygen in the atmosphere has fluctuated over the last 600 million years, reaching a peak of about 30% around 280 million years ago, significantly higher than today's 21%. Two main processes govern changes in the atmosphere: Plants use carbon dioxide from the atmosphere, releasing oxygen. Breakdown of pyrite and volcanic eruptions release sulfur into the atmosphere, which oxidizes and hence reduces the amount of oxygen in the atmosphere. However, volcanic eruptions also release carbon dioxide, which plants can convert to oxygen. The exact cause of the variation of the amount of oxygen in the atmosphere is not known. Periods with much oxygen in the atmosphere are associated with rapid development of animals. Today's atmosphere contains 21% oxygen, which is great enough for this rapid development of animals.

"Air pollution" is the introduction into the atmosphere of chemicals, particulate matter or biological materials that cause harm or discomfort to organisms. Stratospheric ozone depletion is caused by air pollution, chiefly from chlorofluorocarbons and other ozone-depleting substances.

The scientific consensus is that the anthropogenic greenhouse gases currently accumulating in the atmosphere are the main cause of global warming.

On October 19, 2015 NASA started a website containing daily images of the full sunlit side of Earth on http://epic.gsfc.nasa.gov/. The images are taken from the Deep Space Climate Observatory (DSCOVR) and show Earth as it rotates during a day.




</doc>
<doc id="305265" url="https://en.wikipedia.org/wiki?curid=305265" title="Underwater">
Underwater

Underwater refers to the region below the surface of water where the water exists in a swimming pool or a natural feature (called a body of water) such as an ocean, sea, lake, pond, or river.

Three quarters of the planet Earth is covered by water. A majority of the planet's solid surface is abyssal plain, at depths between below the surface of the oceans. The solid surface location on the planet closest to the centre of the orb is the Challenger Deep, located in the Mariana Trench at a depth of . Although a number of human activities are conducted underwater—such as research, scuba diving for work or recreation, or even underwater warfare with submarines, this very extensive environment on planet Earth is hostile to humans in many ways and therefore little explored. But it can be explored by sonar, or more directly via manned or autonomous submersibles. The ocean floors have been surveyed via sonar to at least a coarse resolution; particularly-strategic areas have been mapped in detail, in the name of detecting enemy submarines, or aiding friendly ones, though the resulting maps may still be classified.

An immediate obstacle to human activity under water is the fact that human lungs cannot naturally function in this environment. Unlike the gills of fish, human lungs are adapted to the exchange of gases at atmospheric pressure, not liquids. Aside from simply having insufficient musculature to rapidly move water in and out of the lungs, a more significant problem for all air-breathing animals, such as mammals and birds, is that water contains so little dissolved oxygen compared with atmospheric air. Air is around 21% O; water typically is less than 0.001% dissolved oxygen.

The density of water also causes problems that increase dramatically with depth. The atmospheric pressure at the surface is 14.7 pounds per square inch or around 100 kPa. A comparable water pressure occurs at a depth of only ( for sea water). Thus, at about 10 m below the surface, the water exerts twice the pressure (2 atmospheres or 200 kPa) on the body as air at surface level.

For solids and liquids like bone, muscle and blood, this added pressure is not much of a problem; but it is a problem for any air-filled spaces like the mouth, ears, paranasal sinuses and lungs. This is because the air in those spaces reduces in volume when under pressure and so does not provide those spaces with support against the higher outside pressure. Even at a depth of underwater, an inability to equalize air pressure in the middle ear with outside water pressure can cause pain, and the tympanic membrane (eardrum) can rupture at depths under 10 ft (3 m). The danger of pressure damage is greatest in shallow water because the ratio of pressure change is greatest near the surface of the water. For example, the pressure increase between the surface and 10 m (33 ft) is 100% (100 kPa to 200 kPa), but the pressure increase from 30 m (100 ft) to 40 m (130 ft) is only 25% (400 kPa to 500 kPa).

Any object immersed in water is provided with a buoyant force that counters the force of gravity, appearing to make the object less heavy. If the overall density of the object exceeds the density of water, the object sinks. If the overall density is less than the density of water, the object rises until it floats on the surface.
With increasing depth underwater, sunlight is absorbed, and the amount of visible light diminishes. Because absorption is greater for long wavelengths (red end of the visible spectrum) than for short wavelengths (blue end of the visible spectrum), the colour spectrum is rapidly altered with increasing depth. White objects at the surface appear bluish underwater, and red objects appear dark, even black. Although light penetration will be less if water is turbid, in the very clear water of the open ocean less than 25% of the surface light reaches a depth of 10 m (33 feet). At 100 m (330 ft) the light present from the sun is normally about 0.5% of that at the surface.

The euphotic depth is the depth at which light intensity falls to 1% of the value at the surface. This depth is dependent upon water clarity, being only a few metres underwater in a turbid estuary, but may reach up to 200 metres in the open ocean. At the euphotic depth, plants (such as phytoplankton) have no net energy gain from photosynthesis and thus cannot grow.

There are three layers of ocean temperature: the surface layer, the thermocline, and the deep ocean. The average temperature of surface layer is about 17 °C. About 90% of ocean's water is below the thermocline in the deep ocean, where most of the water is below 4 °C.

Water conducts heat around 25 times more efficiently than air. Hypothermia, a potentially fatal condition, occurs when the human body's core temperature falls below 35 °C. Insulating the body's warmth from water is the main purpose of diving suits and exposure suits when used in water temperatures below 25 °C.

Sound is transmitted about 4.3 times faster in water (1,484 m/s in fresh water) as it is in air (343 m/s). The human brain can determine the direction of sound in air by detecting small differences in the time it takes for sound waves in air to reach each of the two ears. For these reasons divers find it difficult to determine the direction of sound underwater. However, some animals have adapted to this difference and many use sound to navigate underwater.




</doc>
<doc id="31880" url="https://en.wikipedia.org/wiki?curid=31880" title="Universe">
Universe

The Universe is all of space and time and their contents, including planets, stars, galaxies, and all other forms of matter and energy. While the spatial size of the entire Universe is unknown, it is possible to measure the observable universe.

The earliest scientific models of the Universe were developed by ancient Greek and Indian philosophers and were geocentric, placing Earth at the center of the Universe. Over the centuries, more precise astronomical observations led Nicolaus Copernicus to develop the heliocentric model with the Sun at the center of the Solar System. In developing the law of universal gravitation, Isaac Newton built upon Copernicus' work as well as observations by Tycho Brahe and Johannes Kepler's laws of planetary motion.

Further observational improvements led to the realization that the Sun is one of hundreds of billions of stars in the Milky Way, which is one of at least hundreds of billions of galaxies in the Universe. Many of the stars in our galaxy have planets. At the largest scale galaxies are distributed uniformly and the same in all directions, meaning that the Universe has neither an edge nor a center. At smaller scales, galaxies are distributed in clusters and superclusters which form immense filaments and voids in space, creating a vast foam-like structure. Discoveries in the early 20th century have suggested that the Universe had a beginning and that space has been expanding since then, and is currently still expanding at an increasing rate.

The Big Bang theory is the prevailing cosmological description of the development of the Universe. Under this theory, space and time emerged together ago with a fixed amount of energy and matter that has become less dense as the Universe has expanded. After an initial accelerated expansion at around 10 seconds, and the separation of the four known fundamental forces, the Universe gradually cooled and continued to expand, allowing the first subatomic particles and simple atoms to form. Dark matter gradually gathered forming a foam-like structure of filaments and voids under the influence of gravity. Giant clouds of hydrogen and helium were gradually drawn to the places where dark matter was most dense, forming the first galaxies, stars, and everything else seen today. It is possible to see objects that are now further away than 13.799 billion light-years because space itself has expanded, and it is still expanding today. This means that objects which are now up to 46.5 billion light-years away can still be seen in their distant past, because in the past when their light was emitted, they were much closer to the Earth.

From studying the movement of galaxies, it has been discovered that the universe contains much more matter than is accounted for by visible objects; stars, galaxies, nebulas and interstellar gas. This unseen matter is known as dark matter ("dark" means that there is a wide range of strong indirect evidence that it exists, but we have not yet detected it directly). The ΛCDM model is the most widely accepted model of our universe. It suggests that about [2015] of the mass and energy in the universe is a cosmological constant (or, in extensions to ΛCDM, other forms of dark energy such as a scalar field) which is responsible for the current expansion of space, and about [2015] is dark matter. Ordinary ("baryonic") matter is therefore only 4.9% [2015] of the physical universe. Stars, planets, and visible gas clouds only form about 6% of ordinary matter, or about 0.3% of the entire universe.

There are many competing hypotheses about the ultimate fate of the universe and about what, if anything, preceded the Big Bang, while other physicists and philosophers refuse to speculate, doubting that information about prior states will ever be accessible. Some physicists have suggested various multiverse hypotheses, in which the Universe might be one among many universes that likewise exist.

The physical Universe is defined as all of space and time (collectively referred to as spacetime) and their contents. Such contents comprise all of energy in its various forms, including electromagnetic radiation and matter, and therefore planets, moons, stars, galaxies, and the contents of intergalactic space. The Universe also includes the physical laws that influence energy and matter, such as conservation laws, classical mechanics, and relativity.

The Universe is often defined as "the totality of existence", or everything that exists, everything that has existed, and everything that will exist. In fact, some philosophers and scientists support the inclusion of ideas and abstract concepts – such as mathematics and logic – in the definition of the Universe. The word "universe" may also refer to concepts such as "the cosmos", "the world", and "nature".

The word "universe" derives from the Old French word "univers", which in turn derives from the Latin word "universum". The Latin word was used by Cicero and later Latin authors in many of the same senses as the modern English word is used.

A term for "universe" among the ancient Greek philosophers from Pythagoras onwards was , "tò pân" ("the all"), defined as all matter and all space, and , "tò hólon" ("all things"), which did not necessarily include the void. Another synonym was , "ho kósmos" (meaning the world, the cosmos). Synonyms are also found in Latin authors ("totum", "mundus", "natura") and survive in modern languages, e.g., the German words "Das All", "Weltall", and "Natur" for "Universe". The same synonyms are found in English, such as everything (as in the theory of everything), the cosmos (as in cosmology), the world (as in the many-worlds interpretation), and nature (as in natural laws or natural philosophy).

The prevailing model for the evolution of the Universe is the Big Bang theory. The Big Bang model states that the earliest state of the Universe was an extremely hot and dense one, and that the Universe subsequently expanded and cooled. The model is based on general relativity and on simplifying assumptions such as homogeneity and isotropy of space. A version of the model with a cosmological constant (Lambda) and cold dark matter, known as the Lambda-CDM model, is the simplest model that provides a reasonably good account of various observations about the Universe. The Big Bang model accounts for observations such as the correlation of distance and redshift of galaxies, the ratio of the number of hydrogen to helium atoms, and the microwave radiation background.

The initial hot, dense state is called the Planck epoch, a brief period extending from time zero to one Planck time unit of approximately 10 seconds. During the Planck epoch, all types of matter and all types of energy were concentrated into a dense state, and gravity - currently the weakest by far of the four known forces - is believed to have been as strong as the other fundamental forces, and all the forces may have been unified. Since the Planck epoch, space has been expanding to its present scale, with a very short but intense period of cosmic inflation believed to have occurred within the first 10 seconds. This was a kind of expansion different from those we can see around us today. Objects in space did not physically move; instead the metric that defines space itself changed. Although objects in spacetime cannot move faster than the speed of light, this limitation does not apply to the metric governing spacetime itself. This initial period of inflation is believed to explain why space appears to be very flat, and much larger than light could travel since the start of the universe.

Within the first fraction of a second of the universe's existence, the four fundamental forces had separated. As the universe continued to cool down from its inconceivably hot state, various types of subatomic particles were able to form in short periods of time known as the quark epoch, the hadron epoch, and the lepton epoch. Together, these epochs encompassed less than 10 seconds of time following the Big Bang. These elementary particles associated stably into ever larger combinations, including stable protons and neutrons, which then formed more complex atomic nuclei through nuclear fusion. This process, known as Big Bang nucleosynthesis, only lasted for about 17 minutes and ended about 20 minutes after the Big Bang, so only the fastest and simplest reactions occurred. About 25% of the protons and all the neutrons in the universe, by mass, were converted to helium, with small amounts of deuterium (a form of hydrogen) and traces of lithium. Any other element was only formed in very tiny quantities. The other 75% of the protons remained unaffected, as hydrogen nuclei.

After nucleosynthesis ended, the universe entered a period known as the photon epoch. During this period, the Universe was still far too hot for matter to form neutral atoms, so it contained a hot, dense, foggy plasma of negatively charged electrons, neutral neutrinos and positive nuclei. After about 377,000 years, the universe had cooled enough that electrons and nuclei could form the first stable atoms. This is known as recombination for historical reasons; in fact electrons and nuclei were combining for the first time. Unlike plasma, neutral atoms are transparent to many wavelengths of light, so for the first time the universe also became transparent. The photons released ("decoupled") when these atoms formed can still be seen today; they form the cosmic microwave background (CMB).

As the Universe expands, the energy density of electromagnetic radiation decreases more quickly than does that of matter because the energy of a photon decreases with its wavelength. At around 47,000 years, the energy density of matter became larger than that of photons and neutrinos, and began to dominate the large scale behavior of the universe. This marked the end of the radiation-dominated era and the start of the matter-dominated era.

In the earliest stages of the universe, tiny fluctuations within the universe's density led to concentrations of dark matter gradually forming. Ordinary matter, attracted to these by gravity, formed large gas clouds and eventually, stars and galaxies, where the dark matter was most dense, and voids where it was least dense. After around 100 - 300 million years, the first stars formed, known as Population III stars. These were probably very massive, luminous, non metallic and short-lived. They were responsible for the gradual reionization of the Universe between about 200-500 million years and 1 billion years, and also for seeding the universe with elements heavier than helium, through stellar nucleosynthesis. The Universe also contains a mysterious energy - possibly a scalar field - called dark energy, the density of which does not change over time. After about 9.8 billion years, the Universe had expanded sufficiently so that the density of matter was less than the density of dark energy, marking the beginning of the present dark-energy-dominated era. In this era, the expansion of the Universe is accelerating due to dark energy.

Of the four fundamental interactions, gravitation is the dominant at astronomical length scales. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on astronomical length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales.

The Universe appears to have much more matter than antimatter, an asymmetry possibly related to the CP violation. This imbalance between matter and antimatter is partially responsible for the existence of all matter existing today, since matter and antimatter, if equally produced at the Big Bang, would have completely annihilated each other and left only photons as a result of their interaction. The Universe also appears to have neither net momentum nor angular momentum, which follows accepted physical laws if the Universe is finite. These laws are the Gauss's law and the non-divergence of the stress-energy-momentum pseudotensor.

The size of the Universe is somewhat difficult to define. According to the general theory of relativity, far regions of space may never interact with ours even in the lifetime of the Universe due to the finite speed of light and the ongoing expansion of space. For example, radio messages sent from Earth may never reach some regions of space, even if the Universe were to exist forever: space may expand faster than light can traverse it.

Distant regions of space are assumed to exist and to be part of reality as much as we are, even though we can never interact with them. The spatial region that we can affect and be affected by is the observable universe. The observable universe depends on the location of the observer. By traveling, an observer can come into contact with a greater region of spacetime than an observer who remains still. Nevertheless, even the most rapid traveler will not be able to interact with all of space. Typically, the observable universe is taken to mean the portion of the Universe that is observable from our vantage point in the Milky Way.

The proper distance—the distance as would be measured at a specific time, including the present—between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 93 billion light-years (28 billion parsecs). The distance the light from the edge of the observable universe has travelled is very close to the age of the Universe times the speed of light, , but this does not represent the distance at any given time because the edge of the observable universe and the Earth have since moved further apart. For comparison, the diameter of a typical galaxy is 30,000 light-years (9,198 parsecs), and the typical distance between two neighboring galaxies is 3 million light-years (919.8 kiloparsecs). As an example, the Milky Way is roughly 100,000–180,000 light-years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light-years away.

Because we cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the Universe in its totality is finite or infinite. Estimates for the total size of the universe, if finite, reach as high as formula_1 megaparsecs, implied by one resolution of the No-Boundary Proposal.

Astronomers calculate the age of the Universe by assuming that the Lambda-CDM model accurately describes the evolution of the Universe from a very uniform, hot, dense primordial state to its present state and measuring the cosmological parameters which constitute the model. This model is well understood theoretically and supported by recent high-precision astronomical observations such as WMAP and Planck. Commonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for Type Ia supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less accurately measured at present. Assuming that the Lambda-CDM model is correct, the measurements of the parameters using a variety of techniques by numerous experiments yield a best value of the age of the Universe as of 2015 of 13.799 ± 0.021 billion years.
Over time, the Universe and its contents have evolved; for example, the relative population of quasars and galaxies has changed and space itself has expanded. Due to this expansion, scientists on Earth can observe the light from a galaxy 30 billion light-years away even though that light has traveled for only 13 billion years; the very space between them has expanded. This expansion is consistent with the observation that the light from distant galaxies has been redshifted; the photons emitted have been stretched to longer wavelengths and lower frequency during their journey. Analyses of Type Ia supernovae indicate that the spatial expansion is accelerating.

The more matter there is in the Universe, the stronger the mutual gravitational pull of the matter. If the Universe were "too" dense then it would re-collapse into a gravitational singularity. However, if the Universe contained too "little" matter then the self-gravity would be too weak for astronomical structures, like galaxies or planets, to form. Since the Big Bang, the universe has expanded monotonically. Perhaps unsurprisingly, our universe has just the right mass-energy density, equivalent to about 5 protons per cubic meter, which has allowed it to expand for the last 13.8 billion years, giving time to form the universe as observed today.

There are dynamical forces acting on the particles in the Universe which affect the expansion rate. Before 1998, it was expected that the expansion rate would be decreasing as time went on due to the influence of gravitational interactions in the Universe; and thus there is an additional observable quantity in the Universe called the deceleration parameter, which most cosmologists expected to be positive and related to the matter density of the Universe. In 1998, the deceleration parameter was measured by two different groups to be negative, approximately -0.55, which technically implies that the second derivative of the cosmic scale factor formula_2 has been positive in the last 5-6 billion years. This acceleration does not, however, imply that the Hubble parameter is currently increasing; see deceleration parameter for details.

Spacetimes are the arenas in which all physical events take place. The basic elements of spacetimes are events. In any given spacetime, an event is defined as a unique position at a unique time. A spacetime is the union of all events (in the same way that a line is the union of all of its points), formally organized into a manifold.

The Universe appears to be a smooth spacetime continuum consisting of three spatial dimensions and one temporal (time) dimension (an event in the spacetime of the physical Universe can therefore be identified by a set of four coordinates: ("x", "y", "z", "t") ). On the average, space is observed to be very nearly flat (with a curvature close to zero), meaning that Euclidean geometry is empirically true with high accuracy throughout most of the Universe. Spacetime also appears to have a simply connected topology, in analogy with a sphere, at least on the length-scale of the observable Universe. However, present observations cannot exclude the possibilities that the Universe has more dimensions (which is postulated by theories such as the String theory) and that its spacetime may have a multiply connected global topology, in analogy with the cylindrical or toroidal topologies of two-dimensional spaces.
The spacetime of the Universe is usually interpreted from a Euclidean perspective, with space as consisting of three dimensions, and time as consisting of one dimension, the "fourth dimension". By combining space and time into a single manifold called Minkowski space, physicists have simplified a large number of physical theories, as well as described in a more uniform way the workings of the Universe at both the supergalactic and subatomic levels.

Spacetime events are not absolutely defined spatially and temporally but rather are known to be relative to the motion of an observer. Minkowski space approximates the Universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity.

General relativity describes how spacetime is curved and bent by mass and energy (gravity). The topology or geometry of the Universe includes both local geometry in the observable universe and global geometry.
Cosmologists often work with a given space-like slice of spacetime called the comoving coordinates. The section of spacetime which can be observed is the backward light cone, which delimits the cosmological horizon.
The cosmological horizon (also called the particle horizon or the light horizon) is the maximum distance from which particles can have traveled to the observer in the age of the Universe. This horizon represents the boundary between the observable and the unobservable regions of the Universe. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.

An important parameter determining the future evolution of the Universe theory is the density parameter, Omega (Ω), defined as the average matter density of the universe divided by a critical value of that density. This selects one of three possible geometries depending on whether Ω is equal to, less than, or greater than 1. These are called, respectively, the flat, open and closed universes.

Observations, including the Cosmic Background Explorer (COBE), Wilkinson Microwave Anisotropy Probe (WMAP), and Planck maps of the CMB, suggest that the Universe is infinite in extent with a finite age, as described by the Friedmann–Lemaître–Robertson–Walker (FLRW) models. These FLRW models thus support inflationary models and the standard model of cosmology, describing a flat, homogeneous universe presently dominated by dark matter and dark energy.

The Universe may be "fine-tuned"; the Fine-tuned Universe hypothesis is the proposition that the conditions that allow the existence of observable life in the Universe can only occur when certain universal fundamental physical constants lie within a very narrow range of values, so that if any of several fundamental constants were only slightly different, the Universe would have been unlikely to be conducive to the establishment and development of matter, astronomical structures, elemental diversity, or life as it is understood. The proposition is discussed among philosophers, scientists, theologians, and proponents of creationism.

The Universe is composed almost completely of dark energy, dark matter, and ordinary matter. Other contents are electromagnetic radiation (estimated to constitute from 0.005% to close to 0.01% of the total mass of the Universe) and antimatter.

The proportions of all types of matter and energy have changed over the history of the Universe. The total amount of electromagnetic radiation generated within the universe has decreased by 1/2 in the past 2 billion years. Today, ordinary matter, which includes atoms, stars, galaxies, and life, accounts for only 4.9% of the contents of the Universe. The present overall density of this type of matter is very low, roughly 4.5 × 10 grams per cubic centimetre, corresponding to a density of the order of only one proton for every four cubic meters of volume. The nature of both dark energy and dark matter is unknown. Dark matter, a mysterious form of matter that has not yet been identified, accounts for 26.8% of the cosmic contents. Dark energy, which is the energy of empty space and is causing the expansion of the Universe to accelerate, accounts for the remaining 68.3% of the contents.
Matter, dark matter, and dark energy are distributed homogeneously throughout the Universe over length scales longer than 300 million light-years or so. However, over shorter length-scales, matter tends to clump hierarchically; many atoms are condensed into stars, most stars into galaxies, most galaxies into clusters, superclusters and, finally, large-scale galactic filaments. The observable Universe contains approximately 300 sextillion (3) stars and more than 100 billion (10) galaxies. Typical galaxies range from dwarfs with as few as ten million (10) stars up to giants with one trillion (10) stars. Between the larger structures are voids, which are typically 10–150 Mpc (33 million–490 million ly) in diameter. The Milky Way is in the Local Group of galaxies, which in turn is in the Laniakea Supercluster. This supercluster spans over 500 million light-years, while the Local Group spans over 10 million light-years. The Universe also has vast regions of relative emptiness; the largest known void measures 1.8 billion ly (550 Mpc) across.

The observable Universe is isotropic on scales significantly larger than superclusters, meaning that the statistical properties of the Universe are the same in all directions as observed from Earth. The Universe is bathed in highly isotropic microwave radiation that corresponds to a thermal equilibrium blackbody spectrum of roughly 2.72548 kelvins. The hypothesis that the large-scale Universe is homogeneous and isotropic is known as the cosmological principle. A Universe that is both homogeneous and isotropic looks the same from all vantage points and has no center.

An explanation for why the expansion of the Universe is accelerating remains elusive. It is often attributed to "dark energy", an unknown form of energy that is hypothesized to permeate space. On a mass–energy equivalence basis, the density of dark energy (~ 7 × 10 g/cm) is much less than the density of ordinary matter or dark matter within galaxies. However, in the present dark-energy era, it dominates the mass–energy of the universe because it is uniform across space.

Two proposed forms for dark energy are the cosmological constant, a "constant" energy density filling space homogeneously, and scalar fields such as quintessence or moduli, "dynamic" quantities whose energy density can vary in time and space. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to vacuum energy. Scalar fields having only a slight amount of spatial inhomogeneity would be difficult to distinguish from a cosmological constant.

Dark matter is a hypothetical kind of matter that is invisible to the entire electromagnetic spectrum, but which accounts for most of the matter in the Universe. The existence and properties of dark matter are inferred from its gravitational effects on visible matter, radiation, and the large-scale structure of the Universe. Other than neutrinos, a form of hot dark matter, dark matter has not been detected directly, making it one of the greatest mysteries in modern astrophysics. Dark matter neither emits nor absorbs light or any other electromagnetic radiation at any significant level. Dark matter is estimated to constitute 26.8% of the total mass–energy and 84.5% of the total matter in the Universe.

The remaining 4.9% of the mass–energy of the Universe is ordinary matter, that is, atoms, ions, electrons and the objects they form. This matter includes stars, which produce nearly all of the light we see from galaxies, as well as interstellar gas in the interstellar and intergalactic media, planets, and all the objects from everyday life that we can bump into, touch or squeeze. As a matter of fact, the great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10 per cent of the ordinary matter contribution to the mass-energy density of the universe.

Ordinary matter commonly exists in four states (or phases): solid, liquid, gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates.

Ordinary matter is composed of two types of elementary particles: quarks and leptons. For example, the proton is formed of two up quarks and one down quark; the neutron is formed of two down quarks and one up quark; and the electron is a kind of lepton. An atom consists of an atomic nucleus, made up of protons and neutrons, and electrons that orbit the nucleus. Because most of the mass of an atom is concentrated in its nucleus, which is made up of baryons, astronomers often use the term "baryonic matter" to describe ordinary matter, although a small fraction of this "baryonic matter" is electrons.

Soon after the Big Bang, primordial protons and neutrons formed from the quark–gluon plasma of the early Universe as it cooled below two trillion degrees. A few minutes later, in a process known as Big Bang nucleosynthesis, nuclei formed from the primordial protons and neutrons. This nucleosynthesis formed lighter elements, those with small atomic numbers up to lithium and beryllium, but the abundance of heavier elements dropped off sharply with increasing atomic number. Some boron may have been formed at this time, but the next heavier element, carbon, was not formed in significant amounts. Big Bang nucleosynthesis shut down after about 20 minutes due to the rapid drop in temperature and density of the expanding Universe. Subsequent formation of heavier elements resulted from stellar nucleosynthesis and supernova nucleosynthesis.

Ordinary matter and the forces that act on matter can be described in terms of elementary particles. These particles are sometimes described as being fundamental, since they have an unknown substructure, and it is unknown whether or not they are composed of smaller and even more fundamental particles. Of central importance is the Standard Model, a theory that is concerned with electromagnetic interactions and the weak and strong nuclear interactions. The Standard Model is supported by the experimental confirmation of the existence of particles that compose matter: quarks and leptons, and their corresponding "antimatter" duals, as well as the force particles that mediate interactions: the photon, the W and Z bosons, and the gluon. The Standard Model predicted the existence of the recently discovered Higgs boson, a particle that is a manifestation of a field within the Universe that can endow particles with mass. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a "theory of almost everything". The Standard Model does not, however, accommodate gravity. A true force-particle "theory of everything" has not been attained.

A hadron is a composite particle made of quarks held together by the strong force. Hadrons are categorized into two families: baryons (such as protons and neutrons) made of three quarks, and mesons (such as pions) made of one quark and one antiquark. Of the hadrons, protons are stable, and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions and are thus insignificant constituents of the modern Universe.
From approximately 10 seconds after the Big Bang, during a period is known as the hadron epoch, the temperature of the universe had fallen sufficiently to allow quarks to bind together into hadrons, and the mass of the Universe was dominated by hadrons. Initially the temperature was high enough to allow the formation of hadron/anti-hadron pairs, which kept matter and antimatter in thermal equilibrium. However, as the temperature of the Universe continued to fall, hadron/anti-hadron pairs were no longer produced. Most of the hadrons and anti-hadrons were then eliminated in particle-antiparticle annihilation reactions, leaving a small residual of hadrons by the time the Universe was about one second old.

A lepton is an elementary, half-integer spin particle that does not undergo strong interactions but is subject to the Pauli exclusion principle; no two leptons of the same species can be in exactly the same state at the same time. Two main classes of leptons exist: charged leptons (also known as the "electron-like" leptons), and neutral leptons (better known as neutrinos). Electrons are stable and the most common charged lepton in the Universe, whereas muons and taus are unstable particle that quickly decay after being produced in high energy collisions, such as those involving cosmic rays or carried out in particle accelerators.
Charged leptons can combine with other particles to form various composite particles such as atoms and positronium. The electron governs nearly all of chemistry, as it is found in atoms and is directly tied to all chemical properties. Neutrinos rarely interact with anything, and are consequently rarely observed. Neutrinos stream throughout the Universe but rarely interact with normal matter.

The lepton epoch was the period in the evolution of the early Universe in which the leptons dominated the mass of the Universe. It started roughly 1 second after the Big Bang, after the majority of hadrons and anti-hadrons annihilated each other at the end of the hadron epoch. During the lepton epoch the temperature of the Universe was still high enough to create lepton/anti-lepton pairs, so leptons and anti-leptons were in thermal equilibrium. Approximately 10 seconds after the Big Bang, the temperature of the Universe had fallen to the point where lepton/anti-lepton pairs were no longer created. Most leptons and anti-leptons were then eliminated in annihilation reactions, leaving a small residue of leptons. The mass of the Universe was then dominated by photons as it entered the following photon epoch.

A photon is the quantum of light and all other forms of electromagnetic radiation. It is the force carrier for the electromagnetic force, even when static via virtual photons. The effects of this force are easily observable at the microscopic and at the macroscopic level because the photon has zero rest mass; this allows long distance interactions. Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of waves and of particles.

The photon epoch started after most leptons and anti-leptons were annihilated at the end of the lepton epoch, about 10 seconds after the Big Bang. Atomic nuclei were created in the process of nucleosynthesis which occurred during the first few minutes of the photon epoch. For the remainder of the photon epoch the Universe contained a hot dense plasma of nuclei, electrons and photons. About 380,000 years after the Big Bang, the temperature of the Universe fell to the point where nuclei could combine with electrons to create neutral atoms. As a result, photons no longer interacted frequently with matter and the Universe became transparent. The highly redshifted photons from this period form the cosmic microwave background. Tiny variations in temperature and density detectable in the CMB were the early "seeds" from which all subsequent structure formation took place.
General relativity is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. It is the basis of current cosmological models of the Universe. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations. In general relativity, the distribution of matter and energy determines the geometry of spacetime, which in turn describes the acceleration of matter. Therefore, solutions of the Einstein field equations describe the evolution of the Universe. Combined with measurements of the amount, type, and distribution of matter in the Universe, the equations of general relativity describe the evolution of the Universe over time.

With the assumption of the cosmological principle that the Universe is homogeneous and isotropic everywhere, a specific solution of the field equations that describes the Universe is the metric tensor called the Friedmann–Lemaître–Robertson–Walker metric,
where ("r", θ, φ) correspond to a spherical coordinate system. This metric has only two undetermined parameters. An overall dimensionless length scale factor "R" describes the size scale of the Universe as a function of time; an increase in "R" is the expansion of the Universe. A curvature index "k" describes the geometry. The index "k" is defined so that it can take only one of three values: 0, corresponding to flat Euclidean geometry; 1, corresponding to a space of positive curvature; or −1, corresponding to a space of positive or negative curvature. The value of "R" as a function of time "t" depends upon "k" and the cosmological constant "Λ". The cosmological constant represents the energy density of the vacuum of space and could be related to dark energy. The equation describing how "R" varies with time is known as the Friedmann equation after its inventor, Alexander Friedmann.

The solutions for "R(t)" depend on "k" and "Λ", but some qualitative features of such solutions are general. First and most importantly, the length scale "R" of the Universe can remain constant "only" if the Universe is perfectly isotropic with positive curvature ("k"=1) and has one precise value of density everywhere, as first noted by Albert Einstein. However, this equilibrium is unstable: because the Universe is known to be inhomogeneous on smaller scales, "R" must change over time. When "R" changes, all the spatial distances in the Universe change in tandem; there is an overall expansion or contraction of space itself. This accounts for the observation that galaxies appear to be flying apart; the space between them is stretching. The stretching of space also accounts for the apparent paradox that two galaxies can be 40 billion light-years apart, although they started from the same point 13.8 billion years ago and never moved faster than the speed of light.

Second, all solutions suggest that there was a gravitational singularity in the past, when "R" went to zero and matter and energy were infinitely dense. It may seem that this conclusion is uncertain because it is based on the questionable assumptions of perfect homogeneity and isotropy (the cosmological principle) and that only the gravitational interaction is significant. However, the Penrose–Hawking singularity theorems show that a singularity should exist for very general conditions. Hence, according to Einstein's field equations, "R" grew rapidly from an unimaginably hot, dense state that existed immediately following this singularity (when "R" had a small, finite value); this is the essence of the Big Bang model of the Universe. Understanding the singularity of the Big Bang likely requires a quantum theory of gravity, which has not yet been formulated.

Third, the curvature index "k" determines the sign of the mean spatial curvature of spacetime averaged over sufficiently large length scales (greater than about a billion light-years). If "k"=1, the curvature is positive and the Universe has a finite volume. A Universe with positive curvature is often visualized as a three-dimensional sphere embedded in a four-dimensional space. Conversely, if "k" is zero or negative, the Universe has an infinite volume. It may seem counter-intuitive that an infinite and yet infinitely dense Universe could be created in a single instant at the Big Bang when "R"=0, but exactly that is predicted mathematically when "k" does not equal 1. By analogy, an infinite plane has zero curvature but infinite area, whereas an infinite cylinder is finite in one direction and a torus is finite in both. A toroidal Universe could behave like a normal Universe with periodic boundary conditions.

The ultimate fate of the Universe is still unknown, because it depends critically on the curvature index "k" and the cosmological constant "Λ". If the Universe were sufficiently dense, "k" would equal +1, meaning that its average curvature throughout is positive and the Universe will eventually recollapse in a Big Crunch, possibly starting a new Universe in a Big Bounce. Conversely, if the Universe were insufficiently dense, "k" would equal 0 or −1 and the Universe would expand forever, cooling off and eventually reaching the Big Freeze and the heat death of the Universe. Modern data suggests that the rate of expansion of the Universe is not decreasing, as originally expected, but increasing; if this continues indefinitely, the Universe may eventually reach a Big Rip. Observationally, the Universe appears to be flat ("k" = 0), with an overall density that is very close to the critical value between recollapse and eternal expansion.

Some speculative theories have proposed that our Universe is but one of a set of disconnected universes, collectively denoted as the multiverse, challenging or enhancing more limited definitions of the Universe. Scientific multiverse models are distinct from concepts such as alternate planes of consciousness and simulated reality.

Max Tegmark developed a four-part classification scheme for the different types of multiverses that scientists have suggested in response to various Physics problems. An example of such multiverses is the one resulting from the chaotic inflation model of the early universe. Another is the multiverse resulting from the many-worlds interpretation of quantum mechanics. In this interpretation, parallel worlds are generated in a manner similar to quantum superposition and decoherence, with all states of the wave functions being realized in separate worlds. Effectively, in the many-worlds interpretation the multiverse evolves as a universal wavefunction. If the Big Bang that created our multiverse created an ensemble of multiverses, the wave function of the ensemble would be entangled in this sense.

The least controversial category of multiverse in Tegmark's scheme is . The multiverses of this level are composed by distant spacetime events "in our own universe". If space is infinite, or sufficiently large and uniform, identical instances of the history of Earth's entire Hubble volume occur every so often, simply by chance. Tegmark calculated that our nearest so-called doppelgänger, is 10 meters away from us (a double exponential function larger than a googolplex). In principle, it would be impossible to scientifically verify the existence of an identical Hubble volume. However, this existence does follow as a fairly straightforward consequence 

It is possible to conceive of disconnected spacetimes, each existing but unable to interact with one another. An easily visualized metaphor of this concept is a group of separate soap bubbles, in which observers living on one soap bubble cannot interact with those on other soap bubbles, even in principle. According to one common terminology, each "soap bubble" of spacetime is denoted as a "universe", whereas our particular spacetime is denoted as "the Universe", just as we call our moon "the Moon". The entire collection of these separate spacetimes is denoted as the multiverse. With this terminology, different "Universes" are not causally connected to each other. In principle, the other unconnected "Universes" may have different dimensionalities and topologies of spacetime, different forms of matter and energy, and different physical laws and physical constants, although such possibilities are purely speculative. Others consider each of several bubbles created as part of chaotic inflation to be separate "Universes", though in this model these universes all share a causal origin.

Historically, there have been many ideas of the cosmos (cosmologies) and its origin (cosmogonies). Theories of an impersonal Universe governed by physical laws were first proposed by the Greeks and Indians. Ancient Chinese philosophy encompassed the notion of the Universe including both all of space and all of time. Over the centuries, improvements in astronomical observations and theories of motion and gravitation led to ever more accurate descriptions of the Universe. The modern era of cosmology began with Albert Einstein's 1915 general theory of relativity, which made it possible to quantitatively predict the origin, evolution, and conclusion of the Universe as a whole. Most modern, accepted theories of cosmology are based on general relativity and, more specifically, the predicted Big Bang.

Many cultures have stories describing the origin of the world and universe. Cultures generally regard these stories as having some truth. There are however many differing beliefs in how these stories apply amongst those believing in a supernatural origin, ranging from a god directly creating the Universe as it is now to a god just setting the "wheels in motion" (for example via mechanisms such as the big bang and evolution).

Ethnologists and anthropologists who study myths have developed various classification schemes for the various themes that appear in creation stories. For example, in one type of story, the world is born from a world egg; such stories include the Finnish epic poem "Kalevala", the Chinese story of Pangu or the Indian Brahmanda Purana. In related stories, the Universe is created by a single entity emanating or producing something by him- or herself, as in the Tibetan Buddhism concept of Adi-Buddha, the ancient Greek story of Gaia (Mother Earth), the Aztec goddess Coatlicue myth, the ancient Egyptian god Atum story, and the Judeo-Christian Genesis creation narrative in which the Abrahamic God created the Universe. In another type of story, the Universe is created from the union of male and female deities, as in the Maori story of Rangi and Papa. In other stories, the Universe is created by crafting it from pre-existing materials, such as the corpse of a dead god — as from Tiamat in the Babylonian epic "Enuma Elish" or from the giant Ymir in Norse mythology – or from chaotic materials, as in Izanagi and Izanami in Japanese mythology. In other stories, the Universe emanates from fundamental principles, such as Brahman and Prakrti, the creation myth of the Serers, or the yin and yang of the Tao.

The pre-Socratic Greek philosophers and Indian philosophers developed some of the earliest philosophical concepts of the Universe. The earliest Greek philosophers noted that appearances can be deceiving, and sought to understand the underlying reality behind the appearances. In particular, they noted the ability of matter to change forms (e.g., ice to water to steam) and several philosophers proposed that all the physical materials in the world are different forms of a single primordial material, or "arche". The first to do so was Thales, who proposed this material to be water. Thales' student, Anaximander, proposed that everything came from the limitless "apeiron". Anaximenes proposed the primordial material to be air on account of its perceived attractive and repulsive qualities that cause the "arche" to condense or dissociate into different forms. Anaxagoras proposed the principle of "Nous" (Mind), while Heraclitus proposed fire (and spoke of "logos"). Empedocles proposed the elements to be earth, water, air and fire. His four-element model became very popular. Like Pythagoras, Plato believed that all things were composed of number, with Empedocles' elements taking the form of the Platonic solids. Democritus, and later philosophers—most notably Leucippus—proposed that the Universe is composed of indivisible atoms moving through a void (vacuum), although Aristotle did not believe that to be feasible because air, like water, offers resistance to motion. Air will immediately rush in to fill a void, and moreover, without resistance, it would do so indefinitely fast.

Although Heraclitus argued for eternal change, his contemporary Parmenides made the radical suggestion that all change is an illusion, that the true underlying reality is eternally unchanging and of a single nature. Parmenides denoted this reality as (The One). Parmenides' idea seemed implausible to many Greeks, but his student Zeno of Elea challenged them with several famous paradoxes. Aristotle responded to these paradoxes by developing the notion of a potential countable infinity, as well as the infinitely divisible continuum. Unlike the eternal and unchanging cycles of time, he believed that the world is bounded by the celestial spheres and that cumulative stellar magnitude is only finitely multiplicative.

The Indian philosopher Kanada, founder of the Vaisheshika school, developed a notion of atomism and proposed that light and heat were varieties of the same substance. In the 5th century AD, the Buddhist atomist philosopher Dignāga proposed atoms to be point-sized, durationless, and made of energy. They denied the existence of substantial matter and proposed that movement consisted of momentary flashes of a stream of energy.

The notion of temporal finitism was inspired by the doctrine of creation shared by the three Abrahamic religions: Judaism, Christianity and Islam. The Christian philosopher, John Philoponus, presented the philosophical arguments against the ancient Greek notion of an infinite past and future. Philoponus' arguments against an infinite past were used by the early Muslim philosopher, Al-Kindi (Alkindus); the Jewish philosopher, Saadia Gaon (Saadia ben Joseph); and the Muslim theologian, Al-Ghazali (Algazel).

Astronomical models of the Universe were proposed soon after astronomy began with the Babylonian astronomers, who viewed the Universe as a flat disk floating in the ocean, and this forms the premise for early Greek maps like those of Anaximander and Hecataeus of Miletus.

Later Greek philosophers, observing the motions of the heavenly bodies, were concerned with developing models of the Universe-based more profoundly on empirical evidence. The first coherent model was proposed by Eudoxus of Cnidos. According to Aristotle's physical interpretation of the model, celestial spheres eternally rotate with uniform motion around a stationary Earth. Normal matter is entirely contained within the terrestrial sphere.

"De Mundo" (composed before 250 BC or between 350 and 200 BC), stated, "Five elements, situated in spheres in five regions, the less being in each case surrounded by the greater—namely, earth surrounded by water, water by air, air by fire, and fire by ether—make up the whole Universe".

This model was also refined by Callippus and after concentric spheres were abandoned, it was brought into nearly perfect agreement with astronomical observations by Ptolemy. The success of such a model is largely due to the mathematical fact that any function (such as the position of a planet) can be decomposed into a set of circular functions (the Fourier modes). Other Greek scientists, such as the Pythagorean philosopher Philolaus, postulated (according to Stobaeus account) that at the center of the Universe was a "central fire" around which the Earth, Sun, Moon and Planets revolved in uniform circular motion.

The Greek astronomer Aristarchus of Samos was the first known individual to propose a heliocentric model of the Universe. Though the original text has been lost, a reference in Archimedes' book "The Sand Reckoner" describes Aristarchus's heliocentric model. Archimedes wrote:

You, King Gelon, are aware the Universe is the name given by most astronomers to the sphere the center of which is the center of the Earth, while its radius is equal to the straight line between the center of the Sun and the center of the Earth. This is the common account as you have heard from astronomers. But Aristarchus has brought out a book consisting of certain hypotheses, wherein it appears, as a consequence of the assumptions made, that the Universe is many times greater than the Universe just mentioned. His hypotheses are that the fixed stars and the Sun remain unmoved, that the Earth revolves about the Sun on the circumference of a circle, the Sun lying in the middle of the orbit, and that the sphere of fixed stars, situated about the same center as the Sun, is so great that the circle in which he supposes the Earth to revolve bears such a proportion to the distance of the fixed stars as the center of the sphere bears to its surface

Aristarchus thus believed the stars to be very far away, and saw this as the reason why stellar parallax had not been observed, that is, the stars had not been observed to move relative each other as the Earth moved around the Sun. The stars are in fact much farther away than the distance that was generally assumed in ancient times, which is why stellar parallax is only detectable with precision instruments. The geocentric model, consistent with planetary parallax, was assumed to be an explanation for the unobservability of the parallel phenomenon, stellar parallax. The rejection of the heliocentric view was apparently quite strong, as the following passage from Plutarch suggests ("On the Apparent Face in the Orb of the Moon"):

Cleanthes [a contemporary of Aristarchus and head of the Stoics] thought it was the duty of the Greeks to indict Aristarchus of Samos on the charge of impiety for putting in motion the Hearth of the Universe [i.e. the Earth], ... supposing the heaven to remain at rest and the Earth to revolve in an oblique circle, while it rotates, at the same time, about its own axis

The only other astronomer from antiquity known by name who supported Aristarchus's heliocentric model was Seleucus of Seleucia, a Hellenistic astronomer who lived a century after Aristarchus. According to Plutarch, Seleucus was the first to prove the heliocentric system through reasoning, but it is not known what arguments he used. Seleucus' arguments for a heliocentric cosmology were probably related to the phenomenon of tides. According to Strabo (1.1.9), Seleucus was the first to state that the tides are due to the attraction of the Moon, and that the height of the tides depends on the Moon's position relative to the Sun. Alternatively, he may have proved heliocentricity by determining the constants of a geometric model for it, and by developing methods to compute planetary positions using this model, like what Nicolaus Copernicus later did in the 16th century. During the Middle Ages, heliocentric models were also proposed by the Indian astronomer Aryabhata, and by the Persian astronomers Albumasar and Al-Sijzi.

The Aristotelian model was accepted in the Western world for roughly two millennia, until Copernicus revived Aristarchus's perspective that the astronomical data could be explained more plausibly if the Earth rotated on its axis and if the Sun were placed at the center of the Universe.

As noted by Copernicus himself, the notion that the Earth rotates is very old, dating at least to Philolaus (c. 450 BC), Heraclides Ponticus (c. 350 BC) and Ecphantus the Pythagorean. Roughly a century before Copernicus, the Christian scholar Nicholas of Cusa also proposed that the Earth rotates on its axis in his book, "On Learned Ignorance" (1440). Al-Sijzi also proposed that the Earth rotates on its axis. Empirical evidence for the Earth's rotation on its axis, using the phenomenon of comets, was given by Tusi (1201–1274) and Ali Qushji (1403–1474).

This cosmology was accepted by Isaac Newton, Christiaan Huygens and later scientists. Edmund Halley (1720) and Jean-Philippe de Chéseaux (1744) noted independently that the assumption of an infinite space filled uniformly with stars would lead to the prediction that the nighttime sky would be as bright as the Sun itself; this became known as Olbers' paradox in the 19th century. Newton believed that an infinite space uniformly filled with matter would cause infinite forces and instabilities causing the matter to be crushed inwards under its own gravity. This instability was clarified in 1902 by the Jeans instability criterion. One solution to these paradoxes is the Charlier Universe, in which the matter is arranged hierarchically (systems of orbiting bodies that are themselves orbiting in a larger system, "ad infinitum") in a fractal way such that the Universe has a negligibly small overall density; such a cosmological model had also been proposed earlier in 1761 by Johann Heinrich Lambert. A significant astronomical advance of the 18th century was the realization by Thomas Wright, Immanuel Kant and others of nebulae.

In 1919, when Hooker Telescope was completed, the prevailing view still was that the Universe consisted entirely of the Milky Way Galaxy. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that Universe consists of multitude of galaxies.

The modern era of physical cosmology began in 1917, when Albert Einstein first applied his general theory of relativity to model the structure and dynamics of the Universe.




</doc>
<doc id="177602" url="https://en.wikipedia.org/wiki?curid=177602" title="Outer space">
Outer space

Outer space, or just space, is the expanse that exists beyond the Earth and outside of any astronomical object. Outer space is not completely empty—it is a hard vacuum containing a low density of particles, predominantly a plasma of hydrogen and helium as well as electromagnetic radiation, magnetic fields, neutrinos, dust, and cosmic rays. The baseline temperature, as set by the background radiation from the Big Bang, is . The plasma between galaxies accounts for about half of the baryonic (ordinary) matter in the universe; it has a number density of less than one hydrogen atom per cubic metre and a temperature of millions of kelvins; local concentrations of this plasma have condensed into stars and galaxies. Studies indicate that 90% of the mass in most galaxies is in an unknown form, called dark matter, which interacts with other matter through gravitational but not electromagnetic forces. Observations suggest that the majority of the mass-energy in the observable universe is a poorly understood vacuum energy of space, which astronomers label "dark energy". Intergalactic space takes up most of the volume of the universe, but even galaxies and star systems consist almost entirely of empty space.

Outer space does not begin at a definite altitude above the Earth's surface. However, the Kármán line, an altitude of above sea level, is conventionally used as the start of outer space in space treaties and for aerospace records keeping. The framework for international space law was established by the Outer Space Treaty, which entered into force on 10 October 1967. This treaty precludes any claims of national sovereignty and permits all states to freely explore outer space. Despite the drafting of UN resolutions for the peaceful uses of outer space, anti-satellite weapons have been tested in Earth orbit.

Humans began the physical exploration of space during the 20th century with the advent of high-altitude balloon flights, followed by manned rocket launches. Earth orbit was first achieved by Yuri Gagarin of the Soviet Union in 1961, and unmanned spacecraft have since reached all of the known planets in the Solar System. Due to the high cost of getting into space, manned spaceflight has been limited to low Earth orbit and the Moon.

Outer space represents a challenging environment for human exploration because of the hazards of vacuum and radiation. Microgravity also has a negative effect on human physiology that causes both muscle atrophy and bone loss. In addition to these health and environmental issues, the economic cost of putting objects, including humans, into space is very high.

In 350 BCE, Greek philosopher Aristotle suggested that "nature abhors a vacuum", a principle that became known as the "horror vacui". This concept built upon a 5th-century BCE ontological argument by the Greek philosopher Parmenides, who denied the possible existence of a void in space. Based on this idea that a vacuum could not exist, in the West it was widely held for many centuries that space could not be empty. As late as the 17th century, the French philosopher René Descartes argued that the entirety of space must be filled.

In ancient China, the 2nd-century astronomer Zhang Heng became convinced that space must be infinite, extending well beyond the mechanism that supported the Sun and the stars. The surviving books of the Hsüan Yeh school said that the heavens were boundless, "empty and void of substance". Likewise, the "sun, moon, and the company of stars float in the empty space, moving or standing still".

The Italian scientist Galileo Galilei knew that air had mass and so was subject to gravity. In 1640, he demonstrated that an established force resisted the formation of a vacuum. However, it would remain for his pupil Evangelista Torricelli to create an apparatus that would produce a partial vacuum in 1643. This experiment resulted in the first mercury barometer and created a scientific sensation in Europe. The French mathematician Blaise Pascal reasoned that if the column of mercury was supported by air, then the column ought to be shorter at higher altitude where the air pressure is lower. In 1648, his brother-in-law, Florin Périer, repeated the experiment on the Puy de Dôme mountain in central France and found that the column was shorter by three inches. This decrease in pressure was further demonstrated by carrying a half-full balloon up a mountain and watching it gradually expand, then contract upon descent.

In 1650, German scientist Otto von Guericke constructed the first vacuum pump: a device that would further refute the principle of "horror vacui". He correctly noted that the atmosphere of the Earth surrounds the planet like a shell, with the density gradually declining with altitude. He concluded that there must be a vacuum between the Earth and the Moon.

Back in the 15th century, German theologian Nicolaus Cusanus speculated that the Universe lacked a center and a circumference. He believed that the Universe, while not infinite, could not be held as finite as it lacked any bounds within which it could be contained. These ideas led to speculations as to the infinite dimension of space by the Italian philosopher Giordano Bruno in the 16th century. He extended the Copernican heliocentric cosmology to the concept of an infinite Universe filled with a substance he called aether, which did not resist the motion of heavenly bodies. English philosopher William Gilbert arrived at a similar conclusion, arguing that the stars are visible to us only because they are surrounded by a thin aether or a void. This concept of an aether originated with ancient Greek philosophers, including Aristotle, who conceived of it as the medium through which the heavenly bodies move.

The concept of a Universe filled with a luminiferous aether retained support among some scientists until the early 20th century. This form of aether was viewed as the medium through which light could propagate. In 1887, the Michelson–Morley experiment tried to detect the Earth's motion through this medium by looking for changes in the speed of light depending on the direction of the planet's motion. However, the null result indicated something was wrong with the concept. The idea of the luminiferous aether was then abandoned. It was replaced by Albert Einstein's theory of special relativity, which holds that the speed of light in a vacuum is a fixed constant, independent of the observer's motion or frame of reference.

The first professional astronomer to support the concept of an infinite Universe was the Englishman Thomas Digges in 1576. But the scale of the Universe remained unknown until the first successful measurement of the distance to a nearby star in 1838 by the German astronomer Friedrich Bessel. He showed that the star 61 Cygni had a parallax of just 0.31 arcseconds (compared to the modern value of 0.287″). This corresponds to a distance of over 10 light years. In 1917, Heber Curtis noted that novae in spiral nebulae were, on average, 10 magnitudes fainter than galactic novae, suggesting that the former are 100 times further away. The distance to the Andromeda Galaxy was determined in 1923 by American astronomer Edwin Hubble by measuring the brightness of cepheid variables in that galaxy, a new technique discovered by Henrietta Leavitt. This established that the Andromeda galaxy, and by extension all galaxies, lay well outside the Milky Way.

The modern concept of outer space is based on the "Big Bang" cosmology, first proposed in 1931 by the Belgian physicist Georges Lemaître. This theory holds that the universe originated from a very dense form that has since undergone continuous expansion.

The earliest known estimate of the temperature of outer space was by the Swiss physicist Charles É. Guillaume in 1896. Using the estimated radiation of the background stars, he concluded that space must be heated to a temperature of 5–6 K. British physicist Arthur Eddington made a similar calculation to derive a temperature of 3.18 K in 1926. German physicist Erich Regener used the total measured energy of cosmic rays to estimate an intergalactic temperature of 2.8 K in 1933. American physicists Ralph Alpher and Robert Herman predicted 5 K for the temperature of space in 1948, based on the gradual decrease in background energy following the then-new Big Bang theory. The modern measurement of the cosmic microwave background is about 2.7K.

The term "outward space" was used in 1842 by the English poet Lady Emmeline Stuart-Wortley in her poem "The Maiden of Moscow". The expression "outer space" was used as an astronomical term by Alexander von Humboldt in 1845. It was later popularized in the writings of H. G. Wells in 1901. The shorter term "space" is older, first used to mean the region beyond Earth's sky in John Milton's "Paradise Lost" in 1667.

According to the Big Bang theory, the very early Universe was an extremely hot and dense state about 13.8 billion years ago which rapidly expanded. About 380,000 years later the Universe had cooled sufficiently to allow protons and electrons to combine and form hydrogen—the so-called recombination epoch. When this happened, matter and energy became decoupled, allowing photons to travel freely through the continually expanding space. Matter that remained following the initial expansion has since undergone gravitational collapse to create stars, galaxies and other astronomical objects, leaving behind a deep vacuum that forms what is now called outer space. As light has a finite velocity, this theory also constrains the size of the directly observable universe. This leaves open the question as to whether the Universe is finite or infinite.

The present day shape of the universe has been determined from measurements of the cosmic microwave background using satellites like the Wilkinson Microwave Anisotropy Probe. These observations indicate that the spatial geometry of the observable universe is "flat", meaning that photons on parallel paths at one point remain parallel as they travel through space to the limit of the observable universe, except for local gravity. The flat Universe, combined with the measured mass density of the Universe and the accelerating expansion of the Universe, indicates that space has a non-zero vacuum energy, which is called dark energy.

Estimates put the average energy density of the present day Universe at the equivalent of 5.9 protons per cubic meter, including dark energy, dark matter, and baryonic matter (ordinary matter composed of atoms). The atoms account for only 4.6% of the total energy density, or a density of one proton per four cubic meters. The density of the Universe, however, is clearly not uniform; it ranges from relatively high density in galaxies—including very high density in structures within galaxies, such as planets, stars, and black holes—to conditions in vast voids that have much lower density, at least in terms of visible matter. Unlike matter and dark matter, dark energy seems not to be concentrated in galaxies: although dark energy may account for a majority of the mass-energy in the Universe, dark energy's influence is 5 orders of magnitude smaller than the influence of gravity from matter and dark matter within the Milky Way.

Outer space is the closest known approximation to a perfect vacuum. It has effectively no friction, allowing stars, planets, and moons to move freely along their ideal orbits, following the initial formation stage. However, even the deep vacuum of intergalactic space is not devoid of matter, as it contains a few hydrogen atoms per cubic meter. By comparison, the air humans breathe contains about 10 molecules per cubic meter. The low density of matter in outer space means that electromagnetic radiation can travel great distances without being scattered: the mean free path of a photon in intergalactic space is about 10 km, or 10 billion light years. In spite of this, extinction, which is the absorption and scattering of photons by dust and gas, is an important factor in galactic and intergalactic astronomy.

Stars, planets, and moons retain their atmospheres by gravitational attraction. Atmospheres have no clearly delineated upper boundary: the density of atmospheric gas gradually decreases with distance from the object until it becomes indistinguishable from outer space. The Earth's atmospheric pressure drops to about Pa at of altitude, compared to 100,000 Pa for the International Union of Pure and Applied Chemistry (IUPAC) definition of standard pressure. Above this altitude, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar wind. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather.

The temperature of outer space is measured in terms of the kinetic activity of the gas, as it is on Earth. However, the radiation of outer space has a different temperature than the kinetic temperature of the gas, meaning that the gas and radiation are not in thermodynamic equilibrium. All of the observable universe is filled with photons that were created during the Big Bang, which is known as the cosmic microwave background radiation (CMB). (There is quite likely a correspondingly large number of neutrinos called the cosmic neutrino background.) The current black body temperature of the background radiation is about . The gas temperatures in outer space are always at least the temperature of the CMB but can be much higher. For example, the corona of the Sun reaches temperatures over 1.2–2.6 million K.

Magnetic fields have been detected in the space around just about every class of celestial object. Star formation in spiral galaxies can generate small-scale dynamos, creating turbulent magnetic field strengths of around 5–10 μG. The Davis–Greenstein effect causes elongated dust grains to align themselves with a galaxy's magnetic field, resulting in weak optical polarization. This has been used to show ordered magnetic fields exist in several nearby galaxies. Magneto-hydrodynamic processes in active elliptical galaxies produce their characteristic jets and radio lobes. Non-thermal radio sources have been detected even among the most distant, high-z sources, indicating the presence of magnetic fields.

Outside a protective atmosphere and magnetic field, there are few obstacles to the passage through space of energetic subatomic particles known as cosmic rays. These particles have energies ranging from about 10 eV up to an extreme 10 eV of ultra-high-energy cosmic rays. The peak flux of cosmic rays occurs at energies of about 10 eV, with approximately 87% protons, 12% helium nuclei and 1% heavier nuclei. In the high energy range, the flux of electrons is only about 1% of that of protons. Cosmic rays can damage electronic components and pose a health threat to space travelers. According to astronauts, like Don Pettit, space has a burned/metallic odor that clings to their suits and equipment, similar to the scent of an arc welding torch.

Despite the harsh environment, several life forms have been found that can withstand extreme space conditions for extended periods. Species of lichen carried on the ESA BIOPAN facility survived exposure for ten days in 2007. Seeds of "Arabidopsis thaliana" and "Nicotiana tabacum" germinated after being exposed to space for 1.5 years. A strain of "bacillus subtilis" has survived 559 days when exposed to low-Earth orbit or a simulated martian environment. The lithopanspermia hypothesis suggests that rocks ejected into outer space from life-harboring planets may successfully transport life forms to another habitable world. A conjecture is that just such a scenario occurred early in the history of the Solar System, with potentially microorganism-bearing rocks being exchanged between Venus, Earth, and Mars.

Even at relatively low altitudes in the Earth's atmosphere, conditions are hostile to the human body. The altitude where atmospheric pressure matches the vapor pressure of water at the temperature of the human body is called the Armstrong line, named after American physician Harry G. Armstrong. It is located at an altitude of around . At or above the Armstrong line, fluids in the throat and lungs boil away. More specifically, exposed bodily liquids such as saliva, tears, and liquids in the lungs boil away. Hence, at this altitude, human survival requires a pressure suit, or a pressurized capsule.

Once in space, sudden exposure of unprotected humans to very low pressure, such as during a rapid decompression, can cause pulmonary barotrauma—a rupture of the lungs, due to the large pressure differential between inside and outside the chest. Even if the subject's airway is fully open, the flow of air through the windpipe may be too slow to prevent the rupture. Rapid decompression can rupture eardrums and sinuses, bruising and blood seep can occur in soft tissues, and shock can cause an increase in oxygen consumption that leads to hypoxia.

As a consequence of rapid decompression, oxygen dissolved in the blood empties into the lungs to try to equalize the partial pressure gradient. Once the deoxygenated blood arrives at the brain, humans lose consciousness after a few seconds and die of hypoxia within minutes. Blood and other body fluids boil when the pressure drops below 6.3 kPa, and this condition is called ebullism. The steam may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Ebullism is slowed by the pressure containment of blood vessels, so some blood remains liquid. Swelling and ebullism can be reduced by containment in a pressure suit. The Crew Altitude Protection Suit (CAPS), a fitted elastic garment designed in the 1960s for astronauts, prevents ebullism at pressures as low as 2 kPa. Supplemental oxygen is needed at to provide enough oxygen for breathing and to prevent water loss, while above pressure suits are essential to prevent ebullism. Most space suits use around 30–39 kPa of pure oxygen, about the same as on the Earth's surface. This pressure is high enough to prevent ebullism, but evaporation of nitrogen dissolved in the blood could still cause decompression sickness and gas embolisms if not managed.

Humans evolved for life in Earth gravity, and exposure to weightlessness has been shown to have deleterious effects on human health. Initially, more than 50% of astronauts experience space motion sickness. This can cause nausea and vomiting, vertigo, headaches, lethargy, and overall malaise. The duration of space sickness varies, but it typically lasts for 1–3 days, after which the body adjusts to the new environment. Longer-term exposure to weightlessness results in muscle atrophy and deterioration of the skeleton, or spaceflight osteopenia. These effects can be minimized through a regimen of exercise. Other effects include fluid redistribution, slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, nasal congestion, sleep disturbance, and puffiness of the face.

For long-duration space travel, radiation can pose an acute health hazard.
Exposure to high-energy, ionizing cosmic rays can result in fatigue, nausea, vomiting, as well as damage to the immune system and changes to the white blood cell count. Over longer durations, symptoms include an increased risk of cancer, plus damage to the eyes, nervous system, lungs and the gastrointestinal tract. On a round-trip Mars mission lasting three years, a large fraction of the cells in an astronaut's body would be traversed and potentially damaged by high energy nuclei. The energy of such particles is significantly diminished by the shielding provided by the walls of a spacecraft and can be further diminished by water containers and other barriers. However, the impact of the cosmic rays upon the shielding produces additional radiation that can affect the crew. Further research is needed to assess the radiation hazards and determine suitable countermeasures.

There is no clear boundary between Earth's atmosphere and space, as the density of the atmosphere gradually decreases as the altitude increases. There are several standard boundary designations, namely:

In 2009, scientists reported detailed measurements with a Supra-Thermal Ion Imager (an instrument that measures the direction and speed of ions), which allowed them to establish a boundary at above Earth. The boundary represents the midpoint of a gradual transition over tens of kilometers from the relatively gentle winds of the Earth's atmosphere to the more violent flows of charged particles in space, which can reach speeds well over .

The Outer Space Treaty provides the basic framework for international space law. It covers the legal use of outer space by nation states, and includes in its definition of "outer space" the Moon and other celestial bodies. The treaty states that outer space is free for all nation states to explore and is not subject to claims of national sovereignty. It also prohibits the deployment of nuclear weapons in outer space. The treaty was passed by the United Nations General Assembly in 1963 and signed in 1967 by the USSR, the United States of America and the United Kingdom. As of 2017, 105 state parties have either ratified or acceded to the treaty. An additional 25 states signed the treaty, without ratifying it.

Since 1958, outer space has been the subject of multiple United Nations resolutions. Of these, more than 50 have been concerning the international co-operation in the peaceful uses of outer space and preventing an arms race in space. Four additional space law treaties have been negotiated and drafted by the UN's Committee on the Peaceful Uses of Outer Space. Still, there remains no legal prohibition against deploying conventional weapons in space, and anti-satellite weapons have been successfully tested by the US, USSR and China. The 1979 Moon Treaty turned the jurisdiction of all heavenly bodies (including the orbits around such bodies) over to the international community. However, this treaty has not been ratified by any nation that currently practices manned spaceflight.

In 1976, eight equatorial states (Ecuador, Colombia, Brazil, Congo, Zaire, Uganda, Kenya, and Indonesia) met in Bogotá, Colombia. With their "Declaration of the First Meeting of Equatorial Countries", or "the Bogotá Declaration", they claimed control of the segment of the geosynchronous orbital path corresponding to each country. These claims are not internationally accepted.

A spacecraft enters orbit when its centripetal acceleration due to gravity is less than or equal to the centrifugal acceleration due to the horizontal component of its velocity. For a low Earth orbit, this velocity is about ; by contrast, the fastest manned airplane speed ever achieved (excluding speeds achieved by deorbiting spacecraft) was in 1967 by the North American X-15.

To achieve an orbit, a spacecraft must travel faster than a sub-orbital spaceflight. The energy required to reach Earth orbital velocity at an altitude of is about 36 MJ/kg, which is six times the energy needed merely to climb to the corresponding altitude. Spacecraft with a perigee below about are subject to drag from the Earth's atmosphere, which decreases the orbital altitude. The rate of orbital decay depends on the satellite's cross-sectional area and mass, as well as variations in the air density of the upper atmosphere. Below about , decay becomes more rapid with lifetimes measured in days. Once a satellite descends to , it has only hours before it vaporizes in the atmosphere. The escape velocity required to pull free of Earth's gravitational field altogether and move into interplanetary space is about .

Space is a partial vacuum: its different regions are defined by the various atmospheres and "winds" that dominate within them, and extend to the point at which those winds give way to those beyond. Geospace extends from Earth's atmosphere to the outer reaches of Earth's magnetic field, whereupon it gives way to the solar wind of interplanetary space. Interplanetary space extends to the heliopause, whereupon the solar wind gives way to the winds of the interstellar medium. Interstellar space then continues to the edges of the galaxy, where it fades into the intergalactic void.

Geospace is the region of outer space near Earth, including the upper atmosphere and magnetosphere. The Van Allen radiation belts lie within the geospace. The outer boundary of geospace is the magnetopause, which forms an interface between the Earth's magnetosphere and the solar wind. The inner boundary is the ionosphere. The variable space-weather conditions of geospace are affected by the behavior of the Sun and the solar wind; the subject of geospace is interlinked with heliophysics—the study of the Sun and its impact on the planets of the Solar System.

The day-side magnetopause is compressed by solar-wind pressure—the subsolar distance from the center of the Earth is typically 10 Earth radii. On the night side, the solar wind stretches the magnetosphere to form a magnetotail that sometimes extends out to more than 100–200 Earth radii. For roughly four days of each month, the lunar surface is shielded from the solar wind as the Moon passes through the magnetotail.

Geospace is populated by electrically charged particles at very low densities, the motions of which are controlled by the Earth's magnetic field. These plasmas form a medium from which storm-like disturbances powered by the solar wind can drive electrical currents into the Earth's upper atmosphere. Geomagnetic storms can disturb two regions of geospace, the radiation belts and the ionosphere. These storms increase fluxes of energetic electrons that can permanently damage satellite electronics, interfering with shortwave radio communication and GPS location and timing. Magnetic storms can also be a hazard to astronauts, even in low Earth orbit. They also create aurorae seen at high latitudes in an oval surrounding the geomagnetic poles.

Although it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. This region contains material left over from previous manned and unmanned launches that are a potential hazard to spacecraft. Some of this debris re-enters Earth's atmosphere periodically.

Earth's gravity keeps the Moon in orbit at an average distance of . The region outside Earth's atmosphere and extending out to just beyond the Moon's orbit, including the Lagrangian points, is sometimes referred to as cislunar space.

The region of space where Earth's gravity remains dominant against gravitational perturbations from the Sun is called the Hill sphere. This extends well out into translunar space to a distance of roughly 1% of the mean distance from Earth to the Sun, or .

Deep space has different definitions as to where it starts. It has been defined by the United States government and others as any region beyond cislunar space. The International Telecommunication Union responsible for radio communication (including satellites) defines the beginning of deep space at about 5 times that distance ().

Interplanetary space is defined by the solar wind, a continuous stream of charged particles emanating from the Sun that creates a very tenuous atmosphere (the heliosphere) for billions of kilometers into space. This wind has a particle density of 5–10 protons/cm and is moving at a velocity of . Interplanetary space extends out to the heliopause where the influence of the galactic environment starts to dominate over the magnetic field and particle flux from the Sun. The distance and strength of the heliopause varies depending on the activity level of the solar wind. The heliopause in turn deflects away low-energy galactic cosmic rays, with this modulation effect peaking during solar maximum.

The volume of interplanetary space is a nearly total vacuum, with a mean free path of about one astronomical unit at the orbital distance of the Earth. However, this space is not completely empty, and is sparsely filled with cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also gas, plasma and dust, small meteors, and several dozen types of organic molecules discovered to date by microwave spectroscopy. A cloud of interplanetary dust is visible at night as a faint band called the zodiacal light.

Interplanetary space contains the magnetic field generated by the Sun. There are also magnetospheres generated by planets such as Jupiter, Saturn, Mercury and the Earth that have their own magnetic fields. These are shaped by the influence of the solar wind into the approximation of a teardrop shape, with the long tail extending outward behind the planet. These magnetic fields can trap particles from the solar wind and other sources, creating belts of charged particles such as the Van Allen radiation belts. Planets without magnetic fields, such as Mars, have their atmospheres gradually eroded by the solar wind.

Interstellar space is the physical space within a galaxy beyond the influence each star has upon the encompassed plasma. The contents of interstellar space are called the interstellar medium. Approximately 70% of the mass of the interstellar medium consists of lone hydrogen atoms; most of the remainder consists of helium atoms. This is enriched with trace amounts of heavier atoms formed through stellar nucleosynthesis. These atoms are ejected into the interstellar medium by stellar winds or when evolved stars begin to shed their outer envelopes such as during the formation of a planetary nebula. The cataclysmic explosion of a supernova generates an expanding shock wave consisting of ejected materials that further enrich the medium. The density of matter in the interstellar medium can vary considerably: the average is around 10 particles per m, but cold molecular clouds can hold 10–10 per m.

A number of molecules exist in interstellar space, as can tiny 0.1 μm dust particles. The tally of molecules discovered through radio astronomy is steadily increasing at the rate of about four new species per year. Large regions of higher density matter known as molecular clouds allow chemical reactions to occur, including the formation of organic polyatomic species. Much of this chemistry is driven by collisions. Energetic cosmic rays penetrate the cold, dense clouds and ionize hydrogen and helium, resulting, for example, in the trihydrogen cation. An ionized helium atom can then split relatively abundant carbon monoxide to produce ionized carbon, which in turn can lead to organic chemical reactions.

The local interstellar medium is a region of space within 100 parsecs (pc) of the Sun, which is of interest both for its proximity and for its interaction with the Solar System. This volume nearly coincides with a region of space known as the Local Bubble, which is characterized by a lack of dense, cold clouds. It forms a cavity in the Orion Arm of the Milky Way galaxy, with dense molecular clouds lying along the borders, such as those in the constellations of Ophiuchus and Taurus. (The actual distance to the border of this cavity varies from 60 to 250 pc or more.) This volume contains about 10–10 stars and the local interstellar gas counterbalances the astrospheres that surround these stars, with the volume of each sphere varying depending on the local density of the interstellar medium. The Local Bubble contains dozens of warm interstellar clouds with temperatures of up to 7,000 K and radii of 0.5–5 pc.

When stars are moving at sufficiently high peculiar velocities, their astrospheres can generate bow shocks as they collide with the interstellar medium. For decades it was assumed that the Sun had a bow shock. In 2012, data from Interstellar Boundary Explorer (IBEX) and NASA's Voyager probes showed that the Sun's bow shock does not exist. Instead, these authors argue that a subsonic bow wave defines the transition from the solar wind flow to the interstellar medium. A bow shock is the third boundary of an astrosphere after the termination shock and the astropause (called the heliopause in the Solar System).

Intergalactic space is the physical space between galaxies. Studies of the large scale distribution of galaxies show that the Universe has a foam-like structure, with clusters and groups of galaxies lying along filaments that occupy about a tenth of the total space. The remainder forms huge voids that are mostly empty of galaxies. Typically, a void spans a distance of (10–40) "h" Mpc, where "h" is the Hubble constant in units of .

Surrounding and stretching between galaxies, there is a rarefied plasma that is organized in a galactic filamentary structure. This material is called the intergalactic medium (IGM). The density of the IGM is 5–200 times the average density of the Universe. It consists mostly of ionized hydrogen; i.e. a plasma consisting of equal numbers of electrons and protons. As gas falls into the intergalactic medium from the voids, it heats up to temperatures of 10 K to 10 K, which is high enough so that collisions between atoms have enough energy to cause the bound electrons to escape from the hydrogen nuclei; this is why the IGM is ionized. At these temperatures, it is called the warm–hot intergalactic medium (WHIM). (Although the plasma is very hot by terrestrial standards, 10 K is often called "warm" in astrophysics.) Computer simulations and observations indicate that up to half of the atomic matter in the Universe might exist in this warm–hot, rarefied state. When gas falls from the filamentary structures of the WHIM into the galaxy clusters at the intersections of the cosmic filaments, it can heat up even more, reaching temperatures of 10 K and above in the so-called intracluster medium.

For the majority of human history, space was explored by observations made from the Earth's surface—initially with the unaided eye and then with the telescope. Prior to the advent of reliable rocket technology, the closest that humans had come to reaching outer space was through the use of balloon flights. In 1935, the U.S. "Explorer II" manned balloon flight had reached an altitude of . This was greatly exceeded in 1942 when the third launch of the German A-4 rocket climbed to an altitude of about . In 1957, the unmanned satellite Sputnik 1 was launched by a Russian R-7 rocket, achieving Earth orbit at an altitude of . This was followed by the first human spaceflight in 1961, when Yuri Gagarin was sent into orbit on Vostok 1. The first humans to escape low-Earth orbit were Frank Borman, Jim Lovell and William Anders in 1968 on board the U.S. Apollo 8, which achieved lunar orbit and reached a maximum distance of from the Earth.

The first spacecraft to reach escape velocity was the Soviet Luna 1, which performed a fly-by of the Moon in 1959. In 1961, Venera 1 became the first planetary probe. It revealed the presence of the solar wind and performed the first fly-by of Venus, although contact was lost before reaching Venus. The first successful planetary mission was the 1962 fly-by of Venus by Mariner 2. The first fly-by of Mars was by Mariner 4 in 1964. Since that time, unmanned spacecraft have successfully examined each of the Solar System's planets, as well their moons and many minor planets and comets. They remain a fundamental tool for the exploration of outer space, as well as observation of the Earth. In August 2012, "Voyager 1" became the first man-made object to leave the Solar System and enter interstellar space.

The absence of air makes outer space an ideal location for astronomy at all wavelengths of the electromagnetic spectrum. This is evidenced by the spectacular pictures sent back by the Hubble Space Telescope, allowing light from more than 13 billion years ago—almost to the time of the Big Bang—to be observed. However, not every location in space is ideal for a telescope. The interplanetary zodiacal dust emits a diffuse near-infrared radiation that can mask the emission of faint sources such as extrasolar planets. Moving an infrared telescope out past the dust increases its effectiveness. Likewise, a site like the Daedalus crater on the far side of the Moon could shield a radio telescope from the radio frequency interference that hampers Earth-based observations.

Unmanned spacecraft in Earth orbit are an essential technology of modern civilization. They allow direct monitoring of weather conditions, relay long-range communications like television, provide a means of precise navigation, and allow remote sensing of the Earth. The latter role serves a wide variety of purposes, including tracking soil moisture for agriculture, prediction of water outflow from seasonal snow packs, detection of diseases in plants and trees, and surveillance of military activities.

The deep vacuum of space could make it an attractive environment for certain industrial processes, such as those requiring ultraclean surfaces. However, like asteroid mining, space manufacturing requires significant investment with little prospect of immediate return. An important factor in the total expense is the high cost of placing mass into Earth orbit: $– per kg in inflation-adjusted dollars, according to a 2006 estimate. Proposed concepts for addressing this issue include non-rocket spacelaunch, momentum exchange tethers, and space elevators.

Interstellar travel for a human crew remains at present only a theoretical possibility. The distances to the nearest stars will require new technological developments and the ability to safely sustain crews for journeys lasting several decades. For example, the Daedalus Project study, which proposed a spacecraft powered by the fusion of Deuterium and He, would require 36 years to reach the nearby Alpha Centauri system. Other proposed interstellar propulsion systems include light sails, ramjets, and beam-powered propulsion. More advanced propulsion systems could use antimatter as a fuel, potentially reaching relativistic velocities.




</doc>
<doc id="17554500" url="https://en.wikipedia.org/wiki?curid=17554500" title="Biophysical environment">
Biophysical environment

A biophysical environment is a biotic and abiotic surrounding of an organism or population, and consequently includes the factors that have an influence in their survival, development, and evolution. A biophysical environment can vary in scale from microscopic to global in extent. It can also be subdivided according to its attributes. Examples include the marine environment, the atmospheric environment and the terrestrial environment. The number of biophysical environments is countless, given that each living organism has its own environment.

The term "environment" can refer to a singular global environment in relation to humanity, or a local biophysical environment, e.g. the UK's Environment Agency.

All life that has survived must have adapted to conditions of its environment. Temperature, light, humidity, soil nutrients, etc., all influence any species, within any environment. However life in turn modifies, in various forms, its conditions. Some long term modifications along the history of our planet have been significant, such as the incorporation of oxygen to the atmosphere. This process consisted in the breakdown of carbon dioxide by anaerobic microorganisms that used the carbon in their metabolism and released the oxygen to the atmosphere. This led to the existence of oxygen-based plant and animal life, the great oxygenation event. Other interactions are more immediate and simple, such as the smoothing effect that forests have on the temperature cycle, compared to neighboring unforested areas.

Environmental science is the study of the interactions within the biophysical environment. Part of this scientific discipline is the investigation of the effect of human activity on the environment. Ecology, a sub-discipline of biology and a part of environmental sciences, is often mistaken as a study of human induced effects on the environment. Environmental studies is a broader academic discipline that is the systematic study of interaction of humans with their environment. It is a broad field of study that includes the natural environment, built environments and social environments.

Environmentalism is a broad social and philosophical movement that, in a large part, seeks to minimise and compensate the negative effect of human activity on the biophysical environment. The issues of concern for environmentalists usually relate to the natural environment with the more important ones being climate change, species extinction, pollution, and old growth forest loss.

One of the studies related include employing Geographic Information Science to study the biophysical environment.




</doc>
<doc id="916257" url="https://en.wikipedia.org/wiki?curid=916257" title="Regional policy">
Regional policy

Regional policy aims to improve economic conditions in regions of relative disadvantage, either within a nation or within a supranational grouping such as the European Union.

Although the European Union is one of the richest parts of the world, there are large internal disparities of income and opportunity between its regions. The May 2004 Enlargement, followed by accession of Bulgaria and Romania in January 2007, has widened these gaps. Regional policy transfers resources from richer to poorer regions.

The argument for regional policy is that it is both an instrument of financial solidarity and a powerful force for economic integration.

The major Italian experience of regional policy is the Cassa per il Mezzogiorno, set up in the mid-1950s to foster economic development in southern Italy. Originally intended to last for six months, it survived until 1984.

New roads, irrigation projects and developments in infrastructure were built in an area where local communities had suffered seriously from poverty, de-population and high levels of emigration. Tourism projects attempted to exploit Calabria’s beaches.

UK regional policy was born during the economic depression of the 1930s, when heavy industries in the north were devastated. "Assisted Areas" were established, within which companies could acquire grants or capital allowances – known as Regional Selective Assistance (RSA) – in return for protecting jobs.

The overall pattern of policy changed little in the next forty years. Despite criticism by a 1970s Royal Commission that it was "Empiricism run mad; a game of hit and miss played with more enthusiasm than success", governments of both parties maintained Assisted Areas. Under the 1980s Thatcher government, regional policy was significantly rolled back, with Assisted Areas substantially reduced in size.

The post-1997 Labour administration reorganised regional policy, with RSA replaced by Selective Finance for Investment in England and Scotland. UK policy has been subject to EU regional policy framework, with its strong injunctions against unfair competition (generally meaning state aid).





</doc>
<doc id="2311390" url="https://en.wikipedia.org/wiki?curid=2311390" title="WS-Policy">
WS-Policy

WS-Policy is a specification that allows web services to use XML to advertise their policies (on security, quality of service, etc.) and for web service consumers to specify their policy requirements.

WS-Policy is a W3C recommendation as of September 2007.

WS-Policy represents a set of specifications that describe the capabilities and constraints of the security (and other business) policies on intermediaries and end points (for example, required security tokens, supported encryption algorithms, and privacy rules) and how to associate policies with services and end points.

Assertions can either be requirements put upon a web service or an advertisement of the policies of a web service.

Two "operators" (XML tags) are used to make statements about policy combinations:

Logically, an empty "wsp:All" tag makes no assertions.

If both provider and consumer specify a policy, an effective policy will be computed which usually consists of the intersection of both policies. The new policy contains those assertions made by both sides which do not contradict each other. However, synonymous assertions are considered incompatible by a policy intersection. This can easily be explained by the fact that policy intersection is a syntactic approach, which does not incorporate the semantics of the assertions. Furthermore, it ignores the assertion parameters.

Opposed to what the name might suggest, a policy intersection is (although quite similar) not a set-intersection.




</doc>
<doc id="2149716" url="https://en.wikipedia.org/wiki?curid=2149716" title="Haldane principle">
Haldane principle

In British research policy, the Haldane principle is the idea that decisions about what to spend research funds on should be made by researchers rather than politicians. It is named after Richard Burdon Haldane, who in 1904 and from 1909 to 1918 chaired committees and commissions which recommended this policy.

The 1904 committee recommended the creation of the University Grants Committee which has evolved via the Universities Funding Council into the current higher education funding councils: Research Councils UK, Higher Education Funding Council for England, Scottish Funding Council and Higher Education Funding Council for Wales.

In 1918 Haldane's committee produced the "Haldane Report". The report suggested that research required by government departments could be separated into that required by specific departments and that which was more general. It recommended that departments should oversee the specific research but the general research should be under the control of autonomous Research Councils, which would be free from political and administrative pressures that might discourage research in certain areas. The principle of the autonomy of the research councils is now referred to as the Haldane Principle. The first research council to be created as a result of the Haldane Report was the Medical Research Council.

The principle has remained enshrined in British Government policy, but has been criticised and altered over the years. In 1939 J.D. Bernal argued that social good was more important than researchers' freedom in deciding the direction of research. Solly Zuckerman criticised it in 1971 for its artificial separation of basic and applied science, and the consequent elevation of the status of the former.

A major revision to the application of the Haldane Principle in British research funding came in the early 1970s with the Rothschild Report of 1971, and its implementation which transferred about 25% of the then Research Council funds, and the decisions on the research to be funded with them, back to government departments, a move later undone by Margaret Thatcher's government.

There is currently a debate about the extent to which the principle is still applied in practice.

The Higher Education and Research Act 2017, which merged the research councils and the research part of the Higher Education Funding Council for England into UK Research and Innovation, enacted the Haldane principle as section 103(3): "The “Haldane principle” is the principle that decisions on individual research proposals are best taken following an evaluation of the quality and likely impact of the proposals (such as a peer review process)."





</doc>
<doc id="9970" url="https://en.wikipedia.org/wiki?curid=9970" title="Eightfold path (policy analysis)">
Eightfold path (policy analysis)

The eightfold path is a method of policy analysis assembled by Eugene Bardach, a professor at the Goldman School of Public Policy at the University of California, Berkeley. It is outlined in his book "A Practical Guide for Policy Analysis: The Eightfold Path to More Effective Problem Solving", which is now in its fourth edition. The book is commonly referenced in public policy and public administration scholarship.

Bardach's procedure is as follows:

A possible ninth step, based on Bardach's own writing, might be "repeat steps 1 - 8 as necessary."

The New York taxi driver test is a technique for evaluating the effectiveness of communication between policy makers and analysts. Bardach contends that policy explanations must be clear and down-to-earth enough for a taxi driver to be able to understand the premise during a trip through city streets. The New York taxi driver is presumed to be both a non-specialist and a tough customer.




</doc>
<doc id="12344832" url="https://en.wikipedia.org/wiki?curid=12344832" title="Multifunctionality in agriculture">
Multifunctionality in agriculture

Multifunctionality in agriculture (often simply "multifunctionality") refers to the numerous benefits that agricultural policies may provide for a country or region. Generally speaking, multifunctionality refers to the "non-trade" benefits of agriculture, that is, benefits other than commerce and food production. These include, in the WTO definition of multifunctionality, environmental protection, landscape preservation, rural employment, and food security. These can be broadly classified as benefits to society, culture, a national economy as a whole, national security, and other concerns. For example, in addition to providing food and plant-derived products for the population, agriculture may also provide jobs for rural people and contribute to the viability of the area, create a more stable food supply, and provide other desired environmental and rural outputs.

The numerous externalities, both positive and negative, which are associated with agriculture are important considerations for policy makers. Sometimes current agricultural practices and markets produce too much of an undesired effect or not enough of a desired one. Governments may step in to correct such market failures with policies designed to either encourage or discourage a certain practice. However, individual policies may carry consequences for other policies and for other countries. Such policies are therefore a major topic of discussion in the international community.

Removing protectionist policies on agriculture is one step that may need to be taken for a country to maximize positive externalities, minimize negative ones and make sure that the mixture of outputs derived from agriculture corresponds to the needs of society. However, removing agricultural supports is often cause for consternation among public officials, who may predict the loss of certain positive externalities of the policies already in place. At the same time, officials may fear the implementation of new market protections in other countries which are trying to promote the production of such outputs of agriculture. In such cases, advocates for free trade, such as OECD recommend that countries reduce as much as possible their agricultural protections and institute policies which specifically target the production of the positive non-commodity outputs.

To help countries formulate their agricultural policies, OECD has established a framework for analyzing non-commodity outputs of agricultural activities. When analyzing the multifunctionality of agriculture and the appropriate policies to implement, there are several concepts that need to be considered. The first of these is jointness, or the extent to which the intended agricultural product and the incidental non-commodity outputs of agricultural activity are linked. The production of some non-commodity outputs may be inseparable from agricultural commodity outputs while others may be produced independently of agricultural activity. The goal is to separate agricultural commodities and non-commodity outputs as much as possible. The next issue to be addressed is whether or not the production or non-production of the non-commodity output in question constitutes a market failure. If there is no market failure, there is no need for a policy to correct it. Finally policy makers should examine the characteristics of the output in question since it may have both a degree of market failure and jointness associated with it. After considering the matter from these three perspectives, policy makers may find non-governmental ways of addressing dealing with non-commodity outputs or make changes in their agricultural policies.

In agricultural trade discussions in the WTO, the EU and Japan, among others, argue that multifunctionality justifies continued protection and subsidization of agriculture. The United States and the Cairns Group argue that support of multifunctionality should be specific, targeted, and provided in a non-trade distorting manner.


</doc>
<doc id="13686672" url="https://en.wikipedia.org/wiki?curid=13686672" title="Courtesy resolution">
Courtesy resolution

Courtesy resolution is a non-controversial resolution in the nature of congratulations on the birth of a child, celebration of a wedding anniversary, congratulations of an outstanding citizen achievement or a similar event. It is "a resolution expressing thanks for assistance or commending meritorious accomplishments." An example of a courtesy resolution is the resolution at the end of the political convention thanking everyone for their time.

For a Courtesy Resolution, only the affirmative vote is taken and this is usually a voice vote.


</doc>
<doc id="21301794" url="https://en.wikipedia.org/wiki?curid=21301794" title="Association for Public Policy Analysis and Management">
Association for Public Policy Analysis and Management

The Association for Public Policy Analysis and Management (APPAM) is an American organization whose focus is improving public policy and management by fostering excellence in research, analysis, and education. APPAM founded the "Journal of Policy Analysis and Management" ("JPAM") in 1981. The President is Brendan Sullivan.




</doc>
<doc id="11038718" url="https://en.wikipedia.org/wiki?curid=11038718" title="Health administration">
Health administration

Health administration or healthcare administration is the field relating to leadership, management, and administration of public health systems, health care systems, hospitals, and hospital networks.

Health systems management or health care systems management describes the leadership and general management of hospitals, hospital networks, and/or health care systems. In international use, the term refers to management at all levels. In the United States, management of a single institution (e.g. a hospital) is also referred to as "medical and health services management", "healthcare management", or "health administration".

Health systems management ensures that specific outcomes are attained, that departments within a health facility are running smoothly, that the right people are in the right jobs, that people know what is expected of them, that resources are used efficiently and that all departments are working towards a common goal.

Hospital administrators are individuals or groups of people who act as the central point of control within hospitals. These individuals may be previous or current clinicians, or individuals with other backgrounds. There are two types of administrators, generalists and specialists. Generalists are individuals who are responsible for managing or helping to manage an entire facility. Specialists are individuals who are responsible for the efficient operations of a specific department such as policy analysis, finance, accounting, budgeting, human resources, or marketing.

It was reported in September 2014, that the United States spends roughly $218 billion per year on hospital's administration costs, which is equivalent to 1.43 percent of the total U.S. economy. Hospital administration has grown as a percent of the U.S. economy from .9 percent in 2000 to 1.43 percent in 2012, according to "Health Affairs". In 11 different countries, hospitals allocate approximately 12 percent of their budget toward administrative costs. In the United States, hospitals spend 25 percent on administrative costs.

NCHL competencies that require to engage with credibility, creativity, and motivation in complex and dynamic health care environments. 


Health care management is usually studied through healthcare administration or healthcare management programs in a business school or, in some institutions, in a school of public health.

Although many colleges and universities are offering a bachelor's degree in healthcare administration or human resources, a master's degree is considered the "standard credential" for most health administrators in the United States. Research and academic-based doctorate level degrees, such as the Doctor of Philosophy (PhD) in Health Administration and the Doctor of Health Administration (DHA) degree, prepare health care professionals to turn their clinical or administrative experiences into opportunities to develop new knowledge and practice, teach, shape public policy and/or lead complex organizations. There are multiple recognized degree types that are considered equivalent from the perspective of professional preparation.

The Commission on the Accreditation of Healthcare Management Education (CAHME) is the accrediting body overseeing master's-level programs in the United States and Canada on behalf of the United States Department of Education. It accredits several degree program types, including Master of Hospital Administration (MHA), Master of Health Services Administration (MHSA), Master of Business Administration in Hospital Management (MBA-HM), Master of Health Administration (MHA), Master of Public Health (MPH, MSPH, MSHPM), Master of Science (MS-HSM, MS-HA), and Master of Public Administration (MPA).

Health care management study is a new discipline in Nepal. Pokhara University offers a Hospital Management course. National Open College launched a four-year Bachelor's level (BHCM) course in September 2000 with an enrolment of 40 students, and the next year it also started a one-year postgraduate diploma (PGDHCM) and a two-year master's course (MHCM) in health care management. Nobel College at Sinamangal has also been offering a Bachelor’s level (BHCM) course since 2006. MD Hospital administration (MDHA) and Master in Hospital Management (MHM) are being started from 2013. It is uncertain how many citizens of Nepal are gaining healthcare management qualifications in other countries. There is an absence of professional organization and regulation in the health care management profession in Nepal.

There are a variety of different professional associations related to health systems management, which can be subcategorized as either personal or institutional membership groups. Personal membership groups are joined by individuals, and typically have individual skills and career development as their focus. Larger personal membership groups include the Healthcare Financial Management Association, and the Healthcare Information and Management Systems Society. Institutional membership groups are joined by organizations; whereas they typically focus on organizational effectiveness, and may also include data-sharing agreements and other medical related or administrative practice sharing vehicles for member organizations. Prominent examples include the American Hospital Association and the University Healthsystems Consortium.

Early hospital administrators were called patient directors or superintendents. At the time, many were nurses who had taken on administrative responsibilities. Over half of the members of the American Hospital Association were graduate nurses in 1916. Other superintendents were medical doctors, laymen and members of the clergy. 
In the United States, the first degree granting program in the United States was established at Marquette University in Milwaukee, Wisconsin. By 1927, the first two students received their degrees. The original idea is credited to Father Moulinier, associated with the Catholic Hospital Association. The first modern health systems management program was established in 1934 at the University of Chicago. At the time, programs were completed in two years – one year of formal graduate study and one year of practicing internship. In 1958, the Sloan program at Cornell University began offering a special program requiring two years of formal study, which remains the dominant structure in the United States and Canada today (see also "Academic Preparation").

Health systems management has been described as a "hidden" health profession because of the relatively low-profile role managers take in health systems, in comparison to direct-care professions such as nursing and medicine. However the visibility of the management profession within healthcare has been rising in recent years, due largely to the widespread problems developed countries are having in balancing cost, access, and quality in their hospitals and health systems.





</doc>
<doc id="5509535" url="https://en.wikipedia.org/wiki?curid=5509535" title="Overton window">
Overton window

The Overton window is the range of ideas tolerated in public discourse, also known as the window of discourse. The term is named after political scientist Joseph P. Overton, who claimed that an idea's political viability depends mainly on whether it falls within a range acceptable to the public, rather than on politicians' individual preferences. According to Overton, the window contains the range of policies that a politician can recommend without appearing too extreme to gain or keep public office in the current climate of public opinion.

Overton described a spectrum from "more free" to "less free" with regard to government intervention, oriented vertically on an axis, to avoid comparison with the left-right political spectrum. As the spectrum moves or expands, an idea at a given location may become more or less politically acceptable. Political commentator Joshua Treviño postulated that the degrees of acceptance of public ideas are roughly:

The Overton window is an approach to identifying which ideas define the domain of acceptability within a democracy's possible governmental policies. Proponents of policies outside the window seek to convince or persuade the public in order to move and/or expand the window. Proponents of current policies, or similar ones, within the window seek to convince people that policies outside it should be deemed unacceptable.

After Overton's death, others have examined the concept of adjusting the window by the deliberate promotion of ideas outside of it, or "outer fringe" ideas, with the intention of making less fringe ideas acceptable by comparison. The "door-in-the-face" technique of persuasion is similar.

The idea echoes several earlier expressions, the most recent and similarly academic being Hallin's spheres. In his 1986 book "The Uncensored War", communication scholar Daniel C. Hallin posits three areas of media coverage into which a topic may fall. The areas are diagrammed as concentric circles called spheres. From innermost to outermost they are the sphere of consensus, the sphere of legitimate controversy, and the sphere of deviance. Proposals and positions can be placed at varying degrees of distance from the metaphorical center, and political actors can fight over and help change these positions.

Hallin's theory is developed and applied primarily as a theory that explains varying levels of objectivity in media coverage, but it also accounts for the ongoing contest among media and other political actors about what counts as legitimate disagreement, potentially leading to changes in the boundaries between spheres. As one study that applies Hallin's theory explains, "the borders between the three spheres are dynamic, depending on the political climate and on the editorial line of the various media outlets". In this way, the idea also captures the tug-of-war over the boundaries between normal and deviant political discourse.

An idea similar to the Overton window was expressed by Anthony Trollope in 1868 in his novel "Phineas Finn":

In his "West India Emancipation" speech at Canandaigua, New York, in 1857,
abolitionist leader Frederick Douglass described how public opinion limits the ability of those in power to act with impunity:



</doc>
<doc id="12593785" url="https://en.wikipedia.org/wiki?curid=12593785" title="Culture change">
Culture change

Culture change is a term used in public policy making that emphasizes the influence of cultural capital on individual and community behavior. It has been sometimes called repositioning of culture, which means the reconstruction of the cultural concept of a society. It places stress on the social and cultural capital determinants of decision making and the manner in which these interact with other factors like the availability of information or the financial incentives facing individuals to drive behavior.

These cultural capital influences include the role of parenting, families and close associates; organizations such as schools and workplaces; communities and neighborhoods; and wider social influences such as the media. It is argued that this cultural capital manifests into specific values, attitudes or social norms which in turn guide the behavioral "intentions" that individuals adopt in regard to particular decisions or courses of action. These behavioral intentions interact with other factors driving behavior such as financial incentives, regulation and legislation, or levels of information, to drive actual behavior and ultimately feed back into underlying cultural capital.

In general, cultural stereotypes present great resistance to change and to their own redefinition. Culture, often appears fixed to the observer at any one point in time because cultural mutations occur incrementally. Cultural change is a long-term process. Policymakers need to make a great effort to improve some basics aspects of a society’s cultural traits.

The term is used by Knott et al. of the Prime Minister's Strategy Unit in the publication: "Achieving Culture Change: A Policy Framework" (Knott et al., 2008). The paper sets out how public policy can achieve social and cultural change through 'downstream' interventions including fiscal incentives, legislation, regulation and information provision and also 'upstream' interventions such as parenting, peer and mentoring programs, or development of social and community networks.

The key concepts the paper is based on include:


Knott et al. use examples from a range of policy areas to demonstrate how the culture change framework can be applied to policymaking. For example:






</doc>
<doc id="2842539" url="https://en.wikipedia.org/wiki?curid=2842539" title="Security policy">
Security policy

Security policy is a definition of what it means to "be secure" for a system, organization or other entity. For an organization, it addresses the constraints on behavior of its members as well as constraints imposed on adversaries by mechanisms such as doors, locks, keys and walls. For systems, the security policy addresses constraints on functions and flow among them, constraints on access by external systems and adversaries including programs and access to data by people. 

If it is important to be secure, then it is important to be sure all of the security policy is enforced by mechanisms that are strong enough. There are many organized methodologies and risk assessment strategies to assure completeness of security policies and assure that they are completely enforced. In complex systems, such as information systems, policies can be decomposed into sub-policies to facilitate the allocation of security mechanisms to enforce sub-policies. However, this practice has pitfalls. It is too easy to simply go directly to the sub-policies, which are essentially the rules of operation and dispense with the top level policy. That gives the false sense that the rules of operation address some overall definition of security when they do not. Because it is so difficult to think clearly with completeness about security, rules of operation stated as "sub-policies" with no "super-policy" usually turn out to be rambling rules that fail to enforce anything with completeness. Consequently, a top-level security policy is essential to any serious security scheme and sub-policies and rules of operation are meaningless without it.



</doc>
<doc id="317282" url="https://en.wikipedia.org/wiki?curid=317282" title="Science policy">
Science policy

Science policy is concerned with the allocation of resources for the conduct of science towards the goal of best serving the public interest. Topics include the funding of science, the careers of scientists, and the translation of scientific discoveries into technological innovation to promote commercial product development, competitiveness, economic growth and economic development. Science policy focuses on knowledge production and role of knowledge networks, collaborations and the complex distributions of expertise, equipment and know-how. Understanding the processes and organizational context of generating novel and innovative science and engineering ideas is a core concern of science policy. Science policy topics include weapons development, health care and environmental monitoring.

Science policy thus deals with the entire domain of issues that involve science. A large and complex web of factors influences the development of science and
engineering that includes government science policy makers, private firms (including both national and multi-national firms), social
movements, media, non-governmental organizations, universities, and other research institutions. In addition, science policy is increasingly international as defined by the global operations of firms and research institutions as well as by the collaborative networks of non-governmental
organizations and of the nature of scientific inquiry itself. 

State policy has influenced the funding of public works and science for thousands of years, dating at least from the time of the Mohists, who inspired the study of logic during the period of the Hundred Schools of Thought, and the study of defensive fortifications during the Warring States period in China. General levies of labor and grain were collected to fund great public works in China, including the accumulation of grain for distribution in times of famine, for the building of levees to control flooding by the great rivers of China, for the building of canals and locks to connect rivers of China, some of which flowed in opposite directions to each other, and for the building of bridges across these rivers. These projects required a civil service, the scholars, some of whom demonstrated great mastery of hydraulics.

In Italy, Galileo noted that individual taxation of minute amounts could fund large sums to the State, which could then fund his research on the trajectory of cannonballs, noting that "each individual soldier was being paid from coin collected by a general tax of pennies and farthings, while even a million of gold would not suffice to pay the entire army."

In Great Britain, Lord Chancellor Sir Francis Bacon had a formative effect on science policy with his identification of "experiments of .. light, more penetrating into nature [than what others know]", which today we call the crucial experiment. Governmental approval of the Royal Society recognized a scientific community which exists to this day. British prizes for research spurred the development of an accurate, portable chronometer, which directly enabled reliable navigation and sailing on the high seas, and also funded Babbage's computer.

The professionalization of science, begun in the nineteenth century, was partly enabled by the creation of scientific organizations such as the National Academy of Sciences, the Kaiser Wilhelm Institute, and State funding of universities of their respective nations. In the United States, a member of the National Academy of Sciences can sponsor a Direct Submission for publication in the "Proceedings of the National Academy of Sciences". "PNAS" serves as a channel to recognize research of importance to at least one member of the National Academy of Sciences.

Public policy can directly affect the funding of capital equipment, intellectual infrastructure for industrial research, by providing tax incentives to those organizations who fund research. Vannevar Bush, director of the office of scientific research and development for the U.S. government in July 1945, wrote "Science is a proper concern of government" Vannevar Bush directed the forerunner of the National Science Foundation, and his writings directly inspired researchers to invent the hyperlink and the computer mouse. The DARPA initiative to support computing was the impetus for the Internet Protocol stack. In the same way that scientific consortiums like CERN for high-energy physics have a commitment to public knowledge, access to this public knowledge in physics led directly to CERN's sponsorship of development of the World Wide Web and standard Internet access for all.

The programs that are funded are often divided into four basic categories: basic research, applied research, development, and facilities and equipment. Translational research is a newer concept that seeks to bridge the gap between basic science and practical applications.

Basic science attempts to stimulate breakthroughs. Breakthroughs often lead to an explosion of new technologies and approaches. Once the basic result is developed, it is widely published; however conversion into a practical product is left for the free market. However, many governments have developed risk-taking research and development organizations to take basic theoretical research over the edge into practical engineering. In the U.S., this function is performed by DARPA.

On the other hand, technology development is a policy in which engineering, the application of science, is supported rather than basic science. The emphasis is usually given to projects that increase important strategic or commercial engineering knowledge. The most extreme success story is doubtless the Manhattan Project that developed nuclear weapons. Another remarkable success story was the "X-vehicle" studies that gave the US a lasting lead in aerospace technologies.

These exemplify two disparate approaches: The Manhattan Project was huge, and spent unblinkingly on the most risky alternative approaches. The project members believed that failure would result in their enslavement or destruction by Nazi Germany. Each X-project built an aircraft whose only purpose was to develop a particular technology. The plan was to build a few cheap aircraft of each type, fly a test series, often to the destruction of an aircraft, and never design an aircraft for a practical mission. The only mission was technology development.

A number of high-profile technology developments have failed. The US Space Shuttle failed to meet its cost or flight schedule goals. Most observers explain the project as over constrained: the cost goals too aggressive, the technology and mission too underpowered and undefined.

The Japanese fifth generation computer systems project met every technological goal, but failed to produce commercially important artificial intelligence. Many observers believe that the Japanese tried to force engineering beyond available science by brute investment. Half the amount spent on basic research rather might have produced ten times the result.

Utilitarian policies prioritize scientific projects that significantly reduce suffering for larger numbers of people. This approach would mainly consider the numbers of people that can be helped by a research policy. Research is more likely to be supported when it costs less and has greater benefits. Utilitarian research often pursues incremental improvements rather than dramatic advancements in knowledge, or break-through solutions, which are more commercially viable/feasible.

In contrast, monumental science is a policy in which science is supported for the sake of a greater understanding of the universe, rather than for specific short-term practical goals. This designation covers both large projects, often with large facilities, and smaller research that does not have obvious practical applications and are often overlooked. While these projects may not always have obvious practical outcomes, they provide education of future scientists, and advancement of scientific knowledge of lasting worth about the basic building blocks of science.

Practical outcomes do result from many of these "monumental" science programs. Sometimes these practical outcomes are foreseeable and sometimes they are not. A classic example of a monumental science program focused towards a practical outcome is the Manhattan project. An example of a monumental science program that produces unexpected practical outcome is the laser. Coherent light, the principle behind lasing, was first predicted by Einstein in 1916, but not created until 1954 by Charles H. Townes with the maser. The breakthrough with the maser led to the creation of the laser in 1960 by Theodore Maiman. The delay between the theory of coherent light and the production of the laser was partially due to the assumption that it would be of no practical use.

This policy approach prioritizes efficiently teaching all available science to those who can use it, rather than investing in new science. In particular, the goal is not to "lose" any existing knowledge, and to find new practical ways to apply the available knowledge. The classic success stories of this method occurred in the 19th century U.S. land-grant universities, which established a strong tradition of research in practical agricultural and engineering methods. More recently, the Green Revolution prevented mass famine over the last thirty years. The focus, unsurprisingly, is usually on developing a robust curriculum and inexpensive practical methods to meet local needs.

Most developed countries usually have a specific national body overseeing national science (including technology and innovation) policy. In the case developing countries many follow the same fashion. Many governments of developed countries provide considerable funds (primarily to universities) for scientific research (in fields such as physics and geology) as well as social science research (in fields such as economics and history). Much of this is not intended to provide concrete results that may be commercialisable, although research in scientific fields may lead to results that have such potential. Most university research is aimed at gaining publication in peer reviewed academic journals.

A funding body is an organisation that provides research funding in the form of research grants or scholarships. Research councils are the funding bodies that are government-funded agencies engaged in the support of research in different disciplines and postgraduate funding. Funding from research councils is typically competitive. As a general rule, more funding is available in science and engineering disciplines than in the arts and social sciences.

In Australia, the two main research councils are the Australian Research Council and the National Health and Medical Research Council.

In Canada, the three main research councils ("Tri-Council") are the Social Sciences and Humanities Research Council (SSHRC) the Natural Sciences and Engineering Research Council (NSERC) and the Canadian Institutes of Health Research (CIHR). Additional research funding agencies include the Canada Foundation for Innovation, Genome Canada, Sustainable Development Technology Canada and several Tri-Council supported Networks of Centres of Excellence. 

In Brazil, two important research agencies are the National Council for Scientific and Technological Development (CNPq, Portuguese: Conselho Nacional de Desenvolvimento Científico e Tecnológico), an organization of the Brazilian federal government under the Ministry of Science and Technology, and São Paulo Research Foundation (FAPESP, Portuguese: Fundação de Amparo à Pesquisa do Estado de São Paulo), a public foundation located in the state of São Paulo, Brazil .

The science policy of the European Union is carried out through the European Research Area, which is a system that integrates the scientific resources of member nations and acts as a "common market" for research and innovation purpose. The European Union's executive body, the European Commission, has a Directorate-General for Research; which is responsible for the Union's science policy. In addition, the Joint Research Centre provides independent scientific and technical advice to the European Commission and Member States of the European Union (EU) in support of EU policies. There is also the recently established European Research Council, the first European Union funding body set up to support investigator-driven research.

There are also European science agencies that operate independently of the European Union, such as the European Science Foundation, European Space Agency, and the European Higher Education Area; which are created by the Bologna process.

On science policy and on the European Research Area is grounded the European environmental research and innovation policy, which addresses global challenges of pivotal importance for the well-being of the European citizens within the context of sustainable development and environmental protection. Research and innovation in Europe is financially supported by the programme Horizon 2020, which is also open to participation worldwide.

German research funding agencies include the Deutsche Forschungsgemeinschaft, which covers both science and humanities.

Research funding by the Government of India comes from a number of sources. For basic science and technology research, these include the Council for Scientific and Industrial Research (CSIR), Department of Science and Technology (DST), and University Grants Commission (UGC). For medical research, these include the Indian Council for Medical Research (ICMR), CSIR, DST and Department of Biotechnology (DBT). For applied research, these include the CSIR, DBT and Science and Engineering Research Council (SERC).

Other funding authorities are the Defence Research Development Organisation (DRDO), the Indian Council of Agricultural Research (ICAR), the Indian Space Research Organisation (ISRO), the Department of Ocean Development (DOD), the Indian Council for Social Science Research (ICSSR), and the Ministry of Environment and Forests (MEF) etc.

Irish funding councils include the Irish Research Council (IRC) and the Science Foundation Ireland. The prior Irish Research Council for Science, Engineering and Technology (IRCSET) and the Irish Research Council for the Humanities and Social Sciences (IRCHSS) were merged to form the IRC in March 2012.

Dutch research funding agencies include Nederlandse Organisatie voor Wetenschappelijk Onderzoek (NWO) and Agentschap NL .

The Government of Pakistan has mandated that a certain percentage of gross revenue generated by all telecom service providers be allocated to development and research of information and communication technologies. The National ICT R&D Fund was established in January 2007.

Under the Soviet Union, much research was routinely suppressed.
Now science in Russia is supported by state and private funds. From the state: the Russian Humanitarian Scientific Foundation (http://www.rfh.ru), the Russian Foundation for Basic Research (www.rfbr.ru), the Russian Science Foundation (http://rscf.ru)

Swiss research funding agencies include the Swiss National Science Foundation (SNSF), the innovation promotion agency CTI (CTI/KTI), Ressortforschung des Bundes , and Eidgenössische Stiftungsaufsicht .

In the United Kingdom, the Haldane principle, that decisions about what to spend research funds on should be made by researchers rather than politicians, is still influential in research policy. There are several university departments with a focus on science policy, such as the Science Policy Research Unit. There are seven grant-awarding Research Councils:

The United States has a long history of government support specially for science and technology. Science policy in the United States is the responsibility of many organizations throughout the federal government. Much of the large-scale policy is made through the legislative budget process of enacting the yearly federal budget. Further decisions are made by the various federal agencies which spend the funds allocated by Congress, either on in-house research or by granting funds to outside organizations and researchers.

Research funding agencies in the United States are spread among many different departments, which include:




</doc>
<doc id="17596996" url="https://en.wikipedia.org/wiki?curid=17596996" title="Public policy of the United States">
Public policy of the United States

Public policy decisions are often decided by a group of individuals with different beliefs and interests. The policies of the United States of America comprise all actions taken by its federal government. The executive branch is the primary entity through which policies are enacted, however the policies are derived from a collection of laws, executive decisions, and legal precedents.
The policies of the United States The Almanac of Policy Issues, which provides background information, archived documents, and links to major U.S. public policy issues, organized the public policy of the United States into nine categories. The following lists these categories followed by a few examples of specific, respective policies:


Agricultural policy of the United States is the governing policy for agriculture in the United States and is composed primarily of the periodically renewed federal U.S. farm bills.

In "A New Agricultural Policy for the United States," authors Dennis Keeney and Long Kemp summarize the agricultural policy of the United States as follows: "Because of its unique geography, weather, history and policies, the United States has an agriculture that has been dominated by production of commodity crops for use in animal, industrial and export enterprises. Over time agricultural policies evolved to support an industrialized, commodity-based agriculture. This evolution resulted in farmers leaving the land with agriculture moving to an industrial structure." In parallel with the industrialization of agriculture in the United States, the Federal Government also developed the Dietary Guidelines for Americans, which emphasize consumption of foods that are produced by large-scale farming.

The drug policy of the United States is established by The Office of National Drug Control Policy, a former cabinet-level component of the Executive Office of the President of the United States, which was established by the Anti-Drug Abuse Act of 1988. Its stated goal is to establish policies, priorities, and objectives to eradicate illicit drug use, drug manufacturing, and trafficking, drug-related crime and violence, and drug-related health consequences in the U.S.

The Office of National Drug Control Policy's two current specific goals are to "curtail illicit drug consumption in America" and to "improve the public health and public safety of the American people by reducing the consequences of drug abuse." They plan to achieve these goals by taking the following actions:


The energy policy of the United States addresses issues of energy production, distribution, and consumption, such as building codes and gas mileage standards. The United States Department of Energy plays a major role, and its mission is "to ensure America's security and prosperity by addressing its energy, environmental, and nuclear challenges through transformative science and technology solutions."

Moreover, the White House provides a summary of the United States' current condition regarding its energy policy: "For decades it has been clear that the way Americans produce and consume energy is not sustainable. Our addiction to foreign oil and fossil fuels puts our economy, our national security and our environment at risk. To take this country in a new direction, the President is working with Congress to pass comprehensive energy and climate legislation to protect our nation from the serious economic and strategic risks associated with our reliance on foreign oil, to create jobs, and to cut down on the carbon pollution that contributes to the destabilizing effects of climate change."

The following is a snapshot of the United States' current energy policy goals:


The environmental policy of the United States addresses and regulates activities that impact the environment. Its general goal is to protect the environment for the welfare of future generations. The environmental policy goals are detailed below:


The foreign policy of the United States defines how the United States interacts with foreign nations. It only addresses the security of the American people and promotes international order. The following are the most prominent foreign policies of the United States:


The Federal Reserve, Treasury, and Securities and Exchange Commission took several steps on September 19 to intervene in the crisis. To stop the potential run on money market mutual funds, the Treasury also announced on September 19 a new $50 billion program to insure the investments, similar to the Federal Deposit Insurance Corporation (FDIC) program. Part of the announcements included temporary exceptions to section 23A and 23B (Regulation W), allowing financial groups to more easily share funds within their group. The exceptions would expire on January 30, 2009, unless extended by the Federal Reserve Board. The Securities and Exchange Commission announced termination of short-selling of 799 financial stocks, as well as action against naked short selling, as part of its reaction to the mortgage crisis.


</doc>
<doc id="34268968" url="https://en.wikipedia.org/wiki?curid=34268968" title="Policy alienation">
Policy alienation

Policy alienation refers to a framework which examines the experiences of governmental employees with new policies they have to implement. It has been used to describe the experiences of front-line public professionals with new policies. It is defined "as a general cognitive state of psychological disconnection from the policy programme being implemented."

A number of examples can clarify the concept of policy alienation. For example, Bottery (1998:40), examining the pressures on professionals stemming from new policies in education and health care in Great Britain, cites a teacher arguing that: “The changes have been outrageous, and have produced a culture of meritocracy and high flyers. There’s massive paperwork because the politicians don’t believe teachers are to be trusted.” This indicates that professionals had difficulties identifying with the policies they had to implement. A second example refers to the introduction of a new reimbursement policy in mental healthcare in the Netherlands. In one large-scale survey, as many as nine out of ten professionals wanted to abandon this new policy (Palm et al., 2008). Psychologists even went as far as to openly demonstrate on the street against this policy. A major reason for this was that many could not align their professional values with the content of the policy. As one professional noted:

""Within the new healthcare system economic values are dominant. Too little attention is being paid to the content: professionals helping patients. The result is that professionals become more aware of the costs and revenues of their behavior. This comes at the expense of acting according to professional standards.”"

Overall, a number of studies show an increasing discontent among public professionals toward public policies (see also Hebson et al., 2003; White, 1996), although more positive experiences can also be found (Ruiter, 2007). The policy alienation framework was developed to better understand the experiences of front-line public professionals with new policies.

Currently, there is an intense debate concerning professionals in the public sector. Many of the pressures that professionals face are related to the difficulties they have with the policies they have to implement. When implementers are unable to identify with a policy, this can negatively influence policy effectiveness. Furthermore, a high degree of policy alienation can affect the quality of interactions between professionals and citizens, which may eventually influence the output legitimacy of government. The policy alienation framework is used to analyze this topic.
It has been shown that policy alienation increases resistance to a new policy, lowers behavioral support for the policy and decreases job satisfaction of public professionals. Hence, it has both influences on the individual professional, as well as on policy effectiveness.

Alienation broadly refers to a sense of social estrangement, an absence of social support or meaningful social connection. Sociologists, public administration scholars, and other social scientists have used the alienation concept in various studies. As a result, a number of meanings have been attributed to the term. In an attempt to provide clarity, Seeman broke these meanings down into five alienation dimensions: powerlessness, meaninglessness, normlessness, social isolation, and self-estrangement.

Many scholars have used these dimensions to devise operational measures for alienation so that they can examine the concept in a range of settings. Mau, for example, used four dimensions in examining student alienation. Rayce et al., when investigating adolescent alienation, used three of the five dimensions. Further, many other researchers have used Seeman’s classification in examining the concept of work alienation. Blauner devised operational measures for three of the dimensions: powerlessness, meaninglessness, and social isolation.

The policy alienation framework was conceptualized based on the works of sociologists such as Hegel, Marx, Seeman, and Blauner. Furthermore, works of public administration scholars were used, particularly on Lipsky (street-level bureaucracy). Like work alienation, policy alienation is multidimensional, consisting of policy powerlessness and policy meaninglessness dimensions. In the work alienation literature, the dimensions of powerlessness and meaninglessness are also considered very important.

In essence, powerlessness is a person's lack of control over events in their life. In the realm of policy formulation and implementation, policy powerlessness relates to the degree of influence public professionals have over shaping a policy program. Powerlessness can occur when a new policy is drafted without the help of the professionals, by for example not consulting their professionals associations or labor unions. Furthermore, on an operational level professionals can feel powerless when they have to adhere to tight procedures and rules when implementing a policy (see also Lipsky). This kind of powerlessness may be particularly pronounced in professionals whose expectations of discretion and autonomy contradict notions of bureaucratic control (see also Profession).

The second dimension of policy alienation is meaninglessness. In the realm of policy making and implementation, policy meaninglessness refers to a professional’s perception of the contribution that the policy makes to a greater purpose, most notably to society or to their own clients. For instance, a professional can feel that implementing a policy is meaningless, if it does not deliver any apparent beneficial outcomes for society, such as more safety on the streets.

To make the dimensions more specific, five sub-dimensions were identified: strategic, tactical and operational powerlessness, societal and client meaninglessness. This is shown in the table below.

Five sub-dimensions of policy alienation


</doc>
<doc id="20251567" url="https://en.wikipedia.org/wiki?curid=20251567" title="Nosokinetics">
Nosokinetics

Nosokinetics is the science/subject of measuring and modelling the process of care in health and social care systems." Nosokinetics" brings together the Greek words for "noso": disease and "kinetics": movement.

Black box models are currently used to plan changes in health and social care systems. These input-output models overlook the process of inpatient care, as a result suboptimal decisions are made. Nosokinetics, (analogous to Pharmacokinetics), seeks to develop dynamic methods which measure and model the process of inpatient care. The aim is to develop a scientific base to underpin the planning of sustainable health and social care systems.
Nosokinetics is a new "science" that was established in the UK in the early 1990s by Prof Peter H Millard after publishing his PhD thesis. In 2004 Nosokinetics group newsletter was established.

Prof Peter H Millard writes about Nosokinetics : "If the random forces of wind and tide can make such a beautiful statue (referring to an iceberg), how much better could mankind do if a new science was developed which explains the complex processes of health and social care. Until new methods of planning health and social care services to meet the needs of an ageing population are introduced, service delivery will stumble on from crisis to crisis. The world population is ageing and sustainable systems of health care need to be developed."

He has established the nosokinetics group of interested researchers. The group collaborates to organize conferences and disseminates news of nosokinetics and other researchers' research and practical use of modelling to enhance decision making in health and social care systems.

The Nosokinetics Group has succeeded in attracting a lot of researchers. Nosokinetics interested people are present in many countries including Australia, UK & Egypt. They are from different disciplines ranging from health care providers to management scientists. The news related to nosokinetics is shared to the network through the bimonthly newsletter "Nosokinetics News" which helps to communicate papers, conferences and events of interest to the Nosokinetics network.


</doc>
<doc id="10412265" url="https://en.wikipedia.org/wiki?curid=10412265" title="Policy studies">
Policy studies

Policy studies is a subdisicipline of political science that includes the analysis of the process of policymaking (the policy process) and the contents of policy (policy analysis). Policy analysis includes substantive area research (such as health or education policy), program evaluation and impact studies, and policy design. It "involves systematically studying the nature, causes, and effects of alternative public policies, with particular emphasis on determining the policies that will achieve given goals." It emerged in the United States in the 1960s and 1970s.

Policy Studies also examines the conflicts and conflict resolution that arise from the making of policies in civil society, the private sector, or more commonly, in the public sector (e.g. government).

It frequently focuses on the public sector but is equally applicable to other kinds of organizations (e.g., the not-for-profit sector). Some policy study experts graduate from public policy schools with public policy degrees. Alternatively, experts may have backgrounds in policy analysis, program evaluation, sociology, psychology, philosophy, economics, anthropology, geography, law, political science, social work, environmental planning and public administration.

Traditionally, the field of policy studies focused on domestic policy, with the notable exceptions of foreign and defense policies. However, the wave of economic globalization, which ensued in the late 20th and early 21st centuries, created a need for a subset of policy studies that focuses on global governance, especially as it relates to issues that transcend national borders such as climate change, terrorism, nuclear proliferation, and economic development. This subset of policy studies, which is often referred to as international policy studies, typically requires mastery of a second language and attention to cross-cultural issues in order to address national and cultural biases. For example, the Monterey Institute of International Studies at Middlebury College offers Master of Arts programs that focus exclusively on international policy through a mix of interdisciplinary and cross-cultural analysis called the "Monterey Way". Examples of academic programs in policy studies include the Harvard Kennedy School and the LBJ School of Public Affairs.






</doc>
<doc id="12973014" url="https://en.wikipedia.org/wiki?curid=12973014" title="Blanket policy">
Blanket policy

Blanket policy is a policy which behaves similarly to a variety of things. Based on Webster's Dictionary it "covers a group or class of things or properties instead of one or more things mentioned individually, as where a mortgage secures various debts as a group, or subjects a group or class of different pieces of property to one general lien."

Webster 1913 Suppl.


</doc>
<doc id="39208624" url="https://en.wikipedia.org/wiki?curid=39208624" title="Open educational resources policy">
Open educational resources policy

Open educational resource policies (OER policies) are principles or tenets adopted by governing bodies in support of the use of open content—specifically open educational resources (OER) -- and practices in educational institutions. Such policies are emerging increasingly at the national, state/province, and local levels. Creative Commons defines (OER) policies as "legislation, institutional policies, and/or funder mandates that lead to the creation, increased use, and/or support for improving OER." OER are learning materials that reside in the public domain or have been released under an intellectual property license that permits their free use and re-purposing by others.

Creative Commons hosts an open educational resources policy registry which lists 112 current and proposed open education policies from around the world.

Another resource for finding OER policies is the Open Educational Quality Initiative OPAL Best Practice Clearing House. The OPAL Initiative is a partnership between seven organizations including the International Council for Open and Distance Education (ICDE), UNESCO, European Foundation for Quality, the Open University UK, Aalto University and the Catholic University Portugal. Led by the University of Duisburg-Essen in Germany, it is partly funded by the European Commission.

On Friday 22 June 2012, the UNESCO World Open Educational Resources (OER) Congress released the 2012 Paris OER Declaration which called on governments to openly license publicly funded educational materials.

UNESCO member states unanimously approved the declaration, which highlights the importance of open educational resources and gives recommendations to governments and institutions around the globe.

On January 17, 2014, the Council on Higher Education in South Africa published a "White Paper for Post-School Education and Training". This paper emphasized open learning principles and set the stage for supporting national efforts to design and develop high-quality open educational resources. In response the University of South Africa (UNISA)—one of the founding partners of the OERu network and a member of the 2012 Unesco OER conference in Paris—approved an Open Educational Resource (OER) Strategy in March 2014.

An open-access policy enacted by the Faculty of a research university can empower them in choosing how to distribute their own scholarly work. If a faculty member wishes to grant exclusive rights to a publisher, they would first need to request a waiver from their faculty governance body. Some reasons to implement this kind of policy institution-wide are to:

This kind of blanket policy provides support to those whose research is not part of a project that requires open access to the research done. For example, since the February 2013 directive from the United States Office of Science and Technology Policy, U.S. federal agencies have been developing their own policies on making research freely available within a year of publication.

SPARC, the Scholarly Publishing and Academic Resources Coalition, led the collaborative and open effort to create an "Open Access Spectrum" that demonstrates a more sophisticated approach is needed in discussions about the concept of openness in research communications. The "HowOpenIsIt? Guide (as well as an FAQ document and slide deck) is available for download on the SPARC website. Another useful guide has been developed by members of the Harvard Office for Scholarly Communication, the Harvard Open Access Project, and the Berkman Center for Internet and Society. This online guide, "Good practices for university open-access policies" is built on a wiki and is designed to evolve over time, according to the co-authors: Emily Kilcer, Stuart Shieber and Peter Suber.

On June 10, 2013, the Faculty Board of the California Institute of Technology (Caltech) created an institution-wide Open Access Policy. The ruling stated that as of January 1, 2014, all Caltech faculty must agree to grant nonexclusive rights to Caltech to disseminate their scholarly papers either via the authors' own sites or to Caltech AUTHORS, the online repository. The goal is to encourage wider distribution of their work and to simplify the copyright process when posting research on faculty or institutional Web sites. The initiative was put in place to prevent publishers of those journals from threatening legal action or issuing takedown notices to authors who have posted their content on their own sites or to CaltechAUTHORS, an online repository for research papers authored by Caltech faculty and other researchers at Caltech.

On March 21, 2010, the Duke University Academic Council voted to support the University Library's new data repository, DukeSpace, with a blanket policy to provide open access to their scholarly writings. The policy allows for faculty members to opt out at any time, and it is regularly reviewed to determine its effectiveness.

Duke also in 2010 joined the Compact for Open-Access Publishing Equity (COPE) and established a fund to help Duke faculty members to cover any author fees required to publish in open access journals.

On February 12, 2008, the Faculty of Arts and Sciences of Harvard University approved their Open Access Policy, granting to the President and Fellows of Harvard to "make available his or her scholarly articles and to exercise the copyright in those articles ... in a nonexclusive, irrevocable, paid-up, worldwide license..." Since then, several other schools within the University now participate in the Open Access Policies supported by the Office for Scholarly Communication: the Graduate School of Design, the School of Education, the Business School, the Law School, the Kennedy School of Government, the Divinity School, and the School of Public Health. The University's open-access repository is called DASH (Digital Access to Scholarship at Harvard) which is where the faculty upload their scholarly articles for access by all.

Adopted by a unanimous vote on March 18, 2009, the Massachusetts Institute of Technology (MIT) Faculty adopted an open access policy. The policy applies to "all scholarly articles written while the person is a member of the Faculty except for any articles completed before the adoption of this policy and any articles for which the Faculty member entered into an incompatible licensing or assignment agreement before the adoption of this policy." The MIT online repository is called DSpace@MIT and it was designed to work seamlessly with Google Scholar. The Faculty revised and updated the policy in 2010 to take into consideration the various issues associated with the MIT librarians' discussions with publishers.

In 2010 the Dean of the Faculty of Princeton University appointed an ad-hoc committee of faculty and the University Librarian to study the question of open access to faculty publications - and in March 2011, the committee recommended several changes to the Faculty rules to allow for a blanket policy for open access to Princeton faculty scholarship. The faculty approved an open access policy on September 19, 2011, which was last revised in January 2012.

On June 26, 2008, the Stanford University Graduate School of Education (GSE) were the first in that school to grant permission to the University to make their scholarly articles publicly accessible and to exercise the copyright in a "nonexclusive, irrevocable, worldwide license ... provided that the articles are properly attributed to the authors not sold for a profit." The GSE Open Archive houses and makes publicly available the GSE authors' working papers as well as published articles. Between May 21-24th, 2013, the Stanford GSE doctoral students voted in favor of a motion to enact an Open Access policy. At this time, however, despite the strong case made by Professors John Willinsky and Juan Pablo Alperin, no other Stanford academic units have stepped forward.

On July 24, 2013, the Academic Senate of the University of California (UC) approved the UC Open Access Policy for all 8,000 plus faculty at their ten campuses. Some confusion at the local campuses led to online postings of journal articles whose copyright was already owned by publishers. For example, in December 2013, the academic publishing company Elsevier sent several UC faculty notices to take down certain journal articles posted openly on their campus webpages, e.g., on the department websites or faculty profiles. The UC Open Access Policy protected those faculty who had correctly uploaded their articles to the UC eScholarship repository. In another case of misunderstanding by the faculty about open access, in March 2014 the University received a Digital Millennium Copyright Act (DMCA) takedown notice for nine articles owned by the American Society for Civil Engineers (ASCE). The UC faculty authors had uploaded to eScholarship the publisher-formatted articles between 2004 and 2008, before the UC Open Access Policy had been enacted and in violation of the publisher's agreement with the authors when they gave their copyrights to the ASCE.

In 2014 the Faculty Assembly of the University of Colorado Boulder approved the CU Boulder Open Access Policy "in order to allow for broad dissemination of their research." They granted to The Regents of the University of Colorado "a nonexclusive, irrevocable, worldwide license to exercise any and all rights under copyright relating to their scholarly work, as long as the works are properly attributed to the authors and not used for commercial purposes"—and that the individual faculty would retain full ownership of the material. Authors at UC Boulder are expected to inform publishers about the University's policy and that they "have granted a pre-existing License." The digital repository, CU Scholar, is maintained by the University Libraries and functions under a set of policies derived from the Open Access Policy. Contributions from the CU Boulder community can include working papers and technical reports, published scholarly research articles, completed manuscripts, digital art or multimedia, conference papers and proceedings, theses and dissertations, Undergraduate Honors theses, journals published on campus, faculty course-related output primarily of scholarly interest, and data sets. The Chancellor's Executive Committee recently approved the new policy, following the lead of the Council of Deans and the Office of the Provost and Executive Vice Chancellor.

In 2005 the University of Kansas (KU) created KU ScholarWorks, a digital repository for scholarly work created by KU faculty and staff. Faculty Senate President Lisa Wolf-Wendel, professor of education leadership and policy studies, approved a new policy, "Open Access Policy for University of Kansas Scholarship" on April 30, 2009, in order to provide the broadest possible access to the journal literature authored by KU faculty." In June 2009, under a faculty-initiated policy approved by Chancellor Robert Hemenway, KU became the first U.S. public university to implement an open access policy. Unless a KU author sought a waiver, all articles must be submitted to KU ScholarWorks. "Processes to Implement the KU Open Access Policy" were endorsed by the Faculty Senate in February 2010. Theses and dissertations at the University of Kansas are also openly available, however in 2010 KU Graduate Studies established a policy that a student may request permission to embargo its publication for six months, one year or two years. Graduates earning the KU Master of Fine Arts in Creative Writing or PhD in English (Literature and Creative Writing track) may request a permanent embargo.

In the United Kingdom, the Higher Education Funding Council for England (HEFCE) subsidized the JISC Academy Open Educational Resources Programme. JISC refers to a membership organization that provides digital solutions for United Kingdom education and research initiatives. The JISC/HE OER Programme (Phase 3 from October 2011 – October 2012) was meant to build on sustainable procedure indicated in the first two phases eventually expanding in new directions that connect Open Educational Resources to other fields of work. This third phase involved important stakeholders emphasizing fresh challenges and insights about the effect of OER and Open Educational Practice.

During this stage, the concept of electronic books and Massive Open Online Courses (MOOC) also emerged. MOOCs offer courses at the University level without having to finish the whole programme. Many students get the chance to study premium courses online frequently at no cost. HEFCE made significant investments through the JISC and Academy from 2009 until 2012. The objective was to encourage sharing and reusing of resources which provide benefits to higher education in the United Kingdom. More than 80 projects obtained funding during the UK OER Programme. Substantial investments were channelled towards the development of Open Educational Resources even as the benefits for stakeholders have not been explained properly. Sufficient evidence is needed to prove this point. One criticism is that many such programmes are not technically and educationally accessible to a worldwide audience. 





</doc>
<doc id="38772624" url="https://en.wikipedia.org/wiki?curid=38772624" title="Policy transfer">
Policy transfer

Policy transfer

Policy transfer is a process in which knowledge about policies, administrative arrangements, institutions and ideas in one political setting (past or present) is used in the development of policies, administrative arrangements, institutions and ideas in another political setting. Policy transfer has a long history with a variety of policies such as zero tolerance policing, welfare-to-work and Business Improvement Districts moving between different nation states.

Policy transfer has been the subject of considerable academic research, led primarily by political scientists since the late 1990s. Since the mid-2000s geographers have also played an important role in these debates (often use the term "policy mobilities" instead of policy transfer).

Since David Dolowitz and David Marsh's (2000) paper 'Learning from abroad: the role of policy transfer in contemporary policy-making', academic research has focused on the issues of who is involved in policy transfer, what is transferred, from and to where policy is transferred, the degrees of and constraints on transfer, and its success once transferred. More recently there have been attempts to explicate the role of two way communication, and particularly feedback from policy stakeholders for successful policy transfer, along with efforts to acknowledge the "indigenization" of policies as they are modified and adapted to context. The debates in geography have focused on the technologies and methods through which policies and ideas circulate – such as study tours, conferences and best practice guides – as well as looking at how and why the policies change form as they circulate.



</doc>
<doc id="40519627" url="https://en.wikipedia.org/wiki?curid=40519627" title="Asia-Pacific Network for Global Change Research">
Asia-Pacific Network for Global Change Research

The Asia-Pacific Network for Global Change Research (APN) is an intergovernmental network that promotes policy-oriented research and capacity-building activities related to global change in the region. APN receives financial contribution from the governments of the United States, Japan, Republic of Korea and New Zealand, with in-kind contribution from all it 22 member countries. The APN Secretariat is based in Kobe, Japan, hosted by the Hyogo Prefectural Government.

The history of APN dates back to the 1990 White House Conference on Science and Economics Research Related to Global Change, 17–18 April 1990, at which then US President George Bush invited countries of the world to join the United States in creating regional networks for North-South scientific cooperation at the intergovernmental level to deal with global environmental change research. Later in 1992, President Bush and then Prime Minister of Japan Kiichi Miyazawa signed the 1992 US-Japan Global Partnership Agreement, which, among other things, reaffirmed and strengthened Japan-US commitment to global change research.

Discussions along these lines ultimately resulted in the establishment of three global change research networks: ENRICH for Europe and Africa, APN for Asia and the Pacific, and IAI for the Americas.

APN was formally launched in 1996 at its first intergovernmental meeting held at Chiang Mai, Thailand. In 1997, a competitive process was in place, open to funding applications for scientific research projects relating to global environmental change.

Starting from 12 countries in 1996, APN membership has grown to 22 as of April 2013. In addition to the 22 full members, institutions and individuals from a number of “approved countries” are eligible for APN funding.




</doc>
<doc id="41315495" url="https://en.wikipedia.org/wiki?curid=41315495" title="Policy Monitoring">
Policy Monitoring

Policy monitoring comes a range of activities describing and analyzing the development and implementation of policies, identifying potential gaps in the process, outlining areas for improvement, and holding policy implementers accountable for their activities.

Monitoring policy development and implementation is an integral component of the policy cycle and can be applied in sectors including agriculture, health, education, and finance. Policy monitoring can improve policy informatioation among stakeholders, and the use of evaluation techniques to provide feedback to reframe and revise policies.
Waterman and Wood derived policy monitoring from agency theory, describing a process where policymakers monitor the actions of their bureaucratic agents who implement and enforce policies. This monitoring allows policymakers to compensate for their agents’ greater knowledge of the policy process, and enables them to be well-informed decision makers. Thus policy monitoring allows policymakers and interested actors to systematically examine the process of creating a policy, implementing it, and evaluating its effects.
Policy monitoring activities can be used to collect and analyze data related to the development and implementation of specific policies. It can also help link policies to specific outcomes and help identify and evaluate policy impacts. Policy impacts can include specific changes in behavior (e.g., increased number of people wearing seatbelts), finances (e.g., increased tax revenue), health status or epidemiology (e.g., reduced number of new HIV infections) or other social indicators (e.g., reduced crime rates, reduced levels of pollution). Data from policy monitoring can be used to support advocacy efforts and guide the development of new, timely, and relevant policies. Policy monitoring should also include the identification of operational policy barriers that can be addressed through policy and program reform, and findings can support improved implementation of existing policies.

Numerous actors and stakeholders can influence the movement of policy from inception to implementation. Well-maintained documentation and review of all key stakeholders involved in a policy can help advocates for a given policy—such as military reform, water rights, or disability legislation—prepare to address different ideologies, capacities, or interests of key actors. Limiting stakeholder analysis only to government and official policymakers may ignore major groups that can support policy development. Policy monitoring coalitions should agree on what they are monitoring and be succinct in their recommendations to policymakers.
Policy initiatives themselves are often controversial, and policy monitoring can be contentious because it shows how well policy implementers and enforcers are doing their jobs. Those conducting policy monitoring should be thorough in their data collection and unbiased in their presentation of facts. Robust trainings on policy monitoring work can help organizations be systematic and effective in their policy monitoring efforts.
The United States President’s Emergency Plan for AIDS Relief (PEPFAR)’s Monitoring Policy Reform tool outlines the progression of policy development related to HIV from problem identification to monitoring and evaluation. The tool supports a relatively simple and uniform monitoring process which can be applied to any policy area. This tool can guide policy monitoring efforts throughout the policy reform process.

The FAO’s Food and Agriculture Policy Decision Analysis (FAPDA) is a policy monitoring tool that provides a working cycle technique to identify policy problems and improve analysis of policy issues. By incorporating FAPDA outputs, such as a web-based tool, country policy review, and policy analysis report, policy dialogue can be more systematic and encompass different actors interested in FAPDA data.

The World Health Organization has started to develop dedicated monitoring systems for policy interventions on the social determinants of health that improve health equity, such as social protection and gender equity policies. 
Policy monitoring can be performed through different issue-driven lenses, such as gender sensitivity or gender equality. Gender-sensitive policy monitoring analyzes any gender aspects of a policy or policy issue, and considers the impact of the policy on both men and women, as well as its impact on gender relations. For example, a policy that is shown to have improved the welfare of a household may not necessarily affect all household members positively or equally, and may have even exacerbated gender inequity. Gender-sensitive policy monitoring can help advance gender equity and improve policy implementation. Civil society and other stakeholders can use policy monitoring techniques to systematically gather data on the gender aspects of policies and use these data to influence policymakers to favor gender-equitable health policies¬¬¬—these processes are essential to facilitating gender mainstreaming.

A 2012 study analyzed planned policy interventions across the 22 publicly accessible PEPFAR Partnership Frameworks to understand how the interventions are related to PEPFAR and country or regional priorities. The study found that “policy monitoring by donors, partner country governments, and civil society stakeholders can help measure whether policy interventions are occurring as planned in order to further HIV prevention, care, and treatment and health system goals and, if not, can point to needed changes."


</doc>
<doc id="1221086" url="https://en.wikipedia.org/wiki?curid=1221086" title="Mandate (politics)">
Mandate (politics)

In politics, a mandate is the authority granted by a constituency to act as its representative.

The concept of a government having a legitimate mandate to govern via the fair winning of a democratic election is a central idea of representative democracy. New governments who attempt to introduce policies that they did not make public during an election campaign are said not to have a legitimate "mandate" to implement such policies.

Elections, especially ones with a large margin of victory, and are often said to give the newly elected government or elected official an implicit mandate to put into effect certain policies. When a government seeks re-election they may introduce new policies as part of the campaign and are hoping for approval from the voters, and say they are seeking a "new mandate".

In some languages, a 'mandate' can mean a parliamentary seat won in an election rather than the electoral victory itself. In case such a mandate is bound to the wishes of the electorate, it is an imperative mandate, otherwise it is called "free".


Political philosophy:




</doc>
<doc id="639389" url="https://en.wikipedia.org/wiki?curid=639389" title="Policy analysis">
Policy analysis

Policy analysis is a technique used in public administration to enable civil servants, activists, and others to examine and evaluate the available options to implement the goals of laws and elected officials. The process is also used in the administration of large organizations with complex policies. It has been defined as the process of "determining which of various policies will achieve a given set of goals in light of the relations between the policies and the goals." Policy analysis can be divided into two major fields:


The areas of interest and the purpose of analysis determine what types of analysis are conducted. A combination of two kinds of policy analyses together with program evaluation is defined as "policy studies". Policy analysis is frequently deployed in the public sector, but is equally applicable elsewhere, such as nonprofit organizations and non-governmental organizations. Policy analysis has its roots in systems analysis, an approach used by United States Secretary of Defense Robert McNamara in the 1960s.

Various approaches to policy analysis exist. The analysis "for" policy (and analysis "of" policy) is the central approach in social science and educational policy studies. It is linked to two different traditions of policy analysis and research frameworks. The approach of analysis "for" policy refers to research conducted for actual policy development, often commissioned by policymakers inside the bureaucracy (e.g., senior civil servants) within which the policy is developed. Analysis "of" policy is more of an academic exercise, conducted by academic researchers, professors and think tank researchers, who are often seeking to understand why a particular policy was developed at a particular time and assess the effects, intended or otherwise, of that policy when it was implemented.

There are three approaches that can be distinguished: the analysis-centric, the policy process, and the meta-policy approach.

The analysis-centric (or "analycentric") approach focuses on individual problems and their solutions. Its scope is the micro-scale and its problem interpretation or problem resolution usually involves a technical solution. The primary aim is to identify the most effective and efficient solution in technical and economic terms (e.g. the most efficient allocation of resources).

The policy process approach puts its focal point onto political processes and involved stakeholders; its scope is the broader meso-scale and it interprets problems using a political lens (i.e., the interests and goals of elected officials). It aims at determining what processes, means and policy instruments (e.g., regulation, legislation, subsidy) are used. As well, it tries to explain the role and influence of stakeholders within the policy process . In the 2010s, "stakeholders" is defined broadly to include citizens, community groups, non-governmental organizations, businesses and even opposing political parties. By changing the relative power and influence of certain groups (e.g., enhancing public participation and consultation), solutions to problems may be identified that have more "buy in" from a wider group. One way of doing this followed a heuristic model called the "policy cycle". In its simplest form, the policy cycle, which is often depicted visually as a loop or circle, starts with the identification of the problem, proceeds to an examination of the different policy tools that could be used to respond to that problem, then goes on to the implementation stage, in which one or more policies are put into practice (e.g., a new regulation or subsidy is set in place), and then finally, once the policy has been implemented and run for a certain period, the policy is evaluated. A number of different viewpoints can be used during evaluation, including looking at a policy's effectiveness, cost-effectiveness, value for money, outcomes or outputs.

The meta-policy approach is a systems and context approach; i.e., its scope is the macro-scale and its problem interpretation is usually of a structural nature. It aims at explaining the contextual factors of the policy process; i.e., what the political, economic and socio-cultural factors are that influence it. As problems may result because of structural factors (e.g., a certain economic system or political institution), solutions may entail changing the structure itself.

Policy analysis uses both qualitative methods and quantitative methods. Qualitative research includes case studies and interviews with community members. Quantitative research includes survey research, statistical analysis (also called "data analysis") and model building. A common practice is to define the problem and evaluation criteria; identify and evaluate alternatives; and recommend a certain policy accordingly. Promotion of the best agendas are the product of careful "back-room" analysis of policies by "a priori" assessment and "a posteriori" evaluation.

There are six dimensions to policy analysis categorized as the effects and implementation of the policy across a period of time. Also collectively known as "Durability" of the policy, which means the capacity in content of the policy to produce visible effective compatible change or results over time with robustness.

Effects
Implementation
The strategic effects dimensions can pose certain limitations due to data collection. However the analytical dimensions of effects directly influences acceptability. The degree of acceptability is based upon the plausible definitions of actors involved in feasibility. If the feasibility dimension is compromised, it will put the implementation at risk, which will entail additional costs. Finally, implementation dimensions collectively influence a policy's ability to produce results or impacts.

One model of policy analysis is the "five-E approach", which consists of examining a policy in terms of:

Policies are considered as frameworks that can optimize the general well-being. These are commonly analyzed by legislative bodies and lobbyists. Every policy analysis is intended to bring an evaluative outcome. A systemic policy analysis is meant for in depth study for addressing a social problem. Following are steps in a policy analysis:

Many models exist to analyze the development and implementation of public policy. Analysts use these models to identify important aspects of policy, as well as explain and predict policy and its consequences. Each of these models are based upon the types of policies.


Some evidence supported models are:

Public policy is determined by a range of political institutions, which give policy legitimacy to policy measures. In general, the government applies policy to all citizens and monopolizes the use of force in applying or implementing policy (through government control of law enforcement, court systems, imprisonment and armed forces). The legislature, executive and judicial branches of government are examples of institutions that give policy legitimacy. Many countries also have independent, quasi-independent or arm's length bodies which, while funded by government, are independent from elected officials and political leaders. These organizations may include government commissions, tribunals, regulatory agencies and electoral commissions.

Policy creation is a process that typically follows a sequence of steps or stages:

This model, however, has been criticized for being overly linear and simplistic. In reality, stages of the policy process may overlap or never happen. Also, this model fails to take into account the multiple factors attempting to influence the process itself as well as each other, and the complexity this entails.

One of the most widely used model for public institutions are of Herbert A. Simon, the father of rational models. It is also used by private corporations. However, many criticise the model due to characteristics of the model being impractical and relying on unrealistic assumptions. For instance, it is a difficult model to apply in the public sector because social problems can be very complex, ill-defined and interdependent. The problem lies in the thinking procedure implied by the model which is linear and can face difficulties in extraordinary problems or social problems which have no sequences of happenings.

The rational model of decision-making is a process for making sound decisions in policy-making in the public sector. Rationality is defined as “a style of behavior that is appropriate to the achievement of given goals, within the limits imposed by given conditions and constraints”. It is important to note the model makes a series of assumptions, such as: 'The model must be applied in a system that is stable'; 'The government is a rational and unitary actor and that its actions are perceived as rational choices'; 'The policy problem is unambiguous'; 'There are no limitations of time or cost'.

Furthermore, in the context of the public sector policy models are intended to achieve maximum social gain. Simon identifies an outline of a step by step mode of analysis to achieve rational decisions. Ian Thomas describes Simon's steps as follows:


The model of rational decision-making has also proven to be very useful to several decision making processes in industries outside the public sphere. Nonetheless, there are some who criticize the rational model due to the major problems which can be faced & which tend to arise in practice because social and environmental values can be difficult to quantify and forge consensus around. Furthermore, the assumptions stated by Simon are never fully valid in a real world context.

Further criticism of the rational model include: leaving a gap between planning and implementation, ignoring of the role of people, entrepreneurs, leadership, etc., the insufficiency of technical competence (i.e. ignoring the human factor), reflecting too mechanical an approach (i.e. the organic nature of organizations), requiring of multidimensional and complex models, generation of predictions which are often wrong (i.e. simple solutions may be overlooked), & incurring of cost (i.e. costs of rational-comprehensive planning may outweigh the cost savings of the policy).

However, Thomas R. Dye, the president of the Lincoln Center for Public Service, states the rational model provides a good perspective since in modern society rationality plays a central role and everything that is rational tends to be prized. Thus, it does not seem strange that “we ought to be trying for rational decision-making”.

An incremental policy model relies on features of incremental decision-making such as: satisfying, organizational drift, bounded rationality, and limited cognition, among others. Such policies are often called "muddling through" & represent a conservative tendency: new policies are only slightly different from old policies. Policy-makers are too short on time, resources, and brains to make totally new policies; as such, past policies are accepted as having some legitimacy. When existing policies have sunk costs which discourage innovation, incrementalism is an easier approach than rationalism, and the policies are more politically expedient because they don't necessitate any radical redistribution of values. Such models necessarily struggle to improve the acceptability of public policy.

Criticisms of such a policy approach include: challenges to bargaining (i.e. not successful with limited resources), downplaying useful quantitative information, obscuring real relationships between political entities, an anti-intellectual approach to problems (i.e. the preclusion of imagination), and a bias towards conservatism (i.e. bias against far-reaching solutions).

There are many contemporary policies relevant to gender and workplace issues. Actors analyze contemporary gender-related employment issues ranging from parental leave and maternity programs, sexual harassment, and work/life balance to gender mainstreaming. It is by the juxtaposition of a variety of research methodologies focused on a common theme the richness of understanding is gained. This integrates what are usually separate bodies of evaluation on the role of gender in welfare state developments, employment transformations, workplace policies, and work experience.

This policy is formed as a result of forces and pressures from influential groups. Pressure groups are informally co-opted into the policy making process. Regulatory agencies are captured by those they are supposed to regulate. No one group is dominant all the time on all issues. The group is the bridge between the individual and the administration. The executive is thus pressured by interest groups.

The task of the system is to:

There are several other major types of policy analysis, broadly groupable into competing approaches:



The success of a policy can be measured by changes in the behavior of the target population and active support from various actors and institutions involved. A public policy is an authoritative communication prescribing an unambiguous course of action for specified individuals or groups in certain situations. There must be an authority or leader charged with the implementation and monitoring of the policy with a sound social theory underlying the program and the target group. Evaluations can help estimate what effects will be produced by program objectives/alternatives. However, claims of causality can only be made with randomized control trials in which the policy change is applied to one group and not applied to a control group and individuals are randomly assigned to these groups.

To obtain compliance of the actors involved, the government can resort to positive sanctions, such as favorable publicity, price supports, tax credits, grants-in-aid, direct services or benefits; declarations; rewards; voluntary standards; mediation; education; demonstration programs; training, contracts; subsidies; loans; general expenditures; informal procedures, bargaining; franchises; sole-source provider awards...etc.

Policy evaluation is used to examine content, implementation or impact of the policy, which helps to understand the merit, worth and the utility of the policy. Following are National Collaborating Centre for Healthy Public Policy's (NCCHPP) 10 steps:







</doc>
<doc id="50183186" url="https://en.wikipedia.org/wiki?curid=50183186" title="End Use Energy Demand Centres">
End Use Energy Demand Centres

The End Use Energy Demand (EUED) Centres carry out interdisciplinary research and advise policy on reducing energy demand to help achieve the UK government's CO2 emissions targets. The Centres are a £30m investment of the Research Councils UK Energy Programme (with additional funding from industrial partners) that run from 2013-2018. The six large centres are based across 25 institutions and encompass over 200 researchers.

The Centres are:

Centre for Energy Epidemiology (CEE) - Based at University College London and directed by Professor Tadj Oreszczyn.

Centre on Innovation and Energy Demand (CIED) - Based at the University of Sussex and directed by Professor Benjamin Sovacool.

Centre for Industrial Energy, Materials and Products (CIE-MAP) - Based at the University of Leeds and directed by Professor John Barrett.

Centre for Sustainable Energy Use in Food Chains (CSEF) - Based at Brunel University and led by Professor Savvas Tassou.

Dynamics of Energy, Mobility and Demand (DEMAND) - Based at the University of Lancaster and led by Professor Elizabeth Shove.

Interdisciplinary Centre for Storage, Transformation and Upgrading of Thermal Energy (i-STUTE) - Based at the University of Warwick and directed by Professor Robert Critoph.

The UK is committed to achieving large-scale reductions (80%) in greenhouse gas emissions by 2050 (Climate Change Act (1990)). End Use Energy Demand (EUED) research is about reducing energy use on the scale needed to reach this target, whilst maintaining current or achieving better standards of living and economic growth. The EUED Centres were funded in 2013 following a 2011 call for proposals from the Research Councils UK Energy Programme to address these issues.




</doc>
<doc id="323090" url="https://en.wikipedia.org/wiki?curid=323090" title="Centralisation">
Centralisation

Centralisation (British) or centralization (both British and American) is the process by which the activities of an organization, particularly those regarding planning and decision-making, become concentrated within a particular geographical location group. This moves the important decision-making and planning powers within the center of the organisation.

The term has a variety of meanings in several fields. In political science, centralisation refers to the concentration of a government's power—both geographically and politically—into a centralized government.

"Centralisation of authority" is defined as the systematic and consistent concentration of authority at a central point or in a person within the organization. This idea was first introduced in the Qin Dynasty of China. The Qin government was highly bureaucratic and was administered by a hierarchy of officials, all serving the First Emperor, Qin Shi Huang. The Qin Dynasty practised all the things that Han Feizi taught, allowing Qin Shi Huang to own and control all his territories, including those conquered from other countries. Zheng and his advisers ended feudalism in China by setting up new laws and regulations under a centralized and bureaucratic government with a rigid centralization of authority. Under this system, both the military and government thrived. This was because talented individuals were more easily identified and picked out to be trained for specialized functions.


The acts for the implementation are needed after delegation. Therefore, the authority for taking the decisions can be spread with the help of the delegation of the authority.

The centralisation of authority can be done immediately, if complete concentration is given at the decision-making stage for any position. The centralisation can be done with a position or at a level in an organisation. Ideally, the decision-making power is held by a few individuals.

Centralisation of authority has several advantages and disadvantages. The benefits include:


Disadvantages, on the other hand are as follows:


As written in V.I. Lenin’s book, "Imperialism, the Highest Stage of Capitalism", "The remarkably rapid concentration of production in ever-larger enterprises are one of the most characteristic features of capitalism." He researched the development of production and decided to develop the concept of production as a centralised framework, from individual and scattered small workshops into large factories, leading the capitalism to the world. This is guided by the idea that once concentration of production develops into a particular level, it will become a monopoly, like party organisations of Cartel, Syndicate, and Trust.

Most businesses deal with issues relating to the specifics of centralization or decentralization of decision-making. The key question is either whether the authority should manage all the things at the centre of a business (centralised), or whether it should be delegated far away from the centre (decentralised).

The choice between centralised or decentralised varies. Many large businesses necessarily involve some extent of decentralisation and some extent of centralisation when it begins to operate from several places or any new units and markets added.




</doc>
<doc id="44314964" url="https://en.wikipedia.org/wiki?curid=44314964" title="Protected area downgrading, downsizing, and degazettement">
Protected area downgrading, downsizing, and degazettement

Protected area downgrading, downsizing, and degazettement (PADDD) are processes that change the legal status of national parks and other protected areas. "Downgrading" is "a decrease in legal restrictions on the number, magnitude, or extent of human activities within a protected area (i.e., legal authorization for increased human use)." "Downsizing" refers to a "decrease in size of a protected area as a result of excision of land or sea area through a legal boundary change." "Degazettement" is defined as a loss of legal protection for an entire national park or other protected area. Collectively, PADDD represents legal processes that temper regulations, shrink boundaries, or eliminate all legal protections originally associated with establishment of a protected area.

PADDD is a phenomenon that has recently gained attention among scientists and policymakers. Scientific publications have identified more than 600 enacted PADDD events in 57 countries, encompassing a total of more than 550,000 km2 of protected lands.

PADDD was a topic of discussion at the World Parks Congress in Sydney, Australia, in November, 2014.

Scientists have suggested that the global PADDD trend could be combatted via a systematic programme of protected area "upgrading," whereby conserved wild areas are expanded via the purchase or gazetting of surrounding territory. Successful examples of protected-area upgrading include Gorongosa National Park in Mozambique and the Guanacaste Conservation Area in Costa Rica.


</doc>
<doc id="53642969" url="https://en.wikipedia.org/wiki?curid=53642969" title="Policy learning">
Policy learning

Policy learning is the increased understanding that occurs when policymakers compare one set of policy problems to others within their own or in other jurisdictions. It can aid in understanding why a policy was implemented, the policy's effects, and how the policy could apply to the policymakers' jurisdiction. Before a policy is adopted it goes through a process that involves various combinations of elected official(s), political parties, civil servants, advocacy groups, policy experts or consultants, corporations, think tanks, and multiple levels of government. Policies can be challenged in various ways, including questioning its legality. Ideally policymakers develop complete knowledge about the policy; the policy should achieve its intent and efficiently use resources.

Policy learning through globalization has helped government organizations become more competitive. Policymakers have easy access to global policy knowledge through the internet, access to think tanks, international institutions such as the United Nations, International Monetary Fund (IMF) or the World Bank and individual experts.

In the 1960s academics started to study how policymakers learn about policies. During that time countries were experiencing social, political, economic and technological change. Researchers discovered that governments in different countries faced similar problems in policies and programs amidst uncertainty on how to handle problems in financing its welfare programs. Policymakers start to learn about policy through facts, first-hand experiences or from the experiences of others.

Policy instruments and policy implementation are the steps to policy learning. Policymakers review policy objectives, tools and implementation strategies. When implementations fail, reviews look for the cause(s). Adjustments in objectives, tools and implementation are considered.

Instrumental policy learning is the acquisition of knowledge about the effectiveness of various policy instruments and implementatiions, Policymakers must make choices about the appropriate policy intervention tool(s) to use. The intent is to discover the most effective tool(s) that consume the least resources.

Policymakers can employ seven major policy instrument types. 


After choosing a policy instrument, the details for employing the instrument must be arranged. Implementation carries risks of failing in various ways, such as ineffectiveness, unacceptable delays and excessive costs. Practices that improve success rates include setting reasonable expectations, allowing adequate time and sufficient resources, having clear communication and u nderstanding policy objectives, minimizing the number of approvals, simplifying management structures and aligning all relevant groups around the implementation, along with mechanisms to adapt the implementation in accord with subsequent experience to correct problems and take advantage of new opportunities.

The top-down approach involves allowing high-level policymakers set objectives and define implementation strategies. Lower level implementers carry out the policy. Objectives must be clearly defined and the implementation tools must be selected based on the implementation strategy. Policy designers need to assess the commitment of policy implementers who could be teachers, police officers, social workers or private sector workers.

One example of the top-down approach was in 1973 when the US Congress passed a policy limiting the driving speed to 55 mph on America’s freeways under the National Maximum Speed Law. The policy objective was to reduce gasoline consumption. In addition to increased travel times, a side effect was the reduction of freeway fatalities. 

The bottom-up approach helps policymakers to evaluate whether policy goals are open to more than one interpretation. Does the policy implement a statute, or reflect rules, practices and/or norms such as energy policy or criminal procedure? Are the policy goals internally consistent? How will the policy affect the activities of workers who directly provide services? Bottom-up approaches require policymakers to involve both service providers and service recipients in refining goals, strategies and activities. This bottom-up approach starts from consumer-facing bureaucrats and moves up to the top policymakers. Should the policy face pushback, policymakers must be open to negotiations for a compromise approach. The bottom-up approach emphasizes low level policy implementers, but policy learners must not attempt to frustrate the goals of top policymakers.

In America, the No Child Left Behind Act (NCLB) adopted policyies that would have benefited from bottom-up perspectives. When NCLB was passed, many states struggled to figure out what was required. All states had to get their education plans approved by U.S. Department of Education. Once the education plan was approved, each state had to incorporate NCLB into the state's framework of educational governance and to use the legislation to achieve the state's own goals. If the U.S. federal government had consulted with each state about its education policies, performances, and future goals, teachers and the government would have had a better understanding on what policy objectives were achievable.

Policy learning has not been embraced in some countries. Some countries that were once colonized fear that embracing policies recommended by outsiders will allow other countries to exploit their resources. Rockefeller (1966) claimed that in Latin America in the early 1960s free-market policies were in competition with communist propaganda in Latin American countries. At the time American business were claimed to be exploiting the people and their resources. However, companies such as Chase Manhattan Bank launched a program in Panama to improve cattle raising by training ranchersr to follow the scientific advances of seeding, feeding and breeding cattle more effectively. This process improved the quality of beef, which encouraged higher meat consumption, improved dietary standards and made Panama a beef exporter.

European countries created the Euro to simplify trading between European Union countries. Adopting the Euro would remove currency risk and the cost of currency conversion, and provide a common monetary policy among members. Policy learning took places as more European countries learned that joining the Eurozone would give them access to other markets. Citizens of EU member countries could travel to other EU countries within the Schengen area without transiting a border checkpoint. The learning did not reach all policy sectors. Some EU countries kept their budgets in near balance amid strong growth and employment, while others' budgets were so far out of balance that their overall debt created fears about their ability to make their payments. 


</doc>
<doc id="54737875" url="https://en.wikipedia.org/wiki?curid=54737875" title="Index of articles related to terms of service and privacy policies">
Index of articles related to terms of service and privacy policies

This is a list of articles about terms of service and privacy policies. These are also called terms of use, and are rules one must agree to, in order to use a service. The articles fall in two main categories: descriptions of terms used for specific companies or products, and descriptions of different kinds of terms in general. Articles on companies vary widely in the amount of detail they give on terms of service. Annotations show what is available in the article on each company, and need to be updated as those articles are improved.

Terms of service are regularly the subject of news articles throughout the English-language press, such as in the US, UK, Africa, India, Singapore, and Australia. Terms of service are also addressed in a widely reviewed documentary, academic research, and legal research.














</doc>
<doc id="29209828" url="https://en.wikipedia.org/wiki?curid=29209828" title="Imperative mandate">
Imperative mandate

The Imperative mandate is a political system in which "representatives enact policies in accordance with mandates and can be recalled by people’s assemblies". It requires a context in which "power is not monopolized by the state, but distributed in a plurality of municipalities and assemblies with specific political authority". 

The imperative mandate goes back to the Middle Ages. It was embraced by the revolutionary assemblies in Paris in 1793 but then banned by the royalist members of the French National Assembly of 1789 to block greater influence by the people. It was also rejected in the American Revolution. 

It was embraced in the Paris Commune and by the Council Communism movement.

The Imperative Mandate has been used by the United Democratic Front and Abahlali baseMjondolo in South Africa as well as the Zapatistas in Mexico.




</doc>
<doc id="1192971" url="https://en.wikipedia.org/wiki?curid=1192971" title="Wicked problem">
Wicked problem

A wicked problem is a problem that is difficult or impossible to solve because of incomplete, contradictory, and changing requirements that are often difficult to recognize. The use of the term "wicked" here has come to denote resistance to resolution, rather than evil. Another definition is "a problem whose social complexity means that it has no determinable stopping point". Moreover, because of complex interdependencies, the effort to solve one aspect of a wicked problem may reveal or create other problems.

The phrase was originally used in social planning. Its modern sense was introduced in 1967 by C. West Churchman in a guest editorial Churchman wrote in the journal "Management Science", responding to a previous use of the term by Horst Rittel. Churchman discussed the moral responsibility of operations research "to inform the manager in what respect our 'solutions' have failed to tame his wicked problems". Rittel and Melvin M. Webber formally described the concept of wicked problems in a 1973 treatise, contrasting "wicked" problems with relatively "tame", soluble problems in mathematics, chess, or puzzle solving.

Rittel and Webber's 1973 formulation of wicked problems in social policy planning specified ten characteristics:


Conklin later generalized the concept of problem wickedness to areas other than planning and policy; Conklin's defining characteristics are:


Classic examples of wicked problems include economic, environmental, and political issues. A problem whose solution requires a great number of people to change their mindsets and behavior is likely to be a wicked problem. Therefore, many standard examples of wicked problems come from the areas of public planning and policy. These include global climate change, natural hazards, healthcare, the AIDS epidemic, pandemic influenza, international drug trafficking, nuclear weapons, nuclear energy, waste and social injustice.

In recent years, problems in many areas have been identified as exhibiting elements of wickedness; examples range from aspects of design decision making and knowledge management to business strategy.

Rittel and Webber coined the term in the context of problems of social policy, an arena in which a purely scientific-engineering approach cannot be applied because of the lack of a clear problem definition and differing perspectives of stakeholders. In their words, 

Thus wicked problems are also characterised by the following:


Although Rittel and Webber framed the concept in terms of social policy and planning, wicked problems occur in any domain involving stakeholders with differing perspectives. Recognising this, Rittel and Kunz developed a technique called Issue-Based Information System (IBIS), which facilitates documentation of the rationale behind a group decision in an objective manner.

A recurring theme in research and industry literature is the connection between wicked problems and design. Design problems are typically wicked because they are often ill defined (no prescribed way forward), involve stakeholders with different perspectives, and have no "right" or "optimal" solution. Thus wicked problems cannot be solved by the application of standard (or known) methods; they demand creative solutions.

Wicked problems cannot be tackled by the traditional approach in which problems are defined, analysed and solved in sequential steps. The main reason for this is that there is no clear problem definition of wicked problems. In a paper published in 2000, Nancy Roberts identified the following strategies to cope with wicked problems:


In his 1972 paper, Rittel hints at a collaborative approach; one which attempts "to make those people who are being affected into participants of the planning process. They are not merely asked but actively involved in the planning process". A disadvantage of this approach is that achieving a shared understanding and commitment to solving a wicked problem is a time-consuming process. Another difficulty is that, in some matters, at least one group of people may hold an absolute belief that necessarily contradicts other absolute beliefs held by other groups. Collaboration then becomes impossible until one set of beliefs is relativized or abandoned entirely. 

Research over the last two decades has shown the value of computer-assisted argumentation techniques in improving the effectiveness of cross-stakeholder communication. The technique of dialogue mapping has been used in tackling wicked problems in organizations using a collaborative approach. More recently, in a four-year study of interorganizational collaboration across public, private, and voluntary sectors, steering by government was found to perversely undermine a successful collaboration, producing an organizational crisis which led to the collapse of a national initiative.

In "Wholesome Design for Wicked Problems", Robert Knapp stated that there are ways forward in dealing with wicked problems: 

Examining networks designed to tackle wicked problems in health care, such as caring for older people or reducing sexually transmitted infections, Ferlie and colleagues suggest that managed networks may be the "least bad" way of "making wicked problems governable".

A range of approaches called "problem structuring methods" (PSMs) have been developed in operations research since the 1970s to address problems involving complexity, uncertainty and conflict. PSMs are usually used by a group of people in collaboration (rather than by a solitary individual) to create a consensus about, or at least to facilitate negotiations about, what needs to change. Some widely adopted PSMs include soft systems methodology, the strategic choice approach, and strategic options development and analysis (SODA).

Russell L. Ackoff wrote about complex problems as messes: "Every problem interacts with other problems and is therefore part of a set of interrelated problems, a system of problems... I choose to call such a system a mess."

Extending Ackoff, Robert Horn says that "a Social Mess is a set of interrelated problems and other messes. Complexity—systems of systems—is among the factors that makes Social Messes so resistant to analysis and, more importantly, to resolution."

According to Horn, the defining characteristics of a social mess are:

E. F. Schumacher distinguishes between "divergent and convergent problems" in his book "A Guide for the Perplexed". Convergent problems are those for which attempted solutions gradually converge on one solution or answer. Divergent problems are those for which different answers appear to increasingly contradict each other all the more they are elaborated, requiring a different approach involving faculties of a higher order like love and empathy.

In 1990 DeGrace and Stahl introduced the concept of wicked problems to software development. In the last decade, other computer scientists have pointed out that software development shares many properties with other design practices (particularly that people-, process-, and technology-problems have to be considered equally), and have incorporated Rittel's concepts into their software design methodologies. The design and integration of complex software-defined services that use the Web (web services) can be construed as an evolution from previous models of software design, and therefore becomes a wicked problem also.

Kelly Levin, Benjamin Cashore, Graeme Auld and Steven Bernstein introduced the distinction between "wicked problems" and "super wicked problems" in a 2007 conference paper, which was followed by a 2012 journal article in "Policy Sciences". In their discussion of global climate change, they define super wicked problems as having the following additional characteristics:


While the items that define a wicked problem relate to the problem itself, the items that define a super wicked problem relate to the agent trying to solve it. Global warming is a super wicked problem, and the need to intervene to tend to our longer term interests has also been taken up by others, including Richard Lazarus.



</doc>
<doc id="56290147" url="https://en.wikipedia.org/wiki?curid=56290147" title="Sexuality Policy Watch">
Sexuality Policy Watch

Sexuality Policy Watch (SPW) is a global forum of researchers and activists working on sexual rights issues and policies across the world. The forum was launched in 2002 as the International Working Group on Sexuality and Social Policy (IWGSSP), but changed its name to Sexuality Policy Watch in 2006. 

Since its inception, SPW has conducted research on trends in sexuality, advocated to prevent violence against women, built partnerships with sexual rights groups, and published key policy analyses. Thus, together with the Latin American Committee for the Rights of Women /Brazil (CLADEM), the Commission for Citizenship and Reproduction (CCR), PROMSEX - Center for the Promotion and Defense of Sexual and Reproductive Rights and the National Rapporteurship for the Human Right to Sexual and Reproductive Health in Brazil, SPW published a report investigating press claims in 2012 about the earlier forced sterilisation campaigns in Peru.

SPW is hosted at the Brazilian Interdisciplinary AIDS association or Associação Brasileira Interdisciplinar de AIDS (ABIA) in Rio de Janeiro, Brazil.

The SPW co-chairs are Sonia Corrêa from Brazil and Richard Parker from USA.



</doc>
<doc id="375091" url="https://en.wikipedia.org/wiki?curid=375091" title="Policy">
Policy

A policy is a deliberate system of principles to guide decisions and achieve rational outcomes. A policy is a statement of intent, and is implemented as a procedure or protocol. Policies are generally adopted by a governance body within an organization. Policies can assist in both "subjective" and "objective" decision making. Policies to assist in subjective decision making usually assist senior management with decisions that must be based on the relative merits of a number of factors, and as a result are often hard to test objectively, e.g. work-life balance policy. In contrast policies to assist in objective decision making are usually operational in nature and can be objectively tested, e.g. password policy.

The term may apply to government, private sector organizations and groups, as well as individuals. Presidential executive orders, corporate privacy policies, and parliamentary rules of order are all examples of policy. Policy differs from rules or law. While law can compel or prohibit behaviors (e.g. a law requiring the payment of taxes on income), policy merely guides actions toward those that are most likely to achieve a desired outcome.

Policy or policy study may also refer to the process of making important organizational decisions, including the identification of different alternatives such as programs or spending priorities, and choosing among them on the basis of the impact they will have. Policies can be understood as political, managerial, financial, and administrative mechanisms arranged to reach explicit goals. In public corporate finance, a critical accounting policy is a policy for a firm/company or an industry that is considered to have a notably high subjective element, and that has a material impact on the financial statements.

The intended effects of a policy vary widely according to the organization and the context in which they are made. Broadly, policies are typically instituted to avoid some negative effect that has been noticed in the 
organization, or to seek some positive benefit. 

Corporate purchasing policies provide an example of how organizations attempt to avoid negative effects. Many large companies have policies that all purchases above a certain value must be performed through a purchasing process. By requiring this standard purchasing process through policy, the organization can limit waste and standardize the way purchasing is done. 

The State of California provides an example of benefit-seeking policy. In recent years, the numbers of hybrid cars in California has increased dramatically, in part because of policy changes in Federal law that provided USD $1,500 in tax credits (since phased out) as well as the use of high-occupancy vehicle lanes to hybrid owners (no loew hybrid vehicles). In this case, the organization (state and/or federal government) created an effect (increased ownership and use of hybrid vehicles) through policy (tax breaks, highway lanes). 

Policies frequently have side effects or unintended consequences. Because the environments that policies seek to influence or manipulate are typically complex adaptive systems (e.g. governments, societies, large companies), making a policy change can have counterintuitive results. For example, a government may make a policy decision to raise taxes, in hopes of increasing overall tax revenue. Depending on the size of the tax increase, this may have the overall effect of reducing tax revenue by causing capital flight or by creating a rate so high that citizens are deterred from earning the money that is taxed. (See the Laffer curve.) 

The policy formulation process theoretically includes an attempt to assess as many areas of potential policy impact as possible, to lessen the chances that a given policy will have unexpected or unintended consequences. 

In political science, the policy cycle is a tool used for the analyzing of the development of a policy item. It can also be referred to as a "stagist approach", "stages heuristic" or "stages approach". It is thus a rule of thumb rather than the actual reality of how policy is created, but has been influential in how political scientists looked at policy in general. It was developed as a theory from Harold Lasswell's work.

One version by James E. Anderson, in his "Public Policy-Making" (1974) has the following stages:

An eight step policy cycle is developed in detail in "The Australian Policy Handbook" by Peter Bridgman and Glyn Davis: (now with Catherine Althaus in its 4th and 5th editions)


The Althaus, Bridgman & Davis model is heuristic and iterative. It is and not meant to be or predictive. Policy cycles are typically characterized as adopting a classical approach, and tend to describe processes from the perspective of policy decision makers. Accordingly, some postpositivist academics challenge cyclical models as unresponsive and unrealistic, preferring systemic and more complex models. They consider a broader range of actors involved in the policy space that includes civil society organisations, the media, intellectuals, think tanks or policy research institutes, corporations, lobbyists, etc.

Policies are typically promulgated through official written documents. Policy documents often come with the endorsement or signature of the executive powers within an organization to legitimize the policy and demonstrate that it is considered in force. Such documents often have standard formats that are particular to the organization issuing the policy. While such formats differ in form, policy documents usually contain certain standard components including :

Some policies may contain additional sections, including:

The American political scientist Theodore J. Lowi proposed four types of policy, namely distributive, redistributive, regulatory and constituent in his article 'Four systems of Policy, Politics and Choice' and in 'American Business, Public Policy, Case Studies and Political Theory'.
Policy addresses the intent of the organization, whether government, business, professional, or voluntary. Policy is intended to affect the 'real' world, by guiding the decisions that are made. Whether they are formally written or not, most organizations have identified policies.

Policies may be classified in many different ways. The following is a sample of several different types of policies broken down by their effect on members of the organization.

Distributive policies extend goods and services to members of an organization, as well as distributing the costs of the goods/services amongst the members of the organization. Examples include government policies that impact spending for welfare, public education, highways, and public safety, or a professional organization's benefits plan.

Regulatory policies, or mandates, limit the discretion of individuals and agencies, or otherwise compel certain types of behavior. These policies are generally thought to be best applied when good behavior can be easily defined and bad behavior can be easily regulated and punished through fines or sanctions. An example of a fairly successful public regulatory policy is that of a highway speed limit.

Constituent policies create executive power entities, or deal with laws. Constituent policies also deal with Fiscal Policy in some circumstances.

Policies are dynamic; they are not just static lists of goals or laws. Policy blueprints have to be implemented, often with unexpected results. Social policies are what happens 'on the ground' when they are implemented, as well as what happens at the decision making or legislative stage.

When the term policy is used, it may also refer to:

The actions the organization actually takes may often vary significantly from stated policy. This difference is sometimes caused by political compromise over policy, while in other situations it is caused by lack of policy implementation and enforcement. Implementing policy may have unexpected results, stemming from a policy whose reach extends further than the problem it was originally crafted to address. Additionally, unpredictable results may arise from selective or idiosyncratic enforcement of policy.

Types of policy analysis include: 


These qualifiers can be combined, so one could, for example, have a stationary-memoryless-index policy.







</doc>
<doc id="59676043" url="https://en.wikipedia.org/wiki?curid=59676043" title="Media policy">
Media policy

Media policy / M. politics is a term describing all legislation and political action directed towards regulating the media, especially mass media, and the media industry. Those actions will usually be prompted by pressures from public opinion or from industry interest groups.

Print media, public radio and television broadcasting, mobile communications all converge in the digital infrastructure. This digitalisation produces markets that still lack consistent and rigorous regulation. In instances where regulations exist, technical innovations outpace and overtake existing rules and give rise to illegal activities like copyright violations. This has to be dealt with to defend intellectual property rights (see e.g. Digital Economy Act 2010)

Media politics is the subject of studies in media research and cultural studies.
Liberal media policy is adversely affected by the fact that political success itself hinges critically on favorable comments in the media, see politico-media complex.


</doc>
<doc id="58929051" url="https://en.wikipedia.org/wiki?curid=58929051" title="Reshaping Cultural Policies">
Reshaping Cultural Policies

Reshaping Cultural Policies (styled as Re|Shaping Cultural Policies) is a report series published by UNESCO which monitors the implementation of the UNESCO Convention on the Protection and Promotion of the Diversity of Cultural Expressions (2005). The 2005 UNESCO Convention encourages its 146 parties to introduce policies for culture within a global context and commitment to protect and promote the diversity of cultural expressions. The second and most recent report (2018) subtitled “Advancing Creativity for Development” follows the first report (2015) with the subtitle “A Decade Promoting the Diversity of Cultural Expressions for Development”.

Primarily, the report series draws on reports of all parties to the Convention submitted every four years in which they present and describe the actions they have taken in order to implement the Convention. These reports are called quadrennial periodic reports (QPRs). In addition, the report series includes the analysis of other both governmental and non-governmental sources. In general, the report investigates how implementing the convention reshapes cultural policies. Additionally, it provides evidence of how the implementation process contributes to attaining the United Nations 2030 Sustainable Development Goals (SDGs) to end poverty, protect the planet, and ensure prosperity for every human being. The report series also analyses trends and issues concerning the creative economy, which currently is worth $2,250 billion and employs 30 million people worldwide.

The report puts forward a set of policy recommendations for the future, addressing the adaptation of cultural policies to rapid change in the digital environment, based on human rights and fundamental freedoms of expression. 

“Each Report is not an end-result, but a tool to be used in a long-term process that includes the forging of spaces for policy dialogue, reinforcing stakeholders’ capacities to work together to generate data and information, and advocate for policy innovation both nationally and globally.” 

The reports are published in English, French, Spanish, Russian, Portuguese, Arabic, Chinese, Indonesian, Vietnamese and German. UNESCO is the lead institutional author of the Global Report series and coordinates a broader network of independent experts who author chapters. 

In line with the Parties’ quadrennial periodic reporting, the series is produced every four years. The first cycle spanned the years 2012-2015 and the second runs from 2016 to 2019. Accordingly, the third publication will take place in December 2021.

The 2005 Convention is an international standard setting instrument providing a framework for the governance of culture. In this context, governance of culture refers to policies and measures governments establish to regulate, to promote and to protect all forms of creativity and artistic expressions. The most recent UNESCO Convention in the field of culture and ratified by 146 parties, it is the first international legal tool to encourage governments to invest in creativity. It frames the formulation and implementation of different types of legislative, regulatory, institutional and financial interventions to promote the emergence of dynamic cultural and creative industry sectors around the world.

Within the context of the 2005 Convention, the diversity of cultural expressions "″"refers to the manifold ways in which the cultures of groups and societies find expression. These expressions are passed on within and among groups and societies"″". Specifically, the Convention understands cultural expressions as all forms of creativity and artistic expressions, such as in cinema/audiovisual arts, design, digital arts, music, performing arts, publishing and the visual arts. The 2005 Convention was "since its beginnings, permeated by a material and economic perspective of cultural expressions, focused on the production and consumption of cultural goods and services, with a view to promote more balanced exchanges and sustainable development that takes into account cultural diversity concerns." 

The implementation of the 2005 Convention aims to contribute to achieving several Sustainable Development Goals (SDGs), precisely SDG 4 (Quality Education), SDG 5 (Gender Equality), SDG 8 (Decent Work and Economic Growth), SDG 10 (Reduced Inequalities), SDG 16 (Peace, Justice and Strong Institutions) and SDG 17 (Partnerships for the Goals). The implementation process identifies investing in creativity as a priority for sustainable development. At the global level, the convention calls for countries to provide financial assistance for creativity through their Official Development Assistance (ODA) by investing in the Convention’s International Fund for Cultural Diversity. Additionally, UNESCO, through the 2005 Convention, offers technical assistance to strengthen human and institutional capacities in developing countries.

The director general of UNESCO, Audrey Azoulay referring to the UNESCO General Conference's conviction that cultural activities, goods and services have both an economic and a cultural nature stated: ″[c]ulture is not a commodity: it carries values and identities, it gives markers to live together in a globalized world. Our role is to encourage, question, collect data, to understand and energize creative channels, to encourage the mobility of artists, to stimulate a rapidly changing sector in the new digital environment″. Annika Markovic, Ambassador and Permanent Delegate of Sweden to UNESCO, in 2018, claimed that the report is "“the only global document that presents an overview of cultural development world-wide and monitors state action to protect and promote the diversity of cultural expressions at all levels.”" 

The following aspects thematically summarize the core findings identified by the 2018 report with regard to the implementation of the 2005 UNESCO Convention.

For the first time, national development plans and strategies integrate culture, mainly of countries in the Global South. As a result, cities seem to invest more and more in cultural industries for development. The UN’s 2030 Agenda recognized the role of creativity in sustainable development in the implementation of the SDGs. However, the share of development aid spent on culture today is the lowest it has been in over 10 years. 

According to the report, digital revenues make up 50% of the recorded music market, growing almost 18% over the past year due to a sharp increase in the share of streaming revenues. 

The report states that the internet transforms the cultural value chain into a network platform. E-commerce challenges both culture and trade policies that intend to promote the diversity of cultural expressions. It articulates the urgency to improve data collection on revenues generated through digital channels in order to design better policies and negotiate fair trade agreements. The report claims that monitoring the relationship between large platforms, Big Data, artificial intelligence and the diversity of cultural expressions is crucial to ensure that a variety of distribution platforms and providers promote and protect future artistic creations. 

As informed by the report, attacks against artists have increased in the past years, including in the digital environment where surveillance and online trolling pose new threats to artistic freedom. In 2016, 430 cases were reported around the world (compared to 340 in 2015 and 90 in 2014). Musicians are the most threatened group, while authors also often become a target. In 2016, attacks against authors occurred most often in the Asia-Pacific Region (80 cases), the Middle East and North Africa (51 cases) and Europe (47 cases). The report reveals that meanwhile, there exists an increased awareness with regard to such threats leading to a larger number of initiatives to support the social and economic rights of artists, particularly in African countries. While there exists legal action to affirm the freedom of expression for artists, other laws addressing terrorism and state security repress artistic expressions. 

The report states that half of the persons working in the cultural and creative industries are female. However, a gender gap persists worldwide concerning equal pay, access to funding and prices charged for creative works. Consequently, women remain under-represented in key creative roles and are outnumbered in decision-making positions. Women make up only 34% of Ministers for Culture (compared to 24% in 2005) and only 31% of national arts program directors. Generally, women are represented in specific cultural fields such as arts education and training (60%), book publishing and press (54%), audiovisual and interactive media (26%), as well as design and creative services (33%).

The report demonstrates that predominantly restrictions in terms of mobility represent great challenges to persons pursuing careers in the cultural and creative industries, specifically to those from the Global South. It reveals that a holder of a German passport can travel to 176 countries without a visa while a holder of an Afghan passport can only travel to 24 countries without a visa. As a matter of fact, artists and cultural professionals need to travel to perform, to reach new audiences or to attend a residency or to engage in networking. The report exposes that travel restrictions, including difficulties in obtaining visas oftentimes impedes artists from the Global South to participate in art biennales or film festivals, even when invited to receive an award or to promote their works. 

As stated in the report, the 2005 Convention provides legitimacy for the formulation of cultural policies and their adaptation to changing circumstances and needs. The report underscores that collaborative governance and multi-stakeholder policy making have progressed, notably in some developing countries particularly in the creative economy and cultural education. As a result, parties to the Convention have made considerable progress in fostering digital arts creation, supporting creative entrepreneurship, accelerating the modernization of cultural sectors, promoting distribution and updating copyright legislation. However, the report also reveals a lack in civil society participation in policy making. It underlines the urgency for more effort to ensure the creation of open, transparent and participatory policy processes in order to involve civil society participation in policy making. 

In accordance with the report, the 2005 Convention formally recognizes that cultural goods and services not only have important economic value, but also convey identities, meanings and values. As a consequence, at least eight bilateral and regional free trade agreements concluded between 2015 and 2017 have introduced cultural clauses or list of commitments that promote the objectives and principles of the 2005 Convention. Despite the lack of the promotion of the objectives and principles of the 2005 Convention with regard to the negotiation of mega-regional partnership agreements, some Parties to the Trans Pacific Partnership (TTP) have succeeded in introducing important cultural reservations to protect and promote the diversity of cultural expressions. 

The report’s primary objective is “to provide key actors with better knowledge on how to support evidence-based policy, and to strengthen informed, transparent and participatory systems of governance for culture.” It aims to motivate governments and civil society actors to integrate findings and recommendations into their national cultural policy and development strategies and frameworks. 

Following the findings presented above, the implementation of the 2005 Convention "introduce[s] a range of different policy strategies for integrating culture into development processes" and culture is increasingly regarded as "an economic asset in pursuing sustainable development". Based on its analysis and findings, the Global Report of 2018 suggests the following road map for the parties to the 2005 Convention. Accordingly, parties could tackle major challenges in the implementation of the 2005 Convention by:


Speaking about the visibility of the progress in cultural policies shown by the report series, Bárbara Lovrinić stated that "“[u]nfortunately, where UNESCO is concerned, there is a lack of promotion in the media in general. In the long term, the report could have a positive impact on these issues, which would be enhanced if the public were made more aware of such work.”" She also points out that there is ""a risk that many people will not dwell on the 2005 Convention and the Sustainable Development Goals unless they are already somewhat familiar with the topic.”" With reference to the title of the report series, she concludes that ""cultural policy-making is still far from being reshaped, for it takes a serious amount of time to yield valuable results.”"




</doc>
<doc id="9986" url="https://en.wikipedia.org/wiki?curid=9986" title="Outline of education">
Outline of education

The following outline is provided as an overview of and topical guide to education:

Education – in the general sense is any act or experience that has a formative effect on the mind, character, or physical ability of an individual. In its technical sense, education is the process by which society deliberately transmits its accumulated knowledge, skills, and values from one generation to another. Education can also be defined as the process of becoming an educated person.










History of education


















</doc>
<doc id="9252" url="https://en.wikipedia.org/wiki?curid=9252" title="Education">
Education

Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, beliefs, and habits. Educational methods include storytelling, discussion, teaching, training, and directed research. Education frequently takes place under the guidance of educators and also learners may also educate themselves. Education can take place in formal or informal settings and any experience that has a formative effect on the way one thinks, feels, or acts may be considered educational. The methodology of teaching is called pedagogy.

Formal education is commonly divided formally into such stages as preschool or kindergarten, primary school, secondary school and then college, university, or apprenticeship.

A right to education has been recognized by some governments and the United Nations. In most regions, education is compulsory up to a certain age.

Etymologically, the word "education" is derived from the Latin word "ēducātiō" ("A breeding, a bringing up, a rearing") from "ēducō" ("I educate, I train") which is related to the homonym "ēdūcō" ("I lead forth, I take out; I raise up, I erect") from "ē-" ("from, out of") and "dūcō" ("I lead, I conduct").

Education began in prehistory, as adults trained the young in the knowledge and skills deemed necessary in their society. In pre-literate societies, this was achieved orally and through imitation. Story-telling passed knowledge, values, and skills from one generation to the next. As cultures began to extend their knowledge beyond skills that could be readily learned through imitation, formal education developed. Schools existed in Egypt at the time of the Middle Kingdom.

Plato founded the Academy in Athens, the first institution of higher learning in Europe. The city of Alexandria in Egypt, established in 330 BCE, became the successor to Athens as the intellectual cradle of Ancient Greece. There, the great Library of Alexandria was built in the 3rd century BCE. European civilizations suffered a collapse of literacy and organization following the fall of Rome in CE 476.

In China, Confucius (551–479 BCE), of the State of Lu, was the country's most influential ancient philosopher, whose educational outlook continues to influence the societies of China and neighbours like Korea, Japan, and Vietnam. Confucius gathered disciples and searched in vain for a ruler who would adopt his ideals for good governance, but his Analects were written down by followers and have continued to influence education in East Asia into the modern era.

The Aztecs also had a well-developed theory about education, which has an equivalent word in Nahuatl called "tlacahuapahualiztli." It means "the art of raising or educating a person" or "the art of strengthening or bringing up men." This was a broad conceptualization of education, which prescribed that it begins at home, supported by formal schooling, and reinforced by community living. Historians cite that formal education was mandatory for everyone regardless of social class and gender. There was also the word "neixtlamachiliztli", which is "the act of giving wisdom to the face." These concepts underscore a complex set of educational practices, which was oriented towards communicating to the next generation the experience and intellectual heritage of the past for the purpose of individual development and his integration into the community.

After the Fall of Rome, the Catholic Church became the sole preserver of literate scholarship in Western Europe. The church established cathedral schools in the Early Middle Ages as centres of advanced education. Some of these establishments ultimately evolved into medieval universities and forebears of many of Europe's modern universities. During the High Middle Ages, Chartres Cathedral operated the famous and influential Chartres Cathedral School. The medieval universities of Western Christendom were well-integrated across all of Western Europe, encouraged freedom of inquiry, and produced a great variety of fine scholars and natural philosophers, including Thomas Aquinas of the University of Naples, Robert Grosseteste of the University of Oxford, an early expositor of a systematic method of scientific experimentation, and Saint Albert the Great, a pioneer of biological field research. Founded in 1088, the University of Bologne is considered the first, and the oldest continually operating university.

Elsewhere during the Middle Ages, Islamic science and mathematics flourished under the Islamic caliphate which was established across the Middle East, extending from the Iberian Peninsula in the west to the Indus in the east and to the Almoravid Dynasty and Mali Empire in the south.

The Renaissance in Europe ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly. The European Age of Empires saw European ideas of education in philosophy, religion, arts and sciences spread out across the globe. Missionaries and scholars also brought back new ideas from other civilizations – as with the Jesuit China missions who played a significant role in the transmission of knowledge, science, and culture between China and Europe, translating works from Europe like Euclid's Elements for Chinese scholars and the thoughts of Confucius for European audiences. The Enlightenment saw the emergence of a more secular educational outlook in Europe.

In most countries today, full-time education, whether at school or otherwise, is compulsory for all children up to a certain age. Due to this the proliferation of compulsory education, combined with population growth, UNESCO has calculated that in the next 30 years more people will receive formal education than in all of human history thus far.

Formal education occurs in a structured environment whose explicit purpose is teaching students. Usually, formal education takes place in a school environment with classrooms of multiple students learning together with a trained, certified teacher of the subject. Most school systems are designed around a set of values or ideals that govern all educational choices in that system. Such choices include curriculum, organizational models, design of the physical learning spaces (e.g. classrooms), student-teacher interactions, methods of assessment, class size, educational activities, and more.

Preschools provide education from ages approximately three to seven, depending on the country when children enter primary education. These are also known as nursery schools and as kindergarten, except in the US, where kindergarten is a term often used to describe the earliest levels of primary education. Kindergarten "provide[s] a child-centred, preschool curriculum for three- to seven-year-old children that aim[s] at unfolding the child's physical, intellectual, and moral nature with balanced emphasis on each of them."

Primary (or elementary) education consists of the first five to seven years of formal, structured education. In general, primary education consists of six to eight years of schooling starting at the age of five or six, although this varies between, and sometimes within, countries. Globally, around 89% of children aged six to twelve are enrolled in primary education, and this proportion is rising. Under the Education For All programs driven by UNESCO, most countries have committed to achieving universal enrollment in primary education by 2015, and in many countries, it is compulsory. The division between primary and secondary education is somewhat arbitrary, but it generally occurs at about eleven or twelve years of age. Some education systems have separate middle schools, with the transition to the final stage of secondary education taking place at around the age of fourteen. Schools that provide primary education, are mostly referred to as "primary schools "or "elementary schools". Primary schools are often subdivided into infant schools and junior school.

In India, for example, compulsory education spans over twelve years, with eight years of elementary education, five years of primary schooling and three years of upper primary schooling. Various states in the republic of India provide 12 years of compulsory school education based on a national curriculum framework designed by the National Council of Educational Research and Training.

In most contemporary educational systems of the world, secondary education comprises the formal education that occurs during adolescence. It is characterized by transition from the typically compulsory, comprehensive primary education for minors, to the optional, selective tertiary, "postsecondary", or "higher" education (e.g. university, vocational school) for adults. Depending on the system, schools for this period, or a part of it, may be called secondary or high schools, gymnasiums, lyceums, middle schools, colleges, or vocational schools. The exact meaning of any of these terms varies from one system to another. The exact boundary between primary and secondary education also varies from country to country and even within them but is generally around the seventh to the tenth year of schooling.

Secondary education occurs mainly during the teenage years. In the United States, Canada, and Australia, primary and secondary education together are sometimes referred to as K-12 education, and in New Zealand Year 1–13 is used. The purpose of secondary education can be to give common knowledge, to prepare for higher education, or to train directly in a profession.

Secondary education in the United States did not emerge until 1910, with the rise of large corporations and advancing technology in factories, which required skilled workers. In order to meet this new job demand, high schools were created, with a curriculum focused on practical job skills that would better prepare students for white collar or skilled blue collar work. This proved beneficial for both employers and employees, since the improved human capital lowered costs for the employer, while skilled employees received higher wages.

Secondary education has a longer history in Europe, where grammar schools or academies date from as early as the 16th century, in the form of public schools, fee-paying schools, or charitable educational foundations, which themselves date even further back.

Community colleges offer another option at this transitional stage of education. They provide nonresidential junior college courses to people living in a particular area.

Higher education, also called tertiary, third stage, or postsecondary education, is the non-compulsory educational level that follows the completion of a school such as a high school or secondary school. Tertiary education is normally taken to include undergraduate and postgraduate education, as well as vocational education and training. Colleges and universities mainly provide tertiary education. Collectively, these are sometimes known as tertiary institutions. Individuals who complete tertiary education generally receive certificates, diplomas, or academic degrees.

Higher education typically involves work towards a degree-level or foundation degree qualification. In most developed countries, a high proportion of the population (up to 50%) now enter higher education at some time in their lives. Higher education is therefore very important to national economies, both as a significant industry in its own right and as a source of trained and educated personnel for the rest of the economy.

University education includes teaching, research, and social services activities, and it includes both the undergraduate level (sometimes referred to as tertiary education) and the graduate (or postgraduate) level (sometimes referred to as graduate school). Some universities are composed of several colleges.

One type of university education is a liberal arts education, which can be defined as a "college or university curriculum aimed at imparting broad general knowledge and developing general intellectual capacities, in contrast to a professional, vocational, or technical curriculum." Although what is known today as liberal arts education began in Europe, the term "liberal arts college" is more commonly associated with institutions in the United States such as Williams College or Barnard College.

Vocational education is a form of education focused on direct and practical training for a specific trade or craft. Vocational education may come in the form of an apprenticeship or internship as well as institutions teaching courses such as carpentry, agriculture, engineering, medicine, architecture and the arts.

In the past, those who were disabled were often not eligible for public education. Children with disabilities were repeatedly denied an education by physicians or special tutors. These early physicians (people like Itard, Seguin, Howe, Gallaudet) set the foundation for special education today. They focused on individualized instruction and functional skills. In its early years, special education was only provided to people with severe disabilities, but more recently it has been opened to anyone who has experienced difficulty learning.

While considered "alternative" today, most alternative systems have existed since ancient times. After the public school system was widely developed beginning in the 19th century, some parents found reasons to be discontented with the new system. Alternative education developed in part as a reaction to perceived limitations and failings of traditional education. A broad range of educational approaches emerged, including alternative schools, self learning, homeschooling, and unschooling. Example alternative schools include Montessori schools, Waldorf schools (or Steiner schools), Friends schools, Sands School, Summerhill School, Walden's Path, The Peepal Grove School, Sudbury Valley School, Krishnamurti schools, and open classroom schools. Charter schools are another example of alternative education, which have in the recent years grown in numbers in the US and gained greater importance in its public education system.

In time, some ideas from these experiments and paradigm challenges may be adopted as the norm in education, just as Friedrich Fröbel's approach to early childhood education in 19th-century Germany has been incorporated into contemporary kindergarten classrooms. Other influential writers and thinkers have included the Swiss humanitarian Johann Heinrich Pestalozzi; the American transcendentalists Amos Bronson Alcott, Ralph Waldo Emerson, and Henry David Thoreau; the founders of progressive education, John Dewey and Francis Parker; and educational pioneers such as Maria Montessori and Rudolf Steiner, and more recently John Caldwell Holt, Paul Goodman, Frederick Mayer, George Dennison, and Ivan Illich.

Indigenous education refers to the inclusion of indigenous knowledge, models, methods, and content within formal and non-formal educational systems. Often in a post-colonial context, the growing recognition and use of indigenous education methods can be a response to the erosion and loss of indigenous knowledge and language through the processes of colonialism. Furthermore, it can enable indigenous communities to "reclaim and revalue their languages and cultures, and in so doing, improve the educational success of indigenous students."

Informal learning is one of three forms of learning defined by the Organisation for Economic Co-operation and Development (OECD). Informal learning occurs in a variety of places, such as at home, work, and through daily interactions and shared relationships among members of society. For many learners, this includes language acquisition, cultural norms, and manners.

In informal learning, there is often a reference person, a peer or expert, to guide the learner. If learners have a personal interest in what they are informally being taught, learners tend to expand their existing knowledge and conceive new ideas about the topic being learned. For example, a museum is traditionally considered an informal learning environment, as there is room for free choice, a diverse and potentially non-standardized range of topics, flexible structures, socially rich interaction, and no externally imposed assessments.

While informal learning often takes place outside educational establishments and does not follow a specified curriculum, it can also occur within educational settings and even during formal learning situations. Educators can structure their lessons to directly utilize their students informal learning skills within the education setting.

In the late 19th century, education through play began to be recognized as making an important contribution to child development. In the early 20th century, the concept was broadened to include young adults but the emphasis was on physical activities. L.P. Jacks, also an early proponent of lifelong learning, described education through recreation: "A master in the art of living draws no sharp distinction between his work and his play, his labour and his leisure, his mind and his body, his education and his recreation. He hardly knows which is which. He simply pursues his vision of excellence through whatever he is doing and leaves others to determine whether he is working or playing. To himself, he always seems to be doing both. Enough for him that he does it well." Education through recreation is the opportunity to learn in a seamless fashion through all of life's activities. The concept has been revived by the University of Western Ontario to teach anatomy to medical students.

Autodidacticism (also autodidactism) is a term used to describe self-directed learning. One may become an autodidact at nearly any point in one's life. Notable autodidacts include Abraham Lincoln (U.S. president), Srinivasa Ramanujan (mathematician), Michael Faraday (chemist and physicist), Charles Darwin (naturalist), Thomas Alva Edison (inventor), Tadao Ando (architect), George Bernard Shaw (playwright), Frank Zappa (composer, recording engineer, film director), and Leonardo da Vinci (engineer, scientist, mathematician).

Many large university institutions are now starting to offer free or almost free full courses such as Harvard, MIT and Berkeley teaming up to form edX. Other universities offering open education are prestigious private universities such as Stanford, Princeton, Duke, Johns Hopkins, the University of Pennylvania, and Caltech, as well as notable public universities including Tsinghua, Peking, Edinburgh, University of Michigan, and University of Virginia.

Open education has been called the biggest change in the way people learn since the printing press. Despite favourable studies on effectiveness, many people may still desire to choose traditional campus education for social and cultural reasons.

Many open universities are working to have the ability to offer students standardized testing and traditional degrees and credentials.

The conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the Open University in the United Kingdom. Presently, many of the major open education sources offer their own form of certificate. Due to the popularity of open education, these new kind of academic certificates are gaining more respect and equal "academic value" to traditional degrees.

Out of 182 colleges surveyed in 2009 nearly half said tuition for online courses was higher than for campus-based ones.

A recent meta-analysis found that online and blended educational approaches had better outcomes than methods that used solely face-to-face interaction.

The education sector or education system is a group of institutions (ministries of education, local educational authorities, teacher training institutions, schools, universities, etc.) whose primary purpose is to provide education to children and young people in educational settings. It involves a wide range of people (curriculum developers, inspectors, school principals, teachers, school nurses, students, etc.). These institutions can vary according to different contexts.

Schools deliver education, with support from the rest of the education system through various elements such as education policies and guidelines – to which school policies can refer – curricula and learning materials, as well as pre- and in-service teacher training programmes. The school environment – both physical (infrastructures) and psychological (school climate) – is also guided by school policies that should ensure the well-being of students when they are in school. The Organisation for Economic Co-operation and Development has found that schools tend to perform best when principals have full authority and responsibility for ensuring that students are proficient in core subjects upon graduation. They must also seek feedback from students for quality-assurance and improvement. Governments should limit themselves to monitoring student proficiency.

The education sector is fully integrated into society, through interactions with a large number of stakeholders and other sectors. These include parents, local communities, religious leaders, NGOs, stakeholders involved in health, child protection, justice and law enforcement (police), media and political leadership.

Several UN agencies have asserted that comprehensive sexuality education should be integrated into school curriculum.

Chimombo pointed out education's role as a policy instrument, capable of instilling social change and economic advancement in developing countries by giving communities the opportunity to take control of their destinies. The 2030 Agenda for Sustainable Development, adopted by the United Nations (UN) General Assembly in September 2015, calls for a new vision to address the environmental, social and economic concerns facing the world today. The Agenda includes 17 Sustainable Development Goals (SDGs), including SDG 4 on education.

Since 1909, the ratio of children in the developing world attending school has increased. Before then, a small minority of boys attended school. By the start of the 21st century, the majority of all children in most regions of the world attended school.

Universal Primary Education is one of the eight international Millennium Development Goals, towards which progress has been made in the past decade, though barriers still remain. Securing charitable funding from prospective donors is one particularly persistent problem. Researchers at the Overseas Development Institute have indicated that the main obstacles to funding for education include conflicting donor priorities, an immature aid architecture, and a lack of evidence and advocacy for the issue. Additionally, Transparency International has identified corruption in the education sector as a major stumbling block to achieving Universal Primary Education in Africa. Furthermore, demand in the developing world for improved educational access is not as high as foreigners have expected. Indigenous governments are reluctant to take on the ongoing costs involved. There is also economic pressure from some parents, who prefer their children to earn money in the short term rather than work towards the long-term benefits of education.

A study conducted by the UNESCO International Institute for Educational Planning indicates that stronger capacities in educational planning and management may have an important spill-over effect on the system as a whole. Sustainable capacity development requires complex interventions at the institutional, organizational and individual levels that could be based on some foundational principles:

Nearly every country now has Universal Primary Education.

Similarities – in systems or even in ideas – that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program facilitates exchanges across European universities. The Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.

The Programme for International Student Assessment and the International Association for the Evaluation of Educational Achievement objectively monitor and compare the proficiency of students from a wide range of different nations.

The internationalization of education is sometimes equated by critics with the westernization of education. These critics say that the internationalization of education leads to the erosion of local education systems and indigenous values and norms, which are replaced with Western systems and cultural and ideological values and orientation.

Technology plays an increasingly significant role in improving access to education for people living in impoverished areas and developing countries. However, lack of technological advancement is still causing barriers with regards to quality and access to education in developing countries. Charities like One Laptop per Child are dedicated to providing infrastructures through which the disadvantaged may access educational materials.

The OLPC foundation, a group out of MIT Media Lab and supported by several major corporations, has a stated mission to develop a $100 laptop for delivering educational software. The laptops were widely available as of 2008. They are sold at cost or given away based on donations.

In Africa, the New Partnership for Africa's Development (NEPAD) has launched an "e-school program" to provide all 600,000 primary and high schools with computer equipment, learning materials and internet access within 10 years. An International Development Agency project called nabuur.com, started with the support of former American President Bill Clinton, uses the Internet to allow co-operation by individuals on issues of social development.

India is developing technologies that will bypass land-based telephone and Internet infrastructure to deliver distance learning directly to its students. In 2004, the Indian Space Research Organisation launched EDUSAT, a communications satellite providing access to educational materials that can reach more of the country's population at a greatly reduced cost.

Research into LCPS (low-cost private schools) found that over 5 years to July 2013, debate around LCPSs to achieving Education for All (EFA) objectives was polarized and finding growing coverage in international policy. The polarization was due to disputes around whether the schools are affordable for the poor, reach disadvantaged groups, provide quality education, support or undermine equality, and are financially sustainable. The report examined the main challenges encountered by development organizations which support LCPSs. Surveys suggest these types of schools are expanding across Africa and Asia. This success is attributed to excess demand. These surveys found concern for:

The report showed some cases of successful voucher and subsidy programs; evaluations of international support to the sector are not widespread. Addressing regulatory ineffectiveness is a key challenge. Emerging approaches stress the importance of understanding the political economy of the market for LCPS, specifically how relationships of power and accountability between users, government, and private providers can produce better education outcomes for the poor.

Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Although the terms "educational psychology" and "school psychology" are often used interchangeably, researchers and theorists are likely to be identified as , whereas practitioners in schools or school-related settings are identified as school psychologists. Educational psychology is concerned with the processes of educational attainment in the general population and in sub-populations such as gifted children and those with specific disabilities.

Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. Educational psychology, in turn, informs a wide range of specialties within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks (Lucas, Blazek, & Raley, 2006).

Intelligence is an important factor in how the individual responds to education. Those who have higher intelligence tend to perform better at school and go on to higher levels of education. This effect is also observable in the opposite direction, in that education increases measurable intelligence. Studies have shown that while educational attainment is important in predicting intelligence in later life, intelligence at 53 is more closely correlated to intelligence at 8 years old than to educational attainment.

There has been much interest in learning modalities and styles over the last two decades. The most commonly employed learning modalities are:

Other commonly employed modalities include musical, interpersonal, verbal, logical, and intrapersonal.

Dunn and Dunn focused on identifying relevant stimuli that may influence learning and manipulating the school environment, at about the same time as Joseph Renzulli recommended varying teaching strategies. Howard Gardner identified a wide range of modalities in his Multiple Intelligences theories. The Myers-Briggs Type Indicator and Keirsey Temperament Sorter, based on the works of Jung, focus on understanding how people's personality affects the way they interact personally, and how this affects the way individuals respond to each other within the learning environment. The work of David Kolb and Anthony Gregorc's Type Delineator follows a similar but more simplified approach.

Some theories propose that all individuals benefit from a variety of learning modalities, while others suggest that individuals may have preferred learning styles, learning more easily through visual or kinesthetic experiences. A consequence of the latter theory is that effective teaching should present a variety of teaching methods which cover all three learning modalities so that different students have equal opportunities to learn in a way that is effective for them. Guy Claxton has questioned the extent that learning styles such as Visual, Auditory and Kinesthetic(VAK) are helpful, particularly as they can have a tendency to label children and therefore restrict learning. Recent research has argued, "there is no adequate evidence base to justify incorporating learning styles assessments into general educational practice."

Educational neuroscience is an emerging scientific field that brings together researchers in cognitive neuroscience, developmental cognitive neuroscience, educational psychology, educational technology, education theory and other related disciplines to explore the interactions between biological processes and education. Researchers in educational neuroscience investigate the neural mechanisms of reading, numerical cognition, attention, and their attendant difficulties including dyslexia, dyscalculia, and ADHD as they relate to education. Several academic institutions around the world are beginning to devote resources to the establishment of educational neuroscience research.

As an academic field, philosophy of education is "the philosophical study of education and its problems (...) its central subject matter is education, and its methods are those of philosophy". "The philosophy of education may be either the philosophy of the process of education or the philosophy of the discipline of education. That is, it may be part of the discipline in the sense of being concerned with the aims, forms, methods, or results of the process of educating or being educated; or it may be metadisciplinary in the sense of being concerned with the concepts, aims, and methods of the discipline." As such, it is both part of the field of education and a field of applied philosophy, drawing from fields of metaphysics, epistemology, axiology and the philosophical approaches (speculative, prescriptive or analytic) to address questions in and about pedagogy, education policy, and curriculum, as well as the process of learning, to name a few. For example, it might study what constitutes upbringing and education, the values and norms revealed through upbringing and educational practices, the limits and legitimization of education as an academic discipline, and the relation between education theory and practice.

There is no broad consensus as to what education's chief aim or aims are or should be. Different places, and at different times, have used educational systems for different purposes. The Prussian education system in the 19th century, for example, wanted to turn boys and girls into adults who would serve the state's political goals.

Some authors stress its value to the individual, emphasizing its potential for positively influencing students' personal development, promoting autonomy, forming a cultural identity or establishing a career or occupation. Other authors emphasize education's contributions to societal purposes, including good citizenship, shaping students into productive members of society, thereby promoting society's general economic development, and preserving cultural values. 

The purpose of education in a given time and place affects who is taught, what is taught, and how the education system behaves. For example, in the 21st century, many countries treat education as a positional good. In this competitive approach, people want their own students to get a better education than other students. This approach can lead to unfair treatment of some students, especially those from disadvantaged or marginalized groups. For example, in this system, a city's school system may draw school district boundaries so that nearly all the students in one school are from low-income families, and that nearly all the students in the neighboring schools come from more affluent families, even though concentrating low-income students in one school results in worse educational achievement for the entire school system.

In formal education, a curriculum is the set of courses and their content offered at a school or university. As an idea, curriculum stems from the Latin word for "race course", referring to the course of deeds and experiences through which children grow to become mature adults. A curriculum is prescriptive and is based on a more general syllabus which merely specifies what topics must be understood and to what level to achieve a particular grade or standard.

An academic discipline is a branch of knowledge which is formally taught, either at the university – or via some other such method. Each discipline usually has several sub-disciplines or branches, and distinguishing lines are often both arbitrary and ambiguous. Examples of broad areas of academic disciplines include the natural sciences, mathematics, computer science, social sciences, humanities and applied sciences.

Educational institutions may incorporate fine arts as part of K-12 grade curricula or within majors at colleges and universities as electives. The various types of fine arts are music, dance, and theatre.

The Sudbury Valley School offers a model of education without a curricula.

Instruction is the facilitation of another's learning. Instructors in primary and secondary institutions are often called teachers, and they direct the education of students and might draw on many subjects like reading, writing, mathematics, science and history. Instructors in post-secondary institutions might be called teachers, instructors, or professors, depending on the type of institution; and they primarily teach only their specific discipline. Studies from the United States suggest that the quality of teachers is the single most important factor affecting student performance, and that countries which score highly on international tests have multiple policies in place to ensure that the teachers they employ are as effective as possible. With the passing of NCLB in the United States (No Child Left Behind), teachers must be highly qualified. A popular way to gauge teaching performance is to use student evaluations of teachers (SETS), but these evaluations have been criticized for being counterproductive to learning and inaccurate due to student bias.

College basketball coach John Wooden the Wizard of Westwood would teach through quick "This not That" technique. He would show (a) the correct way to perform an action, (b) the incorrect way the player performed it, and again (c) the correct way to perform an action. This helped him to be a responsive teacher and fix errors on the fly. Also, less communication from him meant more time that the player could practice.

It has been argued that high rates of education are essential for countries to be able to achieve high levels of economic growth. Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting edge technologies already tried and tested by rich countries. However, technology transfer requires knowledgeable managers and engineers who are able to operate new machines or production practices borrowed from the leader in order to close the gap through imitation. Therefore, a country's ability to learn from the leader is a function of its stock of "human capital". Recent study of the determinants of aggregate economic growth have stressed the importance of fundamental economic institutions and the role of cognitive skills.

At the level of the individual, there is a large literature, generally related to the work of Jacob Mincer, on how earnings are related to the schooling and other human capital. This work has motivated a large number of studies, but is also controversial. The chief controversies revolve around how to interpret the impact of schooling. Some students who have indicated a high potential for learning, by testing with a high intelligence quotient, may not achieve their full academic potential, due to financial difficulties.

Economists Samuel Bowles and Herbert Gintis argued in 1976 that there was a fundamental conflict in American schooling between the egalitarian goal of democratic participation and the inequalities implied by the continued profitability of capitalist production.

Many countries are now drastically changing the way they educate their citizens. The world is changing at an ever quickening rate, which means that a lot of knowledge becomes obsolete and inaccurate more quickly. The emphasis is therefore shifting to teaching the skills of learning: to picking up new knowledge quickly and in as agile a way as possible. Finnish schools have even begun to move away from the regular subject-focused curricula, introducing instead developments like phenomenon-based learning, where students study concepts like climate change instead. There are also active educational interventions to implement programs and paths specific to non-traditional students, such as first generation students. 

Education is also becoming a commodity no longer reserved for children. Adults need it too. Some governmental bodies, like the Finnish Innovation Fund Sitra in Finland, have even proposed compulsory lifelong education.




</doc>
<doc id="42845482" url="https://en.wikipedia.org/wiki?curid=42845482" title="United States military veteran suicide">
United States military veteran suicide

United States military veteran suicide is an ongoing phenomenon regarding a reportedly high rate of suicide among U.S. military veterans, in comparison to the general public. According to the most recent report published by the United States Department of Veterans Affairs (VA) in 2016, which analyzed 55 million veterans' records from 1979 to 2014, the current analysis indicates that an average of 20 veterans a day die from suicide.

In 2012 alone, an estimated 6,500 former military personnel died by suicide. More active duty veterans, 177, succumbed to suicide that year than were killed in combat, 176. The Army suffered 52% of the suicides from all branches.

In 2013, the VA released a study that covered suicides from 1999 to 2010, which showed that roughly 22 veterans were dying by suicide per day, or one every 65 minutes. Some sources suggest that this rate may be undercounting suicides. A recent analysis found a suicide rate among veterans of about 30 per 100,000 population per year, compared with the civilian rate of 14 per 100,000. However, the comparison was not adjusted for age and sex.

The total number of suicides differs by age group; 31% of these suicides were by veterans 49 and younger while 69% were by veterans aged 50 and older. As with suicides in general, suicide of veterans is primarily male, with about 97 percent of the suicides being male in the states that reported gender.

In 2015, the Clay Hunt Veterans Suicide Prevention Act passed in the Senate and was then enacted as on February 12, 2015.

In August 2016, the VA released a new report which consisted of the nation's largest analysis of veteran suicide. The report reviewed more than 55 million veterans' records from 1979 to 2014 from every state in the nation. The previous report from 2012 was primarily limited to data on veterans who used VHA health services or from mortality records obtained directly from 20 states and approximately 3 million records. Compared to the data from the 2012 report, which estimated the number of Veteran deaths by suicide to be 22 per day, the current analysis indicates that in 2014, an average of 20 veterans a day died from suicide.

The first suicide prevention center in the United States was opened in Los Angeles in 1958 with funding from the U.S. Public Health Service. In 1966, the Center for Studies of Suicide Prevention (later the Suicide Research Unit) was established at the National Institute of Mental Health (NIMH) of the National Institutes of Health (NIH). Later on, in 1970, the NIMH pushed in Phoenix the discussion about the status of suicide prevention, presented relevant findings about suicide rate and identified the future directions and priorities of the topic.

However, it wasn't until mid-1990s when suicide started being the central issue of the political-social agenda of the United States. Survivors from suicide began to mobilize encouraging the development of a national strategy for suicide prevention. Finally, two Congressional Resolutions—S. Res. 84 and H. Res. 212 of the 105th Congress—recognized suicide as a national problem and suicide prevention as a national priority.

As recommended in the U.N. guidelines, these groups set out to establish a public and private partnership that would be responsible for promoting suicide prevention in the United States. This innovative public-private partnership jointly sponsored a national consensus conference on suicide prevention in Reno, Nevada, which developed a list of 81 recommendations


One of the most important laws about Veterans' Suicide Prevention is the Joshua Omvig Veterans Suicide Prevention Act (JOVSPA) of 2007, supporting the creation of a comprehensive program to reduce the incidence of suicide among veterans. Named for a veteran of Operation Iraqi Freedom who died by suicide in 2005, the act directed the Secretary of the U.S. Department of Veterans Affairs (VA) to implement a comprehensive suicide prevention program for veterans. Components include staff education, mental health assessments as part of overall health assessments, a suicide prevention coordinator at each VA medical facility, research efforts, 24-hour mental health care, a toll-free crisis line, and outreach to and education for veterans and their families. In the summer of 2009, VA added a one-to-one “chat service” for veterans who prefer to reach out for assistance using the Internet.

In 2010, the National Action Alliance for Suicide Prevention was created and, in 2012, the National Strategy was revised. With Obama’s administration suicide prevention strategies for veterans expanded and a goal was formed to make the process of finding and obtaining mental health resources easier for veterans, work to retain and recruit mental health professionals, and make the government programs more accountable for the people they serve.

In 2011, the National Veterans Suicide Prevention Hotline was renamed the Veterans Crisis Line (VCL). The primary mission of the VCL is “to provide 24/7, world-class suicide prevention and crisis intervention services to Veterans, Servicemembers, and their family members.” The VCL faces a number of challenges. It must meet the operational and business demands of responding to over 500,000 calls per year, along with thousands of electronic chats and text messages, and initiating rescue processes when indicated. It must also train staff to respond to Veterans and their family members in individual encounters during which a responder must make an accurate assessment of the needs of the caller under stressful, time-sensitive conditions.

Since its inception in July 2007, the VCL has answered over 3 million calls and initiated the dispatch of emergency services to callers in imminent crisis over 84,000 times. Since launching chat in 2009 and text services in November 2011, the VCL has answered nearly 359,000 and nearly 78,000 requests for chat and text services, respectively. In addition, staff has forwarded more than 504,000 referrals to local VA Suicide Prevention Coordinators on behalf of Veterans to ensure continuity of care with Veterans’ local VA providers. For FY 2016, more than 51,000 chats and 17,000 texts were answered by VCL responders. For FY 2017, nearly 54,000 chats and nearly 16,000 texts were answered by VCL responders. Emergency services were dispatched to over 12,000 callers in immediate crisis in FY 2016, and nearly 19,000 callers in immediate crisis in FY 2017. For FY 2016, nearly 87,000 referrals were made to local Suicide Prevention Coordinators for follow-up care and over 95,000 referrals were made in FY 2017.

The 2018 federal budget expanded mental health screenings for veterans.

A study published in the "Cleveland Clinic Journal of Medicine" found that,
The same study also found that in veterans with PTSD related to combat experience, combat-related guilt may be a significant predictor of suicidal ideation and attempts.

Craig Bryan of the University of Utah National Center for Veterans Studies said that veterans have the same risk factors for suicide as the general population, including feelings of depression, hopelessness, post-traumatic stress disorder, a history of trauma, and access to firearms.

A study done by the "Department of Veterans Affairs" discovered that veterans are more likely to develop symptoms of PTSD for a number of reasons such as:


The "Department of Veterans Affairs" also discovered that where you were deployed and which branch of military you are with can also have drastic effects on your mental status after returning from service. As in most combat wars, your experiences will vary depending on where you are stationed.

Critics of this reporting such as author Tim Worstall in Feb. 2013 claim that there is no epidemic when comparing similar demographic cohorts in the civilian population. He points out that since vets are predominantly male, the suicide rate to compare to is not the general civilian rate, but the rate for males.

Veterans have difficulty transitioning from the Military to civilian life. Many choose to transition by Utilizing their GI Bill or other education benefits. The pursuit of education often facilitates the transition to civilian life. The pursuit of education among Veterans can aggravate post service conditions that are linked to a higher likelihood of suicide but often aids in the transition to civilian life Veterans pursuing education, especially those utilizing the post 9/11 GI Bill, are more likely to have protective factors related to socialization and reintegration than those who are not.


Although higher education has presented many difficulties to returning Veterans, research supports that Veterans often benefit from transitioning from the military into higher education. Academic life often requires Student Veterans to work and interact with other classmates. Most Academic Institutions have Student Veteran Organizations and Resources centers specifically to Aid Military Veterans. Military Education benefits, Primarily the Post 9/11 GI Bill, pay the cost of tuition and provide a housing stipend to Student Veterans. Education benefits often give Veteran Students an income, a goal to continue to work towards and socialization with the general population.




</doc>
<doc id="59424058" url="https://en.wikipedia.org/wiki?curid=59424058" title="National Commission on Teaching and America's Future">
National Commission on Teaching and America's Future

The National Commission on Teaching and America's Future (NCTAF) is a non-profit, non-partisan education policy advocacy organization based in Washington, D.C. Founded in 1994 by then-North Carolina governor Jim Hunt and Stanford University professor Linda Darling-Hammond, the NCTAF focuses its research on improving the teaching profession through recruitment, development, and retention of skilled teachers. In 2017, the NCTAF announced that it will merge with Learning Forward and will operate under the Learning Forward name.

In its 1996 report "What Matters Most: Teaching for America's Future," the NCTAF issued broad recommendations for education leaders and state policymakers to, among other things, overhaul teacher education programs, establish state boards of professional teaching standards, strengthen teacher licensure standards, implement teacher mentoring programs, and create teacher compensation policies that reward knowledge and expertise. The report had wide-reaching impact, with seven states, including Illinois, Indiana, Kentucky, Maine, Missouri, North Carolina, and Ohio, signing on to be partners in implementing the report's recommendations.

In 2001, the NCTAF appointed former US federal election official Tom Carrol its executive director. Carroll announced his retirement in 2014. He was succeeded by Melinda George.



</doc>
<doc id="59480374" url="https://en.wikipedia.org/wiki?curid=59480374" title="Reach Every Reader">
Reach Every Reader

Reach Every Reader is a five-year initiative supported by a $30 million grant from Chan Zuckerberg Initiative co-founders Priscilla Chan and Facebook CEO Mark Zuckerberg. Reach Every Reader was launched by faculty at the Harvard Graduate School of Education and Massachusetts Institute of Technology's Integrated Learning Initiative, and involves collaborators at the Florida Center for Reading Research and Florida State University College of Communication and Information, and the Charlotte-Mecklenburg School District in North Carolina.

The collaboration consists of five projects:


Reach Every Reader will develop a web-based screening tool for reading difficulties that diagnoses underlying causes. The diagnostic screening tool will identify kindergarteners who are at high risk for reading difficulty. The goal is to make this type of screening available to all children. 

The collaboration will also examine which interventions work for which students in order to work toward the development of personalized interventions.

Researchers will work with schools to deliver their interventions to kindergarten students in summer programs and eventually implement them in the school curriculum.

Concern has been expressed that the project involves "crisis talk" that creates pressure for children, and that parents may be concerned about the tracking of their children's personal information.


</doc>
<doc id="29974250" url="https://en.wikipedia.org/wiki?curid=29974250" title="State College of Florida Collegiate School">
State College of Florida Collegiate School

State College of Florida Collegiate School (SCFCS) is a college preparatory school located on State College of Florida's Bradenton campus. It is based on a school in Sweden, with similar views of having students work on their own pace. classes are available for grades 6-10. The school is largely technology based, utilizing a service, Canvas, from Instructure to assign and turn in schoolwork. Each student is assigned an iPad based on their grade level, and Apple laptops are available for services not available on the iPad. Each student start classes on the college campus in eleventh grade if they pass an enrollment test, called the PERT, and have at least a 3.0 GPA. After completing the program, they are given an Associate degree at graduation, alongside their high school diploma. Following this, for a two-year period, students can be given a tuition-paid scholarship for the Florida Gulf Coast University. The current headmaster is Kelly Monod.

State College of Florida Collegiate School is recognized for its high academic record and high quality work. In the year 2011, SCFCS participated in the Florida Comprehensive Assessment Test (FCAT) In sixth grade reading, SCFCS students scored 85 compared to Manatee County public school district, 63, and all Florida students’ composite of 67. Sixth grade math scores were SCFCS at 72, Manatee County, 48, and Florida, 57. In seventh grade reading, SCFCS students scored 83 compared to Manatee County, 65, and Florida, 68. SCFCS scored 73 in seventh grade math compared to Manatee County at 59 and Florida at 62.

Many SCF Collegiate Students regularly participate on the college Brain Bowl team, often with great success. SCF Collegiate Students Carlyle Styer and Christopher Medrano were a part of the "SCF Fire Team" which won the 2015 FCSAA Brain Bowl State Championship. Carlyle Styer was joined by fellow SCFCS student Kara Stevens, as well as four other SCF students in winning the 2015 NAQT Community College Championship. It is believed that this may be the only instance in which high school students have played a significant role in a quiz bowl national championship at the collegiate level.



</doc>
<doc id="277206" url="https://en.wikipedia.org/wiki?curid=277206" title="Freethought">
Freethought

Freethought (or "free thought") is a philosophical viewpoint which holds that positions regarding truth should be formed on the basis of logic, reason, and empiricism, rather than authority, tradition, revelation, or dogma. According to the Oxford English Dictionary <ref>


</doc>
<doc id="59739835" url="https://en.wikipedia.org/wiki?curid=59739835" title="Independent working class education">
Independent working class education

Independent working class education is an approach to education, particularly adult education, developed by labour activists, whereby the education of working class people is seen as a specifically political process linked to other aspects of class struggle. The term, abbreviated to (IWCE), is particularly linked to the Plebs' League.



</doc>
<doc id="59774284" url="https://en.wikipedia.org/wiki?curid=59774284" title="Skill assessment">
Skill assessment

Competence assessment is a process in which evidence is gathered by the assessor and evaluated against agreed criteria in order to make a judgement of competence. Skill assessment is the comparison of actual performance of a skill with the specified standard for performance of that skill under the circumstances specified by the standard, and evaluation of whether the performance meets or exceed the requirements. Assessment of a skill should comply with the four principles of validity, reliability, fairness and flexibility.

Formative assessment provides feedback for remedial work and coaching, while summative assessment checks whether the competence has been achieved at the end of training. Assessment of combinations of skills and of foundational knowledge may provide greater efficiency, and in some cases competence in one skill my imply competence in other skills. The thoroughness reqired of assessment may depend on the consequences of occasional poor performance.

Validity is the primary requirement. If the assessment is not valid, then the other characteristics are irrelevant. Validity means that an assessment process effectively assesses what it is claimed and intended to assess. To achieve this the assessment tools must address all requirements of the standard to the appropriate depth (neither too much nor too little) and be repeated often enough to ensure that the required performance is repeatable.

The training standard that specifies the competency is the benchmark for assessment, and to be valid the assessment must comply exactly with its requirements, so that nothing required by the standard is omitted, and nothing that is not required is included.

The assessment tools for a skill therefore need to be designed so that they allow the skill to be tested in compliance with the requirements of the standard. It can be useful to map the assessment tools to the specific competences to ensure that they cover the full scope of the standard.

There may be a requirement for periodical validation of assessment tools. This process generally involves mapping the tools against the standard and checking that the tools comply with the other principles of assessment and the rules of evidence.

After validity, reliability is essential. A reliable assessment is one where the evidence elicited and interpretation of evidence is consistent with the skill required, so that the assessment consistently produces outcomes that are compliant with the standard. The assessment decision of a given observed performance should not vary for different assessors. The same evidence should lead to the same outcome.

To achieve this, the assessment tool must provide sufficient guidance for the assessor. In practice the assessment instrument provided to the candidate should be paired with an assessor guide which provides instructions to the assessor to guide their judgement of satisfactory performance or acceptable answers to questions.

To be fair the assessment process must be clearly understood by the candidates, and there must be agreement by both assessors and candidates that candidates’ reasonable needs circumstances are addressed.

The assessment tool can provide evidence that the process is understood and accepted by the candidate, by having a place where a statement to this effect is signed by the candidate at the start of the assessment. A further statement that the assessor has checked with the candidate for any special circumstances or requirements can also be included. Reasonable adjustment must not compromise the validity or reliability of the assessment.

Flexibility of assessment is desirable where reasonably practicable. This is a feature that should be inherent in the assessment tools for the skill, and should take into account the expected variability of circumstances, including variations in candidates, equipment, location, environmental conditions and other things not entirely under the control of the assessor, but within the scope of the competence requirements. Flexibility does not imply bending the rules, or failing to comply with the specifications of the standards. All performance criteria must be addressed.

Formative assessments are formal and informal tests, tasks, quizzes, discussions or observations taken during the learning process. These assessments identify strengths and weaknesses and provide feedback to modify the consequent learning activities to facilitate efficient learning and skill development.

Summative assessments evaluate skills at or after the end of an instructional unit, to ensure that competence has been achieved. At this point remedial work may no longer be practicable.

Integrated assessment is part of the learning and teaching process, and can take place at various stages of a learning programme. Assessments may combine assessment of theory and practice. Some skills may need separate and specific assessment, but others can be combined for efficiency.

Assessment is not an event that only occurs at the end of training, it is most effective when continuous and when providing constant feedback on progress and problems, allowing timely intervention where useful. In many cases a sample of evidence is sufficient to infer competence over a fairly large range, as competence in a skill that requires competence in other skills may be a proxy for those more foundational skills.

Comprehensive planning is usually necessary to produce robust assessment tools that suit the training programme and do justice to both the training standard and the learners.

Assessment of practical skills is usually best done by direct observation of performance in conditions as close as reasonably practicable to the circumstances in which the skill would normally be practiced. Where this is not reasonably practicable, simulations may be appropriate, to whatever level of accuracy is available. Assessment of realistic combinations of skills may save a lot of time, and scenarios may be devised that allow simultaneous and sequential assessments of several skill in one assessment session.

The number of repetitions required will also depend on how critical the skill is considered to be. A single successful demonstration may be sufficient to show that the candidate can perform a task when the consequences are minor. Several sequential faultless performances may be required if another person's life will depend on correct performance.

Assessment tools for practical skills may describe a task to be done, and the assessors guide should generally list the stages of the task and the details the assessor should check off as they are done. Where order is important, this should be mentioned. A checklist may be provided as permanent record, or a video may be taken. In some cases there will be a product which can be retained as long term evidence along with the paperwork or database records.



</doc>
<doc id="59774016" url="https://en.wikipedia.org/wiki?curid=59774016" title="PeaceJam Ghana">
PeaceJam Ghana

Peacejam Ghana started in 2008 by its official chapter, the West Africa Center for Peace Foundation, Ghana (WACPF). Peacejam Ghana has mentored and trained over 5000 students since its inception. Peacejam Ghana has produced many scholars, some of whom have received the TPG Global Impact Youth Fellows scholarship to pursue higher degrees.

Wisdom Addo is the Founder and Executive Director of the West Africa Center for Peace Foundation, Ghana.

Peacejam is an annual Youth Leadership Conference that is built around the Nobel Peace Prize Laureates who work with young people with the aim of imparting their skills, knowledge and wisdom to them for community and sustainable development.

The conference usually draws students from Junior High and Senior High schools across the country who are usually trained and mentored by mentors on diverse areas including but not limited to commitment to justice and peace, social responsibility, academic excellence and sustainable development .

1.   Osu Presby Senior High School

2.      Kaneshie Senior High Secondary Technical - Accra

3.      Kraboa Presby Senior High Technical

4.      Half Assini Senior High School

5.      Annor Adjaye Senior High School -

6.      Accra High Senior High School – Accra

7.      St Mary's Senior High School (Ghana)

8.      St. Stephen’s R/C -

9.      Prince of Peace -

10.  Star of The Sea R/C -

11.  Mataheko R/C -

12.  St. Kizito -

13.  Bennett Caulley -


</doc>
<doc id="46426065" url="https://en.wikipedia.org/wiki?curid=46426065" title="Logic">
Logic

Logic (from the ), is the systematic study of the form of valid inference, and the most general laws of truth. A valid inference is one where there is a specific relation of logical support between the assumptions of the inference and its conclusion. In ordinary discourse, inferences may be signified by words such as "therefore", "hence", "ergo", and so on.

There is no universal agreement as to the exact scope and subject matter of logic (see , below), but it has traditionally included the classification of arguments, the systematic exposition of the 'logical form' common to all valid arguments, the study of proof and inference, including paradoxes and fallacies, and the study of syntax and semantics. Historically, logic has been studied in philosophy (since ancient times) and mathematics (since the mid-19th century), and recently logic has been studied in computer science, linguistics, psychology, and other fields.

The concept of logical form is central to logic. The validity of an argument is determined by its logical form, not by its content. Traditional Aristotelian syllogistic logic and modern symbolic logic are examples of formal logic.

However, agreement on what logic is has remained elusive, and although the field of universal logic has studied the common structure of logics, in 2007 Mossakowski et al. commented that "it is embarrassing that there is no widely acceptable formal definition of 'a logic'".

Logic is generally considered formal when it analyzes and represents the "form" of any valid argument type. The form of an argument is displayed by representing its sentences in the formal grammar and symbolism of a logical language to make its content usable in formal inference. Simply put, to formalize simply means to translate English sentences into the language of logic.

This is called showing the "logical form" of the argument. It is necessary because indicative sentences of ordinary language show a considerable variety of form and complexity that makes their use in inference impractical. It requires, first, ignoring those grammatical features irrelevant to logic (such as gender and declension, if the argument is in Latin), replacing conjunctions irrelevant to logic (such as "but") with logical conjunctions like "and" and replacing ambiguous, or alternative logical expressions ("any", "every", etc.) with expressions of a standard type (such as "all", or the universal quantifier ∀).

Second, certain parts of the sentence must be replaced with schematic letters. Thus, for example, the expression "all Ps are Qs" shows the logical form common to the sentences "all men are mortals", "all cats are carnivores", "all Greeks are philosophers", and so on. The schema can further be condensed into the formula "A(P,Q)", where the letter "A" indicates the judgement 'all - are -'.

The importance of form was recognised from ancient times. Aristotle uses variable letters to represent valid inferences in "Prior Analytics", leading Jan Łukasiewicz to say that the introduction of variables was "one of Aristotle's greatest inventions". According to the followers of Aristotle (such as Ammonius), only the logical principles stated in schematic terms belong to logic, not those given in concrete terms. The concrete terms "man", "mortal", etc., are analogous to the substitution values of the schematic placeholders "P", "Q", "R", which were called the "matter" (Greek "hyle") of the inference.

There is a big difference between the kinds of formulas seen in traditional term logic and the predicate calculus that is the fundamental advance of modern logic. The formula "A(P,Q)" (all Ps are Qs) of traditional logic corresponds to the more complex formula formula_1 in predicate logic, involving the logical connectives for universal quantification and implication rather than just the predicate letter "A" and using variable arguments formula_2 where traditional logic uses just the term letter "P". With the complexity comes power, and the advent of the predicate calculus inaugurated revolutionary growth of the subject.

The validity of an argument depends upon the meaning or "semantics" of the sentences that make it up.

Aristotle's Organon, especially "On Interpretation", gives a cursory outline of semantics which the scholastic logicians, particularly in the thirteenth and fourteenth century, developed into a complex and sophisticated theory, called Supposition Theory. This showed how the truth of simple sentences, expressed schematically, depend on how the terms 'supposit' or "stand for" certain extra-linguistic items. For example, in part II of his Summa Logicae, William of Ockham presents a comprehensive account of the necessary and sufficient conditions for the truth of simple sentences, in order to show which arguments are valid and which are not. Thus "every A is B' is true if and only if there is something for which 'A' stands, and there is nothing for which 'A' stands, for which 'B' does not also stand." 

Early modern logic defined semantics purely as a relation between ideas. Antoine Arnauld in the Port Royal Logic, says that 'after conceiving things by our ideas, we compare these ideas, and, finding that some belong together and some do not, we unite or separate them. This is called "affirming" or "denying", and in general "judging". Thus truth and falsity are no more than the agreement or disagreement of ideas. This suggests obvious difficulties, leading Locke to distinguish between 'real' truth, when our ideas have 'real existence' and 'imaginary' or 'verbal' truth, where ideas like harpies or centaurs exist only in the mind. This view (psychologism) was taken to the extreme in the nineteenth century, and is generally held by modern logicians to signify a low point in the decline of logic before the twentieth century.

Modern semantics is in some ways closer to the medieval view, in rejecting such psychological truth-conditions. However, the introduction of quantification, needed to solve the problem of multiple generality, rendered impossible the kind of subject-predicate analysis that underlies medieval semantics. The main modern approach is "model-theoretic semantics", based on Alfred Tarski's semantic theory of truth. The approach assumes that the meaning of the various parts of the propositions are given by the possible ways we can give a recursively specified group of interpretation functions from them to some predefined domain of discourse: an interpretation of first-order predicate logic is given by a mapping from terms to a universe of individuals, and a mapping from propositions to the truth values "true" and "false". Model-theoretic semantics is one of the fundamental concepts of model theory. Modern semantics also admits rival approaches, such as the proof-theoretic semantics that associates the meaning of propositions with the roles that they can play in inferences, an approach that ultimately derives from the work of Gerhard Gentzen on structural proof theory and is heavily influenced by Ludwig Wittgenstein's later philosophy, especially his aphorism "meaning is use".

"Inference" is not to be confused with "implication". An implication is a sentence of the form 'If p then q', and can be true or false. The Stoic logician Philo of Megara was the first to define the truth conditions of such an implication: false only when the antecedent p is true and the consequent q is false, in all other cases true. An inference, on the other hand, consists of two separately asserted propositions of the form 'p therefore q'. An inference is not true or false, but valid or invalid. However, there is a connection between implication and inference, as follows: if the implication 'if p then q' is "true", the inference 'p therefore q' is "valid". This was given an apparently paradoxical formulation by Philo, who said that the implication 'if it is day, it is night' is true only at night, so the inference 'it is day, therefore it is night' is valid in the night, but not in the day.

The theory of inference (or 'consequences') was systematically developed in medieval times by logicians such as William of Ockham and Walter Burley. It is uniquely medieval, though it has its origins in Aristotle's Topics and Boethius' "De Syllogismis hypotheticis". This is why many terms in logic are Latin. For example, the rule that licenses the move from the implication 'if p then q' plus the assertion of its antecedent p, to the assertion of the consequent q is known as modus ponens (or 'mode of positing'). Its Latin formulation is 'Posito antecedente ponitur consequens'. The Latin formulations of many other rules such as 'ex falso quodlibet' (anything follows from a falsehood), 'reductio ad absurdum' (disproof by showing the consequence is absurd) also date from this period.

However, the theory of consequences, or of the so-called 'hypothetical syllogism' was never fully integrated into the theory of the 'categorical syllogism'. This was partly because of the resistance to reducing the categorical judgment 'Every S is P' to the so-called hypothetical judgment 'if anything is S, it is P'. The first was thought to imply 'some S is P', the second was not, and as late as 1911 in the Encyclopædia Britannica article on Logic, we find the Oxford logician T.H. Case arguing against Sigwart's and Brentano's modern analysis of the universal proposition.

A formal system is an organization of terms used for the analysis of deduction. It consists of an alphabet, a language over the alphabet to construct sentences, and a rule for deriving sentences. Among the important properties that logical systems can have are:

Some logical systems do not have all four properties. As an example, Kurt Gödel's incompleteness theorems show that sufficiently complex formal systems of arithmetic cannot be consistent and complete; however, first-order predicate logics not extended by specific axioms to be arithmetic formal systems with equality can be complete and consistent.

As the study of argument is of clear importance to the reasons that we hold things to be true, logic is of essential importance to rationality. Here we have defined logic to be "the systematic study of the form of arguments"; the reasoning behind argument is of several sorts, but only some of these arguments fall under the aegis of logic proper.

Deductive reasoning concerns the logical consequence of given premises and is the form of reasoning most closely connected to logic. On a narrow conception of logic (see below) logic concerns just deductive reasoning, although such a narrow conception controversially excludes most of what is called informal logic from the discipline.

There are other forms of reasoning that are rational but that are generally not taken to be part of logic. These include inductive reasoning, which covers forms of inference that move from collections of particular judgements to universal judgements, and abductive reasoning, which is a form of inference that goes from observation to a hypothesis that accounts for the reliable data (observation) and seeks to explain relevant evidence. The American philosopher Charles Sanders Peirce (1839–1914) first introduced the term as "guessing". Peirce said that to "abduce" a hypothetical explanation formula_3 from an observed surprising circumstance formula_4 is to surmise that formula_3 may be true because then formula_4 would be a matter of course. Thus, to abduce formula_3 from formula_4 involves determining that formula_3 is sufficient (or nearly sufficient), but not necessary, for formula_4.

While inductive and abductive inference are not part of logic proper, the methodology of logic has been applied to them with some degree of success. For example, the notion of deductive validity (where an inference is deductively valid if and only if there is no possible situation in which all the premises are true but the conclusion false) exists in an analogy to the notion of inductive validity, or "strength", where an inference is inductively strong if and only if its premises give some degree of probability to its conclusion. Whereas the notion of deductive validity can be rigorously stated for systems of formal logic in terms of the well-understood notions of semantics, inductive validity requires us to define a reliable generalization of some set of observations. The task of providing this definition may be approached in various ways, some less formal than others; some of these definitions may use logical association rule induction, while others may use mathematical models of probability such as decision trees.

Logic arose (see below) from a concern with correctness of argumentation. Modern logicians usually wish to ensure that logic studies just those arguments that arise from appropriately general forms of inference. For example, Thomas Hofweber writes in the "Stanford Encyclopedia of Philosophy" that logic "does not, however, cover good reasoning as a whole. That is the job of the theory of rationality. Rather it deals with inferences whose validity can be traced back to the formal features of the representations that are involved in that inference, be they linguistic, mental, or other representations."

Logic has been defined as "the study of arguments correct in virtue of their form". This has not been the definition taken in this article, but the idea that logic treats special forms of argument, deductive argument, rather than argument in general, has a history in logic that dates back at least to logicism in mathematics (19th and 20th centuries) and the advent of the influence of mathematical logic on philosophy. A consequence of taking logic to treat special kinds of argument is that it leads to identification of special kinds of truth, the logical truths (with logic equivalently being the study of logical truth), and excludes many of the original objects of study of logic that are treated as informal logic. Robert Brandom has argued against the idea that logic is the study of a special kind of logical truth, arguing that instead one can talk of the logic of material inference (in the terminology of Wilfred Sellars), with logic making explicit the commitments that were originally implicit in informal inference.

Logic comes from the Greek word "logos", originally meaning "the word" or "what is spoken", but coming to mean "thought" or "reason". In the Western World, logic was first developed by Aristotle, who called the subject 'analytics'. Aristotelian logic became widely accepted in science and mathematics and remained in wide use in the West until the early 19th century. Aristotle's system of logic was responsible for the introduction of hypothetical syllogism, temporal modal logic, and inductive logic, as well as influential vocabulary such as terms, predicables, syllogisms and propositions. There was also the rival Stoic logic.

In Europe during the later medieval period, major efforts were made to show that Aristotle's ideas were compatible with Christian faith. During the High Middle Ages, logic became a main focus of philosophers, who would engage in critical logical analyses of philosophical arguments, often using variations of the methodology of scholasticism. In 1323, William of Ockham's influential "Summa Logicae" was released. By the 18th century, the structured approach to arguments had degenerated and fallen out of favour, as depicted in Holberg's satirical play "Erasmus Montanus".
The Chinese logical philosopher Gongsun Long () proposed the paradox "One and one cannot become two, since neither becomes two." In China, the tradition of scholarly investigation into logic, however, was repressed by the Qin dynasty following the legalist philosophy of Han Feizi.

In India, the Anviksiki school of logic was founded by Medhatithi Gautama (c. 6th century BCE). Innovations in the scholastic school, called Nyaya, continued from ancient times into the early 18th century with the Navya-Nyaya school. By the 16th century, it developed theories resembling modern logic, such as Gottlob Frege's "distinction between sense and reference of proper names" and his "definition of number", as well as the theory of "restrictive conditions for universals" anticipating some of the developments in modern set theory. Since 1824, Indian logic attracted the attention of many Western scholars, and has had an influence on important 19th-century logicians such as Charles Babbage, Augustus De Morgan, and George Boole. In the 20th century, Western philosophers like Stanislaw Schayer and Klaus Glashoff have explored Indian logic more extensively.

The syllogistic logic developed by Aristotle predominated in the West until the mid-19th century, when interest in the foundations of mathematics stimulated the development of symbolic logic (now called mathematical logic). In 1854, George Boole published "An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities", introducing symbolic logic and the principles of what is now known as Boolean logic. In 1879, Gottlob Frege published "Begriffsschrift", which inaugurated modern logic with the invention of quantifier notation, reconciling the Aristotelian and Stoic logics in a broader system, and solving such problems for which Aristotelian logic was impotent, such as the problem of multiple generality.. From 1910 to 1913, Alfred North Whitehead and Bertrand Russell published "Principia Mathematica" on the foundations of mathematics, attempting to derive mathematical truths from axioms and inference rules in symbolic logic. In 1931, Gödel raised serious problems with the foundationalist program and logic ceased to focus on such issues.

The development of logic since Frege, Russell, and Wittgenstein had a profound influence on the practice of philosophy and the perceived nature of philosophical problems (see analytic philosophy) and philosophy of mathematics. Logic, especially sentential logic, is implemented in computer logic circuits and is fundamental to computer science. Logic is commonly taught by university philosophy departments, often as a compulsory discipline.

The "Organon" was Aristotle's body of work on logic, with the "Prior Analytics" constituting the first explicit work in formal logic, introducing the syllogistic. The parts of syllogistic logic, also known by the name term logic, are the analysis of the judgements into propositions consisting of two terms that are related by one of a fixed number of relations, and the expression of inferences by means of syllogisms that consist of two propositions sharing a common term as premise, and a conclusion that is a proposition involving the two unrelated terms from the premises.

Aristotle's work was regarded in classical times and from medieval times in Europe and the Middle East as the very picture of a fully worked out system. However, it was not alone: the Stoics proposed a system of propositional logic that was studied by medieval logicians. Also, the problem of multiple generality was recognized in medieval times. Nonetheless, problems with syllogistic logic were not seen as being in need of revolutionary solutions.

Today, some academics claim that Aristotle's system is generally seen as having little more than historical value (though there is some current interest in extending term logics), regarded as made obsolete by the advent of propositional logic and the predicate calculus. Others use Aristotle in argumentation theory to help develop and critically question argumentation schemes that are used in artificial intelligence and legal arguments.

A propositional calculus or logic (also a sentential calculus) is a formal system in which formulae representing propositions can be formed by combining atomic propositions using logical connectives, and in which a system of formal proof rules establishes certain formulae as "theorems". An example of a theorem of propositional logic is formula_11, which says that if A holds, then B implies A.

Predicate logic is the generic term for symbolic formal systems such as first-order logic, second-order logic, many-sorted logic, and infinitary logic. It provides an account of quantifiers general enough to express a wide set of arguments occurring in natural language. For example, Bertrand Russell's famous barber paradox, "there is a man who shaves all and only men who do not shave themselves" can be formalised by the sentence formula_12, using the non-logical predicate formula_13 to indicate that "x" is a man, and the non-logical relation formula_14 to indicate that "x" shaves "y"; all other symbols of the formulae are logical, expressing the universal and existential quantifiers, conjunction, implication, negation and biconditional.

Whilst Aristotelian syllogistic logic specifies a small number of forms that the relevant part of the involved judgements may take, predicate logic allows sentences to be analysed into subject and argument in several additional ways—allowing predicate logic to solve the problem of multiple generality that had perplexed medieval logicians.

The development of predicate logic is usually attributed to Gottlob Frege, who is also credited as one of the founders of analytical philosophy, but the formulation of predicate logic most often used today is the first-order logic presented in Principles of Mathematical Logic by David Hilbert and Wilhelm Ackermann in 1928. The analytical generality of predicate logic allowed the formalization of mathematics, drove the investigation of set theory, and allowed the development of Alfred Tarski's approach to model theory. It provides the foundation of modern mathematical logic.

Frege's original system of predicate logic was second-order, rather than first-order. Second-order logic is most prominently defended (against the criticism of Willard Van Orman Quine and others) by George Boolos and Stewart Shapiro.

In languages, modality deals with the phenomenon that sub-parts of a sentence may have their semantics modified by special verbs or modal particles. For example, ""We go to the games" can be modified to give "We should go to the games", and "We can go to the games" and perhaps "We will go to the games"". More abstractly, we might say that modality affects the circumstances in which we take an assertion to be satisfied. Confusing modality is known as the modal fallacy.

Aristotle's logic is in large parts concerned with the theory of non-modalized logic. Although, there are passages in his work, such as the famous sea-battle argument in "De Interpretatione" § 9, that are now seen as anticipations of modal logic and its connection with potentiality and time, the earliest formal system of modal logic was developed by Avicenna, who ultimately developed a theory of "temporally modalized" syllogistic.

While the study of necessity and possibility remained important to philosophers, little logical innovation happened until the landmark investigations of Clarence Irving Lewis in 1918, who formulated a family of rival axiomatizations of the alethic modalities. His work unleashed a torrent of new work on the topic, expanding the kinds of modality treated to include deontic logic and epistemic logic. The seminal work of Arthur Prior applied the same formal language to treat temporal logic and paved the way for the marriage of the two subjects. Saul Kripke discovered (contemporaneously with rivals) his theory of frame semantics, which revolutionized the formal technology available to modal logicians and gave a new graph-theoretic way of looking at modality that has driven many applications in computational linguistics and computer science, such as dynamic logic.

The motivation for the study of logic in ancient times was clear: it is so that one may learn to distinguish good arguments from bad arguments, and so become more effective in argument and oratory, and perhaps also to become a better person. Half of the works of Aristotle's Organon treat inference as it occurs in an informal setting, side by side with the development of the syllogistic, and in the Aristotelian school, these informal works on logic were seen as complementary to Aristotle's treatment of rhetoric.

This ancient motivation is still alive, although it no longer takes centre stage in the picture of logic; typically dialectical logic forms the heart of a course in critical thinking, a compulsory course at many universities. Dialectic has been linked to logic since ancient times, but it has not been until recent decades that European and American logicians have attempted to provide mathematical foundations for logic and dialectic by formalising dialectical logic. Dialectical logic is also the name given to the special treatment of dialectic in Hegelian and Marxist thought. There have been pre-formal treatises on argument and dialectic, from authors such as Stephen Toulmin ("The Uses of Argument"), Nicholas Rescher ("Dialectics"), and van Eemeren and Grootendorst (Pragma-dialectics). Theories of defeasible reasoning can provide a foundation for the formalisation of dialectical logic and dialectic itself can be formalised as moves in a game, where an advocate for the truth of a proposition and an opponent argue. Such games can provide a formal game semantics for many logics.

Argumentation theory is the study and research of informal logic, fallacies, and critical questions as they relate to every day and practical situations. Specific types of dialogue can be analyzed and questioned to reveal premises, conclusions, and fallacies. Argumentation theory is now applied in artificial intelligence and law.

Mathematical logic comprises two distinct areas of research: the first is the application of the techniques of formal logic to mathematics and mathematical reasoning, and the second, in the other direction, the application of mathematical techniques to the representation and analysis of formal logic.

The earliest use of mathematics and geometry in relation to logic and philosophy goes back to the ancient Greeks such as Euclid, Plato, and Aristotle. Many other ancient and medieval philosophers applied mathematical ideas and methods to their philosophical claims.

One of the boldest attempts to apply logic to mathematics was the logicism pioneered by philosopher-logicians such as Gottlob Frege and Bertrand Russell. Mathematical theories were supposed to be logical tautologies, and the programme was to show this by means of a reduction of mathematics to logic. The various attempts to carry this out met with failure, from the crippling of Frege's project in his "Grundgesetze" by Russell's paradox, to the defeat of Hilbert's program by Gödel's incompleteness theorems.

Both the statement of Hilbert's program and its refutation by Gödel depended upon their work establishing the second area of mathematical logic, the application of mathematics to logic in the form of proof theory. Despite the negative nature of the incompleteness theorems, Gödel's completeness theorem, a result in model theory and another application of mathematics to logic, can be understood as showing how close logicism came to being true: every rigorously defined mathematical theory can be exactly captured by a first-order logical theory; Frege's proof calculus is enough to "describe" the whole of mathematics, though not "equivalent" to it.

If proof theory and model theory have been the foundation of mathematical logic, they have been but two of the four pillars of the subject. Set theory originated in the study of the infinite by Georg Cantor, and it has been the source of many of the most challenging and important issues in mathematical logic, from Cantor's theorem, through the status of the Axiom of Choice and the question of the independence of the continuum hypothesis, to the modern debate on large cardinal axioms.

Recursion theory captures the idea of computation in logical and arithmetic terms; its most classical achievements are the undecidability of the Entscheidungsproblem by Alan Turing, and his presentation of the Church–Turing thesis. Today recursion theory is mostly concerned with the more refined problem of complexity classes—when is a problem efficiently solvable?—and the classification of degrees of unsolvability.

Philosophical logic deals with formal descriptions of ordinary, non-specialist ("natural") language, that is strictly only about the arguments within philosophy's other branches. Most philosophers assume that the bulk of everyday reasoning can be captured in logic if a method or methods to translate ordinary language into that logic can be found. Philosophical logic is essentially a continuation of the traditional discipline called "logic" before the invention of mathematical logic. Philosophical logic has a much greater concern with the connection between natural language and logic. As a result, philosophical logicians have contributed a great deal to the development of non-standard logics (e.g. free logics, tense logics) as well as various extensions of classical logic (e.g. modal logics) and non-standard semantics for such logics (e.g. Kripke's supervaluationism in the semantics of logic).

Logic and the philosophy of language are closely related. Philosophy of language has to do with the study of how our language engages and interacts with our thinking. Logic has an immediate impact on other areas of study. Studying logic and the relationship between logic and ordinary speech can help a person better structure his own arguments and critique the arguments of others. Many popular arguments are filled with errors because so many people are untrained in logic and unaware of how to formulate an argument correctly.

Logic cut to the heart of computer science as it emerged as a discipline: Alan Turing's work on the "Entscheidungsproblem" followed from Kurt Gödel's work on the incompleteness theorems. The notion of the general purpose computer that came from this work was of fundamental importance to the designers of the computer machinery in the 1940s.

In the 1950s and 1960s, researchers predicted that when human knowledge could be expressed using logic with mathematical notation, it would be possible to create a machine that reasons, or artificial intelligence. This was more difficult than expected because of the complexity of human reasoning. In logic programming, a program consists of a set of axioms and rules. Logic programming systems such as Prolog compute the consequences of the axioms and rules in order to answer a query.

Today, logic is extensively applied in the fields of artificial intelligence and computer science, and these fields provide a rich source of problems in formal and informal logic. Argumentation theory is one good example of how logic is being applied to artificial intelligence. The ACM Computing Classification System in particular regards:

Furthermore, computers can be used as tools for logicians. For example, in symbolic logic and mathematical logic, proofs by humans can be computer-assisted. Using automated theorem proving, the machines can find and check proofs, as well as work with proofs too lengthy to write out by hand.

The logics discussed above are all "bivalent" or "two-valued"; that is, they are most naturally understood as dividing propositions into true and false propositions. Non-classical logics are those systems that reject various rules of Classical logic.

Hegel developed his own dialectic logic that extended Kant's transcendental logic but also brought it back to ground by assuring us that "neither in heaven nor in earth, neither in the world of mind nor of nature, is there anywhere such an abstract 'either–or' as the understanding maintains. Whatever exists is concrete, with difference and opposition in itself".

In 1910, Nicolai A. Vasiliev extended the law of excluded middle and the law of contradiction and proposed the law of excluded fourth and logic tolerant to contradiction. In the early 20th century Jan Łukasiewicz investigated the extension of the traditional true/false values to include a third value, "possible", so inventing ternary logic, the first multi-valued logic in the Western tradition.

Logics such as fuzzy logic have since been devised with an infinite number of "degrees of truth", represented by a real number between 0 and 1.

Intuitionistic logic was proposed by L.E.J. Brouwer as the correct logic for reasoning about mathematics, based upon his rejection of the law of the excluded middle as part of his intuitionism. Brouwer rejected formalization in mathematics, but his student Arend Heyting studied intuitionistic logic formally, as did Gerhard Gentzen. Intuitionistic logic is of great interest to computer scientists, as it is a constructive logic and sees many applications, such as extracting verified programs from proofs and influencing the design of programming languages through the formulae-as-types correspondence.

Modal logic is not truth conditional, and so it has often been proposed as a non-classical logic. However, modal logic is normally formalized with the principle of the excluded middle, and its relational semantics is bivalent, so this inclusion is disputable.

What is the epistemological status of the laws of logic? What sort of argument is appropriate for criticizing purported principles of logic? In an influential paper entitled "Is Logic Empirical?" Hilary Putnam, building on a suggestion of W. V. Quine, argued that in general the facts of propositional logic have a similar epistemological status as facts about the physical universe, for example as the laws of mechanics or of general relativity, and in particular that what physicists have learned about quantum mechanics provides a compelling case for abandoning certain familiar principles of classical logic: if we want to be realists about the physical phenomena described by quantum theory, then we should abandon the principle of distributivity, substituting for classical logic the quantum logic proposed by Garrett Birkhoff and John von Neumann.

Another paper of the same name by Michael Dummett argues that Putnam's desire for realism mandates the law of distributivity. Distributivity of logic is essential for the realist's understanding of how propositions are true of the world in just the same way as he has argued the principle of bivalence is. In this way, the question, "Is Logic Empirical?" can be seen to lead naturally into the fundamental controversy in metaphysics on realism versus anti-realism.

The notion of implication formalized in classical logic does not comfortably translate into natural language by means of "if ... then ...", due to a number of problems called the paradoxes of material implication.

The first class of paradoxes involves counterfactuals, such as "If the moon is made of green cheese, then 2+2=5", which are puzzling because natural language does not support the principle of explosion. Eliminating this class of paradoxes was the reason for C.I. Lewis's formulation of strict implication, which eventually led to more radically revisionist logics such as relevance logic.

The second class of paradoxes involves redundant premises, falsely suggesting that we know the succedent because of the antecedent: thus "if that man gets elected, granny will die" is materially true since granny is mortal, regardless of the man's election prospects. Such sentences violate the Gricean maxim of relevance, and can be modelled by logics that reject the principle of monotonicity of entailment, such as relevance logic.

Hegel was deeply critical of any simplified notion of the law of non-contradiction. It was based on Gottfried Wilhelm Leibniz's idea that this law of logic also requires a sufficient ground to specify from what point of view (or time) one says that something cannot contradict itself. A building, for example, both moves and does not move; the ground for the first is our solar system and for the second the earth. In Hegelian dialectic, the law of non-contradiction, of identity, itself relies upon difference and so is not independently assertable.

Closely related to questions arising from the paradoxes of implication comes the suggestion that logic ought to tolerate inconsistency. Relevance logic and paraconsistent logic are the most important approaches here, though the concerns are different: a key consequence of classical logic and some of its rivals, such as intuitionistic logic, is that they respect the principle of explosion, which means that the logic collapses if it is capable of deriving a contradiction. Graham Priest, the main proponent of dialetheism, has argued for paraconsistency on the grounds that there are in fact, true contradictions.

The philosophical vein of various kinds of skepticism contains many kinds of doubt and rejection of the various bases on which logic rests, such as the idea of logical form, correct inference, or meaning, typically leading to the conclusion that there are no logical truths. This is in contrast with the usual views in philosophical skepticism, where logic directs skeptical enquiry to doubt received wisdoms, as in the work of Sextus Empiricus.

Friedrich Nietzsche provides a strong example of the rejection of the usual basis of logic: his radical rejection of idealization led him to reject truth as a "... mobile army of metaphors, metonyms, and anthropomorphisms—in short ... metaphors which are worn out and without sensuous power; coins which have lost their pictures and now matter only as metal, no longer as coins." His rejection of truth did not lead him to reject the idea of either inference or logic completely, but rather suggested that "logic [came] into existence in man's head [out] of illogic, whose realm originally must have been immense. Innumerable beings who made inferences in a way different from ours perished". Thus there is the idea that logical inference has a use as a tool for human survival, but that its existence does not support the existence of truth, nor does it have a reality beyond the instrumental: "Logic, too, also rests on assumptions that do not correspond to anything in the real world".

This position held by Nietzsche however, has come under extreme scrutiny for several reasons. Some philosophers, such as Jürgen Habermas, claim his position is self-refuting—and accuse Nietzsche of not even having a coherent perspective, let alone a theory of knowledge. Georg Lukács, in his book "The Destruction of Reason", asserts that, "Were we to study Nietzsche's statements in this area from a logico-philosophical angle, we would be confronted by a dizzy chaos of the most lurid assertions, arbitrary and violently incompatible." Bertrand Russell described Nietzsche's irrational claims with "He is fond of expressing himself paradoxically and with a view to shocking conventional readers" in his book "A History of Western Philosophy".



</doc>
<doc id="1957928" url="https://en.wikipedia.org/wiki?curid=1957928" title="State College of Florida, Manatee–Sarasota">
State College of Florida, Manatee–Sarasota

State College of Florida, Manatee-Sarasota (SCF) is a state college with campuses located in Manatee and Sarasota county, Florida. Part of the Florida College System, it is designated a "state college" because it offers a greater number of four-year bachelor's degrees than traditional two-year community colleges.

Founded in 1957 as Manatee Junior College, it was known as Manatee Community College from 1985 to 2009. Today SCF operates three campuses in Bradenton, Lakewood Ranch, and Venice. The Bradenton campus includes the Family Heritage House Museum, the SCF Collegiate School (SCFCS), The Neel Performing Arts Center, and the SCF Dental Hygiene Clinic, which provides low cost dental care to the public.

State College of Florida was established on September 17, 1957, by the Florida Board of Education as Manatee Junior College. The college came into existence under a plan of the Florida Board of Education to provide accessible higher education to Florida's population.

The first classes were held on September 2, 1958, in what was formerly a senior high school; enrollment in the first term was 502 students. The college began administering classes in its own facilities in 1959, where the 100 acre Bradenton campus stands today.

The Venice center was opened in 1977 by MJC's Board of Trustees. During this period, the center's functions were funded by the donations of residents living in the surrounding communities which included Venice, North Port, and Englewood. It was not until 1983 that the college received an appropriation from the Florida legislature to expand the Venice center into what is now the full-service Venice campus. It was dedicated on March 30, 1985 and the college's name was changed that year to Manatee Community College.

At the beginning of 2003, MCC opened the Lakewood Ranch campus. The land appropriated for this was donated by the Schroeder-Manatee Ranch. The Lakewood Ranch campus offers credit and non-credit programs of study, as well as technical and workforce development courses. In 2007, the Schroeder-Manatee Ranch donated an additional to the Lakewood Ranch campus. MCC obtained supplementary funding from the Florida legislature, which was allocated for the construction of a new classroom/laboratory building.

In 2009, MCC received approval from the State Board of Education to offer baccalaureate degrees and changed its name to State College of Florida, Manatee-Sarasota to reflect its new status as a four year state institution. The first bachelor's degree offered at SCF was a Bachelor of Science in Nursing which started in January of 2010. Several other bachelor's degrees are now available.

Students can attend classes on campuses located in Bradenton, Venice, and Lakewood Ranch. As well as many business and public-sector sites throughout the community, and from their homes via online classes. On September 29th, 2017, State College of Florida purchased of land to build a brand new campus in Parrish that will provide high quality education to those living north of the Manatee River.

The college president is Dr. Carol F. Probstfeld, who was inaugurated as the sixth president of State College of Florida on November 8th, 2013. On June 7th, 2018, Dr. Todd G. Fritch was named the first Executive Vice President and Provost of SCF.

State College of Florida is accredited by the Commission on Colleges of the Southern Association of Colleges and Schools to award associate and baccalaureate degrees. Noncredit education is offered under SCF's Corporate & Community Development programs. More than 50 percent of the college-bound high school students in Manatee and Sarasota counties attend SCF each year, with a current enrollment of over 30,000 students. SCF is among the top 100 producers of associate degrees in the United States.

SCF has over 50 different clubs and organizations for students to participate in such as intramural sports, Phi Theta Kappa, and a Circle K International club. 

State College of Florida's athletics department consist of five intercollegiate sports teams. They include: Men's basketball, Baseball, Softball, Women's tennis, and Woman's volleyball. SCF's athletic teams are nicknamed the Manatees and they participate in the Suncoast Conference of the Florida State College Activities Association (FSCAA) in Division I of NJCAA Region VIII.

State College of Florida's music department is home to over nine different performing ensembles consisting of the Bradenton symphony orchestra, Symphonic Band, Chamber choir, Concert choir, Jazz ensemble, Jazz combo, Guitar ensemble, Keyboard studies, Presidential string quartet, and the Musical theatre ensemble. Music students from SCF perform in multiple concerts throughout the semester as well as various community and state events such as the FSCAA Symposium. 

The theatre and musical theatre department at State College of Florida does a total of four productions per year, or two per semester. The Theatre faculty includes Dean Anthony, Craig Smith, James Thaggard (he is also the Box Office Manager), and Melodie Dickerson. Most theatre graduates have gone on to four-year universities to receive B.A. or B.F.A. degrees in Theatre and Performing Arts.

The Brain Bowl team at State College of Florida, currently coached by Christina Dwyer, has achieved state and national recognition for being one of the top quiz bowl programs in the country. In the 2014-2015 competition season, SCF's "Fire Team" compiled a record of 58-2 against other two-year schools, going on to win championships at tournaments such as the 2014 Delta Burke Invitational, 2015 FCSAA West Central Regional, 2015 FCSAA Brain Bowl State Championship, 2015 NAQT South Florida Community College Sectionals, and the 2015 NAQT Community College Championship Tournament. The team was also invited to compete in NAQT's Intercollegiate Championship Tournament (DII) where the team placed 25th with a record of 7-6, notably defeating four-year schools such as UC Berkeley, Duke University, University of Alabama, and Claremont Colleges in the process.

The 2015 State and National Championship teams consisted of team captain and club president Michael Moore Jr. and players Naimul Chowdhury, Leon Hostetler, Austin Goode, Carlyle Styer, Kara Stevens and Christopher Medrano. In individual competition, Moore and Chowdhury placed third and seventh in the nation respectively.
Former coaches include Dr. Hyun Kim (co-coach with Christina Dwyer during 2014-2015 season) and Dr. Carole Cole.
The 2016 State and National Championship teams consisted of team captain and club president Michael Friedman and players David Espinal, former championship winner Austin Goode, Paul Forester, and Haley Miller. They won first place at State and Second at Nationals. SCF Player David Espinal would lead the 2017 team to Championships and States with players Lacey Anderson, Damien Bobrek, and Nathanael Havlik. The team went on to win first place at Erik Korray, third in State, and third at Nationals.. 
The current 2018 team is lead by David Espinal and Austin Goode with fellow players Ariel Rodriguez, Lacey Anderson, Justin Reitwiesner, and Sierra Beeson.

The Bradenton Campus is home to the Family Heritage House Museum, a gallery and resource center for the study of African-American achievements. Exhibits include a timeline of significant events in African-American history, including slavery, fights for freedom, community building and education, the Harlem Renaissance, the Civil Rights Movement, Kwanzaa, and the modern era in South Africa. There are also displays about the Underground Railroad and a collection of African masks. Admission is free.

Alumni status is open to all graduates of State College of Florida (formerly Manatee Community College), all former students of SCF who regularly matriculated and left SCF in good standing.




</doc>
<doc id="59732997" url="https://en.wikipedia.org/wiki?curid=59732997" title="Inspired Education Group">
Inspired Education Group

Inspired Education Group is a group who operates and builds schools in Europe, the Middle East, Africa, Australia and Latin America.

Inspired was founded by Nadim M Nsouli in 2013, when his group acquired Reddam House in South Africa. Since its founding, Reddam House has acquired and built 9 schools in South Africa, along with the group's flagship school, Reddam House Berkshire, in Wokingham, England. The school grounds in Wokingham were taken over from Bearwood College, in 2015.

Nsouli - the founder, CEO and Chairman - has worked as a lawyer and as an investment banker, and he has led the group since its founding. Inspired’s President is Graeme Crawford, a South African educator and founder of Reddam House. Dr Stephen Spurr is the Group Education Director and was the Head Master of Westminster School in London from 2004 to 2015. He joined Inspired in 2014

The group’s strategy has been described as "buy and build", involving the purchase of existing schools, as well as the building of new schools. It has offices in London, Milan, Auckland, Bogota, Johannesburg,and Dubai.

Schools in other European countries that form part of the Inspired group include St. George's International School in Switzerland, St. John's International School in Belgium, St. Louis School in Italy and Sotogrande International School in Spain. In the Middle East, the group has acquired British School of Bahrain. Inspired's Latin American schools include Blue Valley School in Costa Rica, Colegio San Mateo in Colombia, and Cambridge College Lima in Peru. By 2017, Inspired operated more than 30 schools, and as of 2018 educating over 35,000 students in 46 schools.

Inspired acquired part of New Zealand’s biggest private-education provider ACG Education’s schools division in 2018 from Pacific Equity Partners for about $500 million. 



</doc>
<doc id="1005874" url="https://en.wikipedia.org/wiki?curid=1005874" title="Principle">
Principle

A principle is a proposition or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 "Principles of Operation".

Examples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.

In common English, it is a substantive and collective term referring to rule governance, the absence of which, being "unprincipled", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only "in principle" but not in fact.

A principle represents values that orient and rule the conduct of persons in a particular society. To "act on principle" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.

It represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.

Archimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the "population principle", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.

The principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:

However, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that "the Earth is round", corresponds to a direct relation between the subject and the predicate.

According to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.

The principle of the excluding third or "principium tertium exclusum" is a principle of the traditional logic formulated canonically by Leibniz as: either "A" is "B" or "A" isn't "B". It is read the following way: either "P" is true, or its denial ¬"P" is.
It is also known as "tertium non datur" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).


</doc>
<doc id="169115" url="https://en.wikipedia.org/wiki?curid=169115" title="Preternatural">
Preternatural

The preternatural or praeternatural is that which appears outside or beside (Latin "") the natural. It is "suspended between the mundane and the miraculous".

In theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.

Medieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in "God’s unmediated actions"; the natural is "what happens always or most of the time"; and the preternatural is "what happens rarely, but nonetheless by the agency of created beings ... Marvels belong, properly speaking, to the realm of the preternatural." Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,

By the 16th century, the term "preternatural" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, "being a natural Magician ... may perform many acts in ways above our knowledge, though not transcending our natural power." According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the "normal" pattern of natural phenomena.

With the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, "To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes." The use of the term was especially common in medicine, for example in John Brown's "A Compleat Treatise of Preternatural Tumours" (1678), or William Smellie's "A Collection of Preternatural Cases and Observations in Midwifery" (1754).

In the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. "The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest."

The linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as "gremlins in the engine", a "ghost in the machine", or attributing motives to objects: "the clouds are threatening". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.

In 2011, Penn State Press began publishing a learned journal titled "Preternature: Critical and Historical Studies on the Preternatural". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers "magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore."




</doc>
<doc id="21830" url="https://en.wikipedia.org/wiki?curid=21830" title="Nature">
Nature

Nature, in the broadest sense, is the natural, physical, or material world or universe. "Nature" can refer to the phenomena of the physical world, and also to life in general. The study of nature is a large, if not the only, part of science. Although humans are part of nature, human activity is often understood as a separate category from other natural phenomena.

The word "nature" is derived from the Latin word "natura", or "essential qualities, innate disposition", and in ancient times, literally meant "birth". "Natura" is a Latin translation of the Greek word "physis" (φύσις), which originally related to the intrinsic characteristics that plants, animals, and other features of the world develop of their own accord. The concept of nature as a whole, the physical universe, is one of several expansions of the original notion; it began with certain core applications of the word φύσις by pre-Socratic philosophers, and has steadily gained currency ever since. This usage continued during the advent of modern scientific method in the last several centuries.

Within the various uses of the word today, "nature" often refers to geology and wildlife. Nature can refer to the general realm of living plants and animals, and in some cases to the processes associated with inanimate objects—the way that particular types of things exist and change of their own accord, such as the weather and geology of the Earth. It is often taken to mean the "natural environment" or wilderness—wild animals, rocks, forest, and in general those things that have not been substantially altered by human intervention, or which persist despite human intervention. For example, manufactured objects and human interaction generally are not considered part of nature, unless qualified as, for example, "human nature" or "the whole of nature". This more traditional concept of natural things which can still be found today implies a distinction between the natural and the artificial, with the artificial being understood as that which has been brought into being by a human consciousness or a human mind. Depending on the particular context, the term "natural" might also be distinguished from the or the supernatural.

Earth is the only planet known to support life, and its natural features are the subject of many fields of scientific research. Within the solar system, it is third closest to the sun; it is the largest terrestrial planet and the fifth largest overall. Its most prominent climatic features are its two large polar regions, two relatively narrow temperate zones, and a wide equatorial tropical to subtropical region. Precipitation varies widely with location, from several metres of water per year to less than a millimetre. 71 percent of the Earth's surface is covered by salt-water oceans. The remainder consists of continents and islands, with most of the inhabited land in the Northern Hemisphere.

Earth has evolved through geological and biological processes that have left traces of the original conditions. The outer surface is divided into several gradually migrating tectonic plates. The interior remains active, with a thick layer of plastic mantle and an iron-filled core that generates a magnetic field. This iron core is composed of a solid inner phase, and a fluid outer phase. Convective motion in the core generates electric currents through dynamo action, and these, in turn, generate the geomagnetic field.

The atmospheric conditions have been significantly altered from the original conditions by the presence of life-forms, which create an ecological balance that stabilizes the surface conditions. Despite the wide regional variations in climate by latitude and other geographic factors, the long-term average global climate is quite stable during interglacial periods, and variations of a degree or two of average global temperature have historically had major effects on the ecological balance, and on the actual geography of the Earth.

Geology is the science and study of the solid and liquid matter that constitutes the Earth. The field of geology encompasses the study of the composition, structure, physical properties, dynamics, and history of Earth materials, and the processes by which they are formed, moved, and changed. The field is a major academic discipline, and is also important for mineral and hydrocarbon extraction, knowledge about and mitigation of natural hazards, some Geotechnical engineering fields, and understanding past climates and environments.

The geology of an area evolves through time as rock units are deposited and inserted and deformational processes change their shapes and locations.

Rock units are first emplaced either by deposition onto the surface or intrude into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows, blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.

After the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.

Earth is estimated to have formed 4.54 billion years ago from the solar nebula, along with the Sun and other planets. The moon formed roughly 20 million years later. Initially molten, the outer layer of the Earth cooled, resulting in the solid crust. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, most or all of which came from ice delivered by comets, produced the oceans and other water sources. The highly energetic chemistry is believed to have produced a self-replicating molecule around 4 billion years ago.

Continents formed, then broke up and reformed as the surface of Earth reshaped over hundreds of millions of years, occasionally combining to make a supercontinent. Roughly 750 million years ago, the earliest known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia which broke apart about 540 million years ago, then finally Pangaea, which broke apart about 180 million years ago.

During the Neoproterozoic era, freezing temperatures covered much of the Earth in glaciers and ice sheets. This hypothesis has been termed the "Snowball Earth", and it is of particular interest as it precedes the Cambrian explosion in which multicellular life forms began to proliferate about 530–540 million years ago.

Since the Cambrian explosion there have been five distinctly identifiable mass extinctions. The last mass extinction occurred some 66 million years ago, when a meteorite collision probably triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared small animals such as mammals. Over the past 66 million years, mammalian life diversified.

Several million years ago, a species of small African ape gained the ability to stand upright. The subsequent advent of human life, and the development of agriculture and further civilization allowed humans to affect the Earth more rapidly than any previous life form, affecting both the nature and quantity of other organisms as well as global climate. By comparison, the Great Oxygenation Event, produced by the proliferation of algae during the Siderian period, required about 300 million years to culminate.

The present era is classified as part of a mass extinction event, the Holocene extinction event, the fastest ever to have occurred. Some, such as E. O. Wilson of Harvard University, predict that human destruction of the biosphere could cause the extinction of one-half of all species in the next 100 years. The extent of the current extinction event is still being researched, debated and calculated by biologists.
The Earth's atmosphere is a key factor in sustaining the ecosystem. The thin layer of gases that envelops the Earth is held in place by gravity. Air is mostly nitrogen, oxygen, water vapor, with much smaller amounts of carbon dioxide, argon, etc. The atmospheric pressure declines steadily with altitude. The ozone layer plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.

Terrestrial weather occurs almost exclusively in the lower part of the atmosphere, and serves as a convective system for redistributing heat. Ocean currents are another important factor in determining climate, particularly the major underwater thermohaline circulation which distributes heat energy from the equatorial oceans to the polar regions. These currents help to moderate the differences in temperature between winter and summer in the temperate zones. Also, without the redistributions of heat energy by the ocean currents and atmosphere, the tropics would be much hotter, and the polar regions much colder.

Weather can have both beneficial and harmful effects. Extremes in weather, such as tornadoes or hurricanes and cyclones, can expend large amounts of energy along their paths, and produce devastation. Surface vegetation has evolved a dependence on the seasonal variation of the weather, and sudden changes lasting only a few years can have a dramatic effect, both on the vegetation and on the animals which depend on its growth for their food.

Climate is a measure of the long-term trends in the weather. Various factors are known to influence the climate, including ocean currents, surface albedo, greenhouse gases, variations in the solar luminosity, and changes to the Earth's orbit. Based on historical records, the Earth is known to have undergone drastic climate changes in the past, including ice ages.

The climate of a region depends on a number of factors, especially latitude. A latitudinal band of the surface with similar climatic attributes forms a climate region. There are a number of such regions, ranging from the tropical climate at the equator to the polar climate in the northern and southern extremes. Weather is also influenced by the seasons, which result from the Earth's axis being tilted relative to its orbital plane. Thus, at any given time during the summer or winter, one part of the Earth is more directly exposed to the rays of the sun. This exposure alternates as the Earth revolves in its orbit. At any given time, regardless of season, the northern and southern hemispheres experience opposite seasons.

Weather is a chaotic system that is readily modified by small changes to the environment, so accurate weather forecasting is limited to only a few days. Overall, two things are happening worldwide: (1) temperature is increasing on the average; and (2) regional climates have been undergoing noticeable changes.

Water is a chemical substance that is composed of hydrogen and oxygen and is vital for all known forms of life. In typical usage, "water" refers only to its liquid form or state, but the substance also has a solid state, ice, and a gaseous state, water vapor, or steam. Water covers 71% of the Earth's surface. On Earth, it is found mostly in oceans and other large bodies of water, with 1.6% of water below ground in aquifers and 0.001% in the air as vapor, clouds, and precipitation. Oceans hold 97% of surface water, glaciers, and polar ice caps 2.4%, and other land surface water such as rivers, lakes, and ponds 0.6%. Additionally, a minute amount of the Earth's water is contained within biological bodies and manufactured products.

An ocean is a major body of saline water, and a principal component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 361 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. This concept of a global ocean as a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.

The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean, and the Arctic Ocean. Smaller regions of the oceans are called seas, gulfs, bays and other names. There are also salt lakes, which are smaller bodies of landlocked saltwater that are not interconnected with the World Ocean. Two notable examples of salt lakes are the Aral Sea and the Great Salt Lake.

A lake (from Latin "lacus") is a terrain feature (or physical feature), a body of liquid on the surface of a world that is localized to the bottom of basin (another type of landform or terrain feature; that is, it is not global) and moves slowly if it moves at all. On Earth, a body of water is considered a lake when it is inland, not part of the ocean, is larger and deeper than a pond, and is fed by a river. The only world other than Earth known to harbor lakes is Titan, Saturn's largest moon, which has lakes of ethane, most likely mixed with methane. It is not known if Titan's lakes are fed by rivers, though Titan's surface is carved by numerous river beds. Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.

A pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams via current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.

A river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. In a few cases, a river simply flows into the ground or dries up completely before reaching another body of water. Small rivers may also be called by several other names, including stream, creek, brook, rivulet, and rill; there is no general rule that defines what can be called a river. Many names for small rivers are specific to geographic location; one example is "Burn" in Scotland and North-east England. Sometimes a river is said to be larger than a creek, but this is not always the case, due to vagueness in the language. A river is part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of stored water in natural ice and snowpacks (i.e., from glaciers).

A stream is a flowing body of water with a current, confined within a bed and stream banks. In the United States, a stream is classified as a watercourse less than wide. Streams are important as conduits in the water cycle, instruments in groundwater recharge, and they serve as corridors for fish and wildlife migration. The biological habitat in the immediate vicinity of a stream is called a riparian zone. Given the status of the ongoing Holocene extinction, streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general involves many branches of inter-disciplinary natural science and engineering, including hydrology, fluvial geomorphology, aquatic ecology, fish biology, riparian ecology, and others.

Ecosystems are composed of a variety of abiotic and biotic components that function in an interrelated way. The structure and composition is determined by various environmental factors that are interrelated. Variations of these factors will initiate dynamic modifications to the ecosystem. Some of the more important components are: soil, atmosphere, radiation from the sun, water, and living organisms.

Central to the ecosystem concept is the idea that living organisms interact with every other element in their local environment. Eugene Odum, a founder of ecology, stated: "Any unit that includes all of the organisms (ie: the "community") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem." Within the ecosystem, species are connected and dependent upon one another in the food chain, and exchange energy and matter between themselves as well as with their environment. The human ecosystem concept is based on the human/nature dichotomy and the idea that all species are ecologically dependent on each other, as well as with the abiotic constituents of their biotope.

A smaller unit of size is called a microecosystem. For example, a microsystem can be a stone and all the life under it. A "macroecosystem" might involve a whole ecoregion, with its drainage basin.

Wilderness is generally defined as areas that have not been significantly modified by human activity. Wilderness areas can be found in preserves, estates, farms, conservation preserves, ranches, , national parks, and even in urban areas along rivers, gulches, or otherwise undeveloped areas. Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, and solitude. Some nature writers believe wilderness areas are vital for the human spirit and creativity, and some ecologists consider wilderness areas to be an integral part of the Earth's self-sustaining natural ecosystem (the biosphere). They may also preserve historic genetic traits and that they provide habitat for wild flora and fauna that may be difficult or impossible to recreate in zoos, arboretums, or laboratories.

Although there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli, and reproduction. Life may also be said to be simply the characteristic state of organisms.

Properties common to terrestrial organisms (plants, animals, fungi, protists, archaea, and bacteria) are that they are cellular, carbon-and-water-based with complex organization, having a metabolism, a capacity to grow, respond to stimuli, and reproduce. An entity with these properties is generally considered life. However, not every definition of life considers all of these properties to be essential. Human-made analogs of life may also be considered to be life.

The biosphere is the part of Earth's outer shell – including land, surface rocks, water, air and the atmosphere – within which life occurs, and which biotic processes in turn alter or transform. From the broadest geophysiological point of view, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere (rocks), hydrosphere (water), and atmosphere (air). The entire Earth contains over 75 billion tons (150 "trillion" pounds or about 6.8×10 kilograms) of biomass (life), which lives within various environments within the biosphere.

Over nine-tenths of the total biomass on Earth is plant life, on which animal life depends very heavily for its existence. More than 2 million species of plant and animal life have been identified to date, and estimates of the actual number of existing species range from several million to well over 50 million. The number of individual species of life is constantly in some degree of flux, with new species appearing and others ceasing to exist on a continual basis. The total number of species is in rapid decline.

The origin of life on Earth is not well understood, but it is known to have occurred at least 3.5 billion years ago, during the hadean or archean eons on a primordial Earth that had a substantially different environment than is found at present. These life forms possessed the basic traits of self-replication and inheritable traits. Once life had appeared, the process of evolution by natural selection resulted in the development of ever-more diverse life forms.

Species that were unable to adapt to the changing environment and competition from other life forms became extinct. However, the fossil record retains evidence of many of these older species. Current fossil and DNA evidence shows that all existing species can trace a continual ancestry back to the first primitive life forms.

When basic forms of plant life developed the process of photosynthesis the sun's energy could be harvested to create conditions which allowed for more complex life forms. The resultant oxygen accumulated in the atmosphere and gave rise to the ozone layer. The incorporation of smaller cells within larger ones resulted in the development of yet more complex cells called eukaryotes. Cells within colonies became increasingly specialized, resulting in true multicellular organisms. With the ozone layer absorbing harmful ultraviolet radiation, life colonized the surface of Earth.

The first form of life to develop on the Earth were microbes, and they remained the only form of life until about a billion years ago when multi-cellular organisms began to appear. Microorganisms are single-celled organisms that are generally microscopic, and smaller than the human eye can see. They include Bacteria, Fungi, Archaea, and Protista.

These life forms are found in almost every location on the Earth where there is liquid water, including in the Earth's interior.
Their reproduction is both rapid and profuse. The combination of a high mutation rate and a horizontal gene transfer ability makes them highly adaptable, and able to survive in new environments, including outer space. They form an essential part of the planetary ecosystem. However, some microorganisms are pathogenic and can post health risk to other organisms.

Originally Aristotle divided all living things between plants, which generally do not move fast enough for humans to notice, and animals. In Linnaeus' system, these became the kingdoms Vegetabilia (later Plantae) and Animalia. Since then, it has become clear that the Plantae as originally defined included several unrelated groups, and the fungi and several groups of algae were removed to new kingdoms. However, these are still often considered plants in many contexts. Bacterial life is sometimes included in flora, and some classifications use the term "bacterial flora" separately from "plant flora".

Among the many ways of classifying plants are by regional floras, which, depending on the purpose of study, can also include "fossil flora", remnants<br> of plant life from a previous era. People in many regions and countries take great pride in their individual arrays of characteristic flora, which can vary widely across the globe due to differences in climate and terrain.

Regional floras commonly are divided into categories such as "native flora" and "agricultural and garden flora", the lastly mentioned of which are intentionally grown and cultivated. Some types of "native flora" actually have been introduced centuries ago by people migrating from one region or continent to another, and become an integral part of the native, or natural flora of the place to which they were introduced. This is an example of how human interaction with nature can blur the boundary of what is considered nature.

Another category of plant has historically been carved out for "weeds". Though the term has fallen into disfavor among botanists as a formal way to categorize "useless" plants, the informal use of the word "weeds" to describe those plants that are deemed worthy of elimination is illustrative of the general tendency of people and societies to seek to alter or shape the course of nature. Similarly, animals are often categorized in ways such as "domestic", "farm animals", "wild animals", "pests", etc. according to their relationship to human life.

Animals as a category have several characteristics that generally set them apart from other living things. Animals are eukaryotic and usually multicellular (although see Myxozoa), which separates them from bacteria, archaea, and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking cell walls.

With a few exceptions—most notably the two phyla consisting of sponges and placozoans—animals have bodies that are differentiated into tissues. These include muscles, which are able to contract and control locomotion, and a nervous system, which sends and processes signals. There is also typically an internal digestive chamber. The eukaryotic cells possessed by all animals are surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules, a framework upon which cells can move about and be reorganized during development and maturation, and which supports the complex anatomy required for mobility.

Although humans comprise only a minuscule proportion of the total living biomass on Earth, the human effect on nature is disproportionately large. Because of the extent of human influence, the boundaries between what humans regard as nature and "made environments" is not clear cut except at the extremes. Even at the extremes, the amount of natural environment that is free of discernible human influence is diminishing at an increasingly rapid pace.

The development of technology by the human race has allowed the greater exploitation of natural resources and has helped to alleviate some of the risk from natural hazards. In spite of this progress, however, the fate of human civilization remains closely linked to changes in the environment. There exists a highly complex feedback loop between the use of advanced technology and changes to the environment that are only slowly becoming understood. Man-made threats to the Earth's natural environment include pollution, deforestation, and disasters such as oil spills. Humans have contributed to the extinction of many plants and animals.

Humans employ nature for both leisure and economic activities. The acquisition of natural resources for industrial use remains a sizable component of the world's economic system. Some activities, such as hunting and fishing, are used for both sustenance and leisure, often by different people. Agriculture was first adopted around the 9th millennium BCE. Ranging from food production to energy, nature influences economic wealth.

Although early humans gathered uncultivated plant materials for food and employed the medicinal properties of vegetation for healing, most modern human use of plants is through agriculture. The clearance of large tracts of land for crop growth has led to a significant reduction in the amount available of forestation and wetlands, resulting in the loss of habitat for many plant and animal species as well as increased erosion.

Beauty in nature has historically been a prevalent theme in art and books, filling large sections of libraries and bookstores. That nature has been depicted and celebrated by so much art, photography, poetry, and other literature shows the strength with which many people associate nature and beauty. Reasons why this association exists, and what the association consists of, are studied by the branch of philosophy called aesthetics. Beyond certain basic characteristics that many philosophers agree about to explain what is seen as beautiful, the opinions are virtually endless. Nature and wildness have been important subjects in various eras of world history. An early tradition of landscape art began in China during the Tang Dynasty (618–907). The tradition of representing nature "as it is" became one of the aims of Chinese painting and was a significant influence in Asian art.

Although natural wonders are celebrated in the Psalms and the Book of Job, wilderness portrayals in art became more prevalent in the 1800s, especially in the works of the Romantic movement. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Before that, paintings had been primarily of religious scenes or of human beings. William Wordsworth's poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture. This artistic movement also coincided with the Transcendentalist movement in the Western world. A common classical idea of beautiful art involves the word mimesis, the imitation of nature. Also in the realm of ideas about beauty in nature is that the perfect is implied through perfect mathematical forms and more generally by patterns in nature. As David Rothenburg writes, "The beautiful is the root of science and the goal of art, the highest possibility that humanity can ever hope to see".

Some fields of science see nature as matter in motion, obeying certain laws of nature which science seeks to understand. For this reason the most fundamental science is generally understood to be "physics" – the name for which is still recognizable as meaning that it is the study of nature.

Matter is commonly defined as the substance of which physical objects are composed. It constitutes the observable universe. The visible components of the universe are now believed to compose only 4.9 percent of the total mass. The remainder is believed to consist of 26.8 percent cold dark matter and 68.3 percent dark energy. The exact arrangement of these components is still unknown and is under intensive investigation by physicists.

The behavior of matter and energy throughout the observable universe appears to follow well-defined physical laws. These laws have been employed to produce cosmological models that successfully explain the structure and the evolution of the universe we can observe. The mathematical expressions of the laws of physics employ a set of twenty physical constants that appear to be static across the observable universe. The values of these constants have been carefully measured, but the reason for their specific values remains a mystery.

Outer space, also simply called "space", refers to the relatively empty regions of the universe outside the atmospheres of celestial bodies. "Outer" space is used to distinguish it from airspace (and terrestrial locations). There is no discrete boundary between the Earth's atmosphere and space, as the atmosphere gradually attenuates with increasing altitude. Outer space within the Solar System is called interplanetary space, which passes over into interstellar space at what is known as the heliopause.

Outer space is sparsely filled with several dozen types of organic molecules discovered to date by microwave spectroscopy, blackbody radiation left over from the big bang and the origin of the universe, and cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also some gas, plasma and dust, and small meteors. Additionally, there are signs of human life in outer space today, such as material left over from previous manned and unmanned launches which are a potential hazard to spacecraft. Some of this debris re-enters the atmosphere periodically.

Although the Earth is the only body within the solar system known to support life, evidence suggests that in the distant past the planet Mars possessed bodies of liquid water on the surface. For a brief period in Mars' history, it may have also been capable of forming life. At present though, most of the water remaining on Mars is frozen.
If life exists at all on Mars, it is most likely to be located underground where liquid water can still exist.

Conditions on the other terrestrial planets, Mercury and Venus, appear to be too harsh to support life as we know it. But it has been conjectured that Europa, the fourth-largest moon of Jupiter, may possess a sub-surface ocean of liquid water and could potentially host life.

Astronomers have started to discover extrasolar Earth analogs – planets that lie in the habitable zone of space surrounding a star, and therefore could possibly host life as we know it.

Media:
Organizations:
Philosophy:



</doc>
<doc id="35659147" url="https://en.wikipedia.org/wiki?curid=35659147" title="Patterns in nature">
Patterns in nature

Patterns in nature are visible regularities of form found in the natural world. These patterns recur in different contexts and can sometimes be modelled mathematically. Natural patterns include symmetries, trees, spirals, meanders, waves, foams, tessellations, cracks and stripes. Early Greek philosophers studied pattern, with Plato, Pythagoras and Empedocles attempting to explain order in nature. The modern understanding of visible patterns developed gradually over time.

In the 19th century, Belgian physicist Joseph Plateau examined soap films, leading him to formulate the concept of a minimal surface. German biologist and artist Ernst Haeckel painted hundreds of marine organisms to emphasise their symmetry. Scottish biologist D'Arcy Thompson pioneered the study of growth patterns in both plants and animals, showing that simple equations could explain spiral growth. In the 20th century, British mathematician Alan Turing predicted mechanisms of morphogenesis which give rise to patterns of spots and stripes. Hungarian biologist Aristid Lindenmayer and French American mathematician Benoît Mandelbrot showed how the mathematics of fractals could create plant growth patterns.

Mathematics, physics and chemistry can explain patterns in nature at different levels. Patterns in living things are explained by the biological processes of natural selection and sexual selection. Studies of pattern formation make use of computer models to simulate a wide range of patterns.

Early Greek philosophers attempted to explain order in nature, anticipating modern concepts. Pythagoras (c. 570–c. 495 BC) explained patterns in nature like the harmonies of music as arising from number, which he took to be the basic constituent of existence. Empedocles (c. 494–c. 434 BC) to an extent anticipated Darwin's evolutionary explanation for the structures of organisms. Plato (c. 427–c. 347 BC) argued for the existence of natural universals. He considered these to consist of ideal forms ( "eidos": "form") of which physical objects are never more than imperfect copies. Thus, a flower may be roughly circular, but it is never a perfect circle.

Theophrastus (c. 372–c. 287 BC) noted that plants "that have flat leaves have them in a regular series"; Pliny the Elder (23–79 AD) noted their patterned circular arrangement. Centuries later, Leonardo da Vinci (1452–1519) noted the spiral arrangement of leaf patterns. Johannes Kepler (1571–1630) pointed out the presence of the Fibonacci sequence in nature, using it to explain the pentagonal form of some flowers. In 1754, Charles Bonnet observed that the spiral phyllotaxis of plants were frequently expressed in both clockwise and counter-clockwise golden ratio series. Mathematical observations of phyllotaxis followed with Karl Friedric Schimper and his friend Alexander Braun's 1830 and 1830 work, respectively; Auguste Bravais and his brother Louis connected phyllotaxis ratios to the Fibonacci sequence in 1837, also noting its appearance in pinecones and pineapples. In his 1854 book, German psychologist Adolf Zeising explored the golden ratio expressed in the arrangement of plant parts, the skeletons of animals and the branching patterns of their veins and nerves, as well as in crystals. A. H. Church studied the patterns of phyllotaxis in his 1904 book. In 1917, D'Arcy Thompson published "On Growth and Form"; his description of phyllotaxis and the Fibonacci sequence, the mathematical relationships in the spiral growth patterns of plants showed that simple equations could describe the spiral growth patterns of animal horns and mollusc shells.

In 1202, Leonardo Fibonacci introduced the Fibonacci sequence to the western world with his book "Liber Abaci". Fibonacci presented a thought experiment on the growth of an idealized rabbit population.

In 1658, the English physician and philosopher Sir Thomas Browne discussed "how Nature Geometrizeth" in "The Garden of Cyrus", citing Pythagorean numerology involving the number 5, and the Platonic form of the quincunx pattern. The discourse's central chapter features examples and observations of the quincunx in botany.

The Belgian physicist Joseph Plateau (1801–1883) formulated the mathematical problem of the existence of a minimal surface with a given boundary, which is now named after him. He studied soap films intensively, formulating Plateau's laws which describe the structures formed by films in foams.

Ernst Haeckel (1834–1919) painted beautiful illustrations of marine organisms, in particular Radiolaria, emphasising their symmetry to support his faux-Darwinian theories of evolution.

The American photographer Wilson Bentley took the first micrograph of a snowflake in 1885.
In 1952, Alan Turing (1912–1954), better known for his work on computing and codebreaking, wrote "The Chemical Basis of Morphogenesis", an analysis of the mechanisms that would be needed to create patterns in living organisms, in the process called morphogenesis. He predicted oscillating chemical reactions, in particular the Belousov–Zhabotinsky reaction. These activator-inhibitor mechanisms can, Turing suggested, generate patterns (dubbed "Turing patterns") of stripes and spots in animals, and contribute to the spiral patterns seen in plant phyllotaxis.

In 1968, the Hungarian theoretical biologist Aristid Lindenmayer (1925–1989) developed the L-system, a formal grammar which can be used to model plant growth patterns in the style of fractals. L-systems have an alphabet of symbols that can be combined using production rules to build larger strings of symbols, and a mechanism for translating the generated strings into geometric structures. In 1975, after centuries of slow development of the mathematics of patterns by Gottfried Leibniz, Georg Cantor, Helge von Koch, Wacław Sierpiński and others, Benoît Mandelbrot wrote a famous paper, "How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension", crystallising mathematical thought into the concept of the fractal.

Living things like orchids, hummingbirds, and the peacock's tail have abstract designs with a beauty of form, pattern and colour that artists struggle to match. The beauty that people perceive in nature has causes at different levels, notably in the mathematics that governs what patterns can physically form, and among living things in the effects of natural selection, that govern how patterns evolve.

Mathematics seeks to discover and explain abstract patterns or regularities of all kinds.
Visual patterns in nature find explanations in chaos theory, fractals, logarithmic spirals, topology and other mathematical patterns. For example, L-systems form convincing models of different patterns of tree growth.
The laws of physics apply the abstractions of mathematics to the real world, often as if it were perfect. For example, a crystal is perfect when it has no structural defects such as dislocations and is fully symmetric. Exact mathematical perfection can only approximate real objects. Visible patterns in nature are governed by physical laws; for example, meanders can be explained using fluid dynamics.

In biology, natural selection can cause the development of patterns in living things for several reasons, including camouflage, sexual selection, and different kinds of signalling, including mimicry and cleaning symbiosis. In plants, the shapes, colours, and patterns of insect-pollinated flowers like the lily have evolved to attract insects such as bees. Radial patterns of colours and stripes, some visible only in ultraviolet light serve as nectar guides that can be seen at a distance.

Symmetry is pervasive in living things. Animals mainly have bilateral or mirror symmetry, as do the leaves of plants and some flowers such as orchids. Plants often have radial or rotational symmetry, as do many flowers and some groups of animals such as sea anemones. Fivefold symmetry is found in the echinoderms, the group that includes starfish, sea urchins, and sea lilies.

Among non-living things, snowflakes have striking sixfold symmetry; each flake's structure forms a record of the varying conditions during its crystallization, with nearly the same pattern of growth on each of its six arms. Crystals in general have a variety of symmetries and crystal habits; they can be cubic or octahedral, but true crystals cannot have fivefold symmetry (unlike quasicrystals). Rotational symmetry is found at different scales among non-living things, including the crown-shaped splash pattern formed when a drop falls into a pond, and both the spheroidal shape and rings of a planet like Saturn.

Symmetry has a variety of causes. Radial symmetry suits organisms like sea anemones whose adults do not move: food and threats may arrive from any direction. But animals that move in one direction necessarily have upper and lower sides, head and tail ends, and therefore a left and a right. The head becomes specialised with a mouth and sense organs (cephalisation), and the body becomes bilaterally symmetric (though internal organs need not be). More puzzling is the reason for the fivefold (pentaradiate) symmetry of the echinoderms. Early echinoderms were bilaterally symmetrical, as their larvae still are. Sumrall and Wray argue that the loss of the old symmetry had both developmental and ecological causes.

Fractals are infinitely self-similar, iterated mathematical constructs having fractal dimension. Infinite iteration is not possible in nature so all 'fractal' patterns are only approximate. For example, the leaves of ferns and umbellifers (Apiaceae) are only self-similar (pinnate) to 2, 3 or 4 levels. Fern-like growth patterns occur in plants and in animals including bryozoa, corals, hydrozoa like the air fern, "Sertularia argentea", and in non-living things, notably electrical discharges. Lindenmayer system fractals can model different patterns of tree growth by varying a small number of parameters including branching angle, distance between nodes or branch points (internode length), and number of branches per branch point.

Fractal-like patterns occur widely in nature, in phenomena as diverse as clouds, river networks, geologic fault lines, mountains, coastlines, animal coloration, snow flakes, crystals, blood vessel branching, actin cytoskeleton, and ocean waves.

Spirals are common in plants and in some animals, notably molluscs. For example, in the nautilus, a cephalopod mollusc, each chamber of its shell is an approximate copy of the next one, scaled by a constant factor and arranged in a logarithmic spiral. Given a modern understanding of fractals, a growth spiral can be seen as a special case of self-similarity.

Plant spirals can be seen in phyllotaxis, the arrangement of leaves on a stem, and in the arrangement (parastichy) of other parts as in composite flower heads and seed heads like the sunflower or fruit structures like the pineapple and snake fruit, as well as in the pattern of scales in pine cones, where multiple spirals run both clockwise and anticlockwise. These arrangements have explanations at different levels – mathematics, physics, chemistry, biology – each individually correct, but all necessary together. Phyllotaxis spirals can be generated mathematically from Fibonacci ratios: the Fibonacci sequence runs 1, 1, 2, 3, 5, 8, 13... (each subsequent number being the sum of the two preceding ones). For example, when leaves alternate up a stem, one rotation of the spiral touches two leaves, so the pattern or ratio is 1/2. In hazel the ratio is 1/3; in apricot it is 2/5; in pear it is 3/8; in almond it is 5/13. In disc phyllotaxis as in the sunflower and daisy, the florets are arranged in Fermat's spiral with Fibonacci numbering, at least when the flowerhead is mature so all the elements are the same size. Fibonacci ratios approximate the golden angle, 137.508°, which governs the curvature of Fermat's spiral.

From the point of view of physics, spirals are lowest-energy configurations which emerge spontaneously through self-organizing processes in dynamic systems. From the point of view of chemistry, a spiral can be generated by a reaction-diffusion process, involving both activation and inhibition. Phyllotaxis is controlled by proteins that manipulate the concentration of the plant hormone auxin, which activates meristem growth, alongside other mechanisms to control the relative angle of buds around the stem. From a biological perspective, arranging leaves as far apart as possible in any given space is favoured by natural selection as it maximises access to resources, especially sunlight for photosynthesis.

In mathematics, a dynamical system is chaotic if it is (highly) sensitive to initial conditions (the so-called "butterfly effect"), which requires the mathematical properties of topological mixing and dense periodic orbits.

Alongside fractals, chaos theory ranks as an essentially universal influence on patterns in nature. There is a relationship between chaos and fractals—the "strange attractors" in chaotic systems have a fractal dimension. Some cellular automata, simple sets of mathematical rules that generate patterns, have chaotic behaviour, notably Stephen Wolfram's Rule 30.

Vortex streets are zigzagging patterns of whirling vortices created by the unsteady separation of flow of a fluid, most often air or water, over obstructing objects. Smooth (laminar) flow starts to break up when the size of the obstruction or the velocity of the flow become large enough compared to the viscosity of the fluid.

Meanders are sinuous bends in rivers or other channels, which form as a fluid, most often water, flows around bends. As soon as the path is slightly curved, the size and curvature of each loop increases as helical flow drags material like sand and gravel across the river to the inside of the bend. The outside of the loop is left clean and unprotected, so erosion accelerates, further increasing the meandering in a powerful positive feedback loop.

Waves are disturbances that carry energy as they move. Mechanical waves propagate through a medium – air or water, making it oscillate as they pass by. Wind waves are sea surface waves that create the characteristic chaotic pattern of any large body of water, though their statistical behaviour can be predicted with wind wave models. As waves in water or wind pass over sand, they create patterns of ripples. When winds blow over large bodies of sand, they create dunes, sometimes in extensive dune fields as in the Taklamakan desert. Dunes may form a range of patterns including crescents, very long straight lines, stars, domes, parabolas, and longitudinal or seif ('sword') shapes.

Barchans or crescent dunes are produced by wind acting on desert sand; the two horns of the crescent and the slip face point downwind. Sand blows over the upwind face, which stands at about 15 degrees from the horizontal, and falls onto the slip face, where it accumulates up to the angle of repose of the sand, which is about 35 degrees. When the slip face exceeds the angle of repose, the sand avalanches, which is a nonlinear behaviour: the addition of many small amounts of sand causes nothing much to happen, but then the addition of a further small amount suddenly causes a large amount to avalanche. Apart from this nonlinearity, barchans behave rather like solitary waves.

A soap bubble forms a sphere, a surface with minimal area — the smallest possible surface area for the volume enclosed. Two bubbles together form a more complex shape: the outer surfaces of both bubbles are spherical; these surfaces are joined by a third spherical surface as the smaller bubble bulges slightly into the larger one.

A foam is a mass of bubbles; foams of different materials occur in nature. Foams composed of soap films obey Plateau's laws, which require three soap films to meet at each edge at 120° and four soap edges to meet at each vertex at the tetrahedral angle of about 109.5°. Plateau's laws further require films to be smooth and continuous, and to have a constant average curvature at every point. For example, a film may remain nearly flat on average by being curved up in one direction (say, left to right) while being curved downwards in another direction (say, front to back). Structures with minimal surfaces can be used as tents. Lord Kelvin identified the problem of the most efficient way to pack cells of equal volume as a foam in 1887; his solution uses just one solid, the bitruncated cubic honeycomb with very slightly curved faces to meet Plateau's laws. No better solution was found until 1993 when Denis Weaire and Robert Phelan proposed the Weaire–Phelan structure; the Beijing National Aquatics Center adapted the structure for their outer wall in the 2008 Summer Olympics.

At the scale of living cells, foam patterns are common; radiolarians, sponge spicules, silicoflagellate exoskeletons and the calcite skeleton of a sea urchin, "Cidaris rugosa", all resemble mineral casts of Plateau foam boundaries. The skeleton of the Radiolarian, "Aulonia hexagona", a beautiful marine form drawn by Ernst Haeckel, looks as if it is a sphere composed wholly of hexagons, but this is mathematically impossible. The Euler characteristic states that for any convex polyhedron, the number of faces plus the number of vertices (corners) equals the number of edges plus two. A result of this formula is that any closed polyhedron of hexagons has to include exactly 12 pentagons, like a soccer ball, Buckminster Fuller geodesic dome, or fullerene molecule. This can be visualised by noting that a mesh of hexagons is flat like a sheet of chicken wire, but each pentagon that is added forces the mesh to bend (there are fewer corners, so the mesh is pulled in).

Tessellations are patterns formed by repeating tiles all over a flat surface. There are 17 wallpaper groups of tilings. While common in art and design, exactly repeating tilings are less easy to find in living things. The cells in the paper nests of social wasps, and the wax cells in honeycomb built by honey bees are well-known examples. Among animals, bony fish, reptiles or the pangolin, or fruits like the salak are protected by overlapping scales or osteoderms, these form more-or-less exactly repeating units, though often the scales in fact vary continuously in size. Among flowers, the snake's head fritillary, "Fritillaria meleagris", have a tessellated chequerboard pattern on their petals. The structures of minerals provide good examples of regularly repeating three-dimensional arrays. Despite the hundreds of thousands of known minerals, there are rather few possible types of arrangement of atoms in a crystal, defined by crystal structure, crystal system, and point group; for example, there are exactly 14 Bravais lattices for the 7 lattice systems in three-dimensional space.

Cracks are linear openings that form in materials to relieve stress. When an elastic material stretches or shrinks uniformly, it eventually reaches its breaking strength and then fails suddenly in all directions, creating cracks with 120 degree joints, so three cracks meet at a node. Conversely, when an inelastic material fails, straight cracks form to relieve the stress. Further stress in the same direction would then simply open the existing cracks; stress at right angles can create new cracks, at 90 degrees to the old ones. Thus the pattern of cracks indicates whether the material is elastic or not. In a tough fibrous material like oak tree bark, cracks form to relieve stress as usual, but they do not grow long as their growth is interrupted by bundles of strong elastic fibres. Since each species of tree has its own structure at the levels of cell and of molecules, each has its own pattern of splitting in its bark.

Leopards and ladybirds are spotted; angelfish and zebras are striped. These patterns have an evolutionary explanation: they have functions which increase the chances that the offspring of the patterned animal will survive to reproduce. One function of animal patterns is camouflage; for instance, a leopard that is harder to see catches more prey. Another function is signalling — for instance, a ladybird is less likely to be attacked by predatory birds that hunt by sight, if it has bold warning colours, and is also distastefully bitter or poisonous, or mimics other distasteful insects. A young bird may see a warning patterned insect like a ladybird and try to eat it, but it will only do this once; very soon it will spit out the bitter insect; the other ladybirds in the area will remain undisturbed. The young leopards and ladybirds, inheriting genes that somehow create spottedness, survive. But while these evolutionary and functional arguments explain why these animals need their patterns, they do not explain how the patterns are formed.

Alan Turing, and later the mathematical biologist James Murray, described a mechanism that spontaneously creates spotted or striped patterns: a reaction-diffusion system. The cells of a young organism have genes that can be switched on by a chemical signal, a morphogen, resulting in the growth of a certain type of structure, say a darkly pigmented patch of skin. If the morphogen is present everywhere, the result is an even pigmentation, as in a black leopard. But if it is unevenly distributed, spots or stripes can result. Turing suggested that there could be feedback control of the production of the morphogen itself. This could cause continuous fluctuations in the amount of morphogen as it diffused around the body. A second mechanism is needed to create standing wave patterns (to result in spots or stripes): an inhibitor chemical that switches off production of the morphogen, and that itself diffuses through the body more quickly than the morphogen, resulting in an activator-inhibitor scheme. The Belousov–Zhabotinsky reaction is a non-biological example of this kind of scheme, a chemical oscillator.

Later research has managed to create convincing models of patterns as diverse as zebra stripes, giraffe blotches, jaguar spots (medium-dark patches surrounded by dark broken rings) and ladybird shell patterns (different geometrical layouts of spots and stripes, see illustrations). Richard Prum's activation-inhibition models, developed from Turing's work, use six variables to account for the observed range of nine basic within-feather pigmentation patterns, from the simplest, a central pigment patch, via concentric patches, bars, chevrons, eye spot, pair of central spots, rows of paired spots and an array of dots. More elaborate models simulate complex feather patterns in the guineafowl "Numida meleagris" in which the individual feathers feature transitions from bars at the base to an array of dots at the far (distal) end. These require an oscillation created by two inhibiting signals, with interactions in both space and time.

Patterns can form for other reasons in the vegetated landscape of tiger bush and fir waves. Tiger bush stripes occur on arid slopes where plant growth is limited by rainfall. Each roughly horizontal stripe of vegetation effectively collects the rainwater from the bare zone immediately above it. Fir waves occur in forests on mountain slopes after wind disturbance, during regeneration. When trees fall, the trees that they had sheltered become exposed and are in turn more likely to be damaged, so gaps tend to expand downwind. Meanwhile, on the windward side, young trees grow, protected by the wind shadow of the remaining tall trees. Natural patterns are sometimes formed by animals, as in the Mima mounds of the Northwestern United States and some other areas, which appear to be created over many years by the burrowing activities of pocket gophers, while the so-called fairy circles of Namibia appear to be created by the interaction of competing groups of sand termites, along with competition for water among the desert plants.

In permafrost soils with an active upper layer subject to annual freeze and thaw, patterned ground can form, creating circles, nets, ice wedge polygons, steps, and stripes. Thermal contraction causes shrinkage cracks to form; in a thaw, water fills the cracks, expanding to form ice when next frozen, and widening the cracks into wedges. These cracks may join up to form polygons and other shapes.

The fissured pattern that develops on vertebrate brains are caused by a physical process of constrained expansion dependent on two geometric parameters: relative tangential cortical expansion and relative thickness of the cortex. Similar patterns of gyri (peaks) and sulci (troughs) have been demonstrated in models of the brain starting from smooth, layered gels, with the patterns caused by compressive mechanical forces resulting from the expansion of the outer layer (representing the cortex) after the addition of a solvent. Numerical models in computer simulations support natural and experimental observations that the surface folding patterns increase in larger brains.


Footnotes
Citations
Pioneering authors


General books

Patterns from nature (as art)



</doc>
<doc id="14389994" url="https://en.wikipedia.org/wiki?curid=14389994" title="Natural landscape">
Natural landscape

A natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.

In "Silent Spring" (1962) Rachel Carson describes a roadside verge as it used to look: "Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year" and then how it looks now following the use of herbicides: "The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.

The phrase "natural landscape" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape "separate" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape ("Urlandschaft") and its opposite cultural landscape ("Kulturlandschaft") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase "natural landscape" by a geographer can be found in Carl O. Sauer's paper "The Morphology of Landscape" (1925).

The concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, "that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape." With regard to landscape gardening John Aikin, commented in 1794: "Whatever, therefore, there be of "novelty" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: "straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape".

In his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.

Subsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a "Landschaftskunde" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the "Urlandschaft" (original landscape) or landscape that existed before major human induced changes and the "Kulturlandschaft" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.

The term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.

Matters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: "The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.

The dualism of the first definition has its roots is an "ancient concept", because early people viewed "nature, or the nonhuman world […] as a divine "Other", godlike in its separation from humans". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. 
With this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.

What is meant by natural, within the American conservation movement, has been changing over the last century and a half.

In the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and "avoid disturbances such as logging, grazing, fire and insect outbreaks". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.

A century later, in the mid-twentieth century, it began to be believed that the earlier policy of "protection from disturbance was inadequate to preserve park values", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that "A national park should represent a vignette of primitive America". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.

However, recent research in various disciplines indicates that a pristine natural or "primitive" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.

Also important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: "Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it "involves a dualistic vision in which the human is entirely outside the natural" and affirms that "wildness (as opposed to wilderness) can be found anywhere" even "in the cracks of a Manhattan sidewalk". According to Cronon we have to "abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild." Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.

The landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only " the high summits of the Cairngorm Mountains, consist entirely of natural elements. These "high summits" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.

The Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.
Visitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.

No place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.
Even the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.

Cultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.



</doc>
<doc id="37205291" url="https://en.wikipedia.org/wiki?curid=37205291" title="Aesthetics of nature">
Aesthetics of nature

Aesthetics of nature is a sub-field of philosophical ethics, and refers to the study of natural objects from their aesthetical perspective.

Aesthetics of nature developed as a sub-field of philosophical ethics. In the 18th and 19th century, the aesthetics of nature advanced the concepts of disinterestedness, the pictures, and the introduction of the idea of positive aesthetics. The first major developments of nature occurred in the 18th century. The concept of disinterestedness had been explained by many thinkers. Anthony Ashley-Cooper introduced the concept as a way of characterizing the notion of the aesthetic, later magnified by Francis Hutcheson, who expanded it to exclude personal and utilitarianism interests and associations of a more general nature from aesthetic experience. This concept was further developed by Archibald Alison who referred it to a particular state of mind.

The theory of disinterestedness opened doors for a better understanding of the aesthetics dimensions of nature in terms of three conceptualizations: 

Objects experienced as beautiful tend to be small, smooth, and fair in color. In contrast, objects viewed as sublime tend to be powerful, intense and terrifying. Picturesque items are a mixture of both, which can be seen as varied and irregular, rich and forceful, and even vibrant.

Cognitive and non-cognitive approaches of nature have directed their focus from natural environments to the consideration of human and human influenced environments and developed aesthetic investigations of everyday life.(Carlson and Lintott, 2007; Parsons 2008a; Carlson 2010)

People may be mistaken by the art object analogy. For instance, a sandhill crane is not an art object; an art object is not a sandhill crane. In fact, an art object should be called an "artifact". The crane is wildlife on its own and is not an art object. This can be related to Satio's definition of the cognitive view. In elaboration, the crane lives through various ecosystems such as Yellowstone. Nature is a living system which includes animals, plants, and Eco-systems. In contrast, an art object has no regeneration, evolutionary history, or metabolism. An individual may be in the forest and perceive it as beautiful because of the plethora of colors such as red, green, and yellow. This is a result of the chemicals interacting with chlorophyll. An individual's aesthetic experience may increase; however, none of the things mentioned have anything to do with what is really going on in the forest. The chlorophyll is capturing solar energy and the residual chemicals protect the trees from insect grazing.

Any color perceived by human visitors for a few hours is entirely different from what is really happening. According to Leopold, the three features of ecosystems that generate land ethic are integrity, stability and beauty. None of the mentioned features are real in nature. Ecosystems are not stable: they are dramatically changing and they have little integration; ergo, beauty is in the eye of the beholder.

In a Post-Modern approach, when an individual engages in aesthetically appreciating a natural thing, we give meaning to the thing we appreciate and in that meaning, we express and develop our own attitudes, values and beliefs. Our interest in natural things are not only a passive reflection of our inclinations, as Croce describes as the appreciation of nature as looking in a mirror, or what we might call our inward life; but may instead be the things we come across in nature that engage and stimulate our imagination. As a result, we are challenged to think differently and apply thoughts and associations to in new situations and ways.
As a characterization of the appreciation of art, nature aestheticists argue that post modernism is a mistaken view because we do not have a case of anything goes.The aesthetics appreciation of art is governed by some normative standards. In the world of art, criticism may take place when people come together and discuss books and films or critics write appraisals for publications. On the contrary, there are not obvious instances of debate and appraisals where different judgments about the aesthetics of character of nature are evaluated.


</doc>
<doc id="3759820" url="https://en.wikipedia.org/wiki?curid=3759820" title="Physis">
Physis

Physis (Greek: "phusis") is a Greek theological, philosophical, and scientific term usually translated into English as "nature".

The term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.
In pre-Socratic usage, "phusis" was contrasted with , , "law, human convention."
Since Aristotle, however, the "physical" (the subject matter of "physics", properly "natural things") has more typically been juxtaposed to the "metaphysical".

The word φύσις is a verbal noun based on φύω "to grow, to appear" (cognate with English "to be"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. 

In pre-Socratic philosophy, beginning with Heraclitus, "phusis" in keeping with its etymology of "growing, becoming" is always used in the sense of the "natural" "development", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used 
in the comprehensive sense, as referring to ""all" things", as it were "Nature" in the sense of "Universe".

In the Sophist tradition, the term stood in opposition to "nomos" (), "law" or "custom", in the debate on which parts of human existence are natural, and which are due to convention. 
The contrast of "phisis" vs. "nomos" could be applied to any subject, much like the modern contrast of "nature vs. nurture".

In book 10 of "Laws", Plato criticizes those who write works "peri phuseōs". The criticism is that such authors tend to focus on a purely "naturalistic" explanation of the world, ignoring the role of "intention" or "technē", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod "grow" out of primordial entities after the physical universe had been established.

"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist "by" nature; and this would be true if you proved that the soul is older than the body, but not otherwise."

Aristotle sought out the definition of "physis" to prove that there was more than one definition of "physis", and more than one way to interpret nature. "Though Aristotle retains the ancient sense of "physis" as growth, he insists that an adequate definition of "physis" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final." Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of "physis" was his relationship between art and nature. Aristotle said that "physis" (nature) is dependent on techne (art). "The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself." What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).
Though φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in "Romans 2:14" where Paul writes "For when Gentiles, who do not have the law, by "nature" do what the law requires, they are a law to themselves, even though they do not have the law." Another use of φύσις in the sense of "natural order" is "Romans 1:26" where he writes "the men likewise gave up "natural" relations with women and were consumed with passion for one another". In "1 Corinthians 11:14", Paul asks "Does not nature itself teach you that if a man wears long hair it is a disgrace for him?"

This use of φύσις as referring to a "natural order" in "Romans 1:26" and "1 Corinthians 11:14" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of "natural law" in the sense that was made possible by Judeo-Christian theology.

As part of the Pauline theology of salvation by grace, Paul writes in "Ephesians 2:3" that "we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by "nature" children of wrath, like the rest of mankind. In the next verse he writes, "by grace you have been saved." 

Theologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.

The Greek adjective "phusikos" is represented in various forms in modern English:
As "physics" "the study of nature", as "physical" (via Middle Latin "physicalis") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology ("physiologia") is of 16th-century coinage (Jean Fernel). The term "physique", for "the bodily constitution of a person", is a 19th-century loan from French. 

In medicine the suffix "-physis" occurs in such compounds as "symphysis", "epiphysis", and a few others, in the sense of "a growth". The physis also refers to the "growth plate", or site of growth at the end of long bones.



</doc>
<doc id="40159918" url="https://en.wikipedia.org/wiki?curid=40159918" title="Ecosystem health">
Ecosystem health

Ecosystem health is a metaphor used to describe the condition of an ecosystem. Ecosystem condition can vary as a result of fire, flooding, drought, extinctions, invasive species, climate change, mining, overexploitation in fishing, farming or logging, chemical spills, and a host of other reasons. There is no universally accepted benchmark for a healthy ecosystem, rather the apparent health status of an ecosystem can vary depending upon which health metrics are employed in judging it and which societal aspirations are driving the assessment. Advocates of the health metaphor argue for its simplicity as a communication tool. "Policy-makers and the public need simple, understandable concepts like health." Critics worry that ecosystem health, a "value-laden construct", is often "passed off as science to unsuspecting policy makers and the public."

The health metaphor applied to the environment has been in use at least since the early 1800s and the great American conservationist Aldo Leopold (1887–1948) spoke metaphorically of land health, land sickness, mutilation, and violence when describing land use practices. The term "ecosystem management" has been in use at least since the 1950s. The term "ecosystem health" has become widespread in the ecological literature, as a general metaphor meaning something good, and as an environmental quality goal in field assessments of rivers, lakes, seas, and forests.

Recently however this metaphor has been subject of quantitative formulation using complex systems concepts such as criticality, meaning that a healthy ecosystem is in some sort of balance between adaptability (randomness) and robustness (order) . Nevertheless the universality of criticality is still under examination and is known as the Criticality Hypothesis, which states that systems in a dynamic regime shifting between order and disorder, attain the highest level of computational capabilities and achieve an optimal trade-off between robustness and flexibility. Recent results in cell and evolutionary biology, neuroscience and computer science have great interest in the criticality hypothesis, emphasizing its role as a viable candidate general law in the realm of adaptive complex systems (see and references therein).

The term ecosystem health has been employed to embrace some suite of environmental goals deemed desirable. Edward Grumbine's highly cited paper "What is ecosystem management?" surveyed ecosystem management and ecosystem health literature and summarized frequently encountered goal statements:

Grumbine describes each of these goals as a "value statement" and stresses the role of human values in setting ecosystem management goals.

It is the last goal mentioned in the survey, accommodating humans, that is most contentious. "We have observed that when groups of stakeholders work to define … visions, this leads to debate over whether to emphasize ecosystem health or human well-being … Whether the priority is ecosystems or people greatly influences stakeholders' assessment of desirable ecological and social states." and, for example, "For some, wolves are critical to ecosystem health and an essential part of nature, for others they are a symbol of government overreach threatening their livelihoods and cultural values."

Measuring ecosystem health requires extensive goal-driven environmental sampling. For example, a vision for ecosystem health of Lake Superior was developed by a public forum and a series of objectives were prepared for protection of habitat and maintenance of populations of some 70 indigenous fish species. A suite of 80 lake health indicators was developed for the Great Lakes Basin including monitoring native fish species, exotic species, water levels, phosphorus levels, toxic chemicals, phytoplankton, zooplankton, fish tissue contaminants, etc.

Some authors have attempted broad definitions of ecosystem health, such as benchmarking as healthy the historical ecosystem state "prior to the onset of anthropogenic stress." A difficulty is that the historical composition of many human-altered ecosystems is unknown or unknowable. Also, fossil and pollen records indicate that the species that occupy an ecosystem reshuffle through time, so it is difficult to identify one snapshot in time as optimum or "healthy.".

A commonly cited broad definition states that a healthy ecosystem has three attributes:

While this captures significant ecosystem properties, a generalization is elusive as those properties do not necessarily co-vary in nature. For example, there is not necessarily a clear or consistent relationship between productivity and species richness. Similarly, the relationship between resilience and diversity is complex, and ecosystem stability may depend upon one or a few species rather than overall diversity. And some undesirable ecosystems are highly productive.

"Resilience is not desirable per se. There can be highly resilient states of ecosystems which are very undesirable from some human perspectives , such as algal-dominated coral reefs." Ecological resilience is a "capacity" that varies depending upon which properties of the ecosystem are to be studied and depending upon what kinds of disturbances are considered and how they are to be quantified. Approaches to assessing it "face high uncertainties and still require a considerable amount of empirical and theoretical research."

Other authors have sought a numerical index of ecosystem health that would permit quantitative comparisons among ecosystems and within ecosystems over time. One such system employs ratings of the three properties mentioned above: Health = system vigor x system organization x system resilience. Ecologist Glenn Suter argues that such indices employ "nonsense units," the indices have "no meaning; they cannot be predicted, so they are not applicable to most regulatory problems; they have no diagnostic power; effects of one component are eclipsed by responses of other components, and the reason for a high or low index value is unknown."

Health metrics are determined by stakeholder goals, which drive ecosystem definition. An ecosystem is an abstraction. "Ecosystems cannot be identified or found in nature. Instead, they must be delimited by an observer. This can be done in many different ways for the same chunk of nature, depending on the specific perspectives of interest."

Ecosystem definition determines the acceptable range of variability (reference conditions) and determines measurement variables. The latter are used as indicators of ecosystem structure and function, and can be used as indicators of "health".

An indicator is a variable, such as a chemical or biological property, that when measured, is used to infer trends in another (unmeasured) environmental variable or cluster of unmeasured variables (the indicandum). For example, rising mortality rate of canaries in a coal mine is an indicator of rising carbon monoxide levels. Rising chlorophyll-a levels in a lake may signal eutrophication.

Ecosystem assessments employ two kinds of indicators, descriptive indicators and normative indicators. "Indicators can be used descriptively for a scientific purpose or normatively for a political purpose."

Used descriptively, high chlorophyll-a is an indicator of eutrophication, but it may also be used as an ecosystem health indicator. When used as a normative (health) indicator, it indicates a rank on a health scale, a rank that can vary widely depending on societal preferences as to what is desirable. A high chlorophyll-a level in a natural successional wetland might be viewed as healthy whereas a human-impacted wetland with the "same" indicator value may be judged unhealthy.

Estimation of ecosystem health has been criticized for intermingling the two types of environmental indicators. A health indicator is a normative indicator, and if conflated with descriptive indicators "implies that normative values can be measured objectively, which is certainly not true. Thus, implicit values are insinuated to the reader, a situation which has to be avoided."

It can be argued that the very act of selecting indicators of any kind is biased by the observer's perspective but separation of goals from descriptions has been advocated as a step toward transparency: "A separation of descriptive and normative indicators is essential from the perspective of the philosophy of science … Goals and values cannot be deduced directly from descriptions … a fact that is emphasized repeatedly in the literature of environmental ethics … Hence, we advise always specifying the definition of indicators and propose clearly distinguishing ecological indicators in science from policy indicators used for decision-making processes."

And integration of multiple, possibly conflicting, normative indicators into a single measure of "ecosystem health" is problematic. Using 56 indicators, "determining environmental status and assessing marine ecosystems health in an integrative way is still one of the grand challenges in marine ecosystems ecology, research and management"

Another issue with indicators is validity. Good indicators must have an independently validated high predictive value, that is high sensitivity (high probability of indicating a significant change in the indicandum) and high specificity (low probability of wrongly indicating a change). The reliability of various health metrics has been questioned and "what combination of measurements should be used to evaluate ecosystems is a matter of current scientific debate." Most attempts to identify ecological indicators have been correlative rather than derived from prospective testing of their predictive value and the selection process for many indicators has been based upon weak evidence or has been lacking in evidence.

In some cases no reliable indicators are known: "We found no examples of invertebrates successfully used in [forest] monitoring programs. Their richness and abundance ensure that they play significant roles in ecosystem function but thwart focus on a few key species." And, "Reviews of species-based monitoring approaches reveal that no single species, nor even a group of species, accurately reflects entire communities. Understanding the response of a single species may not provide reliable predictions about a group of species even when the group is a few very similar species."

A trade-off between human health and the "health" of nature has been termed the "health paradox" and it illuminates how human values drive perceptions of ecosystem health.

Human health has benefited by sacrificing the "health" of wild ecosystems, such as dismantling and damming of wild valleys, destruction of mosquito-bearing wetlands, diversion of water for irrigation, conversion of wilderness to farmland, timber removal, and extirpation of tigers, whales, ferrets, and wolves.

There has been an acrimonious schism among conservationists and resource managers over the question of whether to "ratchet back human domination of the biosphere" or whether to embrace it. These two perspectives have been characterized as utilitarian vs protectionist.

The utilitarian view treats human health and well-being as criteria of ecosystem health. For example, destruction of wetlands to control malaria mosquitoes "resulted in an improvement in ecosystem health."
The protectionist view treats humans as an invasive species: "If there was ever a species that qualified as an invasive pest, it is "Homo sapiens","

Proponents of the utilitarian view argue that "healthy ecosystems are characterized by their capability to sustain healthy human populations," and "healthy ecosystems must be economically viable," as it is "unhealthy" ecosystems that are likely to result in increases in contamination, infectious diseases, fires, floods, crop failures and fishery collapse.

Protectionists argue that privileging of human health is a conflict of interest as humans have demolished massive numbers of ecosystems to maintain their welfare, also disease and parasitism are historically normal in pre-industrial nature. Diseases and parasites promote ecosystem functioning, driving biodiversity and productivity, and parasites may constitute a significant fraction of ecosystem biomass.

The very choice of the word "health" applied to ecology has been questioned as lacking in neutrality in a BioScience article on responsible use of scientific language: "Some conservationists fear that these terms could endorse human domination of the planet … and could exacerbate the shifting cognitive baseline whereby humans tend to become accustomed to new and often degraded ecosystems and thus forget the nature of the past."

Criticism of ecosystem health largely targets the failure of proponents to explicitly distinguish the normative dimension from the descriptive dimension, and has included the following:

Alternatives have been proposed for the term ecosystem health, including more neutral language such as ecosystem status, ecosystem prognosis, and ecosystem sustainability. Another alternative to the use of a health metaphor is to "express exactly and clearly the public policy and the management objective", to employ habitat descriptors and real properties of ecosystems. An example of a policy statement is "The maintenance of viable natural populations of wildlife and ecological functions always takes precedence over any human use of wildlife." An example of a goal is "Maintain viable populations of all native species in situ." An example of a management objective is "Maintain self-sustaining populations of lake whitefish within the range of abundance observed during 1990-99."

Kurt Jax presented an ecosystem assessment format that avoids imposing a preconceived notion of normality, that avoids the muddling of normative and descriptive, and that gives serious attention to ecosystem definition. (1) Societal purposes for the ecosystem are negotiated by stakeholders, (2) a functioning ecosystem is defined with emphasis on phenomena relevant to stakeholder goals, (3) benchmark reference conditions and permissible variation of the system are established, (4) measurement variables are chosen for use as indicators, and (5) the time scale and spatial scale of assessment are decided.

Ecological health has been used as a medical term in reference to human allergy and multiple chemical sensitivity and as a public health term for programs to modify health risks (diabetes, obesity, smoking, etc.). Human health itself, when viewed in its broadest sense, is viewed as having ecological foundations. It is also an urban planning term in reference to "green" cities (composting, recycling), and has been used loosely with regard to various environmental issues, and as the condition of human-disturbed environmental sites. Ecosystem integrity implies a condition of an ecosystem exposed to a minimum of human influence. Ecohealth is the relationship of human health to the environment, including the effect of climate change, wars, food production, urbanization, and ecosystem structure and function. Ecosystem management and ecosystem-based management refer to the sustainable management of ecosystems and in some cases may employ the terms ecosystem health or ecosystem integrity as a goal.


</doc>
<doc id="14272151" url="https://en.wikipedia.org/wiki?curid=14272151" title="Nature religion">
Nature religion

A nature religion is a religious movement that believes nature and the natural world is an embodiment of divinity, sacredness or spiritual power. Nature religions include indigenous religions practiced in various parts of the world by cultures who consider the environment to be imbued with spirits and other sacred entities. It also includes contemporary Pagan faiths which are primarily concentrated in Europe and North America.

The term "nature religion" was first coined by the American religious studies scholar Catherine Albanese, who used it in her work "Nature Religion in America: From the Algonkian Indians to the New Age" (1991) and later went on to use it in other studies. Following on from Albanese's development of the term it has since been used by other academics working in the discipline.

Catherine Albanese described nature religion as "a symbolic center and the cluster of beliefs, behaviours, and values that encircles it", deeming it to be useful for shining a light on aspects of history that are rarely viewed as religious.
In a paper of his on the subject, the Canadian religious studies scholar Peter Beyer described "nature religion" as a "useful analytical abstraction" to refer to "any religious belief or practice in which devotees consider nature to be the embodiment of divinity, sacredness, transcendence, spiritual power, or whatever cognate term one wishes to use". He went on to note that in this way nature religion was not an "identifiable religious tradition" such as Buddhism or Christianity are, but that it instead covers "a range of religious and quasi-religious movements, groups and social networks whose participants may or may not identify with one of the many constructed religions of global society which referred to many other nature religion."

Peter Beyer noted the existence of a series of common characteristics which he believed were shared by different nature religions. He remarked that although "one must be careful not to overgeneralise", he suspected that there were a series of features which "occur sufficiently often" in those nature religions known to recorded scholarship to constitute a pattern.

The first of these common characteristics was nature religion's "comparative resistance to institutionalisation and legitimisation in terms of identifiable socio-religious authorities and organisations", meaning that nature religionists rarely formed their religious beliefs into large, visible socio-political structures such as churches. Furthermore, Beyer noted, nature religionists often held a "concomitant distrust of and even eschewing of politically orientated power". Instead of this, he felt that among nature religious communities, there was "a valuing of community as non-hierarchical" and a "conditional optimism with regard to human capacity and the future."

In the sphere of the environment, Beyer noted that nature religionists held to a "holistic conception of reality" and "a valorisation of physical place as vital aspects of their spiritualities". Similarly, Beyer noted the individualism which was favoured by nature religionists. He remarked that those adhering to such beliefs typically had respect for "charismatic and hence purely individual authority" and place a "strong emphasis on individual paths" which led them to believe in "the equal value of individuals and groups". Along similar lines, he also commented on the "strong experiential basis" to nature religionist beliefs "where personal experience is a final arbiter of truth or validity".

In April 1996, the University of Lancaster in North West England held a conference on contemporary Paganism entitled "Nature Religion Today: Western Paganism, Shamanism and Esotericism in the 1990s", and ultimately led to the publication of an academic anthology of the same name two years later. This book, "Nature Religion Today: Paganism in the Modern World", was edited by members of the University's Department of Religious Studies, a postgraduate named Joanne Pearson and two professors, Richard H. Roberts and Geoffrey Samuel.

In his study of Wicca, the Pagan studies scholar Ethan Doyle White expressed the view that the category of "nature religion" was problematic from a "historical perspective" because it solely emphasises the "commonalities of belief and attitude to the natural world" that are found between different religions and in doing so divorces these different belief systems from their distinctive socio-cultural and historical backgrounds.




</doc>
<doc id="52634071" url="https://en.wikipedia.org/wiki?curid=52634071" title="Nature-based solutions">
Nature-based solutions

Nature-based solutions (NBS) refers to the sustainable management and use of nature for tackling socio-environmental challenges. The challenges include issues such as climate change, water security, water pollution, food security, human health, and disaster risk management.

A definition by the European Union states that these solutions are "inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. The Nature-based Solutions Initiative meanwhile defines them as "actions that work with and enhance nature so as to help people adapt to change and disasters". Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions". With NBS, healthy, resilient and diverse ecosystems (whether natural, managed or newly created) can provide solutions for the benefit of societies and overall biodiversity.

For instance, the restoration or protection of mangroves along coastlines utilizes a nature-based solution to accomplish several things. Mangroves moderate the impact of waves and wind on coastal settlements or cities and sequester CO . They also provide safe nurseries for marine life that can be the basis for sustaining populations of fish that local populations may depend on. Additionally, the mangrove forests can help control coastal erosion resulting from sea level rise. Similarly, in cities green roofs or walls are nature-based solutions that can be used to moderate the impact of high temperatures, capture storm water, abate pollution, and act as carbon sinks, while enhancing biodiversity.

Conservation approaches and environment management initiatives have been carried out for decades. What is new is that the benefits of such nature-based solutions to human well-being have been articulated well more recently. Even if the term itself is still being framed, examples of nature-based solutions can be found all over the world, and imitated. Nature-based solutions are on their way to being mainstreamed in national and international policies and programmes (e.g. climate change policy, law, infrastructure investment and financing mechanisms). For example, the theme for World Water Day 2018 was "Nature for water" and by UN-Water's accompanying UN World Water Development Report had the title "Nature-based Solutions for Water".

Societies increasingly face challenges such as climate change, urbanization, jeopardized food security and water resource provision, and disaster risk. One approach to answer these challenges is to singularly rely on technological strategies. An alternative approach is to manage the (socio-)ecological systems in a comprehensive way in order to sustain and potentially increase the delivery of ecosystem services to humans. In this context, nature-based solutions (NBS) have recently been put forward by practitioners and quickly thereafter by policymakers. These solutions stress the sustainable use of nature in solving coupled environmental-social-economic challenges.

While ecosystem services are often valued in terms of immediate benefits to human well-being and economy, NBS focus on the benefits to people and the environment itself, to allow for sustainable solutions that are able to respond to environmental change and hazards in the long-term. NBS go beyond the traditional biodiversity conservation and management principles by "re-focusing" the debate on humans and specifically integrating societal factors such as human well-being and poverty reduction, socio-economic development, and governance principles.

With respect to water issues, NBS can achieve the following, according to the World Water Development Report 2018 by UN-Water: 

In 2015, the European network BiodivERsA highlighted how NBS relate to concepts like ecosystem approaches and ecological engineering. NBS are strongly connected to ideas such as natural systems agriculture, natural solutions, ecosystem-based approaches, adaptation services, natural infrastructure, green infrastructure and ecological engineering. For instance, ecosystem-based approaches are increasingly promoted for climate change adaptation and mitigation by organisations like United Nations Environment Programme and non-governmental organisations such as The Nature Conservancy. These organisations refer to "policies and measures that take into account the role of ecosystem services in reducing the vulnerability of society to climate change, in a multi-sectoral and multi-scale approach".

Likewise, natural infrastructure is defined as a "strategically planned and managed network of natural lands, such as forests and wetlands, working landscapes, and other open spaces that conserves or enhances ecosystem values and functions and provides associated benefits to human populations"; and green infrastructure refers to an "interconnected network of green spaces that conserves natural systems and provides assorted benefits to human populations".

Similarly, the concept of ecological engineering generally refers to "protecting, restoring (i.e. ecosystem restoration) or modifying ecological systems to increase the quantity, quality and sustainability of particular services they provide, or to build new ecological systems that provide services that would otherwise be provided through more conventional engineering, based on non-renewable resources".

The International Union for the Conservation of Nature (IUCN) defines NBS as actions to protect, sustainably manage, and restore natural or modified ecosystems, that address societal challenges effectively and adaptively, simultaneously providing human well-being and biodiversity benefits, with climate change, food security, disaster risks, water security, social and economic development as well as human health being the common societal challenges.

IUCN proposes to consider NBS as an umbrella concept. Categories and examples of NBS approaches according to IUCN include:

The general objective of NBS is clear, namely the sustainable management and use of nature for tackling societal challenges. However, different stakeholders view NBS from other perspectives. For instance, IUCN defines NBS as "actions to protect, sustainably manage and restore natural or modified ecosystems, which address societal challenges effectively and adaptively, while simultaneously providing human well-being and biodiversity benefits". This framing puts the need for well-managed and restored ecosystems at the heart of NBS, with the overarching goal of "Supporting the achievement of society's development goals and safeguard human well-being in ways that reflect cultural and societal values and enhance the resilience of ecosystems, their capacity for renewal and the provision of services".

In the context of the ongoing political debate on jobs and growth (main drivers of the current EU policy agenda), the European Commission underlines that NBS can transform environmental and societal challenges into innovation opportunities, by turning natural capital into a source for green growth and sustainable development. In their view, NBS to societal challenges are "solutions that are inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions."

This framing is somewhat broader, and puts economy and social assets at the heart of NBS as importantly as sustaining environmental conditions. It shares similarities with the definition proposed by Maes and Jacobs (2015) defining NBS as "any transition to a use of ES with decreased input of non-renewable natural capital and increased investment in renewable natural processes". In their view, development and evaluation of NBS spans three basic requirements: (1) decrease of fossil fuel input per produced unit; (2) lowering of systemic trade-offs and increasing synergies between ES; and (3) increasing labor input and jobs. Here, nature is seen as a tool to inspire more systemic solutions to societal problems.

Whatever definition used, promoting sustainability and the increased role of natural, self-sustained processes relying on biodiversity, are inherent to NBS. They constitute actions easily seen as positive for a wide range of stakeholders, as they bring about benefits at environmental, economic and social levels. As a consequence, the concept of NBS is gaining acceptance outside the conservation community (e.g. urban planning) and is now on its way to be mainstreamed into policies and programmes (climate change policy, law, infrastructure investment and financing mechanisms).

In 2014-2015, the European network BiodivERsA mobilized a range of scientists, research donors and stakeholders and proposed a typology characterizing NBS along two gradients. 1. "how much engineering of biodiversity and ecosystems is involved in NBS", and 2. "how many ecosystem services and stakeholder groups are targeted by a given NBS". The typology highlights that NBS can involve very different actions on ecosystems (from protection to management and even creation of new ecosystems) and is based on the assumption that the higher the number of services and stakeholder groups targeted, the lower the capacity to maximize the delivery of each service and simultaneously fulfil the specific needs of all stakeholder groups. As such, three types of NBS are distinguished (Figure 2):

Type 1 NBS consists of no or minimal intervention in ecosystems, with the objectives of maintaining or improving the delivery of a range of ES both inside and outside of these conserved ecosystems. Examples include the protection of mangroves in coastal areas to limit risks associated to extreme weather conditions and provide benefits and opportunities to local populations; and the establishment of marine protected areas to conserve biodiversity within these areas while exporting biomass into fishing grounds. This type of NBS is connected to, for example, the concept of biosphere reserves which incorporates core protected areas for nature conservation and buffer zones and transition areas where people live and work in a sustainable way.

Type 2 NBS corresponds to management approaches that develop sustainable and multifunctional ecosystems and landscapes (extensively or intensively managed). These types improve the delivery of selected ES compared to what would be obtained with a more conventional intervention. Examples include innovative planning of agricultural landscapes to increase their multi-functionality; and approaches for enhancing tree species and genetic diversity to increase forest resilience to extreme events. This type of NBS is strongly connected to concepts like natural systems agriculture, agro-ecology, and evolutionary-orientated forestry.

Type 3 NBS consists of managing ecosystems in very extensive ways or even creating new ecosystems (e.g., artificial ecosystems with new assemblages of organisms for green roofs and walls to mitigate city warming and clean polluted air). Type 3 is linked to concepts like green and blue infrastructures and objectives like restoration of heavily degraded or polluted areas and greening cities.

Type 1 and 2 would typically fall within the IUCN NBS framework, whereas Type 2 and moreover Type 3 are often exemplified by EC for turning natural capital into a source for green growth and sustainable development.

Hybrid solutions exist along this gradient both in space and time. For instance, at landscape scale, mixing protected and managed areas could be needed to fulfil multi-functionality and sustainability goals. Similarly, a constructed wetland can be developed as a type 3 but, when well established, may subsequently be preserved and surveyed as a type 1.

Demonstrating the benefits of nature and healthy ecosystems and showcasing the return on investment they can offer is necessary in order to increase awareness, but also to provide support and guidance on how to implement NBS. A large number of initiatives around the world already highlight the effectiveness of NBS approaches to address a wide range of societal challenges.

The following table shows examples from around the world:

In 2018, The Hindu reported that the East Kolkata wetlands, the world's largest organic sewage treatment facility had been used to clean the sewage of Kolkata in an organic manner by using algae for several decades. In use since the 1930s, the natural system was discovered by Dhrubajyoti Ghosh, an ecologist and a municipal engineer in the 1970s while working in the region. Ghosh worked for decades to protect the wetlands. It had been a practice in Kolkata, one of the five largest cities in India, for the municipal authorities to pump sewage into shallow ponds ("bheris"). Under the heat of the tropical sun, algae proliferated in them, converting the sewage into clean water, which in turn was used by villagers to grow paddy and vegetables. This system has been in use in the region since the 1930s and treats 750 million litres of wastewater per day, giving livelihood to 100,000 people in the vicinity. For his work, Ghosh was included in the UN Global 500 Roll of Honour in 1990 and received the Luc Hoffmann award in 2016.

There is currently no accepted basis on which a government agency, municipality or private company can systematically assess the efficiency, effectiveness and sustainability of a particular nature-based solution. However, a series of principles are proposed to guide effective and appropriate implementation, and thus to upscale NBS in practice. For example, NBS embrace and are not meant to replace nature conservation norms. Also, NBS are determined by site-specific natural and cultural contexts that include traditional, local and scientific knowledge. NBS are an integral part of the overall design of policies, and measure or actions, to address a specific challenges. Finally, NBS can be implemented alone or in an integrated manner with other solutions to societal challenges (e.g. technological and engineering solutions) and they are applied at the landscape scale.

Implementing NBS requires political, economic, and scientific challenges to be tackled. First and foremost, private sector investment is needed, not to replace but to supplement traditional sources of capital such as public funding or philanthropy. The challenge is therefore to provide a robust evidence base for the contribution of nature to economic growth and jobs, and to demonstrate the economic viability of these solutions – compared to technological ones – on a timescale compatible with that of global change. Furthermore, it requires measures like adaptation of economic subsidy schemes, and the creation of opportunities for conservation finance, to name a few. Indeed, such measures will be needed to scale up NBS interventions, and strengthen their impact in mitigating the world's most pressing challenges.

Since 2016, the EU is supporting a multi-stakeholder dialogue platform (called ThinkNature) to promote the co-design, testing and deployment of improved and innovative NBS in an integrated way. Creation of such science-policy-business-society interfaces could promote the market uptake of NBS. The project is part of the EU’s Horizon 2020 – Research and Innovation programme, and will last for 3 years. There are a total of 17 international partners involved, including the Technical University of Crete (Project Leader), the University of Helsinki and BiodivERsA.

In 2017, as part of the Presidency of the Estonian Republic of the Council of the European Union, a conference called “Nature-based Solutions: From Innovation to Common-use” was organized by the Ministry of the Environment of Estonia and the University of Tallinn. This conference aimed to strengthen synergies among various recent initiatives and programs related to NBS launched by the European Commission and by the EU Member States, focusing on policy and governance of NBS, and on research and innovation.

In recognition of the importance of natural ecosystems for mitigation and adaptation, the Paris Agreement calls on all Parties to acknowledge “the importance of the conservation and enhancement, as appropriate, of sinks and reservoirs of the greenhouse gases” and to “note the importance of ensuring the integrity of all ecosystems, including oceans, and the protection of biodiversity, recognized by some cultures as Mother Earth”. It then includes in its Articles several references to nature-based solutions. For example, Article 5.2 encourages Parties to adopt “…policy approaches and positive incentives for activities relating to reducing emissions from deforestation and forest degradation, and the role of conservation and sustainable management of forests and enhancement of forest carbon stocks in developing countries; and alternative policy approaches, such as joint mitigation and adaptation approaches for the integral and sustainable management of forests, while reaffirming the importance of incentivizing, as appropriate, non-carbon benefits associated with such approaches”. Article 7.1 further encourages Parties to build the resilience of socioeconomic and ecological systems, including through economic diversification and sustainable management of natural resources. In total, the Agreement refers to nature (ecosystems, natural resources, forests) in 13 distinct places. An in-depth analysis of all Nationally Determined Contributions submitted to UNFCCC, revealed that around 130 NDCs or 65% of signatories commit to nature-based solutions in their climate pledges, suggesting broad consensus for the role of nature in helping meet climate change goals. However, high-level commitments rarely translate into robust, measurable actions on-the-ground.

The term NBS was put forward by practitioners in the late 2000s (in particular the International Union for the Conservation of Nature and the World Bank) and thereafter by policymakers in Europe (most notably the European Commission).

The term "nature-based solutions" was first used in the late 2000s. It was used in the context of finding new solutions to mitigate and adapt to climate change effects, whilst simultaneously protecting biodiversity and improving sustainable livelihoods.

The IUCN referred to NBS in a position paper for the United Nations Framework Convention on Climate Change. The term was also adopted by European policymakers, in particular by the European Commission in a report stressing that NBS can offer innovative means to create jobs and growth as part of a green economy. The term started to make appearances in the mainstream media around the time of the Global Climate Action Summit in California in September 2018 




</doc>
<doc id="558685" url="https://en.wikipedia.org/wiki?curid=558685" title="Natural environment">
Natural environment

The natural environment encompasses all living and non-living things occurring naturally, meaning in this case not artificial. The term is most often applied to the Earth or some parts of Earth. This environment encompasses the interaction of all living species, climate, weather and natural resources that affect human survival and economic activity.

The concept of the "natural environment" can be distinguished as components:

In contrast to the natural environment is the built environment. In such areas where man has fundamentally transformed landscapes such as urban settings and agricultural land conversion, the natural environment is greatly modified into a simplified human environment. Even acts which seem less extreme, such as building a mud hut or a photovoltaic system in the desert, the modified environment becomes an artificial one. Though many animals build things to provide a better environment for themselves, they are not human, hence beaver dams, and the works of Mound-building termites, are thought of as natural.

People seldom find "absolutely natural" environments on Earth, and naturalness usually varies in a continuum, from 100% natural in one extreme to 0% natural in the other. More precisely, we can consider the different aspects or components of an environment, and see that their degree of naturalness is not uniform. If, for instance, in an agricultural field, the mineralogic composition and the structure of its soil are similar to those of an undisturbed forest soil, but the structure is quite different.

"Natural environment" is often used as a synonym for habitat. For instance, when we say that the natural environment of giraffes is the savanna.

Earth science generally recognizes 4 spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere as correspondent to rocks, water, air, and life respectively. Some scientists include, as part of the spheres of the Earth, the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere, as well as the pedosphere (corresponding to soil) as an active and intermixed sphere. Earth science (also known as geoscience, the geographical sciences or the Earth Sciences), is an all-embracing term for the sciences related to the planet Earth. There are four major disciplines in earth sciences, namely geography, geology, geophysics and geodesy. These major disciplines use physics, chemistry, biology, chronology and mathematics to build a qualitative and quantitative understanding of the principal areas or "spheres" of Earth.

The Earth's crust, or lithosphere, is the outermost solid surface of the planet and is chemically and mechanically different from underlying mantle. It has been generated greatly by igneous processes in which magma cools and solidifies to form solid rock. Beneath the lithosphere lies the mantle which is heated by the decay of radioactive elements. The mantle though solid is in a state of rheic convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics. Volcanoes result primarily from the melting of subducted crust material or of rising mantle at mid-ocean ridges and mantle plumes.

Most water is found in one or another natural kind of body of water.

An ocean is a major body of saline water, and a component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 362 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over 3,000 meters (9,800 ft) deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. The deep seabeds are more than half the Earth's surface, and are among the least-modified natural environments. The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean and the Arctic Ocean.

A river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. A few rivers simply flow into the ground and dry up completely before reaching another body of water. 
The water in a river is usually in a channel, made up of a stream bed between banks. In larger rivers there is also a wider floodplain shaped by waters over-topping the channel. Flood plains may be very wide in relation to the size of the river channel. Rivers are a part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of water stored in glaciers and snowpacks.

Small rivers may also be termed by several other names, including stream, creek and brook. Their current is confined within a bed and stream banks. Streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general is known as "surface hydrology." 

A lake (from Latin "lacus") is a terrain feature, a body of water that is localized to the bottom of basin. A body of water is considered a lake when it is inland, is not part of an ocean, and is larger and deeper than a pond.
Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.

A pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams by their current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.

Humans impact the water in different ways such as modifying rivers (through dams and stream channelization), urbanization, and deforestation. These impact lake levels, groundwater conditions, water pollution, thermal pollution, and marine pollution. Humans modify rivers by using direct channel manipulation. They are building dams and reservoirs and manipulating the direction of the rivers and water path. Dams are good for humans, some communities need the reservoirs to survive. However, reservoirs and dams may negatively impact the environment and wildlife. Dams stops fish migration and the moving of organisms down stream. Urbanization effects the environment because of deforestation and changing lake levels, groundwater conditions, etc. Deforestation and urbanization go hand in hand. Deforestation may cause flooding, declining stream flow, and changes in riverside vegetation. The changing vegetation occurs because when trees cannot get adequate water they start to deteriorate, leading to a decreased food supply for the wildlife in an area.

The atmosphere of the Earth serves as a key factor in sustaining the planetary ecosystem. The thin layer of gases that envelops the Earth is held in place by the planet's gravity. Dry air consists of 78% nitrogen, 21% oxygen, 1% argon and other inert gases, such as carbon dioxide. The remaining gases are often referred to as trace gases, among which are the greenhouse gases such as water vapor, carbon dioxide, methane, nitrous oxide, and ozone. Filtered air includes trace amounts of many other chemical compounds. Air also contains a variable amount of water vapor and suspensions of water droplets and ice crystals seen as clouds. Many natural substances may be present in tiny amounts in an unfiltered air sample, including dust, pollen and spores, sea spray, volcanic ash, and meteoroids. Various industrial pollutants also may be present, such as chlorine (elementary or in compounds), fluorine compounds, elemental mercury, and sulphur compounds such as sulphur dioxide [SO].

The ozone layer of the Earth's atmosphere plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.

Earth's atmosphere can be divided into five main layers. These layers are mainly determined by whether temperature increases or decreases with altitude. From highest to lowest, these layers are:

Within the five principal layers determined by temperature there are several layers determined by other properties.

The dangers of global warming are being increasingly studied by a wide global consortium of scientists. These scientists are increasingly concerned about the potential long-term effects of global warming on our natural environment and on the planet. Of particular concern is how climate change and global warming caused by anthropogenic, or human-made releases of greenhouse gases, most notably carbon dioxide, can act interactively, and have adverse effects upon the planet, its natural environment and humans' existence. It is clear the planet is warming, and warming rapidly. This is due to the greenhouse effect, which is caused by greenhouse gases, which trap heat inside the Earth's atmosphere because of their more complex molecular structure which allows them to vibrate and in turn trap heat and release it back towards the Earth. This warming is also responsible for the extinction of natural habitats, which in turn leads to a reduction in wildlife population.The most recent report from the Intergovernmental Panel on Climate Change (the group of the leading climate scientists in the world) concluded that the earth will warm anywhere from 2.7 to almost 11 degrees Fahrenheit (1.5 to 6 degrees Celsius) between 1990 and 2100.
Efforts have been increasingly focused on the mitigation of greenhouse gases that are causing climatic changes, on developing adaptative strategies to global warming, to assist humans, other animal, and plant species, ecosystems, regions and nations in adjusting to the effects of global warming. Some examples of recent collaboration to address climate change and global warming include:

A significantly profound challenge is to identify the natural environmental dynamics in contrast to environmental changes not within natural variances. A common solution is to adapt a static view neglecting natural variances to exist. Methodologically, this view could be defended when looking at processes which change slowly and short time series, while the problem arrives when fast processes turns essential in the object of the study.

Climate looks at the statistics of temperature, humidity, atmospheric pressure, wind, rainfall, atmospheric particle count and other meteorological elements in a given region over long periods of time. Weather, on the other hand, is the present condition of these same elements over periods up to two weeks.

Climates can be classified according to the average and typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme is the one originally developed by Wladimir Köppen. The Thornthwaite system, in use since 1948, uses evapotranspiration as well as temperature and precipitation information to study animal species diversity and the potential impacts of climate changes.

Weather is a set of all the phenomena occurring in a given atmospheric area at a given time. Most weather phenomena occur in the troposphere, just below the stratosphere. Weather refers, generally, to day-to-day temperature and precipitation activity, whereas climate is the term for the average atmospheric conditions over longer periods of time. When used without qualification, "weather" is understood to be the weather of Earth.

Weather occurs due to density (temperature and moisture) differences between one place and another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On the Earth's surface, temperatures usually range ±40 °C (100 °F to −40 °F) annually. Over thousands of years, changes in the Earth's orbit have affected the amount and distribution of solar energy received by the Earth and influence long-term climate

Surface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The atmosphere is a chaotic system, and small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that civilized human activity such as agriculture and industry has inadvertently modified weather patterns.

Evidence suggests that life on Earth has existed for about 3.7 billion years. All known life forms share fundamental molecular mechanisms, and based on these observations, theories on the origin of life attempt to find a mechanism explaining the formation of a primordial single cell organism from which all life originates. There are many different hypotheses regarding the path that might have been taken from simple organic molecules via pre-cellular life to protocells and metabolism.

Although there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli and reproduction. Life may also be said to be simply the characteristic state of organisms. In biology, the science of living organisms, "life" is the condition which distinguishes active organisms from inorganic matter, including the capacity for growth, functional activity and the continual change preceding death.

A diverse variety of living organisms (life forms) can be found in the biosphere on Earth, and properties common to these organisms—plants, animals, fungi, protists, archaea, and bacteria—are a carbon- and water-based cellular form with complex organization and heritable genetic information. Living organisms undergo metabolism, maintain homeostasis, possess a capacity to grow, respond to stimuli, reproduce and, through natural selection, adapt to their environment in successive generations. More complex living organisms can communicate through various means.

An ecosystem (also called as environment) is a natural unit consisting of all plants, animals and micro-organisms (biotic factors) in an area functioning together with all of the non-living physical (abiotic) factors of the environment.

Central to the ecosystem concept is the idea that living organisms are continually engaged in a highly interrelated set of relationships with every other element constituting the environment in which they exist. Eugene Odum, one of the founders of the science of ecology, stated: "Any unit that includes all of the organisms (i.e.: the "community") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem."

A greater number or variety of species or biological diversity of an ecosystem may contribute to greater resilience of an ecosystem, because there are more species present at a location to respond to change and thus "absorb" or reduce its effects. This reduces the effect before the ecosystem's structure is fundamentally changed to a different state. This is not universally the case and there is no proven relationship between the species diversity of an ecosystem and its ability to provide goods and services on a sustainable level.

The term ecosystem can also pertain to human-made environments, such as human ecosystems and human-influenced ecosystems, and can describe any situation where there is relationship between living organisms and their environment. Fewer areas on the surface of the earth today exist free from human contact, although some genuine wilderness areas continue to exist without any forms of human intervention.

Biomes are terminologically similar to the concept of ecosystems, and are climatically and geographically defined areas of ecologically similar climatic conditions on the Earth, such as communities of plants, animals, and soil organisms, often referred to "as" ecosystems. Biomes are defined on the basis of factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation.

Global biogeochemical cycles are critical to life, most notably those of water, oxygen, carbon, nitrogen and phosphorus.

Wilderness is generally defined as a natural environment on Earth that has not been significantly modified by human activity. The WILD Foundation goes into more detail, defining wilderness as: "The most intact, undisturbed wild natural areas left on our planet - those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure." Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity.

The word, "wilderness", derives from the notion of wildness; in other words that which is not controllable by humans. The word's etymology is from the Old English "wildeornes", which in turn derives from "wildeor" meaning "wild beast" (wild + deor = beast, deer). From this point of view, it is the wildness of a place that makes it a wilderness. The mere presence or activity of people does not disqualify an area from being "wilderness." Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered "wild." This way of looking at wilderness includes areas within which natural processes operate without very noticeable human interference.

Wildlife includes all non-domesticated plants, animals and other organisms. Domesticating wild plant and animal species for human benefit has occurred many times all over the planet, and has a major impact on the environment, both positive and negative. Wildlife can be found in all ecosystems. Deserts, rain forests, plains, and other areas—including the most developed urban sites—all have distinct forms of wildlife. While the term in popular culture usually refers to animals that are untouched by civilized human factors, most scientists agree that wildlife around the world is (now) impacted by human activities.

It is the common understanding of "natural environment" that underlies environmentalism — a broad political, social, and philosophical movement that advocates various actions and policies in the interest of protecting what nature remains in the natural environment, or restoring or expanding the role of nature in this environment. While true wilderness is increasingly rare, "wild" nature (e.g., unmanaged forests, uncultivated grasslands, wildlife, wildflowers) can be found in many locations previously inhabited by humans.

Goals for the benefit of people and natural systems, commonly expressed by environmental scientists and environmentalists include:


In some cultures the term environment is meaningless because there is no separation between people and what they view as the natural world, or their surroundings. Specifically in the United States, many native cultures do not recognize the "environment", or see themselves as environmentalists.



</doc>
<doc id="53806811" url="https://en.wikipedia.org/wiki?curid=53806811" title="Final Straw: Food, Earth, Happiness">
Final Straw: Food, Earth, Happiness

Final Straw: Food, Earth, Happiness is a documentary/art film released in June 2015 that takes audiences through farms and urban landscapes in Japan, South Korea, and the United States, interviewing leading practitioners in the Natural Farming movement. The film began when an environmental artist (Patrick M. Lydon) and an environmental book editor (Suhee Kang), had a chance meeting in Seoul, South Korea, and began conducting short interviews together with leaders in the ecology and social justice movements. Upon meeting Korean farmer Seong Hyun Choi however, the two were so impressed by his ecological mindset and way of working, that they set out to produce a feature film about the movement. Lydon and Kang ended up quitting their jobs, giving away most of their possessions, and becoming voluntarily homeless for four years in order to afford producing the film.

The film is split into three sections 1) Modern Life, 2) Foundations and Mindset of Natural Farming, and 3) Natural Farming in Practice and Life. According to the filmmakers, as they began to understand more about how natural farming itself was not rooted in methods, but in a way of thinking, they chose to explore the life philosophies and ways of thinking of natural farming practitioners in a more free-flowing and artistic way, rather than an instructive one; the result is an unconventional documentary that features slow paced musical interludes alongside interviews. Reviewers have called both "meditative, and mindful," and "an inspiring call to action." Author and musician Alicia Bay Laurel called the film "both art and documentary".

Lydon and Kang spent what they call a "meager" life savings to make the film, along with the volunteer efforts of farmers, translators, writers, musicians they had met during their journey. Although the film was filmed, written, and edited entirely by the two directors, they readily admit that the process of making the film was co-operative effort, with more than 200 volunteers directly involved in the process in some way. The soundtrack was recorded with professional musicians from each of the three countries where filming took place, all of whom donated their time to contribute to the film project. With the continued help of international volunteers, the film is available in four languages (English, Korean, Japanese, Vietnamese), and three more (Chinese, Portuguese, French) are in progress.

Frustrated by the lack of distribution and film festival options for low- and no-budget films, the filmmakers made the decision to manage distribution and touring in the same way they went about filming, through co-operative effort. With the help of volunteers, independent theater owners, and community organizers, they launched an extensive tour throughout Japan and South Korea from 2015-2016, eventually screening the film at over 130 venues.

Rather than simply screening the film, the filmmakers decided to transition their existing media production organization "SocieCity," into a vehicle for art and community engagement. They made a point of hosting interactive events along with their screenings and in several cases, stayed in communities for up to three months at a time to build natural gardens and host a project they call REALtimeFOOD, a grown-to-order restaurant which connects the ideas from the film with real-world practices in farming, food, and crafts. In most cases, these efforts were funded by grants from local philanthropic organizations and/or supported by the communities themselves.

Interested in the unconventional way the film was being made and toured, multiple magazines and newspapers in Japan and Korea followed the directors during several parts of their journey, notably ESSEN, Bar and Dining, and Road magazines, and Shikoku Shinbun and Huffington Post newspapers.

During the tour, the film was eventually picked up by festivals including Tassie Eco Film Festival and Belleville Doc Fest. 



</doc>
<doc id="19468941" url="https://en.wikipedia.org/wiki?curid=19468941" title="Balance of nature">
Balance of nature

The balance of nature is a theory that proposes that ecological systems are usually in a stable equilibrium or homeostasis, which is to say that a small change in some particular parameter (the size of a particular population, for example) will be corrected by some negative feedback that will bring the parameter back to its original "point of balance" with the rest of the system. It may apply where populations depend on each other, for example in predator/prey systems, or relationships between herbivores and their food source. It is also sometimes applied to the relationship between the Earth's ecosystem, the composition of the atmosphere, and the world's weather.

The Gaia hypothesis is a balance of nature-based theory that suggests that the Earth and its ecology may act as co-ordinated systems in order to maintain the balance of nature.

The theory that nature is permanently in balance has been largely discredited by scientists working in ecology, as it has been found that chaotic changes in population levels are common, but nevertheless the idea continues to be popular in the general public. During the later half of the twentieth century the theory was superseded by catastrophe theory and chaos theory.

The concept that nature maintains its condition is of ancient provenance; Herodotus commented on the wonderful relationship between predator and prey species, which remained in a steady proportion to one another, with predators never excessively consuming their prey populations. The "balance of nature" concept once ruled ecological research, as well as once governing the management of natural resources. This led to a doctrine popular among some conservationists that nature was best left to its own devices, and that human intervention into it was by definition unacceptable. The validity of a "balance of nature" was already questioned in the early 1900s, but the general abandonment of the theory by scientists working in ecology only happened in the last quarter of that century when studies showed that it did not match what could be observed among plant and animal populations.

Predator-prey populations tend to show chaotic behavior within limits, where the sizes of populations change in a way that may appear random, but is in fact obeying deterministic laws based only on the relationship between a population and its food source illustrated by the Lotka–Volterra equation. An experimental example of this was shown in an eight-year study on small Baltic Sea creatures such as plankton, which were isolated from the rest of the ocean. Each member of the food web was shown to take turns multiplying and declining, even though the scientists kept the outside conditions constant. An article in the journal "Nature" stated; "Advanced mathematical techniques proved the indisputable presence of chaos in this food web ... short-term prediction is possible, but long-term prediction is not."

Although some conservationist organizations argue that human activity is incompatible with a balanced ecosystem, there are numerous examples in history showing that several modern day habitats originate from human activity: some of Latin America's rain forests owe their existence to humans planting and transplanting them, while the abundance of grazing animals in the Serengeti plain of Africa is thought by some ecologists to be partly due to human-set fires that created savanna habitats.

Possibly one of the best examples of an ecosystem fundamentally modified by human activity can be observed as a consequence of the Australian Aboriginal practice of "Fire-stick farming". The legacy of this practice over long periods has resulted in forests being converted to grasslands capable of sustaining larger populations of faunal prey, particularly in the northern and western regions of the continent. So thorough has been the effect of these deliberate regular burnings that many plant and tree species from affected regions have now completely adapted to the annual fire regime in that they require the passage of a fire before their seeds will even germinate. One school in Los Angeles states, " “We have let our kids go to the forest area of the playground. However, five years later, we found that none of the flowers were growing, the natural damp soil had been hardened, and all of the beautiful grass had been plucked,”.

Despite being discredited among ecologists, the theory is widely held to be true by the general public, with one authority calling it an "enduring myth". At least in Midwestern America, the "balance of nature" idea was shown to be widely held by both science majors and the general student population. In a study at the University of Patras, educational sciences students were asked to reason about the future of ecosystems which suffered human-driven disturbances. Subjects agreed that it was very likely for the ecosystems to fully recover their initial state, referring to either a 'recovery process' which restores the initial 'balance', or specific 'recovery mechanisms' as an ecosystem's inherent characteristic. In a 2017 study, Ampatzidis and Ergazaki discuss the learning objectives and design criteria that a learning environment for non-biology major students should meet to support them challenge the "balance of nature" idea.



</doc>
<doc id="9228" url="https://en.wikipedia.org/wiki?curid=9228" title="Earth">
Earth

Earth is the third planet from the Sun and the only astronomical object known to harbor life. According to radiometric dating and other sources of evidence, Earth formed over 4.5 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. Earth revolves around the Sun in 365.26 days, a period known as an Earth year. During this time, Earth rotates about its axis about 366.26 times.

Earth's axis of rotation is tilted with respect to its orbital plane, producing seasons on Earth. The gravitational interaction between Earth and the Moon causes ocean tides, stabilizes Earth's orientation on its axis, and gradually slows its rotation. Earth is the densest planet in the Solar System and the largest of the four terrestrial planets.

Earth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.

Within the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.6 billion humans live on Earth and depend on its biosphere and natural resources for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.

The modern English word "Earth" developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled '. It has cognates in every Germanic language, and their proto-Germanic root has been reconstructed as *"erþō". In its earliest appearances, "eorðe" was already being used to translate the many senses of Latin ' and Greek ("gē"): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus as among the devotees of Nerthus, and later Norse mythology included Jörð, a giantess often given as the mother of Thor.

Originally, "earth" was written in lowercase, and from early Middle English, its definite sense as "the globe" was expressed as "the earth". By Early Modern English, many nouns were capitalized, and "the earth" became (and often remained) "the Earth", particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as "Earth", by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes "Earth" when appearing as a name (e.g. "Earth's atmosphere") but writes it in lowercase when preceded by "the" (e.g. "the atmosphere of the earth"). It almost always appears in lowercase in colloquial expressions such as "what on earth are you doing?"

The oldest material found in the Solar System is dated to (Bya). By the primordial Earth had formed. The bodies in the Solar System formed and evolved with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth taking 10– (Mys) to form.

A subject of research is the formation of the Moon, some 4.53 Bya. A leading hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object, named Theia, hit Earth. In this view, the mass of Theia was approximately 10 percent of Earth, it hit Earth with a glancing blow and some of its mass merged with Earth. Between approximately 4.1 and , numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon and, by inference, to that of Earth.

Earth's atmosphere and oceans were formed by volcanic activity and outgassing. Water vapor from these sources condensed into the oceans, augmented by water and ice from asteroids, protoplanets, and comets. In this model, atmospheric "greenhouse gases" kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By , Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.

A crust formed when the molten outer layer of Earth cooled to form a solid. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. Over the period of hundreds of millions of years, the supercontinents have assembled and broken apart. Roughly (Mya), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia , then finally Pangaea, which also broke apart .

The present pattern of ice ages began about and then intensified during the Pleistocene about . High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every . The last continental glaciation ended ago.

Chemical reactions led to the first self-replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all current life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen () accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer () in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, and remains of biotic material found in 4.1 billion-year-old rocks in Western Australia. The earliest direct evidence of life on Earth is contained in 3.45 billion-year-old Australian rocks showing fossils of microorganisms.

During the Neoproterozoic, , much of Earth might have been covered in ice. This hypothesis has been termed "Snowball Earth", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, , there have been five mass extinctions. The most recent such event was , when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared some small animals such as mammals, which at the time resembled shrews. Mammalian life has diversified over the past , and several million years ago an African ape-like animal such as "Orrorin tugenensis" gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which led to the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues to this day.

Earth's expected long-term future is tied to that of the Sun. Over the next , solar luminosity will increase by 10%, and over the next by 40%. The Earth's increasing surface temperature will accelerate the inorganic carbon cycle, reducing concentration to levels lethally low for plants ( for C4 photosynthesis) in approximately . The lack of vegetation will result in the loss of oxygen in the atmosphere, making animal life impossible. After another billion years all surface water will have disappeared and the mean global temperature will reach . From that point, the Earth is expected to be habitable for another , possibly up to if nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.

The Sun will evolve to become a red giant in about . Models predict that the Sun will expand to roughly , about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit from the Sun when the star reaches its maximum radius. Most, if not all, remaining life will be destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that Earth's orbit will eventually decay due to tidal effects and drag, causing it to enter the Sun's atmosphere and be vaporized.

The shape of Earth is approximately oblate spheroidal. Due to rotation, the Earth is flattened at the poles and bulging around the equator. The diameter of the Earth at the equator is larger than the pole-to-pole diameter. Thus the point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador (). The average diameter of the reference spheroid is . Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: The maximum deviation of only 0.17% is at the Mariana Trench ( below local sea level), whereas Mount Everest ( above local sea level) represents a deviation of 0.14%.

In geodesy, the exact shape that Earth's oceans would adopt in the absence of land and perturbations such as tides and winds is called the geoid. More precisely, the geoid is the surface of gravitational equipotential at mean sea level.

Earth's mass is approximately (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.

The most common rock constituents of the crust are nearly all oxides: chlorine, sulfur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. Over 99% of the crust is composed of 11 oxides, principally silica, alumina, iron oxides, lime, magnesia, potash and soda.

Earth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about under the oceans to for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are composed. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The Earth's inner core might rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year. The radius of the inner core is about one fifth of that of Earth.

Earth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to , and the pressure could reach . Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately , twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.

The mean heat loss from Earth is , for a global heat loss of . A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.

Earth's mechanically rigid outer layer, the lithosphere, is divided into tectonic plates. These plates are rigid segments that move relative to each other at one of three boundaries types: At convergent boundaries, two plates come together; at divergent boundaries, two plates are pulled apart; and at transform boundaries, two plates slide past one another laterally. Along these plate boundaries, earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.

As the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than old. The oldest oceanic crust is located in the Western Pacific and is estimated to be old. By comparison, the oldest dated continental crust is .

The seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between . The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of and the Pacific Plate moving . At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of .

The total surface area of Earth is about . Of this, 70.8%, or , is below sea level and covered by ocean water. Below the ocean's surface are much of the continental shelf, mountains, volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2%, or , not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. Tectonics and erosion, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape the Earth's surface over geological time.

The continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.

The elevation of the land surface varies from the low point of at the Dead Sea, to a maximum altitude of at the top of Mount Everest. The mean height of land above sea level is about .

The pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland. Close to 40% of Earth's land surface is used for agriculture, or an estimated of cropland and of pastureland.

The abundance of water on Earth's surface is a unique feature that distinguishes the "Blue Planet" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of . The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of .

The mass of the oceans is approximately 1.35 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of with a mean depth of , resulting in an estimated volume of . If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be .

About 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.

The average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño–Southern Oscillation.

The atmospheric pressure at Earth's sea level averages , with a scale height of about . A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules. Water vapor content varies between 0.01% and 4% but averages about 1%. The height of the troposphere varies with latitude, ranging between at the poles to at the equator, with some variation resulting from weather and seasonal factors.

Earth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved , forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric into. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be , in contrast to the current , and life on Earth probably would not exist in its current form. In May 2017, glints of light, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.

Earth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.

The primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.

Water vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.

The amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.

This latitudinal rule has several anomalies:

The commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions of terrain based on observed temperature and precipitation.

The highest air temperature ever measured on Earth was in Furnace Creek, California, in Death Valley, in 1913. The lowest air temperature ever directly measured on Earth was at Vostok Station in 1983, but satellites have used remote sensing to measure temperatures as low as in East Antarctica. These temperature records are only measurements made with modern instruments from the 20th century onwards and likely do not reflect the full range of temperature on Earth.

Above the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.

Thermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.

The gravity of Earth is the acceleration that is imparted to objects due to the distribution of mass within the Earth. Near the Earth's surface, gravitational acceleration is approximately . Local differences in topography, geology, and deeper tectonic structure cause local and broad, regional differences in the Earth's gravitational field, known as gravity anomalies.

The main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is , with global magnetic dipole moment of . The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.

The extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which waves propagate through the solar wind, a supersonic bowshock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates; the ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belt are formed by high-energy particles whose motion is essentially random, but otherwise contained by the magnetosphere.

During magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.

Earth's rotation period relative to the Sun—its mean solar day—is of mean solar time (). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between longer.

Earth's rotation period relative to the fixed stars, called its "stellar day" by the International Earth Rotation and Reference Systems Service (IERS), is of mean solar time (UT1), or Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its "sidereal day", is of mean solar time (UT1) . Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.

Apart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.

Earth orbits the Sun at an average distance of about every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about , which is fast enough to travel a distance equal to Earth's diameter, about , in seven minutes, and the distance to the Moon, , in about 3.5 hours.

The Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.

The Hill sphere, or the sphere of gravitational influence, of the Earth is about in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.

Earth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.

The axial tilt of the Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. In northern temperate latitudes, the Sun rises north of true east during the summer solstice, and sets north of true west, reversing in the winter. The Sun rises south of true east in the summer for the southern temperate zone and sets south of true west.

Above the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the Southern Hemisphere, the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.

By astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.

The angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.

In modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.

A study from 2016 suggested that Planet Nine tilted all Solar System planets, including Earth's, by about six degrees.

A planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.

A planet's life forms inhabit ecosystems, whose total is sometimes said to form a "biosphere". Earth's biosphere is thought to have begun evolving about . The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.

In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.

Earth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.

Large deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.

Earth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, of Earth's land surface consisted of forest and woodlands, was grasslands and pasture, and was cultivated as croplands. The estimated amount of irrigated land in 1993 was . Humans also live on the land by using building materials to construct shelters.

Large areas of Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.

Many localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.

There is a scientific consensus linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.

Cartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.

Earth's human population reached approximately seven billion on 31 October 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.

68% of the land mass of the world is in the northern hemisphere. Partly due to the predominance of land mass, 90% of humans live in the northern hemisphere.

It is estimated that one-eighth of Earth's surface is suitable for humans to live on – three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%), high mountains (27%), or other unsuitable terrains. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen–Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)
Independent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica, a few land parcels along the Danube river's western bank, and the unclaimed area of Bir Tawil between Egypt and Sudan. , there are 193 sovereign states that are member states of the United Nations, plus two observer states and 72 dependent territories and states with limited recognition. Earth has never had a sovereign government with authority over the entire globe, although some nation-states have striven for world domination and failed.

The United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.

The first human to orbit Earth was Yuri Gagarin on 12 April 1961. In total, about 487 people have visited outer space and reached orbit , and, of these, twelve have walked on the Moon. Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months. The farthest that humans have traveled from Earth is , achieved during the Apollo 13 mission in 1970.

The Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as "moons", after Earth's.

The gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.
Due to their tidal interaction, the Moon recedes from Earth at the rate of approximately . Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Devonian period, for example, (approximately ) there were 400 days in a year, with each day lasting 21.8 hours.

The Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.

Viewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.

The most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.

Earth has at least five co-orbital asteroids, including 3753 Cruithne and . A trojan asteroid companion, , is librating around the leading Lagrange triangular point, L4, in the Earth's orbit around the Sun.

The tiny near-Earth asteroid makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.

, there are 1,886 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.

The standard astronomical symbol of Earth consists of a cross circumscribed by a circle, , representing the four corners of the world.

Human cultures have developed many views of the planet. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity, and by the mid-20th century, the Gaia Principle compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.

Scientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in the Greek colonies of southern Italy during the late 6th century BC by the idea of spherical Earth, which was attributed to both the philosophers Pythagoras and Parmenides. By the end of the 5th century BC, the sphericity of Earth was universally accepted among Greek intellectuals. Earth was generally believed to be the center of the universe until the 16th century, when scientists first conclusively demonstrated that it was a moving object, comparable to the other planets in the Solar System. Due to the efforts of influential Christian scholars and clerics such as James Ussher, who sought to determine the age of Earth through analysis of genealogies in Scripture, Westerners before the 19th century generally believed Earth to be a few thousand years old at most. It was only during the 19th century that geologists realized Earth's age was at least many millions of years.

Lord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old. The perception of Earth shifted again in the 20th century when humans first viewed it from orbit, and especially with photographs of Earth returned by the Apollo program.

</math>, where "m" is the mass of Earth, "a" is an astronomical unit, and "M" is the mass of the Sun. So the radius in AU is about formula_1.</ref>



</doc>
<doc id="58591936" url="https://en.wikipedia.org/wiki?curid=58591936" title="Selvaggio Blu (Sardinia)">
Selvaggio Blu (Sardinia)

The Selvaggio Blu (Wild Blue) is a trekking route in the territory of the district of Baunei (Sardinia). It was conceived in 1987 by Mario Verin, (photographer and alpinist) and Peppino Cicalò (architect), President of the Nuoro section of the Italian Alpine Club. The itinerary extends for over 40 kilometers (approximately 25 miles) from the touristic port of Santa Maria Navarrese (Baunei) to the beach of Cala Sisine (Baunei). It takes on average 4 days to complete.

The Selvaggio Blu is considered one of the last wild trekking routes of the Mediterranean because, for the major part of the itinerary, it can only be accessed by boat or by following the path along the coast of the Gulf of Orosei.

Verin and Cicalò used the name 'Selvaggio Blu' to reflect the main characteristics of the journey: "Selvaggio" to reflect the wildness and pureness of the experience, and "Blu" because the trek goes along the coast, where the color of the sea and the sky is predominant.

The Selvaggio Blu is located entirely in the territory of the district of Baunei which extends for 211.9 km2 on the east coast of Sardinia, in the province of Ogliastra.

The Baunei area is considered one of the wildest in Sardinia, going from the coastal town of Santa Maria Navarrese and traversing Limestone plateaux and coastal scenery to the beach of Cala Luna. It contains all the main centers on the route, including: Santa Maria Navarrese, Pedra Longa, Portu Pedrosu, Cala Goloritzè, Su Feilau, Cala Sisine and also all the centers included in Selvaggio Blu variations: Cala Mariolu, Cala Biriala; S'Istrada Longa, Grotta del Fico and the plateau of Golgo. 

Selvaggio Blu has a strategic location to see the geological history of Sardinia, as it is located 40 km along the coast. There are several important geology observations on Selvaggio Blu's hiking route which can be reached both by land and sea.

In the northern part of Santa Maria Navarrese, hikers have the possibility to move along a section with fractured granite from the Palazoic age, where rocks are several meters thick. This area contains some of Sardinia's oldest rocks, covered with layers of Cambrian-Ordovician metasandstones, phyllites and quartzites. In Pedra Longa the section with granite ends, and there is a transition to the limestones, which are a common feature of the Gulf of Orosei coast. Because of climate changes the limestone has been affected by rising and falling of sea level. Changes caused by the mixing of fresh water from the limestone and salt sea water include larger limestones cavities and fluctuations in different colours.

Grotta del Fico is a karst cave located on the Selvaggio Blu between Santa Maria Navarrese and Cala Luna. Grotta del Fico is accessible by boat or walking. It was discovered by fishermen in the early 20th century, and opened to the public with guided tours in 2003. Inside Grotta del Fico there is a lake with clear water which reflects inside the cave. The lake is created by karst water that flows in the main part of the cave and fills up the lake.


Selvaggio Blu is a 40 kilometers trek that has an estimated travel time of 4 days, but many people take about 6 – 7 days, depending on experience and fitness.

The Selvaggio Blu starts at Pedra Longa (40°1'38"N 9°42'25"E) and goes to Portu Pedrosu (40°4'5"N 9°44'2"E). It has an estimated travel time of 9 hours to cover its 12 km of length. In this stage it is reached the maximum height of all the Selvaggio Blu, which is 770 m. This section gets a climbing grade of EEA on the UAII Scale, which makes this stage the third hardest in technical difficulty of the entire journey. The path crosses a small valley into the woods until it reaches a point where it is possible to see the sea. The path continues into another small valley with holm oaks, walking on very sharp limestone flakes. The path then crosses two gates, after which it descends toward the sea following the cliffs. Then, passing on the top of the Grotta dei Colombi, the path once again entersa small valley. Descending this valley, on the left there is a section equipped with juniper trunks that allow climbers to descend into Bacu Tenadili. The path at this point challenges the orientation skills of the climbers because of the lack of a GPS signal. After this section, there is a cracked limestone area where the shepherds are used to collecting water; after this the path becomes less clear. Following a steep zigzag, the path leads to the mooring of Portu Pedrosu were the first stage ends. Walking for 10 minutes more to Portu Cuau(40°5'11"N 9°43'55"E) there is a large space for camping

The itinerary of the second day starts at Portu Pedrosu and ends in Cala Goloritzè (40°6'29"N 9°41'23"E), the maximum height reached during this day is about 495 m. This stage is 9.5 km and the estimated travel time is of 6 hours. It is not as difficult as the other paths; its difficulty has been ranked as EE on the UAII scale, which makes this the easiest technical section of the Selvaggio Blu. From Portu Pedrosu the path is well defined for a short section due to a good muletrack surrounded by vegetation. Passing a small valley, the path climbs up to a rocky plateau where the track becomes less well defined. Making a large curve towards the North, which is partially covered by the vegetation, the path follows the edge of a small valley, continuing until it reaches a balcony overlooking the sea (2 km from Portu Cuau). Keeping the sea behind, the path proceeds through small rocks and holm oak sections, until it reaches the ovile of Kenos Trainos, one of the most important along the Selvaggio Blu. From this point, the path becomes less clear because of thick vegetation and a small rocky step. Reaching Su Runcu'e su Press it is possible to escape from the Selvaggio Blu and in 15 minutes reach a clearing which is accessible by a Suv. 

The itinerary of the third day is 7.4 km long, with a maximum height of 485 m. It is the most difficult stage of the whole route; its difficulty has been ranked IV+ of the UIAA scale. Some people decide to stop during this stage because of all the obstacles on the path to Bacu Su Feilau. This section requires the capability to travel on all types of terrain, with difficulties including the exposure, difficult vegetation, and the lack of GPS signal. During this stage, which starts from Cala Golorizè and ends in Bacu su Feilau (40°3'59"N 9°34'52"E), the itinerary climbs two rock walls, the first one of 20 meters and the second one of 4 meters. These represent the hardest technical climbing difficulties on the Selvaggio Blu. This section includes several caves and woods, and 2 abseils of 20 meters each. Bacu su Feliau, which is a big hole through the rocky spur that overlooks Bacu Padente, was originally a Bivouac shelter on Selvaggio Blu, but for large groups it is recommended to descend it and climb up towards Ololbizzi, a charcoal burners' circle in the upper part of the Bacu.

The last day starts from Bacu Su Feliau and ends 7 km later in Cala Sisine(40°10'45"N 9°38'1"E). It offers a large variety of scenery, the highest altitude reached during this stage is 480 m. The estimated travel time is 6.5 hours and the technical difficulty is ranked IV on the UIAA scale. This makes this stage the second most difficult of the Selvaggio Blu. At the start of this stage, it is necessary to climb a juniper trunk with giant moss-covered oaks. The route then traverses a gully and is then marked with blue waymarks painted on rocks. These mark the way to exit the woods, utilizing ancient mule tracks, and gives a panoramic view of the sea. Before descending, there is the possibility to climb a cliff named "rottura delle altezze" (which means "breaking the heights") from which walkers can look at the sea 200 m below. One of the paths leads to the ovule Piddi, meaning Mandragora or Mandrake in Sards; this poisonous plant grows all over the Supramonte. Helped by the locals, Verin and Cicalò built a network of mule tracks that, with a series of bends, pass through the most impervious gullies and lead to the sea, which can be seen only in Cala Sisine; here there are docks built to transport charcoal and woods. From Cala Sisine it is possible to return to the starting point by the sea with an inflatable boat or by land with a SUV.

There are several different versions of the Selvaggio Blu:


There are other variations of the Selvaggio Blu as daily excursions:



The guides that conceived the trek are:


The official guides of the trek are:


In the first stage of Selvaggio Blu, oleanders find their ideal habitat and a limestone plateau characterise all the area. True natural monuments as holm oaks can be easily found. The path is coloured by the bushy euphorbias (Euphorbia dendroides). Snakes are integral parts of the fauna, as for example biacco (coluber viridiflavus).

In the second stage the cistus (rockrose) can be found in numerous pink and white varieties.

Ferula is a plant belonging to the Apiaceae family that are in the third stage of Selvaggio Blu.

During the fourth part of the path the possibility to find shepherds is very high because of the great number of ovili. Goats are basic animals of the fauna.

Sardinian mouflon can also be seen. 
Many books and guides about Selvaggio Blu. These include:


Selvaggio Blu is not the only trekking route that you can find in Sardinia. There are 4 more trekking routes called:




</doc>
