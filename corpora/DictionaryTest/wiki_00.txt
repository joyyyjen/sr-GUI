product
teardown
a
product
teardown
or
simply
teardown
is
the
act
of
disassembling
a
product
such
that
it
helps
to
identify
its
component
parts
chip
system
functionality
and
component
costing
information
for
products
having
'secret'
technology
such
as
the
mikoyan-gurevich
mig-25
the
process
may
be
secret
for
others
including
consumer
electronics
the
results
are
typically
disseminated
through
photographs
and
component
lists
so
that
others
can
make
use
of
the
information
without
having
to
disassemble
the
product
themselves
this
information
is
important
to
designers
of
semiconductors
displays
batteries
packaging
companies
integrated
design
firms
and
semiconductor
fabs
and
the
systems
they
operate
within
this
information
can
be
of
interest
to
hobbyists
but
can
also
be
used
commercially
by
the
technical
community
to
find
out
for
example
what
semiconductor
components
are
being
utilized
in
consumer
electronic
products
such
as
the
wii
video
game
console
or
apple's
iphone
such
knowledge
can
aid
understanding
of
how
the
product
works
including
innovative
design
features
and
can
facilitate
estimating
the
bill
of
materials
(bom)
the
financial
community
therefore
has
an
interest
in
teardowns
as
knowing
how
a
company's
products
are
built
can
help
guide
a
stock
valuation
manufacturers
are
often
not
allowed
to
announce
what
components
are
present
in
a
product
due
to
non-disclosure
agreements
(nda)
teardowns
can
also
play
a
part
in
evidence
of
use
in
court
and
litigation
proceedings
where
a
companies
parts
may
have
been
used
without
their
permission
counterfeited
or
to
show
where
intellectual
property
or
patents
might
be
infringed
by
another
firms
part
or
system
identifying
semiconductor
components
in
systems
has
become
more
difficult
over
the
past
years
the
most
notable
change
started
with
apple's
8gb
ipod
nano
which
were
repackaged
with
apple
branding
this
makes
it
more
difficult
to
identify
the
actual
device
manufacturer
and
function
of
the
component
without
performing
a
'decap'
–
removing
the
outer
packaging
to
analyze
the
die
within
it
typically
there
are
markings
on
the
die
inside
the
package
that
can
lead
experienced
engineers
to
see
who
actually
created
the
device
and
what
functionality
it
performs
in
the
system
technology
fusion
technology
fusion
involves
a
transformation
of
core
technologies
through
a
combination
process
facilitated
by
technological
advances
such
as
the
phone
and
the
internet
which
ensure
that
labs
are
no
longer
isolated
this
results
in
profitable
advances
that
can
be
made
cheaply
by
combining
knowledge
from
different
fields
companies
industries
and
geographies
the
technological
fusion
is
distinguished
from
the
so-called
breakthrough
approach
which
is
the
linear
technological
development
that
replaces
an
older
generation
of
technology
through
its
focus
on
combining
existing
technologies
into
hybrid
products
that
can
revolutionize
markets
the
fusion
of
technologies
goes
beyond
mere
combination
fusion
is
more
than
complementarism
because
it
creates
a
new
market
and
new
growth
opportunities
for
each
participant
in
the
innovation
it
blends
incremental
improvements
from
several
(often
previously
separate)
fields
to
create
a
product
an
example
is
the
fusion
of
mechanical
and
electronic
engineering
to
create
mechatronics
there
is
also
the
case
of
fusing
chemical
and
electronics
technology
to
produce
the
liquid
crystal
display
(lcd)
technology
technology
assessment
technology
assessment
(ta
german:
french:
)
is
a
scientific
interactive
and
communicative
process
that
aims
to
contribute
to
the
formation
of
public
and
political
opinion
on
societal
aspects
of
science
and
technology
ta
is
the
study
and
evaluation
of
new
technologies
it
is
based
on
the
conviction
that
new
developments
within
and
discoveries
by
the
scientific
community
are
relevant
for
the
world
at
large
rather
than
just
for
the
scientific
experts
themselves
and
that
technological
progress
can
never
be
free
of
ethical
implications
also
technology
assessment
recognizes
the
fact
that
scientists
normally
are
not
trained
ethicists
themselves
and
accordingly
ought
to
be
very
careful
when
passing
ethical
judgement
on
their
own
or
their
colleagues
new
findings
projects
or
work
in
progress
technology
assessment
assumes
a
global
perspective
and
is
future-oriented
not
anti-technological
ta
considers
its
task
as
an
interdisciplinary
approach
to
solving
already
existing
problems
and
preventing
potential
damage
caused
by
the
uncritical
application
and
the
commercialization
of
new
technologies
therefore
any
results
of
technology
assessment
studies
must
be
published
and
particular
consideration
must
be
given
to
communication
with
political
decision-makers
an
important
problem
concerning
technology
assessment
is
the
so-called
collingridge
dilemma:
on
the
one
hand
impacts
of
new
technologies
cannot
be
easily
predicted
until
the
technology
is
extensively
developed
and
widely
used;
on
the
other
hand
control
or
change
of
a
technology
is
difficult
as
soon
as
it
is
widely
used
technology
assessments
which
are
a
form
of
cost–benefit
analysis
are
difficult
if
not
impossible
to
carry
out
in
an
objective
manner
since
subjective
decisions
and
value
judgments
have
to
be
made
regarding
a
number
of
complex
issues
such
as
(a)
the
boundaries
of
the
analysis
(ie
what
costs
are
internalized
and
externalized)
(b)
the
selection
of
appropriate
indicators
of
potential
positive
and
negative
consequences
of
the
new
technology
(c)
the
monetization
of
non-market
values
and
(d)
a
wide
range
of
ethical
perspectives
consequently
most
technology
assessments
are
neither
objective
nor
value-neutral
exercises
but
instead
are
greatly
influenced
and
biased
by
the
values
of
the
most
powerful
stakeholders
which
are
in
many
cases
the
developers
and
proponents
(ie
corporations
and
governments)
of
new
technologies
under
consideration
in
the
most
extreme
view
as
expressed
by
ian
barbour
in
'’technology
environment
and
human
values'’
technology
assessment
is
"a
one-sided
apology
for
contemporary
technology
by
people
with
a
stake
in
its
continuation"
some
of
the
major
fields
of
ta
are:
information
technology
hydrogen
technologies
nuclear
technology
molecular
nanotechnology
pharmacology
organ
transplants
gene
technology
artificial
intelligence
the
internet
and
many
more
health
technology
assessment
is
related
but
profoundly
different
despite
the
similarity
in
the
name
the
following
types
of
concepts
of
ta
are
those
that
are
most
visible
and
practiced
there
are
however
a
number
of
further
ta
forms
that
are
only
proposed
as
concepts
in
the
literature
or
are
the
label
used
by
a
particular
ta
institution
many
ta
institutions
are
members
of
the
european
parliamentary
technology
assessment
(epta)
network
some
are
working
for
the
stoa
panel
of
the
european
parliament
and
formed
the
european
technology
assessment
group
(etag)
technology
technology
("science
of
craft"
from
greek
"techne"
"art
skill
cunning
of
hand";
and
"-logia")
is
the
collection
of
techniques
skills
methods
and
processes
used
in
the
production
of
goods
or
services
or
in
the
accomplishment
of
objectives
such
as
scientific
investigation
technology
can
be
the
knowledge
of
techniques
processes
and
the
like
or
it
can
be
embedded
in
machines
to
allow
for
operation
without
detailed
knowledge
of
their
workings
systems
(e
g
machines)
applying
technology
by
taking
an
input
changing
it
according
to
the
system's
use
and
then
producing
an
outcome
are
referred
to
as
technology
systems
or
technological
systems
the
simplest
form
of
technology
is
the
development
and
use
of
basic
tools
the
prehistoric
discovery
of
how
to
control
fire
and
the
later
neolithic
revolution
increased
the
available
sources
of
food
and
the
invention
of
the
wheel
helped
humans
to
travel
in
and
control
their
environment
developments
in
historic
times
including
the
printing
press
the
telephone
and
the
internet
have
lessened
physical
barriers
to
communication
and
allowed
humans
to
interact
freely
on
a
global
scale
technology
has
many
effects
it
has
helped
develop
more
advanced
economies
(including
today's
global
economy)
and
has
allowed
the
rise
of
a
leisure
class
many
technological
processes
produce
unwanted
by-products
known
as
pollution
and
deplete
natural
resources
to
the
detriment
of
earth's
environment
innovations
have
always
influenced
the
values
of
a
society
and
raised
new
questions
of
the
ethics
of
technology
examples
include
the
rise
of
the
notion
of
efficiency
in
terms
of
human
productivity
and
the
challenges
of
bioethics
philosophical
debates
have
arisen
over
the
use
of
technology
with
disagreements
over
whether
technology
improves
the
human
condition
or
worsens
it
neo-luddism
anarcho-primitivism
and
similar
reactionary
movements
criticize
the
pervasiveness
of
technology
arguing
that
it
harms
the
environment
and
alienates
people;
proponents
of
ideologies
such
as
transhumanism
and
techno-progressivism
view
continued
technological
progress
as
beneficial
to
society
and
the
human
condition
the
use
of
the
term
"technology"
has
changed
significantly
over
the
last
200
years
before
the
20th
century
the
term
was
uncommon
in
english
and
it
was
used
either
to
refer
to
the
description
or
study
of
the
useful
arts
or
to
allude
to
technical
education
as
in
the
massachusetts
institute
of
technology
(chartered
in
1861)
the
term
"technology"
rose
to
prominence
in
the
20th
century
in
connection
with
the
second
industrial
revolution
the
term's
meanings
changed
in
the
early
20th
century
when
american
social
scientists
beginning
with
thorstein
veblen
translated
ideas
from
the
german
concept
of
""
into
"technology"
in
german
and
other
european
languages
a
distinction
exists
between
"technik"
and
"technologie"
that
is
absent
in
english
which
usually
translates
both
terms
as
"technology"
by
the
1930s
"technology"
referred
not
only
to
the
study
of
the
industrial
arts
but
to
the
industrial
arts
themselves
in
1937
the
american
sociologist
read
bain
wrote
that
"technology
includes
all
tools
machines
utensils
weapons
instruments
housing
clothing
communicating
and
transporting
devices
and
the
skills
by
which
we
produce
and
use
them"
bain's
definition
remains
common
among
scholars
today
especially
social
scientists
scientists
and
engineers
usually
prefer
to
define
technology
as
applied
science
rather
than
as
the
things
that
people
make
and
use
more
recently
scholars
have
borrowed
from
european
philosophers
of
"technique"
to
extend
the
meaning
of
technology
to
various
forms
of
instrumental
reason
as
in
foucault's
work
on
technologies
of
the
self
("techniques
de
soi")
dictionaries
and
scholars
have
offered
a
variety
of
definitions
the
"merriam-webster
learner's
dictionary"
offers
a
definition
of
the
term:
"the
use
of
science
in
industry
engineering
etc
to
invent
useful
things
or
to
solve
problems"
and
"a
machine
piece
of
equipment
method
etc
that
is
created
by
technology"
ursula
franklin
in
her
1989
"real
world
of
technology"
lecture
gave
another
definition
of
the
concept;
it
is
"practice
the
way
we
do
things
around
here"
the
term
is
often
used
to
imply
a
specific
field
of
technology
or
to
refer
to
high
technology
or
just
consumer
electronics
rather
than
technology
as
a
whole
bernard
stiegler
in
"technics
and
time
1"
defines
technology
in
two
ways:
as
"the
pursuit
of
life
by
means
other
than
life"
and
as
"organized
inorganic
matter"
technology
can
be
most
broadly
defined
as
the
entities
both
material
and
immaterial
created
by
the
application
of
mental
and
physical
effort
in
order
to
achieve
some
value
in
this
usage
technology
refers
to
tools
and
machines
that
may
be
used
to
solve
real-world
problems
it
is
a
far-reaching
term
that
may
include
simple
tools
such
as
a
crowbar
or
wooden
spoon
or
more
complex
machines
such
as
a
space
station
or
particle
accelerator
tools
and
machines
need
not
be
material;
virtual
technology
such
as
computer
software
and
business
methods
fall
under
this
definition
of
technology
w
brian
arthur
defines
technology
in
a
similarly
broad
way
as
"a
means
to
fulfill
a
human
purpose"
the
word
"technology"
can
also
be
used
to
refer
to
a
collection
of
techniques
in
this
context
it
is
the
current
state
of
humanity's
knowledge
of
how
to
combine
resources
to
produce
desired
products
to
solve
problems
fulfill
needs
or
satisfy
wants;
it
includes
technical
methods
skills
processes
techniques
tools
and
raw
materials
when
combined
with
another
term
such
as
"medical
technology"
or
"space
technology"
it
refers
to
the
state
of
the
respective
field's
knowledge
and
tools
"state-of-the-art
technology"
refers
to
the
high
technology
available
to
humanity
in
any
field
technology
can
be
viewed
as
an
activity
that
forms
or
changes
culture
additionally
technology
is
the
application
of
math
science
and
the
arts
for
the
benefit
of
life
as
it
is
known
a
modern
example
is
the
rise
of
communication
technology
which
has
lessened
barriers
to
human
interaction
and
as
a
result
has
helped
spawn
new
subcultures;
the
rise
of
cyberculture
has
at
its
basis
the
development
of
the
internet
and
the
computer
not
all
technology
enhances
culture
in
a
creative
way;
technology
can
also
help
facilitate
political
oppression
and
war
via
tools
such
as
guns
as
a
cultural
activity
technology
predates
both
science
and
engineering
each
of
which
formalize
some
aspects
of
technological
endeavor
the
distinction
between
science
engineering
and
technology
is
not
always
clear
science
is
systematic
knowledge
of
the
physical
or
material
world
gained
through
observation
and
experimentation
technologies
are
not
usually
exclusively
products
of
science
because
they
have
to
satisfy
requirements
such
as
utility
usability
and
safety
engineering
is
the
goal-oriented
process
of
designing
and
making
tools
and
systems
to
exploit
natural
phenomena
for
practical
human
means
often
(but
not
always)
using
results
and
techniques
from
science
the
development
of
technology
may
draw
upon
many
fields
of
knowledge
including
scientific
engineering
mathematical
linguistic
and
historical
knowledge
to
achieve
some
practical
result
technology
is
often
a
consequence
of
science
and
engineering
although
technology
as
a
human
activity
precedes
the
two
fields
for
example
science
might
study
the
flow
of
electrons
in
electrical
conductors
by
using
already-existing
tools
and
knowledge
this
new-found
knowledge
may
then
be
used
by
engineers
to
create
new
tools
and
machines
such
as
semiconductors
computers
and
other
forms
of
advanced
technology
in
this
sense
scientists
and
engineers
may
both
be
considered
technologists;
the
three
fields
are
often
considered
as
one
for
the
purposes
of
research
and
reference
the
exact
relations
between
science
and
technology
in
particular
have
been
debated
by
scientists
historians
and
policymakers
in
the
late
20th
century
in
part
because
the
debate
can
inform
the
funding
of
basic
and
applied
science
in
the
immediate
wake
of
world
war
ii
for
example
it
was
widely
considered
in
the
united
states
that
technology
was
simply
"applied
science"
and
that
to
fund
basic
science
was
to
reap
technological
results
in
due
time
an
articulation
of
this
philosophy
could
be
found
explicitly
in
vannevar
bush's
treatise
on
postwar
science
policy
"science
–
the
endless
frontier":
"new
products
new
industries
and
more
jobs
require
continuous
additions
to
knowledge
of
the
laws
of
nature 
this
essential
new
knowledge
can
be
obtained
only
through
basic
scientific
research"
in
the
late-1960s
however
this
view
came
under
direct
attack
leading
towards
initiatives
to
fund
science
for
specific
tasks
(initiatives
resisted
by
the
scientific
community)
the
issue
remains
contentious
though
most
analysts
resist
the
model
that
technology
simply
is
a
result
of
scientific
research
the
use
of
tools
by
early
humans
was
partly
a
process
of
discovery
and
of
evolution
early
humans
evolved
from
a
species
of
foraging
hominids
which
were
already
bipedal
with
a
brain
mass
approximately
one
third
of
modern
humans
tool
use
remained
relatively
unchanged
for
most
of
early
human
history
approximately
50000
years
ago
the
use
of
tools
and
complex
set
of
behaviors
emerged
believed
by
many
archaeologists
to
be
connected
to
the
emergence
of
fully
modern
language
hominids
started
using
primitive
stone
tools
millions
of
years
ago
the
earliest
stone
tools
were
little
more
than
a
fractured
rock
but
approximately
75000
years
ago
pressure
flaking
provided
a
way
to
make
much
finer
work
the
discovery
and
utilization
of
fire
a
simple
energy
source
with
many
profound
uses
was
a
turning
point
in
the
technological
evolution
of
humankind
the
exact
date
of
its
discovery
is
not
known;
evidence
of
burnt
animal
bones
at
the
cradle
of
humankind
suggests
that
the
domestication
of
fire
occurred
before
1
ma;
scholarly
consensus
indicates
that
"homo
erectus"
had
controlled
fire
by
between
500
and
400
ka
fire
fueled
with
wood
and
charcoal
allowed
early
humans
to
cook
their
food
to
increase
its
digestibility
improving
its
nutrient
value
and
broadening
the
number
of
foods
that
could
be
eaten
other
technological
advances
made
during
the
paleolithic
era
were
clothing
and
shelter;
the
adoption
of
both
technologies
cannot
be
dated
exactly
but
they
were
a
key
to
humanity's
progress
as
the
paleolithic
era
progressed
dwellings
became
more
sophisticated
and
more
elaborate;
as
early
as
380
ka
humans
were
constructing
temporary
wood
huts
clothing
adapted
from
the
fur
and
hides
of
hunted
animals
helped
humanity
expand
into
colder
regions;
humans
began
to
migrate
out
of
africa
by
200
ka
and
into
other
continents
such
as
eurasia
human's
technological
ascent
began
in
earnest
in
what
is
known
as
the
neolithic
period
("new
stone
age")
the
invention
of
polished
stone
axes
was
a
major
advance
that
allowed
forest
clearance
on
a
large
scale
to
create
farms
this
use
of
polished
stone
axes
increased
greatly
in
the
neolithic
but
were
originally
used
in
the
preceding
mesolithic
in
some
areas
such
as
ireland
agriculture
fed
larger
populations
and
the
transition
to
sedentism
allowed
simultaneously
raising
more
children
as
infants
no
longer
needed
to
be
carried
as
nomadic
ones
must
additionally
children
could
contribute
labor
to
the
raising
of
crops
more
readily
than
they
could
to
the
hunter-gatherer
economy
with
this
increase
in
population
and
availability
of
labor
came
an
increase
in
labor
specialization
what
triggered
the
progression
from
early
neolithic
villages
to
the
first
cities
such
as
uruk
and
the
first
civilizations
such
as
sumer
is
not
specifically
known;
however
the
emergence
of
increasingly
hierarchical
social
structures
and
specialized
labor
of
trade
and
war
amongst
adjacent
cultures
and
the
need
for
collective
action
to
overcome
environmental
challenges
such
as
irrigation
are
all
thought
to
have
played
a
role
continuing
improvements
led
to
the
furnace
and
bellows
and
provided
for
the
first
time
the
ability
to
smelt
and
forge
of
gold
copper
silver
and
lead
native
metals
found
in
relatively
pure
form
in
nature
the
advantages
of
copper
tools
over
stone
bone
and
wooden
tools
were
quickly
apparent
to
early
humans
and
native
copper
was
probably
used
from
near
the
beginning
of
neolithic
times
(about
10
ka)
native
copper
does
not
naturally
occur
in
large
amounts
but
copper
ores
are
quite
common
and
some
of
them
produce
metal
easily
when
burned
in
wood
or
charcoal
fires
eventually
the
working
of
metals
led
to
the
discovery
of
alloys
such
as
bronze
and
brass
(about
4000
bce)
the
first
uses
of
iron
alloys
such
as
steel
dates
to
around
1800
bce
meanwhile
humans
were
learning
to
harness
other
forms
of
energy
the
earliest
known
use
of
wind
power
is
the
sailing
ship;
the
earliest
record
of
a
ship
under
sail
is
that
of
a
nile
boat
dating
to
the
8th
millennium
bce
from
prehistoric
times
egyptians
probably
used
the
power
of
the
annual
flooding
of
the
nile
to
irrigate
their
lands
gradually
learning
to
regulate
much
of
it
through
purposely
built
irrigation
channels
and
"catch"
basins
the
ancient
sumerians
in
mesopotamia
used
a
complex
system
of
canals
and
levees
to
divert
water
from
the
tigris
and
euphrates
rivers
for
irrigation
according
to
archaeologists
the
wheel
was
invented
around
4000
bce
probably
independently
and
nearly
simultaneously
in
mesopotamia
(in
present-day
iraq)
the
northern
caucasus
(maykop
culture)
and
central
europe
estimates
on
when
this
may
have
occurred
range
from
5500
to
3000
bce
with
most
experts
putting
it
closer
to
4000
bce
the
oldest
artifacts
with
drawings
depicting
wheeled
carts
date
from
about
3500
bce;
however
the
wheel
may
have
been
in
use
for
millennia
before
these
drawings
were
made
more
recently
the
oldest-known
wooden
wheel
in
the
world
was
found
in
the
ljubljana
marshes
of
slovenia
the
invention
of
the
wheel
revolutionized
trade
and
war
it
did
not
take
long
to
discover
that
wheeled
wagons
could
be
used
to
carry
heavy
loads
the
ancient
sumerians
used
the
potter's
wheel
and
may
have
invented
it
a
stone
pottery
wheel
found
in
the
city-state
of
ur
dates
to
around
3429
bce
and
even
older
fragments
of
wheel-thrown
pottery
have
been
found
in
the
same
area
fast
(rotary)
potters'
wheels
enabled
early
mass
production
of
pottery
but
it
was
the
use
of
the
wheel
as
a
transformer
of
energy
(through
water
wheels
windmills
and
even
treadmills)
that
revolutionized
the
application
of
nonhuman
power
sources
the
first
two-wheeled
carts
were
derived
from
travois
and
were
first
used
in
mesopotamia
and
iran
in
around
3000
bce
the
oldest
known
constructed
roadways
are
the
stone-paved
streets
of
the
city-state
of
ur
dating
to
circa
4000
bce
and
timber
roads
leading
through
the
swamps
of
glastonbury
england
dating
to
around
the
same
time
period
the
first
long-distance
road
which
came
into
use
around
3500
bce
spanned
1500
miles
from
the
persian
gulf
to
the
mediterranean
sea
but
was
not
paved
and
was
only
partially
maintained
in
around
2000
bce
the
minoans
on
the
greek
island
of
crete
built
a
fifty-kilometer
(thirty-mile)
road
leading
from
the
palace
of
gortyn
on
the
south
side
of
the
island
through
the
mountains
to
the
palace
of
knossos
on
the
north
side
of
the
island
unlike
the
earlier
road
the
minoan
road
was
completely
paved
ancient
minoan
private
homes
had
running
water
a
bathtub
virtually
identical
to
modern
ones
was
unearthed
at
the
palace
of
knossos
several
minoan
private
homes
also
had
toilets
which
could
be
flushed
by
pouring
water
down
the
drain
the
ancient
romans
had
many
public
flush
toilets
which
emptied
into
an
extensive
sewage
system
the
primary
sewer
in
rome
was
the
cloaca
maxima;
construction
began
on
it
in
the
sixth
century
bce
and
it
is
still
in
use
today
the
ancient
romans
also
had
a
complex
system
of
aqueducts
which
were
used
to
transport
water
across
long
distances
the
first
roman
aqueduct
was
built
in
312
bce
the
eleventh
and
final
ancient
roman
aqueduct
was
built
in
226
ce
put
together
the
roman
aqueducts
extended
over
450
kilometers
but
less
than
seventy
kilometers
of
this
was
above
ground
and
supported
by
arches
innovations
continued
through
the
middle
ages
with
innovations
such
as
silk
the
horse
collar
and
horseshoes
in
the
first
few
hundred
years
after
the
fall
of
the
roman
empire
medieval
technology
saw
the
use
of
simple
machines
(such
as
the
lever
the
screw
and
the
pulley)
being
combined
to
form
more
complicated
tools
such
as
the
wheelbarrow
windmills
and
clocks
the
renaissance
brought
forth
many
of
these
innovations
including
the
printing
press
(which
facilitated
the
greater
communication
of
knowledge)
and
technology
became
increasingly
associated
with
science
beginning
a
cycle
of
mutual
advancement
the
advancements
in
technology
in
this
era
allowed
a
more
steady
supply
of
food
followed
by
the
wider
availability
of
consumer
goods
starting
in
the
united
kingdom
in
the
18th
century
the
industrial
revolution
was
a
period
of
great
technological
discovery
particularly
in
the
areas
of
agriculture
manufacturing
mining
metallurgy
and
transport
driven
by
the
discovery
of
steam
power
technology
took
another
step
in
a
second
industrial
revolution
with
the
harnessing
of
electricity
to
create
such
innovations
as
the
electric
motor
light
bulb
and
countless
others
scientific
advancement
and
the
discovery
of
new
concepts
later
allowed
for
powered
flight
and
advancements
in
medicine
chemistry
physics
and
engineering
the
rise
in
technology
has
led
to
skyscrapers
and
broad
urban
areas
whose
inhabitants
rely
on
motors
to
transport
them
and
their
food
supply
communication
was
also
greatly
improved
with
the
invention
of
the
telegraph
telephone
radio
and
television
the
late
19th
and
early
20th
centuries
saw
a
revolution
in
transportation
with
the
invention
of
the
airplane
and
automobile
the
20th
century
brought
a
host
of
innovations
in
physics
the
discovery
of
nuclear
fission
has
led
to
both
nuclear
weapons
and
nuclear
power
computers
were
also
invented
and
later
miniaturized
utilizing
transistors
and
integrated
circuits
information
technology
subsequently
led
to
the
creation
of
the
internet
which
ushered
in
the
current
information
age
humans
have
also
been
able
to
explore
space
with
satellites
(later
used
for
telecommunication)
and
in
manned
missions
going
all
the
way
to
the
moon
in
medicine
this
era
brought
innovations
such
as
open-heart
surgery
and
later
stem
cell
therapy
along
with
new
medications
and
treatments
complex
manufacturing
and
construction
techniques
and
organizations
are
needed
to
make
and
maintain
these
new
technologies
and
entire
industries
have
arisen
to
support
and
develop
succeeding
generations
of
increasingly
more
complex
tools
modern
technology
increasingly
relies
on
training
and
education –
their
designers
builders
maintainers
and
users
often
require
sophisticated
general
and
specific
training
moreover
these
technologies
have
become
so
complex
that
entire
fields
have
been
created
to
support
them
including
engineering
medicine
and
computer
science
and
other
fields
have
been
made
more
complex
such
as
construction
transportation
and
architecture
generally
technicism
is
the
belief
in
the
utility
of
technology
for
improving
human
societies
taken
to
an
extreme
technicism
"reflects
a
fundamental
attitude
which
seeks
to
control
reality
to
resolve
all
problems
with
the
use
of
scientific–technological
methods
and
tools"
in
other
words
human
beings
will
someday
be
able
to
master
all
problems
and
possibly
even
control
the
future
using
technology
some
such
as
stephen
v
monsma
connect
these
ideas
to
the
abdication
of
religion
as
a
higher
moral
authority
optimistic
assumptions
are
made
by
proponents
of
ideologies
such
as
transhumanism
and
singularitarianism
which
view
technological
development
as
generally
having
beneficial
effects
for
the
society
and
the
human
condition
in
these
ideologies
technological
development
is
morally
good
transhumanists
generally
believe
that
the
point
of
technology
is
to
overcome
barriers
and
that
what
we
commonly
refer
to
as
the
human
condition
is
just
another
barrier
to
be
surpassed
singularitarians
believe
in
some
sort
of
"accelerating
change";
that
the
rate
of
technological
progress
accelerates
as
we
obtain
more
technology
and
that
this
will
culminate
in
a
"singularity"
after
artificial
general
intelligence
is
invented
in
which
progress
is
nearly
infinite;
hence
the
term
estimates
for
the
date
of
this
singularity
vary
but
prominent
futurist
ray
kurzweil
estimates
the
singularity
will
occur
in
2045
kurzweil
is
also
known
for
his
history
of
the
universe
in
six
epochs:
(1)
the
physical/chemical
epoch
(2)
the
life
epoch
(3)
the
human/brain
epoch
(4)
the
technology
epoch
(5)
the
artificial
intelligence
epoch
and
(6)
the
universal
colonization
epoch
going
from
one
epoch
to
the
next
is
a
singularity
in
its
own
right
and
a
period
of
speeding
up
precedes
it
each
epoch
takes
a
shorter
time
which
means
the
whole
history
of
the
universe
is
one
giant
singularity
event
some
critics
see
these
ideologies
as
examples
of
scientism
and
techno-utopianism
and
fear
the
notion
of
human
enhancement
and
technological
singularity
which
they
support
some
have
described
karl
marx
as
a
techno-optimist
on
the
somewhat
skeptical
side
are
certain
philosophers
like
herbert
marcuse
and
john
zerzan
who
believe
that
technological
societies
are
inherently
flawed
they
suggest
that
the
inevitable
result
of
such
a
society
is
to
become
evermore
technological
at
the
cost
of
freedom
and
psychological
health
many
such
as
the
luddites
and
prominent
philosopher
martin
heidegger
hold
serious
although
not
entirely
deterministic
reservations
about
technology
(see
"the
question
concerning
technology")
according
to
heidegger
scholars
hubert
dreyfus
and
charles
spinosa
"heidegger
does
not
oppose
technology
he
hopes
to
reveal
the
essence
of
technology
in
a
way
that
'in
no
way
confines
us
to
a
stultified
compulsion
to
push
on
blindly
with
technology
or
what
comes
to
the
same
thing
to
rebel
helplessly
against
it'
indeed
he
promises
that
'when
we
once
open
ourselves
expressly
to
the
essence
of
technology
we
find
ourselves
unexpectedly
taken
into
a
freeing
claim'
what
this
entails
is
a
more
complex
relationship
to
technology
than
either
techno-optimists
or
techno-pessimists
tend
to
allow"
some
of
the
most
poignant
criticisms
of
technology
are
found
in
what
are
now
considered
to
be
dystopian
literary
classics
such
as
aldous
huxley's
"brave
new
world"
anthony
burgess's
"a
clockwork
orange"
and
george
orwell's
"nineteen
eighty-four"
in
goethe's
"faust"
faust
selling
his
soul
to
the
devil
in
return
for
power
over
the
physical
world
is
also
often
interpreted
as
a
metaphor
for
the
adoption
of
industrial
technology
more
recently
modern
works
of
science
fiction
such
as
those
by
philip
k
dick
and
william
gibson
and
films
such
as
"blade
runner"
and
"ghost
in
the
shell"
project
highly
ambivalent
or
cautionary
attitudes
toward
technology's
impact
on
human
society
and
identity
the
late
cultural
critic
neil
postman
distinguished
tool-using
societies
from
technological
societies
and
from
what
he
called
"technopolies"
societies
that
are
dominated
by
the
ideology
of
technological
and
scientific
progress
to
the
exclusion
or
harm
of
other
cultural
practices
values
and
world-views
darin
barney
has
written
about
technology's
impact
on
practices
of
citizenship
and
democratic
culture
suggesting
that
technology
can
be
construed
as
(1)
an
object
of
political
debate
(2)
a
means
or
medium
of
discussion
and
(3)
a
setting
for
democratic
deliberation
and
citizenship
as
a
setting
for
democratic
culture
barney
suggests
that
technology
tends
to
make
ethical
questions
including
the
question
of
what
a
good
life
consists
in
nearly
impossible
because
they
already
give
an
answer
to
the
question:
a
good
life
is
one
that
includes
the
use
of
more
and
more
technology
nikolas
kompridis
has
also
written
about
the
dangers
of
new
technology
such
as
genetic
engineering
nanotechnology
synthetic
biology
and
robotics
he
warns
that
these
technologies
introduce
unprecedented
new
challenges
to
human
beings
including
the
possibility
of
the
permanent
alteration
of
our
biological
nature
these
concerns
are
shared
by
other
philosophers
scientists
and
public
intellectuals
who
have
written
about
similar
issues
(eg
francis
fukuyama
jürgen
habermas
william
joy
and
michael
sandel)
another
prominent
critic
of
technology
is
hubert
dreyfus
who
has
published
books
such
as
"on
the
internet"
and
"what
computers
still
can't
do"
a
more
infamous
anti-technological
treatise
is
""
written
by
the
unabomber
ted
kaczynski
and
printed
in
several
major
newspapers
(and
later
books)
as
part
of
an
effort
to
end
his
bombing
campaign
of
the
techno-industrial
infrastructure
there
are
also
subcultures
that
disapprove
of
some
or
most
technology
such
as
self-identified
off-gridders
the
notion
of
appropriate
technology
was
developed
in
the
20th
century
by
thinkers
such
as
ef
schumacher
and
jacques
ellul
to
describe
situations
where
it
was
not
desirable
to
use
very
new
technologies
or
those
that
required
access
to
some
centralized
infrastructure
or
parts
or
skills
imported
from
elsewhere
the
ecovillage
movement
emerged
in
part
due
to
this
concern
"this
section
mainly
focuses
on
american
concerns
even
if
it
can
reasonably
be
generalized
to
other
western
countries
"
in
his
article
jared
bernstein
a
senior
fellow
at
the
center
on
budget
and
policy
priorities
questions
the
widespread
idea
that
automation
and
more
broadly
technological
advances
have
mainly
contributed
to
this
growing
labor
market
problem
his
thesis
appears
to
be
a
third
way
between
optimism
and
skepticism
essentially
he
stands
for
a
neutral
approach
of
the
linkage
between
technology
and
american
issues
concerning
unemployment
and
declining
wages
he
uses
two
main
arguments
to
defend
his
point
first
because
of
recent
technological
advances
an
increasing
number
of
workers
are
losing
their
jobs
yet
scientific
evidence
fails
to
clearly
demonstrate
that
technology
has
displaced
so
many
workers
that
it
has
created
more
problems
than
it
has
solved
indeed
automation
threatens
repetitive
jobs
but
higher-end
jobs
are
still
necessary
because
they
complement
technology
and
manual
jobs
that
"requires
flexibility
judgment
and
common
sense"
remain
hard
to
replace
with
machines
second
studies
have
not
shown
clear
links
between
recent
technology
advances
and
the
wage
trends
of
the
last
decades
therefore
according
to
bernstein
instead
of
focusing
on
technology
and
its
hypothetical
influences
on
current
american
increasing
unemployment
and
declining
wages
one
needs
to
worry
more
about
"bad
policy
that
fails
to
offset
the
imbalances
in
demand
trade
income
and
opportunity"
for
people
who
use
both
the
internet
and
mobile
devices
in
excessive
quantities
it
is
likely
for
them
to
experience
fatigue
and
over
exhaustion
as
a
result
of
disruptions
in
their
sleeping
patterns
continuous
studies
have
shown
that
increased
bmi
and
weight
gain
are
associated
with
people
who
spend
long
hours
online
and
not
exercising
frequently
heavy
internet
use
is
also
displayed
in
the
school
lower
grades
of
those
who
use
it
in
excessive
amounts
it
has
also
been
noted
that
the
use
of
mobile
phones
whilst
driving
has
increased
the
occurrence
of
road
accidents
—
particularly
amongst
teen
drivers
statistically
teens
reportedly
have
fourfold
the
number
of
road
traffic
incidents
as
those
who
are
20
years
or
older
and
a
very
high
percentage
of
adolescents
write
(81%)
and
read
(92%)
texts
while
driving
in
this
context
mass
media
and
technology
have
a
negative
impact
on
people
on
both
their
mental
and
physical
health
thomas
p
hughes
stated
that
because
technology
has
been
considered
as
a
key
way
to
solve
problems
we
need
to
be
aware
of
its
complex
and
varied
characters
to
use
it
more
efficiently
what
is
the
difference
between
a
wheel
or
a
compass
and
cooking
machines
such
as
an
oven
or
a
gas
stove?
can
we
consider
all
of
them
only
a
part
of
them
or
none
of
them
as
technologies?
technology
is
often
considered
too
narrowly;
according
to
hughes
"technology
is
a
creative
process
involving
human
ingenuity"
this
definition's
emphasis
on
creativity
avoids
unbounded
definitions
that
may
mistakenly
include
cooking
"technologies"
but
it
also
highlights
the
prominent
role
of
humans
and
therefore
their
responsibilities
for
the
use
of
complex
technological
systems
yet
because
technology
is
everywhere
and
has
dramatically
changed
landscapes
and
societies
hughes
argues
that
engineers
scientists
and
managers
have
often
believed
that
they
can
use
technology
to
shape
the
world
as
they
want
they
have
often
supposed
that
technology
is
easily
controllable
and
this
assumption
has
to
be
thoroughly
questioned
for
instance
evgeny
morozov
particularly
challenges
two
concepts:
"internet-centrism"
and
"solutionism"
internet-centrism
refers
to
the
idea
that
our
society
is
convinced
that
the
internet
is
one
of
the
most
stable
and
coherent
forces
solutionism
is
the
ideology
that
every
social
issue
can
be
solved
thanks
to
technology
and
especially
thanks
to
the
internet
in
fact
technology
intrinsically
contains
uncertainties
and
limitations
according
to
alexis
madrigal's
review
of
morozov's
theory
to
ignore
it
will
lead
to
"unexpected
consequences
that
could
eventually
cause
more
damage
than
the
problems
they
seek
to
address"
benjamin
r
cohen
and
gwen
ottinger
also
discussed
the
multivalent
effects
of
technology
therefore
recognition
of
the
limitations
of
technology
and
more
broadly
scientific
knowledge
is
needed –
especially
in
cases
dealing
with
environmental
justice
and
health
issues
ottinger
continues
this
reasoning
and
argues
that
the
ongoing
recognition
of
the
limitations
of
scientific
knowledge
goes
hand
in
hand
with
scientists
and
engineers’
new
comprehension
of
their
role
such
an
approach
of
technology
and
science
"[require]
technical
professionals
to
conceive
of
their
roles
in
the
process
differently
[they
have
to
consider
themselves
as]
collaborators
in
research
and
problem
solving
rather
than
simply
providers
of
information
and
technical
solutions"
technology
is
properly
defined
as
any
application
of
science
to
accomplish
a
function
the
science
can
be
leading
edge
or
well
established
and
the
function
can
have
high
visibility
or
be
significantly
more
mundane
but
it
is
all
technology
and
its
exploitation
is
the
foundation
of
all
competitive
advantage
technology-based
planning
is
what
was
used
to
build
the
us
industrial
giants
before
wwii
(eg
dow
dupont
gm)
and
it
is
what
was
used
to
transform
the
us
into
a
superpower
it
was
not
economic-based
planning
the
use
of
basic
technology
is
also
a
feature
of
other
animal
species
apart
from
humans
these
include
primates
such
as
chimpanzees
some
dolphin
communities
and
crows
considering
a
more
generic
perspective
of
technology
as
ethology
of
active
environmental
conditioning
and
control
we
can
also
refer
to
animal
examples
such
as
beavers
and
their
dams
or
bees
and
their
honeycombs
the
ability
to
make
and
use
tools
was
once
considered
a
defining
characteristic
of
the
genus
homo
however
the
discovery
of
tool
construction
among
chimpanzees
and
related
primates
has
discarded
the
notion
of
the
use
of
technology
as
unique
to
humans
for
example
researchers
have
observed
wild
chimpanzees
utilising
tools
for
foraging:
some
of
the
tools
used
include
leaf
sponges
termite
fishing
probes
pestles
and
levers
west
african
chimpanzees
also
use
stone
hammers
and
anvils
for
cracking
nuts
as
do
capuchin
monkeys
of
boa
vista
brazil
theories
of
technology
often
attempt
to
predict
the
future
of
technology
based
on
the
high
technology
and
science
of
the
time
as
with
all
predictions
of
the
future
however
technology's
is
uncertain
in
2005
futurist
ray
kurzweil
predicted
that
the
future
of
technology
would
mainly
consist
of
an
overlapping
"gnr
revolution"
of
genetics
nanotechnology
and
robotics
with
robotics
being
the
most
important
of
the
three
outline
of
technology
the
following
outline
is
provided
as
an
overview
of
and
topical
guide
to
technology:
technology
–
collection
of
tools
including
machinery
modifications
arrangements
and
procedures
used
by
humans
engineering
is
the
discipline
that
seeks
to
study
and
design
new
technologies
technologies
significantly
affect
human
as
well
as
other
animal
species'
ability
to
control
and
adapt
to
their
natural
environments
history
of
technology
potential
technology
of
the
future
includes:
hypothetical
technology
–
philosophy
of
technology
–
fictional
technology
–
technology
readiness
level
technology
readiness
levels
(trl)
are
a
method
of
estimating
technology
maturity
of
critical
technology
elements
(cte)
of
a
program
during
the
acquisition
process
they
are
determined
during
a
technology
readiness
assessment
(tra)
that
examines
program
concepts
technology
requirements
and
demonstrated
technology
capabilities
trl
are
based
on
a
scale
from
1
to
9
with
9
being
the
most
mature
technology
the
use
of
trls
enables
consistent
uniform
discussions
of
technical
maturity
across
different
types
of
technology
trl
has
been
in
widespread
use
at
nasa
since
the
1980s
where
it
was
originally
invented
in
1999
the
us
department
of
defense
was
advised
by
gao
to
use
the
scale
for
procurement
which
it
did
from
the
early
2000s
by
2008
the
scale
was
also
in
use
at
the
european
space
agency
(esa)
as
it
is
evidenced
by
their
handbook
the
european
commission
advised
eu-funded
research
and
innovation
projects
to
adopt
the
scale
in
2010
which
they
did
from
2014
in
its
horizon
2020
program
in
2013
trl
was
further
canonized
by
the
iso
16290:2013
standard
a
comprehensive
approach
and
discussion
about
trls
has
been
published
by
the
european
association
of
research
and
technology
organisations
(earto)
extensive
criticism
of
the
adoption
of
trl
scale
by
the
european
union
was
published
in
the
innovation
journal
in
that
"concreteness
and
sophistication
of
the
trl
scale
gradually
diminished
as
its
usage
spread
outside
its
original
context
(space
programs)"
technology
readiness
levels
were
originally
conceived
at
nasa
in
1974
and
formally
defined
in
1989
the
original
definition
included
seven
levels
but
in
the
1990s
nasa
adopted
the
current
nine-level
scale
that
subsequently
gained
widespread
acceptance
original
nasa
trl
definitions
(1989)
the
trl
methodology
was
originated
by
stan
sadin
at
nasa
headquarters
in
1974
at
that
time
ray
chase
was
the
jpl
propulsion
division
representative
on
the
jupiter
orbiter
design
team
at
the
suggestion
of
stan
sadin
mr
chase
used
this
methodology
to
assess
the
technology
readiness
of
the
proposed
jpl
jupiter
orbiter
spacecraft
design
later
mr
chase
spent
a
year
at
nasa
headquarters
helping
mr
sadin
institutionalize
the
trl
methodology
mr
chase
joined
anser
in
1978
where
he
used
the
trl
methodology
to
evaluate
the
technology
readiness
of
proposed
air
force
development
programs
he
published
several
articles
during
the
1980s
and
90s
on
reusable
launch
vehicles
utilizing
the
trl
methodology
these
documented
an
expanded
version
of
the
methodology
that
included
design
tools
test
facilities
and
manufacturing
readiness
on
the
air
force
have
not
program
the
have
not
program
manager
greg
jenkins
and
ray
chase
published
the
expanded
version
of
the
trl
methodology
which
included
design
and
manufacturing
leon
mckinney
and
mr
chase
used
the
expanded
version
to
assess
the
technology
readiness
of
the
anser
team's
highly
reusable
space
transportation
("hrst")
concept
anser
also
created
an
adapted
version
of
the
trl
methodology
for
proposed
homeland
security
agency
programs
the
united
states
air
force
adopted
the
use
of
technology
readiness
levels
in
the
1990s
in
1995
john
c
mankins
nasa
wrote
a
paper
that
discussed
nasa's
use
of
trl
extended
the
scale
and
proposed
expanded
descriptions
for
each
trl
in
1999
the
united
states
general
accounting
office
produced
an
influential
report
that
examined
the
differences
in
technology
transition
between
the
dod
and
private
industry
it
concluded
that
the
dod
takes
greater
risks
and
attempts
to
transition
emerging
technologies
at
lesser
degrees
of
maturity
than
does
private
industry
the
gao
concluded
that
use
of
immature
technology
increased
overall
program
risk
the
gao
recommended
that
the
dod
make
wider
use
of
technology
readiness
levels
as
a
means
of
assessing
technology
maturity
prior
to
transition
in
2001
the
deputy
under
secretary
of
defense
for
science
and
technology
issued
a
memorandum
that
endorsed
use
of
trls
in
new
major
programs
guidance
for
assessing
technology
maturity
was
incorporated
into
the
"defense
acquisition
guidebook"
subsequently
the
dod
developed
detailed
guidance
for
using
trls
in
the
2003
dod
technology
readiness
assessment
deskbook
the
european
space
agency
adopted
the
trl
scale
in
the
mid-2000s
its
handbook
closely
follows
the
nasa
definition
of
trls
the
universal
usage
of
trl
in
eu
policy
was
proposed
in
the
final
report
of
the
first
high
level
expert
group
on
key
enabling
technologies
and
it
was
indeed
implemented
in
the
subsequent
eu
framework
program
called
h2020
running
from
2013
to
2020
this
means
not
only
space
and
weapons
programs
but
everything
from
nanotechnology
to
informatics
and
communication
technology
a
"technology
readiness
level
calculator"
was
developed
by
the
united
states
air
force
this
tool
is
a
standard
set
of
questions
implemented
in
microsoft
excel
that
produces
a
graphical
display
of
the
trls
achieved
this
tool
is
intended
to
provide
a
snapshot
of
technology
maturity
at
a
given
point
in
time
the
"technology
program
management
model"
was
developed
by
the
united
states
army
the
tpmm
is
a
trl-gated
high-fidelity
activity
model
that
provides
a
flexible
management
tool
to
assist
technology
managers
in
planning
managing
and
assessing
their
technologies
for
successful
technology
transition
the
model
provides
a
core
set
of
activities
including
systems
engineering
and
program
management
tasks
that
are
tailored
to
the
technology
development
and
management
goals
this
approach
is
comprehensive
yet
it
consolidates
the
complex
activities
that
are
relevant
to
the
development
and
transition
of
a
specific
technology
program
into
one
integrated
model
the
primary
purpose
of
using
technology
readiness
levels
is
to
help
management
in
making
decisions
concerning
the
development
and
transitioning
of
technology
it
should
be
viewed
as
one
of
several
tools
that
are
needed
to
manage
the
progress
of
research
and
development
activity
within
an
organization
among
the
advantages
of
trls:
some
of
the
characteristics
of
trls
that
limit
their
utility:
current
trl
models
tend
to
disregard
negative
and
obsolescence
factors
there
have
been
suggestions
made
for
incorporating
such
factors
into
assessments
for
complex
technologies
that
incorporate
various
development
stages
a
more
detailed
scheme
called
the
technology
readiness
pathway
matrix
has
been
developed
going
from
basic
units
to
applications
in
society
this
tool
aims
to
show
that
a
readiness
level
of
a
technology
is
based
on
a
less
linear
process
but
on
a
more
complex
pathway
through
its
application
in
society
multimodal
anthropology
multimodal
anthropology
is
an
emerging
subfield
of
social
cultural
anthropology
that
encompasses
anthropological
research
and
knowledge
production
across
multiple
traditional
and
new
media
platforms
and
practices
including
film
video
photography
theatre
design
podcast
mobile
apps
interactive
games
web-based
social
networking
immersive
360
video
and
augmented
reality
as
characterized
in
american
anthropologist""
multimodal
anthropology
is
an
"anthropology
that
works
across
multiple
media
but
one
that
also
engages
in
public
anthropology
and
collaborative
anthropology
through
a
field
of
differentially
linked
media
platforms"
(collins
durington
gill)
a
multimodal
approach
also
encourages
anthropologist
to
reconsider
the
ways
in
which
they
conduct
their
research
to
pay
close
attention
to
the
role
various
media
technologies
and
digital
devices
plays
in
the
lives
of
their
interlocutors
and
how
they
these
technologies
redefine
what
fieldwork
looks
like
multimodal
anthropology
is
not
a
new
concept
it
has
been
a
fundamental
part
of
anthropological
research
and
fieldwork
from
the
early
days
of
the
disciple
anthropologists
have
been
experimenting
with
different
forms
media
technologies
throughout
the
twentieth
century
whenever
confronted
with
the
limitation
of
text-based
ethnography
multimodal
is
a
term
that
has
readily
been
used
since
the
1970s
in
varied
disciplines
as
psychotherapy
phonetics
genetics
literature
and
medicine
to
characterize
different
approaches
to
carrying
out
scientific
research
that
involves
to
a
certain
degree
thinking
outside
of
the
box
in
the
early
1990s
semioticians
used
the
terms
to
discuss
different
forms
of
communication
across
different
media
eventually
including
digital
media
technological
advances
in
the
later
part
of
the
twentieth
century
the
accessibility
to
photography
film
cameras
and
audio
recorders
led
to
the
emergence
of
visual
anthropology
as
a
sub
discipline
dedicated
to
the
study
and
production
of
ethnographic
photography
film
and
media
building
in
this
legacy
multimodal
anthropology
seeks
to
expand
the
boundaries
of
visual
anthropology
to
incorporate
emerging
technologies
of
twenty-first
century
including
mobile
networking
social
media
geo-mapping
virtual
reality
podcasting
interactive
design
along
with
other
traditional
forms
of
learning
and
knowledge
production
like
art
and
drawing
that
were
often
sidelined
within
visual
anthropology
such
as
interactive
gaming
theatre
performance
graphic
novels
ethnofiction
and
experimental
ethnography
as
samuel
collins
matthew
durington
and
harjant
gill
note
in
their
introductory
essay
on
title
"multimodality:
an
invitation"
published
in
american
anthropologist
"multimodal
anthropologies
does
not
attempt
–
or
desire
–
to
supplant
visual
anthropology
rather
it
seeks
to
include
traditional
forms
of
visual
anthropology
while
simultaneously
broadening
the
purview
of
the
discipline
to
engage
in
variety
of
media
forms
that
exist
today"
enchroma
enchroma
lenses
are
glasses
designed
to
improve
and
modify
some
aspects
of
color
vision
deficiency
for
color
blind
people
the
glasses
were
invented
by
dr
donald
mcpherson
in
2002
wearing
the
glasses
results
in
subtle
differences
when
color
blind
people
look
longer
and
more
carefully
glass
scientist
dr
donald
mcpherson
invented
enchroma
glasses
by
accident
he
originally
invented
this
type
of
lens
to
protect
surgeons
during
laser
operations
in
2002
at
the
ultimate
frisbee
tournament
in
santa
cruz
california
mcpherson
lent
a
pair
to
a
friend
who
was
color
blind
his
friend
saw
colors
he
had
never
seen
before
mcpherson
started
studying
color
blindness
and
with
andrew
schmeder
founded
the
company
enchroma
inc
in
2010
to
sell
glasses
that
compensate
for
color
vision
deficiency
enchroma
glasses
target
people
with
difficulties
in
distinguishing
reds
and
greens
the
first
pair
of
commercial
glasses
were
released
in
2012
the
optical
filter
used
in
enchroma
lenses
can
improve
or
modify
aspects
of
color
vision
deficiency
(cvd)
enchroma
lenses
focus
on
the
most
common
color
vision
deficiency
which
is
caused
by
the
red
and
green
retinal
cone
cells
that
when
responding
to
light
coincide
the
most
common
form
of
color
blindness
is
known
as
deuteranomaly
it
is
genetic
and
an
x-linked
trait
that
affects
up
to
one
in
every
12
men
(8%)
and
one
in
200
women
(05%)
to
eliminate
the
overlapping
of
the
wavelengths
of
light
there
is
an
optical
material
called
a
'multi-notch'
filter
is
capable
of
removing
the
exact
wavelengths
of
light
in
the
location
where
it
overlaps
getting
a
simplified
differentiation
of
colors
the
glasses
block
specific
wavelengths
to
create
a
clearer
separation
of
different
color
signals
so
that
they
can
be
better
calculated
by
the
brain
the
separation
of
signals
allows
most
people
with
cvd
to
distinguish
colors
but
the
glasses
will
have
little
to
no
effect
on
the
20%
of
color
blind
people
who
have
severe
color
impairment
the
filters
designed
by
the
method
have
a
spectral
transmittance
that
can
be
essentially
described
as
a
multi-band
filter
made
up
by
a
plurality
of
passbands
interleaved
with
stopbands
this
technology
makes
it
possible
to
remove
the
overlapping
of
colours
the
use
of
glass
in
lens
manufacturing
has
reduced
due
to
the
availability
of
new
lens
materials
offering
different
characteristics
lens
research
is
aimed
at
finding
the
optimum
materials
for
the
most
common
vision
deficiencies
enchroma
technology
is
one
of
the
few
materials
to
compenste
for
cvd
trivex
eyeglass
lenses
were
first
developed
by
ppg
industries
in
the
united
states
the
lenses
can
be
progressive
or
photochromic
they
are
significant
in
allowing
better
vision
sharpness
and
eye
protection
and
are
also
comfortable
to
wear
since
the
weight
of
the
material
is
minimized
the
tinted
plastic
lens
used
to
make
enchroma
glasses
is
coated
in
approximtely
100
layers
of
dielectric
material
a
coating
can
be
applied
on
both
sides
of
the
lens
to
eliminate
light
reflection
and
to
protect
the
lens
itself
for
scratches
a
predecessor
of
the
trivex
lens
with
similar
characteristics
is
made
from
polycarbonate
enchroma
lenses
are
used
in
various
types
of
glasses
including
sunglasses
indoor
glasses
infant
industrial
safety
and
sports
glasses
sunglasses
were
the
first
product
to
use
enchroma
lenses
they
can
be
used
in
various
lighting
conditions
even
in
bright
indoor
environments
and
assist
people
affected
by
deuteranopia
and
protanopia
they
protect
those
who
are
affected
by
color
blindness
from
solar
ultraviolet
and
blue
light
and
do
not
let
solar
radiation
with
a
wavelength
shorter
than
450
nanometers
pass
through
they
block
just
one
or
two
colors
(usually
green
and
blue)
permitting
people
to
see
colors
such
as
dull
brown
or
yellow
this
is
due
to
a
specific
coat
that
increases
the
brightness
of
colors
and
provides
protection
from
uv
rays
different
models
of
sunglasses
exist
according
to
the
color
deficiency
the
person
suffers
from
this
version
of
the
glasses
for
people
with
cvd
can
be
used
indoors
but
they
work
best
in
bright
lighting
and
have
applications
for
people
who
spend
a
lot
of
time
facing
a
computer
monitor
students
with
color
blindness
can
have
a
benefit
from
the
glasses
because
they
can
understand
particular
color-coded
information
better
at
their
school
or
at
university
there
are
also
particular
glasses
for
athletic
people
for
children
and
for
industrial
safety
use
which
are
made
of
particular
materials
such
as
polycarbonate
this
makes
the
glasses
stronger
and
less
breakable
these
kind
of
lenses
are
able
to
correct
severe
vision
deficiency
but
they
do
not
need
additional
lens
thickness
to
achieve
this
trivex
eyeglass
lenses
offer
the
same
characteristics
as
those
that
are
made
for
athletic
people
or
children
and
provide
an
alternative
material
to
polycarbonate
the
glasses
are
available
in
single
vision
lenses
bifocals
and
progressive
lens
the
sunglasses
are
also
used
by
some
senior
citizens
to
help
with
sunlight
sensitivity
or
due
to
medical
conditions
such
as
diabetes
glaucoma
macular
degeneration
cataracts
multiple
sclerosis
and
eye
cancer
the
qassim
university
and
the
pacific
university
conducted
joint
experiments
the
qassim
university
experiment
involved
25
males
aged
from
20
to
25
years
two
of
those
25
were
excluded
because
of
a
vision
disease
that
may
affect
the
cvd
deficiency
the
pacific
university
college
of
optometry's
experiment
tested
whether
the
glasses
helped
people
with
particular
deficiencies
improve
the
way
they
see
colors
the
study
involved
ten
individuals
with
hereditary
deficiencies
(nine
males
and
one
female
from
age
19
to
52)
the
enchroma
cx-14
filters
did
not
significantly
influence
the
vision
of
cvd
subjects
but
for
two
of
them
the
error
score
was
improved
in
2018
researchers
from
the
university
of
granada
studied
enchroma
lenses
and
proved
that
they
merely
helped
color
blind
people
to
see
the
same
colors
in
a
different
way
since
the
colored
filter
altered
the
way
colors
appeared
in
their
eyes
the
research
involved
200
volunteers
including
48
who
were
color
blind
people
out
of
the
48
four
were
women
and
44
were
men
aged
between
14
and
64
these
participants
were
already
aware
of
their
own
condition
of
cvd
the
study
concluded
that
although
an
improvement
in
color
vision
has
been
accomplished
since
the
colors
are
enhanced
by
the
effect
of
the
glasses
the
technology
could
not
be
considered
completely
effective
because
the
color
vision
it
offers
ws
not
identical
to
the
non-colour
blind
individuals
in
order
to
evaluate
the
effectiveness
of
the
glasses
the
color
vision
of
the
participants
was
evaluated
without
the
enchroma
glasses
using
the
ishihara
test
farnsworth-munsell
100
hue
test
and
a
color-naming
test
(with
21
colors
from
an
x-rite
gretagmacbeth
chart)
the
tests
were
repeated
under
the
same
conditions
while
wearing
enchroma
glasses
in
order
to
measure
the
full
effect
of
enchroma
lenses
an
adaptation
time
of
30
minutes
was
given
to
the
participants
between
the
two
sections
of
the
tests
more
than
2
weeks
passed
in
order
to
reduce
the
likelihood
of
participants
memorizing
the
color
patterns
the
researchers
claimed
that
the
effect
of
using
enchroma
glasses
is
similar
to
glasses
where
the
use
of
color
filters
changes
the
user’s
perception
and
increases
the
contrast
among
the
colors
such
as
those
used
for
shooting
or
hunting
the
research
showed
that
enchroma
glasses
did
not
reveal
any
improvement
in
the
ishihara
test
and
farnsworth
munsell
100
hue
test
in
other
research
on
the
effects
of
wearing
enchroma
glasses
on
red-green
cvd
r
mastey
recruited
27
males:
eight
deuteranomalous
ten
deuteranopic
and
nine
protanopic
while
e
j
patterson
recruited
fifteen
males:
seven
deuteranopic
six
deuteranomalous
one
protanomalous
and
one
protanopic
for
the
research
the
researchers
used
the
color
assessment
and
diagnosis
(cad)
test
that
provides
chromatic
discrimination
thresholds
in
2015
dr
blake
porter
of
the
university
of
otago
conducted
an
experiment
with
406
participants
of
whom
42
already
used
enchroma
glasses
the
participants
were
asked
questions
about
their
color
blindness
their
personal
experience
with
the
lens
and
the
effects
of
those
on
their
life
and
on
the
way
they
saw
the
colors
90%
of
the
participants
said
they
would
recommend
them
to
other
people
and
more
than
50%
of
those
people
stated
that
enchroma
glasses
improved
their
life
10%
of
the
remaining
participants
reported
that
the
glasses
did
not
have
any
impact
on
their
life
40%
of
the
participants
identified
green
as
the
color
range
that
changed
the
most
in
some
subjects
improvements
due
to
the
technology
do
not
happen
immediately
because
the
brain
requires
time
to
rewire
and
create
new
links
neuroplasticity
neuroplasticity
depends
on
the
age
of
the
individual
younger
people
reported
that
they
perceived
little
changes
from
the
first
time
they
wore
glasses
and
a
second
wearing
in
which
they
have
had
time
to
adapt
to
the
technology
middle
age
people
reported
that
the
colors
continued
to
change
from
the
first
time
they
wore
the
lens
and
that
color
brightness
and
enhancement
got
better
with
time
most
participants
related
that
the
biggest
effects
of
the
glasses
are
not
perceived
initially
but
after
a
few
days
confirming
that
the
neuroplasticity
takes
time
the
experiment
demonstrated
that
the
glasses
have
a
positive
effect
on
the
everyday
life
of
those
subject
to
color
blindness
the
glasses
achieve
better
results
with
some
colors
than
others
the
biggest
improvements
are
achieved
with
greens
followed
by
purples
pinks
and
reds
subtle
differences
emerged
when
users
looked
longer
and
more
carefully
shivani
siroya
shivani
siroya
is
the
founder
and
chief
executive
officer
of
tala
a
smartphone
lending
app
she
founded
the
app
in
2011
to
offer
instant
credit
scores
to
people
in
underrepresented
markets
such
as
kenya
tanzania
and
the
philippines
the
app
also
acts
as
a
lender
and
has
granted
more
than
$225m
in
microloans
as
of
2018
in
april
2018
the
app
secured
$65
million
in
funding
from
female
founders
fund
lowercase
capital
and
revolution
capital
among
others
its
total
funding
is
over
$105
million
prior
to
founding
tala
siroya
worked
for
the
un
population
fund
she
also
worked
at
citi
ubs
and
credit
suisse
in
microfinance
jobs
siroya
has
a
mph
in
health
economics
and
policy
from
columbia
university
and
a
ba
from
wesleyan
university
in
international
relations
melinda
gates
nominated
siroya
in
2018
as
a
wired
icon
science
science
(from
the
latin
word
"scientia"
meaning
"knowledge")
is
a
systematic
enterprise
that
builds
and
organizes
knowledge
in
the
form
of
testable
explanations
and
predictions
about
the
universe
the
earliest
roots
of
science
can
be
traced
to
ancient
egypt
and
mesopotamia
in
around
3500
to
3000
bce
their
contributions
to
mathematics
astronomy
and
medicine
entered
and
shaped
greek
natural
philosophy
of
classical
antiquity
whereby
formal
attempts
were
made
to
explain
events
of
the
physical
world
based
on
natural
causes
after
the
fall
of
the
western
roman
empire
knowledge
of
greek
conceptions
of
the
world
deteriorated
in
western
europe
during
the
early
centuries
(400
to
1000
ce)
of
the
middle
ages
but
was
preserved
in
the
muslim
world
during
the
islamic
golden
age
the
recovery
and
assimilation
of
greek
works
and
islamic
inquiries
into
western
europe
from
the
10th
to
13th
century
revived
natural
philosophy
which
was
later
transformed
by
the
scientific
revolution
that
began
in
the
16th
century
as
new
ideas
and
discoveries
departed
from
previous
greek
conceptions
and
traditions
the
scientific
method
soon
played
a
greater
role
in
knowledge
creation
and
it
was
not
until
the
19th
century
that
many
of
the
institutional
and
professional
features
of
science
began
to
take
shape
modern
science
is
typically
divided
into
three
major
branches
that
consist
of
the
natural
sciences
(eg
biology
chemistry
and
physics)
which
study
nature
in
the
broadest
sense;
the
social
sciences
(eg
economics
psychology
and
sociology)
which
study
individuals
and
societies;
and
the
formal
sciences
(eg
logic
mathematics
and
theoretical
computer
science)
which
study
abstract
concepts
there
is
disagreement
however
on
whether
the
formal
sciences
actually
constitute
a
science
as
they
do
not
rely
on
empirical
evidence
disciplines
that
use
existing
scientific
knowledge
for
practical
purposes
such
as
engineering
and
medicine
are
described
as
applied
sciences
science
is
based
on
research
which
is
commonly
conducted
in
academic
and
research
institutions
as
well
as
in
government
agencies
and
companies
the
practical
impact
of
scientific
research
has
led
to
the
emergence
of
science
policies
that
seek
to
influence
the
scientific
enterprise
by
prioritizing
the
development
of
commercial
products
armaments
health
care
and
environmental
protection
science
in
a
broad
sense
existed
before
the
modern
era
and
in
many
historical
civilizations
modern
science
is
distinct
in
its
approach
and
successful
in
its
results
so
it
now
defines
what
science
is
in
the
strictest
sense
of
the
term
science
in
its
original
sense
was
a
word
for
a
type
of
knowledge
rather
than
a
specialized
word
for
the
pursuit
of
such
knowledge
in
particular
it
was
the
type
of
knowledge
which
people
can
communicate
to
each
other
and
share
for
example
knowledge
about
the
working
of
natural
things
was
gathered
long
before
recorded
history
and
led
to
the
development
of
complex
abstract
thought
this
is
shown
by
the
construction
of
complex
calendars
techniques
for
making
poisonous
plants
edible
public
works
at
national
scale
such
as
those
which
harnessed
the
floodplain
of
the
yangtse
with
reservoirs
dams
and
dikes
and
buildings
such
as
the
pyramids
however
no
consistent
conscious
distinction
was
made
between
knowledge
of
such
things
which
are
true
in
every
community
and
other
types
of
communal
knowledge
such
as
mythologies
and
legal
systems
metallurgy
was
known
in
prehistory
and
the
vinča
culture
was
the
earliest
known
producer
of
bronze-like
alloys
it
is
thought
that
early
experimentation
with
heating
and
mixing
of
substances
over
time
developed
into
alchemy
neither
the
words
nor
the
concepts
"science"
and
"nature"
were
part
of
the
conceptual
landscape
in
the
ancient
near
east
the
ancient
mesopotamians
used
knowledge
about
the
properties
of
various
natural
chemicals
for
manufacturing
pottery
faience
glass
soap
metals
lime
plaster
and
waterproofing;
they
also
studied
animal
physiology
anatomy
and
behavior
for
divinatory
purposes
and
made
extensive
records
of
the
movements
of
astronomical
objects
for
their
study
of
astrology
the
mesopotamians
had
intense
interest
in
medicine
and
the
earliest
medical
prescriptions
appear
in
sumerian
during
the
third
dynasty
of
ur
(
2112
bce
–
2004
bce)
nonetheless
the
mesopotamians
seem
to
have
had
little
interest
in
gathering
information
about
the
natural
world
for
the
mere
sake
of
gathering
information
and
mainly
only
studied
scientific
subjects
which
had
obvious
practical
applications
or
immediate
relevance
to
their
religious
system
in
the
classical
world
there
is
no
real
ancient
analog
of
a
modern
scientist
instead
well-educated
usually
upper-class
and
almost
universally
male
individuals
performed
various
investigations
into
nature
whenever
they
could
afford
the
time
before
the
invention
or
discovery
of
the
concept
of
"nature"
(ancient
greek
"phusis")
by
the
pre-socratic
philosophers
the
same
words
tend
to
be
used
to
describe
the
"natural"
"way"
in
which
a
plant
grows
and
the
"way"
in
which
for
example
one
tribe
worships
a
particular
god
for
this
reason
it
is
claimed
these
men
were
the
first
philosophers
in
the
strict
sense
and
also
the
first
people
to
clearly
distinguish
"nature"
and
"convention"
natural
philosophy
the
precursor
of
natural
science
was
thereby
distinguished
as
the
knowledge
of
nature
and
things
which
are
true
for
every
community
and
the
name
of
the
specialized
pursuit
of
such
knowledge
was
"philosophy" –
the
realm
of
the
first
philosopher-physicists
they
were
mainly
speculators
or
theorists
particularly
interested
in
astronomy
in
contrast
trying
to
use
knowledge
of
nature
to
imitate
nature
(artifice
or
technology
greek
"technē")
was
seen
by
classical
scientists
as
a
more
appropriate
interest
for
lower
class
artisans
the
early
greek
philosophers
of
the
milesian
school
which
was
founded
by
thales
of
miletus
and
later
continued
by
his
successors
anaximander
and
anaximenes
were
the
first
to
attempt
to
explain
natural
phenomena
without
relying
on
the
supernatural
the
pythagoreans
developed
a
complex
number
philosophy
and
contributed
significantly
to
the
development
of
mathematical
science
the
theory
of
atoms
was
developed
by
the
greek
philosopher
leucippus
and
his
student
democritus
the
greek
doctor
hippocrates
established
the
tradition
of
systematic
medical
science
and
is
known
as
"the
father
of
medicine"
a
turning
point
in
the
history
of
early
philosophical
science
was
socrates'
example
of
applying
philosophy
to
the
study
of
human
matters
including
human
nature
the
nature
of
political
communities
and
human
knowledge
itself
the
socratic
method
as
documented
by
plato's
dialogues
is
a
dialectic
method
of
hypothesis
elimination:
better
hypotheses
are
found
by
steadily
identifying
and
eliminating
those
that
lead
to
contradictions
this
was
a
reaction
to
the
sophist
emphasis
on
rhetoric
the
socratic
method
searches
for
general
commonly
held
truths
that
shape
beliefs
and
scrutinizes
them
to
determine
their
consistency
with
other
beliefs
socrates
criticized
the
older
type
of
study
of
physics
as
too
purely
speculative
and
lacking
in
self-criticism
socrates
was
later
in
the
words
of
his
"apology"
accused
of
corrupting
the
youth
of
athens
because
he
did
"not
believe
in
the
gods
the
state
believes
in
but
in
other
new
spiritual
beings"
socrates
refuted
these
claims
but
was
sentenced
to
death
aristotle
later
created
a
systematic
programme
of
teleological
philosophy:
motion
and
change
is
described
as
the
actualization
of
potentials
already
in
things
according
to
what
types
of
things
they
are
in
his
physics
the
sun
goes
around
the
earth
and
many
things
have
it
as
part
of
their
nature
that
they
are
for
humans
each
thing
has
a
formal
cause
a
final
cause
and
a
role
in
a
cosmic
order
with
an
unmoved
mover
the
socratics
also
insisted
that
philosophy
should
be
used
to
consider
the
practical
question
of
the
best
way
to
live
for
a
human
being
(a
study
aristotle
divided
into
ethics
and
political
philosophy)
aristotle
maintained
that
man
knows
a
thing
scientifically
"when
he
possesses
a
conviction
arrived
at
in
a
certain
way
and
when
the
first
principles
on
which
that
conviction
rests
are
known
to
him
with
certainty"
the
greek
astronomer
aristarchus
of
samos
(310–230
bce)
was
the
first
to
propose
a
heliocentric
model
of
the
universe
with
the
sun
at
the
center
and
all
the
planets
orbiting
it
aristarchus's
model
was
widely
rejected
because
it
was
believed
to
violate
the
laws
of
physics
the
inventor
and
mathematician
archimedes
of
syracuse
made
major
contributions
to
the
beginnings
of
calculus
and
has
sometimes
been
credited
as
its
inventor
although
his
proto-calculus
lacked
several
defining
features
pliny
the
elder
was
a
roman
writer
and
polymath
who
wrote
the
seminal
encyclopedia
"natural
history"
dealing
with
history
geography
medicine
astronomy
earth
science
botany
and
zoology
other
scientists
or
proto-scientists
in
antiquity
were
theophrastus
euclid
herophilos
hipparchus
ptolemy
and
galen
during
late
antiquity
in
the
byzantine
empire
many
greek
classical
texts
were
preserved
many
syriac
translations
were
done
by
groups
such
as
the
nestorians
and
monophysites
they
played
a
role
when
they
translated
greek
classical
texts
into
arabic
under
the
caliphate
during
which
many
types
of
classical
learning
were
preserved
and
in
some
cases
improved
upon
in
addition
the
neighboring
sassanid
empire
established
the
medical
academy
of
gondeshapur
where
greek
syriac
and
persian
physicians
established
the
most
important
medical
center
of
the
ancient
world
during
the
6th
and
7th
centuries
because
of
the
collapse
of
the
western
roman
empire
due
to
the
migration
period
an
intellectual
decline
took
place
in
the
western
part
of
europe
in
the
400s
in
contrast
the
byzantine
empire
resisted
the
attacks
from
the
barbarians
and
preserved
and
improved
upon
the
learning
john
philoponus
a
byzantine
scholar
in
the
500s
was
the
first
scholar
ever
to
question
aristotle's
teaching
of
physics
and
to
note
its
flaws
john
philoponus'
criticism
of
aristotelian
principles
of
physics
served
as
an
inspiration
to
medieval
scholars
as
well
as
to
galileo
galilei
who
ten
centuries
later
during
the
scientific
revolution
extensively
cited
philoponus
in
his
works
while
making
the
case
as
to
why
aristotelian
physics
was
flawed
during
late
antiquity
and
the
early
middle
ages
the
aristotelian
approach
to
inquiries
on
natural
phenomena
was
used
aristotle's
four
causes
prescribed
that
four
"why"
questions
should
be
answered
in
order
to
explain
things
scientifically
some
ancient
knowledge
was
lost
or
in
some
cases
kept
in
obscurity
during
the
fall
of
the
western
roman
empire
and
periodic
political
struggles
however
the
general
fields
of
science
(or
"natural
philosophy"
as
it
was
called)
and
much
of
the
general
knowledge
from
the
ancient
world
remained
preserved
through
the
works
of
the
early
latin
encyclopedists
like
isidore
of
seville
however
aristotle's
original
texts
were
eventually
lost
in
western
europe
and
only
one
text
by
plato
was
widely
known
the
"timaeus"
which
was
the
only
platonic
dialogue
and
one
of
the
few
original
works
of
classical
natural
philosophy
available
to
latin
readers
in
the
early
middle
ages
another
original
work
that
gained
influence
in
this
period
was
ptolemy's
"almagest"
which
contains
a
geocentric
description
of
the
solar
system
in
the
byzantine
empire
many
greek
classical
texts
were
preserved
many
syriac
translations
were
done
by
groups
such
as
the
nestorians
and
monophysites
they
played
a
role
when
they
translated
greek
classical
texts
into
arabic
under
the
caliphate
during
which
many
types
of
classical
learning
were
preserved
and
in
some
cases
improved
upon
the
house
of
wisdom
was
established
in
abbasid-era
baghdad
iraq
where
the
islamic
study
of
aristotelianism
flourished
al-kindi
(801–873)
was
the
first
of
the
muslim
peripatetic
philosophers
and
is
known
for
his
efforts
to
introduce
greek
and
hellenistic
philosophy
to
the
arab
world
the
islamic
golden
age
flourished
from
this
time
until
the
mongol
invasions
of
the
13th
century
ibn
al-haytham
(alhazen)
as
well
as
his
predecessor
ibn
sahl
was
familiar
with
ptolemy's
"optics"
and
used
experiments
as
a
means
to
gain
knowledge
furthermore
doctors
and
alchemists
such
as
the
persians
avicenna
and
al-razi
also
greatly
developed
the
science
of
medicine
with
the
former
writing
the
canon
of
medicine
a
medical
encyclopedia
used
until
the
18th
century
and
the
latter
discovering
multiple
compounds
like
alcohol
avicenna's
canon
is
considered
to
be
one
of
the
most
important
publications
in
medicine
and
they
both
contributed
significantly
to
the
practice
of
experimental
medicine
using
clinical
trials
and
experiments
to
back
their
claims
in
classical
antiquity
greek
and
roman
taboos
had
meant
that
dissection
was
usually
banned
in
ancient
times
but
in
middle
ages
it
changed:
medical
teachers
and
students
at
bologna
began
to
open
human
bodies
and
mondino
de
luzzi
(c
1275–1326)
produced
the
ﬁrst
known
anatomy
textbook
based
on
human
dissection
by
the
eleventh
century
most
of
europe
had
become
christian;
stronger
monarchies
emerged;
borders
were
restored;
technological
developments
and
agricultural
innovations
were
made
which
increased
the
food
supply
and
population
in
addition
classical
greek
texts
started
to
be
translated
from
arabic
and
greek
into
latin
giving
a
higher
level
of
scientific
discussion
in
western
europe
by
1088
the
first
university
in
europe
(the
university
of
bologna)
had
emerged
from
its
clerical
beginnings
demand
for
latin
translations
grew
(for
example
from
the
toledo
school
of
translators);
western
europeans
began
collecting
texts
written
not
only
in
latin
but
also
latin
translations
from
greek
arabic
and
hebrew
manuscript
copies
of
alhazen's
"book
of
optics"
also
propagated
across
europe
before
1240
as
evidenced
by
its
incorporation
into
vitello's
"perspectiva"
avicenna's
canon
was
translated
into
latin
in
particular
the
texts
of
aristotle
ptolemy
and
euclid
preserved
in
the
houses
of
wisdom
and
also
in
the
byzantine
empire
were
sought
amongst
catholic
scholars
the
influx
of
ancient
texts
caused
the
renaissance
of
the
12th
century
and
the
flourishing
of
a
synthesis
of
catholicism
and
aristotelianism
known
as
scholasticism
in
western
europe
which
became
a
new
geographic
center
of
science
an
"experiment"
in
this
period
would
be
understood
as
a
careful
process
of
observing
describing
and
classifying
one
prominent
scientist
in
this
era
was
roger
bacon
scholasticism
had
a
strong
focus
on
revelation
and
dialectic
reasoning
and
gradually
fell
out
of
favour
over
the
next
centuries
as
alchemy's
focus
on
experiments
that
include
direct
observation
and
meticulous
documentation
slowly
increased
in
importance
alhazen
disproved
ptolemy's
theory
of
vision
but
did
not
make
any
corresponding
changes
to
aristotle's
metaphysics
the
scientific
revolution
ran
concurrently
to
a
process
where
elements
of
aristotle's
metaphysics
such
as
ethics
teleology
and
formal
causality
slowly
fell
out
of
favour
scholars
slowly
came
to
realize
that
the
universe
itself
might
well
be
devoid
of
both
purpose
and
ethical
imperatives
the
development
from
a
physics
infused
with
goals
ethics
and
spirit
toward
a
physics
where
these
elements
do
not
play
an
integral
role
took
centuries
this
development
was
enhanced
by
the
condemnations
of
1277
where
aristotle's
books
were
banned
by
the
catholic
church
this
allowed
the
theoretical
possibility
of
vacuum
and
motion
in
a
vacuum
a
direct
result
was
the
emergence
of
the
science
of
dynamics
new
developments
in
optics
played
a
role
in
the
inception
of
the
renaissance
both
by
challenging
long-held
metaphysical
ideas
on
perception
as
well
as
by
contributing
to
the
improvement
and
development
of
technology
such
as
the
camera
obscura
and
the
telescope
before
what
we
now
know
as
the
renaissance
started
roger
bacon
vitello
and
john
peckham
each
built
up
a
scholastic
ontology
upon
a
causal
chain
beginning
with
sensation
perception
and
finally
apperception
of
the
individual
and
universal
forms
of
aristotle
a
model
of
vision
later
known
as
perspectivism
was
exploited
and
studied
by
the
artists
of
the
renaissance
this
theory
uses
only
three
of
aristotle's
four
causes:
formal
material
and
final
in
the
sixteenth
century
copernicus
formulated
a
heliocentric
model
of
the
solar
system
unlike
the
geocentric
model
of
ptolemy's
"almagest"
this
was
based
on
a
theorem
that
the
orbital
periods
of
the
planets
are
longer
as
their
orbs
are
farther
from
the
centre
of
motion
which
he
found
not
to
agree
with
ptolemy's
model
kepler
and
others
challenged
the
notion
that
the
only
function
of
the
eye
is
perception
and
shifted
the
main
focus
in
optics
from
the
eye
to
the
propagation
of
light
kepler
modelled
the
eye
as
a
water-filled
glass
sphere
with
an
aperture
in
front
of
it
to
model
the
entrance
pupil
he
found
that
all
the
light
from
a
single
point
of
the
scene
was
imaged
at
a
single
point
at
the
back
of
the
glass
sphere
the
optical
chain
ends
on
the
retina
at
the
back
of
the
eye
kepler
is
best
known
however
for
improving
copernicus'
heliocentric
model
through
the
discovery
of
kepler's
laws
of
planetary
motion
kepler
did
not
reject
aristotelian
metaphysics
and
described
his
work
as
a
search
for
the
harmony
of
the
spheres
galileo
made
innovative
use
of
experiment
and
mathematics
however
he
became
persecuted
after
pope
urban
viii
blessed
galileo
to
write
about
the
copernican
system
galileo
had
used
arguments
from
the
pope
and
put
them
in
the
voice
of
the
simpleton
in
the
work
"dialogue
concerning
the
two
chief
world
systems"
which
greatly
offended
urban
viii
in
northern
europe
the
new
technology
of
the
printing
press
was
widely
used
to
publish
many
arguments
including
some
that
disagreed
widely
with
contemporary
ideas
of
nature
rené
descartes
and
francis
bacon
published
philosophical
arguments
in
favor
of
a
new
type
of
non-aristotelian
science
descartes
emphasized
individual
thought
and
argued
that
mathematics
rather
than
geometry
should
be
used
in
order
to
study
nature
bacon
emphasized
the
importance
of
experiment
over
contemplation
bacon
further
questioned
the
aristotelian
concepts
of
formal
cause
and
final
cause
and
promoted
the
idea
that
science
should
study
the
laws
of
"simple"
natures
such
as
heat
rather
than
assuming
that
there
is
any
specific
nature
or
"formal
cause"
of
each
complex
type
of
thing
this
new
science
began
to
see
itself
as
describing
"laws
of
nature"
this
updated
approach
to
studies
in
nature
was
seen
as
mechanistic
bacon
also
argued
that
science
should
aim
for
the
first
time
at
practical
inventions
for
the
improvement
of
all
human
life
as
a
precursor
to
the
age
of
enlightenment
isaac
newton
and
gottfried
wilhelm
leibniz
succeeded
in
developing
a
new
physics
now
referred
to
as
classical
mechanics
which
could
be
confirmed
by
experiment
and
explained
using
mathematics
leibniz
also
incorporated
terms
from
aristotelian
physics
but
now
being
used
in
a
new
non-teleological
way
for
example
"energy"
and
"potential"
(modern
versions
of
aristotelian
""energeia"
and
"potentia"")
this
implied
a
shift
in
the
view
of
objects:
where
aristotle
had
noted
that
objects
have
certain
innate
goals
that
can
be
actualized
objects
were
now
regarded
as
devoid
of
innate
goals
in
the
style
of
francis
bacon
leibniz
assumed
that
different
types
of
things
all
work
according
to
the
same
general
laws
of
nature
with
no
special
formal
or
final
causes
for
each
type
of
thing
it
is
during
this
period
that
the
word
"science"
gradually
became
more
commonly
used
to
refer
to
a
"type
of
pursuit"
of
a
type
of
knowledge
especially
knowledge
of
nature –
coming
close
in
meaning
to
the
old
term
"natural
philosophy"
during
this
time
the
declared
purpose
and
value
of
science
became
producing
wealth
and
inventions
that
would
improve
human
lives
in
the
materialistic
sense
of
having
more
food
clothing
and
other
things
in
bacon's
words
"the
real
and
legitimate
goal
of
sciences
is
the
endowment
of
human
life
with
new
inventions
and
riches"
and
he
discouraged
scientists
from
pursuing
intangible
philosophical
or
spiritual
ideas
which
he
believed
contributed
little
to
human
happiness
beyond
"the
fume
of
subtle
sublime
or
pleasing
speculation"
science
during
the
enlightenment
was
dominated
by
scientific
societies
and
academies
which
had
largely
replaced
universities
as
centres
of
scientific
research
and
development
societies
and
academies
were
also
the
backbone
of
the
maturation
of
the
scientific
profession
another
important
development
was
the
popularization
of
science
among
an
increasingly
literate
population
philosophes
introduced
the
public
to
many
scientific
theories
most
notably
through
the
"encyclopédie"
and
the
popularization
of
newtonianism
by
voltaire
as
well
as
by
émilie
du
châtelet
the
french
translator
of
newton's
"principia"
some
historians
have
marked
the
18th
century
as
a
drab
period
in
the
history
of
science;
however
the
century
saw
significant
advancements
in
the
practice
of
medicine
mathematics
and
physics;
the
development
of
biological
taxonomy;
a
new
understanding
of
magnetism
and
electricity;
and
the
maturation
of
chemistry
as
a
discipline
which
established
the
foundations
of
modern
chemistry
enlightenment
philosophers
chose
a
short
history
of
scientific
predecessors –
galileo
boyle
and
newton
principally –
as
the
guides
and
guarantors
of
their
applications
of
the
singular
concept
of
nature
and
natural
law
to
every
physical
and
social
field
of
the
day
in
this
respect
the
lessons
of
history
and
the
social
structures
built
upon
it
could
be
discarded
the
nineteenth
century
is
a
particularly
important
period
in
the
history
of
science
since
during
this
era
many
distinguishing
characteristics
of
contemporary
modern
science
began
to
take
shape
such
as:
transformation
of
the
life
and
physical
sciences
frequent
use
of
precision
instruments
emergence
of
terms
like
"biologist"
"physicist"
"scientist";
slowly
moving
away
from
antiquated
labels
like
"natural
philosophy"
and
"natural
history"
increased
professionalization
of
those
studying
nature
lead
to
reduction
in
amateur
naturalists
scientists
gained
cultural
authority
over
many
dimensions
of
society
economic
expansion
and
industrialization
of
numerous
countries
thriving
of
popular
science
writings
and
emergence
of
science
journals
early
in
the
19th
century
john
dalton
suggested
the
modern
atomic
theory
based
on
democritus's
original
idea
of
individible
particles
called
"atoms"
both
john
herschel
and
william
whewell
systematized
methodology:
the
latter
coined
the
term
scientist
when
charles
darwin
published
"on
the
origin
of
species"
he
established
evolution
as
the
prevailing
explanation
of
biological
complexity
his
theory
of
natural
selection
provided
a
natural
explanation
of
how
species
originated
but
this
only
gained
wide
acceptance
a
century
later
the
laws
of
conservation
of
energy
conservation
of
momentum
and
conservation
of
mass
suggested
a
highly
stable
universe
where
there
could
be
little
loss
of
resources
with
the
advent
of
the
steam
engine
and
the
industrial
revolution
there
was
however
an
increased
understanding
that
all
forms
of
energy
as
defined
by
newton
were
not
equally
useful;
they
did
not
have
the
same
energy
quality
this
realization
led
to
the
development
of
the
laws
of
thermodynamics
in
which
the
cumulative
energy
quality
of
the
universe
is
seen
as
constantly
declining:
the
entropy
of
the
universe
increases
over
time
the
electromagnetic
theory
was
also
established
in
the
19th
century
and
raised
new
questions
which
could
not
easily
be
answered
using
newton's
framework
the
phenomena
that
would
allow
the
deconstruction
of
the
atom
were
discovered
in
the
last
decade
of
the
19th
century:
the
discovery
of
x-rays
inspired
the
discovery
of
radioactivity
in
the
next
year
came
the
discovery
of
the
first
subatomic
particle
the
electron
einstein's
theory
of
relativity
and
the
development
of
quantum
mechanics
led
to
the
replacement
of
classical
mechanics
with
a
new
physics
which
contains
two
parts
that
describe
different
types
of
events
in
nature
in
the
first
half
of
the
century
the
development
of
antibiotics
and
artificial
fertilizer
made
global
human
population
growth
possible
at
the
same
time
the
structure
of
the
atom
and
its
nucleus
was
discovered
leading
to
the
release
of
"atomic
energy"
(nuclear
power)
in
addition
the
extensive
use
of
technological
innovation
stimulated
by
the
wars
of
this
century
led
to
revolutions
in
transportation
(automobiles
and
aircraft)
the
development
of
icbms
a
space
race
and
a
nuclear
arms
race
the
molecular
structure
of
dna
was
discovered
in
1953
the
discovery
of
the
cosmic
microwave
background
radiation
in
1964
led
to
a
rejection
of
the
steady
state
theory
of
the
universe
in
favour
of
the
big
bang
theory
of
georges
lemaître
the
development
of
spaceflight
in
the
second
half
of
the
century
allowed
the
first
astronomical
measurements
done
on
or
near
other
objects
in
space
including
manned
landings
on
the
moon
space
telescopes
lead
to
numerous
discoveries
in
astronomy
and
cosmology
widespread
use
of
integrated
circuits
in
the
last
quarter
of
the
20th
century
combined
with
communications
satellites
led
to
a
revolution
in
information
technology
and
the
rise
of
the
global
internet
and
mobile
computing
including
smartphones
the
need
for
mass
systematization
of
long
intertwined
causal
chains
and
large
amounts
of
data
led
to
the
rise
of
the
fields
of
systems
theory
and
computer-assisted
scientific
modelling
which
are
partly
based
on
the
aristotelian
paradigm
harmful
environmental
issues
such
as
ozone
depletion
acidification
eutrophication
and
climate
change
came
to
the
public's
attention
in
the
same
period
and
caused
the
onset
of
environmental
science
and
environmental
technology
in
a
1967
article
lynn
townsend
white
jr
blamed
the
ecological
crisis
on
the
historical
decline
of
the
notion
of
spirit
in
nature
with
the
discovery
of
the
higgs
boson
in
2012
the
last
particle
predicted
by
the
standard
model
of
particle
physics
was
found
in
2015
gravitational
waves
predicted
by
general
relativity
a
century
before
were
first
observed
the
human
genome
project
was
completed
in
2003
determining
the
sequence
of
nucleotide
base
pairs
that
make
up
human
dna
and
identifying
and
mapping
all
of
the
genes
of
the
human
genome
induced
pluripotent
stem
cells
were
developed
in
2006
a
technology
allowing
adult
cells
to
be
transformed
into
stem
cells
capable
of
giving
rise
to
any
cell
type
found
in
the
body
potentially
of
huge
importance
to
the
field
of
regenerative
medicine
modern
science
is
commonly
divided
into
three
major
branches
that
consist
of
the
natural
sciences
social
sciences
and
formal
sciences
each
of
these
branches
comprise
various
specialized
yet
overlapping
scientific
disciplines
that
often
possess
their
own
nomenclature
and
expertise
both
natural
and
social
sciences
are
empirical
sciences
as
their
knowledge
are
based
on
empirical
observations
and
are
capable
of
being
tested
for
its
validity
by
other
researchers
working
under
the
same
conditions
there
are
also
closely
related
disciplines
that
use
science
such
as
engineering
and
medicine
natural
science
is
concerned
with
the
description
prediction
and
understanding
of
natural
phenomena
based
on
empirical
evidence
from
observation
and
experimentation
it
can
be
divided
into
two
main
branches:
life
science
(or
biological
science)
and
physical
science
physical
science
is
subdivided
into
branches
including
physics
chemistry
astronomy
and
earth
science
these
two
branches
may
be
further
divided
into
more
specialized
disciplines
modern
natural
science
is
the
successor
to
the
natural
philosophy
that
began
in
ancient
greece
galileo
descartes
bacon
and
newton
debated
the
benefits
of
using
approaches
which
were
more
mathematical
and
more
experimental
in
a
methodical
way
still
philosophical
perspectives
conjectures
and
presuppositions
often
overlooked
remain
necessary
in
natural
science
systematic
data
collection
including
discovery
science
succeeded
natural
history
which
emerged
in
the
16th
century
by
describing
and
classifying
plants
animals
minerals
and
so
on
today
"natural
history"
suggests
observational
descriptions
aimed
at
popular
audiences
social
science
is
concerned
with
society
and
the
relationships
among
individuals
within
a
society
it
has
many
branches
that
include
but
are
not
limited
to
anthropology
archaeology
communication
studies
economics
history
human
geography
jurisprudence
linguistics
political
science
psychology
public
health
and
sociology
social
scientists
may
adopt
various
philosophical
theories
to
study
individuals
and
society
for
example
positivist
social
scientists
use
methods
resembling
those
of
the
natural
sciences
as
tools
for
understanding
society
and
so
define
science
in
its
stricter
modern
sense
interpretivist
social
scientists
by
contrast
may
use
social
critique
or
symbolic
interpretation
rather
than
constructing
empirically
falsifiable
theories
and
thus
treat
science
in
its
broader
sense
in
modern
academic
practice
researchers
are
often
eclectic
using
multiple
methodologies
(for
instance
by
combining
both
quantitative
and
qualitative
research)
the
term
"social
research"
has
also
acquired
a
degree
of
autonomy
as
practitioners
from
various
disciplines
share
in
its
aims
and
methods
formal
science
is
involved
in
the
study
of
formal
systems
it
includes
mathematics
systems
theory
robotics
and
theoretical
computer
science
the
formal
sciences
share
similarities
with
the
other
two
branches
by
relying
on
objective
careful
and
systematic
study
of
an
area
of
knowledge
they
are
however
different
from
the
empirical
sciences
as
they
rely
exclusively
on
deductive
reasoning
without
the
need
for
empirical
evidence
to
verify
their
abstract
concepts
the
formal
sciences
are
therefore
"a
priori"
disciplines
and
because
of
this
there
is
disagreement
on
whether
they
actually
constitute
a
science
nevertheless
the
formal
sciences
play
an
important
role
in
the
empirical
sciences
calculus
for
example
was
initially
invented
to
understand
motion
in
physics
natural
and
social
sciences
that
rely
heavily
on
mathematical
applications
include
mathematical
physics
mathematical
chemistry
mathematical
biology
mathematical
finance
and
mathematical
economics
scientific
research
can
be
labeled
as
either
basic
or
applied
research
basic
research
is
the
search
for
knowledge
and
applied
research
is
the
search
for
solutions
to
practical
problems
using
this
knowledge
although
some
scientific
research
is
applied
research
into
specific
problems
a
great
deal
of
our
understanding
comes
from
the
curiosity-driven
undertaking
of
basic
research
this
leads
to
options
for
technological
advance
that
were
not
planned
or
sometimes
even
imaginable
this
point
was
made
by
michael
faraday
when
allegedly
in
response
to
the
question
"what
is
the
"use"
of
basic
research?"
he
responded:
"sir
what
is
the
use
of
a
new-born
child?"
for
example
research
into
the
effects
of
red
light
on
the
human
eye's
rod
cells
did
not
seem
to
have
any
practical
purpose;
eventually
the
discovery
that
our
night
vision
is
not
troubled
by
red
light
would
lead
search
and
rescue
teams
(among
others)
to
adopt
red
light
in
the
cockpits
of
jets
and
helicopters
finally
even
basic
research
can
take
unexpected
turns
and
there
is
some
sense
in
which
the
scientific
method
is
built
to
harness
luck
scientific
research
involves
using
the
scientific
method
which
seeks
to
objectively
explain
the
events
of
nature
in
a
reproducible
way
an
explanatory
thought
experiment
or
hypothesis
is
put
forward
as
explanation
using
principles
such
as
parsimony
(also
known
as
"occam's
razor")
and
are
generally
expected
to
seek
consilience –
fitting
well
with
other
accepted
facts
related
to
the
phenomena
this
new
explanation
is
used
to
make
falsifiable
predictions
that
are
testable
by
experiment
or
observation
the
predictions
are
to
be
posted
before
a
confirming
experiment
or
observation
is
sought
as
proof
that
no
tampering
has
occurred
disproof
of
a
prediction
is
evidence
of
progress
this
is
done
partly
through
observation
of
natural
phenomena
but
also
through
experimentation
that
tries
to
simulate
natural
events
under
controlled
conditions
as
appropriate
to
the
discipline
(in
the
observational
sciences
such
as
astronomy
or
geology
a
predicted
observation
might
take
the
place
of
a
controlled
experiment)
experimentation
is
especially
important
in
science
to
help
establish
causal
relationships
(to
avoid
the
correlation
fallacy)
when
a
hypothesis
proves
unsatisfactory
it
is
either
modified
or
discarded
if
the
hypothesis
survived
testing
it
may
become
adopted
into
the
framework
of
a
scientific
theory
a
logically
reasoned
self-consistent
model
or
framework
for
describing
the
behavior
of
certain
natural
phenomena
a
theory
typically
describes
the
behavior
of
much
broader
sets
of
phenomena
than
a
hypothesis;
commonly
a
large
number
of
hypotheses
can
be
logically
bound
together
by
a
single
theory
thus
a
theory
is
a
hypothesis
explaining
various
other
hypotheses
in
that
vein
theories
are
formulated
according
to
most
of
the
same
scientific
principles
as
hypotheses
in
addition
to
testing
hypotheses
scientists
may
also
generate
a
model
an
attempt
to
describe
or
depict
the
phenomenon
in
terms
of
a
logical
physical
or
mathematical
representation
and
to
generate
new
hypotheses
that
can
be
tested
based
on
observable
phenomena
while
performing
experiments
to
test
hypotheses
scientists
may
have
a
preference
for
one
outcome
over
another
and
so
it
is
important
to
ensure
that
science
as
a
whole
can
eliminate
this
bias
this
can
be
achieved
by
careful
experimental
design
transparency
and
a
thorough
peer
review
process
of
the
experimental
results
as
well
as
any
conclusions
after
the
results
of
an
experiment
are
announced
or
published
it
is
normal
practice
for
independent
researchers
to
double-check
how
the
research
was
performed
and
to
follow
up
by
performing
similar
experiments
to
determine
how
dependable
the
results
might
be
taken
in
its
entirety
the
scientific
method
allows
for
highly
creative
problem
solving
while
minimizing
any
effects
of
subjective
bias
on
the
part
of
its
users
(especially
the
confirmation
bias)
mathematics
is
essential
in
the
formation
of
hypotheses
theories
and
laws
in
the
natural
and
social
sciences
for
example
it
is
used
in
quantitative
scientific
modeling
which
can
generate
new
hypotheses
and
predictions
to
be
tested
it
is
also
used
extensively
in
observing
and
collecting
measurements
statistics
a
branch
of
mathematics
is
used
to
summarize
and
analyze
data
which
allow
scientists
to
assess
the
reliability
and
variability
of
their
experimental
results
computational
science
applies
computing
power
to
simulate
real-world
situations
enabling
a
better
understanding
of
scientific
problems
than
formal
mathematics
alone
can
achieve
according
to
the
society
for
industrial
and
applied
mathematics
computation
is
now
as
important
as
theory
and
experiment
in
advancing
scientific
knowledge
john
ziman
points
out
that
intersubjective
verifiability
is
fundamental
to
the
creation
of
all
scientific
knowledge
ziman
shows
how
scientists
can
identify
patterns
to
each
other
across
centuries;
he
refers
to
this
ability
as
"perceptual
consensibility"
he
then
makes
consensibility
leading
to
consensus
the
touchstone
of
reliable
knowledge
scientists
usually
take
for
granted
a
set
of
basic
assumptions
that
are
needed
to
justify
the
scientific
method:
(1)
that
there
is
an
objective
reality
shared
by
all
rational
observers;
(2)
that
this
objective
reality
is
governed
by
natural
laws;
(3)
that
these
laws
can
be
discovered
by
means
of
systematic
observation
and
experimentation
philosophy
of
science
seeks
a
deep
understanding
of
what
these
underlying
assumptions
mean
and
whether
they
are
valid
the
belief
that
scientific
theories
should
and
do
represent
metaphysical
reality
is
known
as
realism
it
can
be
contrasted
with
anti-realism
the
view
that
the
success
of
science
does
not
depend
on
it
being
accurate
about
unobservable
entities
such
as
electrons
one
form
of
anti-realism
is
idealism
the
belief
that
the
mind
or
consciousness
is
the
most
basic
essence
and
that
each
mind
generates
its
own
reality
in
an
idealistic
world
view
what
is
true
for
one
mind
need
not
be
true
for
other
minds
there
are
different
schools
of
thought
in
philosophy
of
science
the
most
popular
position
is
empiricism
which
holds
that
knowledge
is
created
by
a
process
involving
observation
and
that
scientific
theories
are
the
result
of
generalizations
from
such
observations
empiricism
generally
encompasses
inductivism
a
position
that
tries
to
explain
the
way
general
theories
can
be
justified
by
the
finite
number
of
observations
humans
can
make
and
hence
the
finite
amount
of
empirical
evidence
available
to
confirm
scientific
theories
this
is
necessary
because
the
number
of
predictions
those
theories
make
is
infinite
which
means
that
they
cannot
be
known
from
the
finite
amount
of
evidence
using
deductive
logic
only
many
versions
of
empiricism
exist
with
the
predominant
ones
being
bayesianism
and
the
hypothetico-deductive
method
empiricism
has
stood
in
contrast
to
rationalism
the
position
originally
associated
with
descartes
which
holds
that
knowledge
is
created
by
the
human
intellect
not
by
observation
critical
rationalism
is
a
contrasting
20th-century
approach
to
science
first
defined
by
austrian-british
philosopher
karl
popper
popper
rejected
the
way
that
empiricism
describes
the
connection
between
theory
and
observation
he
claimed
that
theories
are
not
generated
by
observation
but
that
observation
is
made
in
the
light
of
theories
and
that
the
only
way
a
theory
can
be
affected
by
observation
is
when
it
comes
in
conflict
with
it
popper
proposed
replacing
verifiability
with
falsifiability
as
the
landmark
of
scientific
theories
and
replacing
induction
with
falsification
as
the
empirical
method
popper
further
claimed
that
there
is
actually
only
one
universal
method
not
specific
to
science:
the
negative
method
of
criticism
trial
and
error
it
covers
all
products
of
the
human
mind
including
science
mathematics
philosophy
and
art
another
approach
instrumentalism
colloquially
termed
"shut
up
and
multiply"
emphasizes
the
utility
of
theories
as
instruments
for
explaining
and
predicting
phenomena
it
views
scientific
theories
as
black
boxes
with
only
their
input
(initial
conditions)
and
output
(predictions)
being
relevant
consequences
theoretical
entities
and
logical
structure
are
claimed
to
be
something
that
should
simply
be
ignored
and
that
scientists
shouldn't
make
a
fuss
about
(see
interpretations
of
quantum
mechanics)
close
to
instrumentalism
is
constructive
empiricism
according
to
which
the
main
criterion
for
the
success
of
a
scientific
theory
is
whether
what
it
says
about
observable
entities
is
true
thomas
kuhn
argued
that
the
process
of
observation
and
evaluation
takes
place
within
a
paradigm
a
logically
consistent
"portrait"
of
the
world
that
is
consistent
with
observations
made
from
its
framing
he
characterized
"normal
science"
as
the
process
of
observation
and
"puzzle
solving"
which
takes
place
within
a
paradigm
whereas
"revolutionary
science"
occurs
when
one
paradigm
overtakes
another
in
a
paradigm
shift
each
paradigm
has
its
own
distinct
questions
aims
and
interpretations
the
choice
between
paradigms
involves
setting
two
or
more
"portraits"
against
the
world
and
deciding
which
likeness
is
most
promising
a
paradigm
shift
occurs
when
a
significant
number
of
observational
anomalies
arise
in
the
old
paradigm
and
a
new
paradigm
makes
sense
of
them
that
is
the
choice
of
a
new
paradigm
is
based
on
observations
even
though
those
observations
are
made
against
the
background
of
the
old
paradigm
for
kuhn
acceptance
or
rejection
of
a
paradigm
is
a
social
process
as
much
as
a
logical
process
kuhn's
position
however
is
not
one
of
relativism
finally
another
approach
often
cited
in
debates
of
scientific
skepticism
against
controversial
movements
like
"creation
science"
is
methodological
naturalism
its
main
point
is
that
a
difference
between
natural
and
supernatural
explanations
should
be
made
and
that
science
should
be
restricted
methodologically
to
natural
explanations
that
the
restriction
is
merely
methodological
(rather
than
ontological)
means
that
science
should
not
consider
supernatural
explanations
itself
but
should
not
claim
them
to
be
wrong
either
instead
supernatural
explanations
should
be
left
a
matter
of
personal
belief
outside
the
scope
of
science
methodological
naturalism
maintains
that
proper
science
requires
strict
adherence
to
empirical
study
and
independent
verification
as
a
process
for
properly
developing
and
evaluating
explanations
for
observable
phenomena
the
absence
of
these
standards
arguments
from
authority
biased
observational
studies
and
other
common
fallacies
are
frequently
cited
by
supporters
of
methodological
naturalism
as
characteristic
of
the
non-science
they
criticize
a
scientific
theory
is
empirical
and
is
always
open
to
falsification
if
new
evidence
is
presented
that
is
no
theory
is
ever
considered
strictly
certain
as
science
accepts
the
concept
of
fallibilism
the
philosopher
of
science
karl
popper
sharply
distinguished
truth
from
certainty
he
wrote
that
scientific
knowledge
"consists
in
the
search
for
truth"
but
it
"is
not
the
search
for
certainty 
all
human
knowledge
is
fallible
and
therefore
uncertain"
new
scientific
knowledge
rarely
results
in
vast
changes
in
our
understanding
according
to
psychologist
keith
stanovich
it
may
be
the
media's
overuse
of
words
like
"breakthrough"
that
leads
the
public
to
imagine
that
science
is
constantly
proving
everything
it
thought
was
true
to
be
false
while
there
are
such
famous
cases
as
the
theory
of
relativity
that
required
a
complete
reconceptualization
these
are
extreme
exceptions
knowledge
in
science
is
gained
by
a
gradual
synthesis
of
information
from
different
experiments
by
various
researchers
across
different
branches
of
science;
it
is
more
like
a
climb
than
a
leap
theories
vary
in
the
extent
to
which
they
have
been
tested
and
verified
as
well
as
their
acceptance
in
the
scientific
community
for
example
heliocentric
theory
the
theory
of
evolution
relativity
theory
and
germ
theory
still
bear
the
name
"theory"
even
though
in
practice
they
are
considered
factual
philosopher
barry
stroud
adds
that
although
the
best
definition
for
"knowledge"
is
contested
being
skeptical
and
entertaining
the
"possibility"
that
one
is
incorrect
is
compatible
with
being
correct
therefore
scientists
adhering
to
proper
scientific
approaches
will
doubt
themselves
even
once
they
possess
the
truth
the
fallibilist
c s
peirce
argued
that
inquiry
is
the
struggle
to
resolve
actual
doubt
and
that
merely
quarrelsome
verbal
or
hyperbolic
doubt
is
fruitless –
but
also
that
the
inquirer
should
try
to
attain
genuine
doubt
rather
than
resting
uncritically
on
common
sense
he
held
that
the
successful
sciences
trust
not
to
any
single
chain
of
inference
(no
stronger
than
its
weakest
link)
but
to
the
cable
of
multiple
and
various
arguments
intimately
connected
stanovich
also
asserts
that
science
avoids
searching
for
a
"magic
bullet";
it
avoids
the
single-cause
fallacy
this
means
a
scientist
would
not
ask
merely
"what
is
"the"
cause
of "
but
rather
"what
"are"
the
most
significant
"causes"
of "
this
is
especially
the
case
in
the
more
macroscopic
fields
of
science
(eg
psychology
physical
cosmology)
research
often
analyzes
few
factors
at
once
but
these
are
always
added
to
the
long
list
of
factors
that
are
most
important
to
consider
for
example
knowing
the
details
of
only
a
person's
genetics
or
their
history
and
upbringing
or
the
current
situation
may
not
explain
a
behavior
but
a
deep
understanding
of
all
these
variables
combined
can
be
very
predictive
an
area
of
study
or
speculation
that
masquerades
as
science
in
an
attempt
to
claim
a
legitimacy
that
it
would
not
otherwise
be
able
to
achieve
is
sometimes
referred
to
as
pseudoscience
fringe
science
or
junk
science
physicist
richard
feynman
coined
the
term
"cargo
cult
science"
for
cases
in
which
researchers
believe
they
are
doing
science
because
their
activities
have
the
outward
appearance
of
science
but
actually
lack
the
"kind
of
utter
honesty"
that
allows
their
results
to
be
rigorously
evaluated
various
types
of
commercial
advertising
ranging
from
hype
to
fraud
may
fall
into
these
categories
there
can
also
be
an
element
of
political
or
ideological
bias
on
all
sides
of
scientific
debates
sometimes
research
may
be
characterized
as
"bad
science"
research
that
may
be
well-intended
but
is
actually
incorrect
obsolete
incomplete
or
over-simplified
expositions
of
scientific
ideas
the
term
"scientific
misconduct"
refers
to
situations
such
as
where
researchers
have
intentionally
misrepresented
their
published
data
or
have
purposely
given
credit
for
a
discovery
to
the
wrong
person
scientific
research
is
published
in
an
enormous
range
of
scientific
literature
scientific
journals
communicate
and
document
the
results
of
research
carried
out
in
universities
and
various
other
research
institutions
serving
as
an
archival
record
of
science
the
first
scientific
journals
"journal
des
sçavans"
followed
by
the
"philosophical
transactions"
began
publication
in
1665
since
that
time
the
total
number
of
active
periodicals
has
steadily
increased
in
1981
one
estimate
for
the
number
of
scientific
and
technical
journals
in
publication
was
11500
the
united
states
national
library
of
medicine
currently
indexes
5516
journals
that
contain
articles
on
topics
related
to
the
life
sciences
although
the
journals
are
in
39
languages
91
percent
of
the
indexed
articles
are
published
in
english
most
scientific
journals
cover
a
single
scientific
field
and
publish
the
research
within
that
field;
the
research
is
normally
expressed
in
the
form
of
a
scientific
paper
science
has
become
so
pervasive
in
modern
societies
that
it
is
generally
considered
necessary
to
communicate
the
achievements
news
and
ambitions
of
scientists
to
a
wider
populace
science
magazines
such
as
"new
scientist"
"science
vie"
and
"scientific
american"
cater
to
the
needs
of
a
much
wider
readership
and
provide
a
non-technical
summary
of
popular
areas
of
research
including
notable
discoveries
and
advances
in
certain
fields
of
research
science
books
engage
the
interest
of
many
more
people
tangentially
the
science
fiction
genre
primarily
fantastic
in
nature
engages
the
public
imagination
and
transmits
the
ideas
if
not
the
methods
of
science
recent
efforts
to
intensify
or
develop
links
between
science
and
non-scientific
disciplines
such
as
literature
or
more
specifically
poetry
include
the
"creative
writing
science"
resource
developed
through
the
royal
literary
fund
discoveries
in
fundamental
science
can
be
world-changing
for
example:
the
scientific
community
is
a
group
of
all
interacting
scientists
along
with
their
respective
societies
and
institutions
scientists
are
individuals
who
conduct
scientific
research
to
advance
knowledge
in
an
area
of
interest
the
term
"scientist"
was
coined
by
william
whewell
in
1833
in
modern
times
many
professional
scientists
are
trained
in
an
academic
setting
and
upon
completion
attain
an
academic
degree
with
the
highest
degree
being
a
doctorate
such
as
a
doctor
of
philosophy
(phd)
doctor
of
medicine
(md)
or
doctor
of
engineering
(deng)
many
scientists
pursue
careers
in
various
sectors
of
the
economy
such
as
academia
industry
government
and
nonprofit
environments
scientists
exhibit
a
strong
curiosity
about
reality
with
some
scientists
having
a
desire
to
apply
scientific
knowledge
for
the
benefit
of
health
nations
environment
or
industries
other
motivations
include
recognition
by
their
peers
and
prestige
the
nobel
prize
a
widely
regarded
prestigious
award
is
awarded
annually
to
those
who
have
achieved
scientific
advances
in
the
fields
of
medicine
physics
chemistry
and
economics
science
has
historically
been
a
male-dominated
field
with
some
notable
exceptions
women
faced
considerable
discrimination
in
science
much
as
they
did
in
other
areas
of
male-dominated
societies
such
as
frequently
being
passed
over
for
job
opportunities
and
denied
credit
for
their
work
for
example
christine
ladd
(1847–1930)
was
able
to
enter
a
phd
program
as
"c
ladd";
christine
"kitty"
ladd
completed
the
requirements
in
1882
but
was
awarded
her
degree
only
in
1926
after
a
career
which
spanned
the
algebra
of
logic
(see
truth
table)
color
vision
and
psychology
her
work
preceded
notable
researchers
like
ludwig
wittgenstein
and
charles
sanders
peirce
the
achievements
of
women
in
science
have
been
attributed
to
their
defiance
of
their
traditional
role
as
laborers
within
the
domestic
sphere
in
the
late
20th
century
active
recruitment
of
women
and
elimination
of
institutional
discrimination
on
the
basis
of
sex
greatly
increased
the
number
of
women
scientists
but
large
gender
disparities
remain
in
some
fields;
in
the
early
21st
century
over
half
of
new
biologists
were
female
while
80%
of
phds
in
physics
are
given
to
men
in
the
early
part
of
the
21st
century
women
in
the
united
states
earned
503%
of
bachelor's
degrees
456%
of
master's
degrees
and
407%
of
phds
in
science
and
engineering
fields
they
earned
more
than
half
of
the
degrees
in
psychology
(about
70%)
social
sciences
(about
50%)
and
biology
(about
50-60%)
but
earned
less
than
half
the
degrees
in
the
physical
sciences
earth
sciences
mathematics
engineering
and
computer
science
lifestyle
choice
also
plays
a
major
role
in
female
engagement
in
science;
women
with
young
children
are
28%
less
likely
to
take
tenure-track
positions
due
to
work-life
balance
issues
and
female
graduate
students'
interest
in
careers
in
research
declines
dramatically
over
the
course
of
graduate
school
whereas
that
of
their
male
colleagues
remains
unchanged
learned
societies
for
the
communication
and
promotion
of
scientific
thought
and
experimentation
have
existed
since
the
renaissance
many
scientists
belong
to
a
learned
society
that
promotes
their
respective
scientific
discipline
profession
or
group
of
related
disciplines
membership
may
be
open
to
all
may
require
possession
of
some
qualifications
or
may
be
an
honor
conferred
by
election
membership
often
requires
possession
of
some
scientific
credentials
or
may
be
an
honor
conferred
by
election
most
scientific
societies
are
non-profit
organizations
and
many
are
professional
associations
their
activities
typically
include
holding
regular
conferences
for
the
presentation
and
discussion
of
new
research
results
and
publishing
or
sponsoring
academic
journals
in
their
discipline
some
also
act
as
professional
bodies
regulating
the
activities
of
their
members
in
the
public
interest
or
the
collective
interest
of
the
membership
scholars
in
the
sociology
of
science
argue
that
learned
societies
are
of
key
importance
and
their
formation
assists
in
the
emergence
and
development
of
new
disciplines
or
professions
the
professionalization
of
science
begun
in
the
19th
century
was
partly
enabled
by
the
creation
of
distinguished
academy
of
sciences
in
a
number
of
countries
such
as
the
italian
in
1603
the
british
royal
society
in
1660
the
french
in
1666
the
american
national
academy
of
sciences
in
1863
the
german
kaiser
wilhelm
institute
in
1911
and
the
chinese
academy
of
sciences
in
1928
international
scientific
organizations
such
as
the
international
council
for
science
have
since
been
formed
to
promote
cooperation
between
the
scientific
communities
of
different
nations
science
policy
is
an
area
of
public
policy
concerned
with
the
policies
that
affect
the
conduct
of
the
scientific
enterprise
including
research
funding
often
in
pursuance
of
other
national
policy
goals
such
as
technological
innovation
to
promote
commercial
product
development
weapons
development
health
care
and
environmental
monitoring
science
policy
also
refers
to
the
act
of
applying
scientific
knowledge
and
consensus
to
the
development
of
public
policies
science
policy
thus
deals
with
the
entire
domain
of
issues
that
involve
the
natural
sciences
in
accordance
with
public
policy
being
concerned
about
the
well-being
of
its
citizens
science
policy's
goal
is
to
consider
how
science
and
technology
can
best
serve
the
public
state
policy
has
influenced
the
funding
of
public
works
and
science
for
thousands
of
years
particularly
within
civilizations
with
highly
organized
governments
such
as
imperial
china
and
the
roman
empire
prominent
historical
examples
include
the
great
wall
of
china
completed
over
the
course
of
two
millennia
through
the
state
support
of
several
dynasties
and
the
grand
canal
of
the
yangtze
river
an
immense
feat
of
hydraulic
engineering
begun
by
sunshu
ao
(孫叔敖
7th
c
bce)
ximen
bao
(西門豹
5th
cbce)
and
shi
chi
(4th
c
bce)
this
construction
dates
from
the
6th
century
bce
under
the
sui
dynasty
and
is
still
in
use
today
in
china
such
state-supported
infrastructure
and
scientific
research
projects
date
at
least
from
the
time
of
the
mohists
who
inspired
the
study
of
logic
during
the
period
of
the
hundred
schools
of
thought
and
the
study
of
defensive
fortifications
like
the
great
wall
of
china
during
the
warring
states
period
public
policy
can
directly
affect
the
funding
of
capital
equipment
and
intellectual
infrastructure
for
industrial
research
by
providing
tax
incentives
to
those
organizations
that
fund
research
vannevar
bush
director
of
the
office
of
scientific
research
and
development
for
the
united
states
government
the
forerunner
of
the
national
science
foundation
wrote
in
july
1945
that
"science
is
a
proper
concern
of
government"
scientific
research
is
often
funded
through
a
competitive
process
in
which
potential
research
projects
are
evaluated
and
only
the
most
promising
receive
funding
such
processes
which
are
run
by
government
corporations
or
foundations
allocate
scarce
funds
total
research
funding
in
most
developed
countries
is
between
15%
and
3%
of
gdp
in
the
oecd
around
two-thirds
of
research
and
development
in
scientific
and
technical
fields
is
carried
out
by
industry
and
20%
and
10%
respectively
by
universities
and
government
the
government
funding
proportion
in
certain
industries
is
higher
and
it
dominates
research
in
social
science
and
humanities
similarly
with
some
exceptions
(eg
biotechnology)
government
provides
the
bulk
of
the
funds
for
basic
scientific
research
many
governments
have
dedicated
agencies
to
support
scientific
research
prominent
scientific
organizations
include
the
national
science
foundation
in
the
united
states
the
national
scientific
and
technical
research
council
in
argentina
commonwealth
scientific
and
industrial
research
organisation
(csiro)
in
australia
in
france
the
max
planck
society
and
in
germany
and
csic
in
spain
in
commercial
research
and
development
all
but
the
most
research-oriented
corporations
focus
more
heavily
on
near-term
commercialisation
possibilities
rather
than
"blue-sky"
ideas
or
technologies
(such
as
nuclear
fusion)
the
public
awareness
of
science
relates
to
the
attitudes
behaviors
opinions
and
activities
that
make
up
the
relations
between
science
and
the
general
public
it
integrates
various
themes
and
activities
such
as
science
communication
science
museums
science
festivals
science
fairs
citizen
science
and
science
in
popular
culture
social
scientists
have
devised
various
metrics
to
measure
the
public
understanding
of
science
such
as
factual
knowledge
self-reported
knowledge
and
structural
knowledge
the
mass
media
face
a
number
of
pressures
that
can
prevent
them
from
accurately
depicting
competing
scientific
claims
in
terms
of
their
credibility
within
the
scientific
community
as
a
whole
determining
how
much
weight
to
give
different
sides
in
a
scientific
debate
may
require
considerable
expertise
regarding
the
matter
few
journalists
have
real
scientific
knowledge
and
even
beat
reporters
who
know
a
great
deal
about
certain
scientific
issues
may
be
ignorant
about
other
scientific
issues
that
they
are
suddenly
asked
to
cover
politicization
of
science
occurs
when
government
business
or
advocacy
groups
use
legal
or
economic
pressure
to
influence
the
findings
of
scientific
research
or
the
way
it
is
disseminated
reported
or
interpreted
many
factors
can
act
as
facets
of
the
politicization
of
science
such
as
populist
anti-intellectualism
perceived
threats
to
religious
beliefs
postmodernist
subjectivism
and
fear
for
business
interests
politicization
of
science
is
usually
accomplished
when
scientific
information
is
presented
in
a
way
that
emphasizes
the
uncertainty
associated
with
the
scientific
evidence
tactics
such
as
shifting
conversation
failing
to
acknowledge
facts
and
capitalizing
on
doubt
of
scientific
consensus
have
been
used
to
gain
more
attention
for
views
that
have
been
undermined
by
scientific
evidence
examples
of
issues
that
have
involved
the
politicization
of
science
include
the
global
warming
controversy
health
effects
of
pesticides
and
health
effects
of
tobacco
publications
resources
outline
of
science
the
following
outline
is
provided
as
a
topical
overview
of
science:
science
–
the
systematic
effort
of
acquiring
knowledge—through
observation
and
experimentation
coupled
with
logic
and
reasoning
to
find
out
what
can
be
proved
or
not
proved—and
the
knowledge
thus
acquired
the
word
"science"
comes
from
the
latin
word
"scientia"
meaning
knowledge
a
practitioner
of
science
is
called
a
"scientist"
modern
science
respects
objective
logical
reasoning
and
follows
a
set
of
core
procedures
or
rules
in
order
to
determine
the
nature
and
underlying
natural
laws
of
the
universe
and
everything
in
it
some
scientists
do
not
know
of
the
rules
themselves
but
follow
them
through
research
policies
these
procedures
are
known
as
the
scientific
method
scientific
method
(outline)
–
body
of
techniques
for
investigating
phenomena
and
acquiring
new
knowledge
as
well
as
for
correcting
and
integrating
previous
knowledge
it
is
based
on
observable
empirical
measurable
evidence
and
subject
to
laws
of
reasoning
both
deductive
and
inductive
branches
of
science
–
divisions
within
science
with
respect
to
the
entity
or
system
concerned
which
typically
embodies
its
own
terminology
and
nomenclature
natural
science
(outline)
–
major
branch
of
science
that
tries
to
explain
and
predict
nature's
phenomena
based
on
empirical
evidence
in
natural
science
hypotheses
must
be
verified
scientifically
to
be
regarded
as
scientific
theory
validity
accuracy
and
social
mechanisms
ensuring
quality
control
such
as
peer
review
and
repeatability
of
findings
are
amongst
the
criteria
and
methods
used
for
this
purpose
natural
science
can
be
broken
into
two
main
branches:
biology
and
physical
science
each
of
these
branches
and
all
of
their
sub-branches
are
referred
to
as
natural
sciences
formal
science
–
branches
of
knowledge
that
are
concerned
with
formal
systems
such
as
those
under
the
branches
of:
logic
mathematics
computer
science
statistics
and
some
aspects
of
linguistics
unlike
other
sciences
the
formal
sciences
are
not
concerned
with
the
validity
of
theories
based
on
observations
in
the
real
world
but
instead
with
the
properties
of
formal
systems
based
on
definitions
and
rules
social
science
–
study
of
the
social
world
constructed
between
humans
the
social
sciences
usually
limit
themselves
to
an
anthropomorphically
centric
view
of
these
interactions
with
minimal
emphasis
on
the
inadvertent
impact
of
social
human
behavior
on
the
external
environment
(physical
biological
ecological
etc)
'social'
is
the
concept
of
exchange/influence
of
ideas
thoughts
and
relationship
interactions
(resulting
in
harmony
peace
self
enrichment
favoritism
maliciousness
justice
seeking
etc)
between
humans
the
scientific
method
is
utilized
in
many
social
sciences
albeit
adapted
to
the
needs
of
the
social
construct
being
studied
applied
science
–
branch
of
science
that
applies
existing
scientific
knowledge
to
develop
more
practical
applications
including
inventions
and
other
technological
advancements
see
–
the
scientific
fields
mentioned
below
are
generally
described
by
the
science
they
study
science
education
branches
of
science
the
branches
of
science
also
referred
to
as
sciences
"scientific
fields"
or
"scientific
disciplines"
are
commonly
divided
into
three
major
groups:
natural
and
social
sciences
are
empirical
sciences
meaning
that
the
knowledge
must
be
based
on
observable
phenomena
and
must
be
capable
of
being
verified
by
other
researchers
working
under
the
same
conditions
natural
social
and
formal
science
make
up
the
fundamental
sciences
which
form
the
basis
of
interdisciplinary
and
applied
sciences
such
as
engineering
and
medicine
specialized
scientific
disciplines
that
exist
in
multiple
categories
may
include
parts
of
other
scientific
disciplines
but
often
possess
their
own
terminologies
and
expertises
natural
science
is
a
branch
of
science
that
seeks
to
elucidate
the
rules
that
govern
the
natural
world
by
applying
an
empirical
and
scientific
method
to
the
study
of
the
universe
the
term
natural
sciences
is
used
to
distinguish
it
from
the
social
sciences
which
apply
the
scientific
method
to
study
human
behavior
and
social
patterns;
the
humanities
which
use
a
critical
or
analytical
approach
to
the
study
of
the
human
condition;
and
the
formal
sciences
physical
science
is
an
encompassing
term
for
the
branches
of
natural
science
and
science
that
study
non-living
systems
in
contrast
to
the
life
sciences
however
the
term
"physical"
creates
an
unintended
somewhat
arbitrary
distinction
since
many
branches
of
physical
science
also
study
biological
phenomena
there
is
a
difference
between
physical
science
and
physics
"physics"
(from
)
is
a
natural
science
that
involves
the
study
of
matter
and
its
motion
through
spacetime
along
with
related
concepts
such
as
energy
and
force
more
broadly
it
is
the
general
analysis
of
nature
conducted
in
order
to
understand
how
the
universe
behaves
physics
is
one
of
the
oldest
academic
disciplines
perhaps
the
oldest
through
its
inclusion
of
astronomy
over
the
last
two
millennia
physics
was
a
part
of
natural
philosophy
along
with
chemistry
certain
branches
of
mathematics
and
biology
but
during
the
scientific
revolution
in
the
16th
century
the
natural
sciences
emerged
as
unique
research
programs
in
their
own
right
certain
research
areas
are
interdisciplinary
such
as
biophysics
and
quantum
chemistry
which
means
that
the
boundaries
of
physics
are
not
rigidly
defined
in
the
nineteenth
and
twentieth
centuries
physicalism
emerged
as
a
major
unifying
feature
of
the
philosophy
of
science
as
physics
provides
fundamental
explanations
for
every
observed
natural
phenomenon
new
ideas
in
physics
often
explain
the
fundamental
mechanisms
of
other
sciences
while
opening
to
new
research
areas
in
mathematics
and
philosophy
"chemistry"
(the
etymology
of
the
word
has
been
much
disputed)
is
the
science
of
matter
and
the
changes
it
undergoes
the
science
of
matter
is
also
addressed
by
physics
but
while
physics
takes
a
more
general
and
fundamental
approach
chemistry
is
more
specialized
being
concerned
by
the
composition
behavior
(or
reaction)
structure
and
properties
of
matter
as
well
as
the
changes
it
undergoes
during
chemical
reactions
it
is
a
physical
science
which
studies
various
substances
atoms
molecules
and
matter
(especially
carbon
based);
biochemistry
the
study
of
substances
found
in
biological
organisms;
physical
chemistry
the
study
of
chemical
processes
using
physical
concepts
such
as
thermodynamics
and
quantum
mechanics;
and
analytical
chemistry
the
analysis
of
material
samples
to
gain
an
understanding
of
their
chemical
composition
and
structure
many
more
specialized
disciplines
have
emerged
in
recent
years
eg
neurochemistry
the
chemical
study
of
the
nervous
system
(see
subdisciplines)
"earth
science"
(also
known
as
"geoscience"
"the
geosciences"
or
"the
earth
sciences")
is
an
all-embracing
term
for
the
sciences
related
to
the
planet
earth
it
is
arguably
a
special
case
in
planetary
science
the
earth
being
the
only
known
life-bearing
planet
there
are
both
reductionist
and
holistic
approaches
to
earth
sciences
the
formal
discipline
of
earth
sciences
may
include
the
study
of
the
atmosphere
hydrosphere
oceans
and
biosphere
as
well
as
the
solid
earth
typically
earth
scientists
will
use
tools
from
physics
chemistry
biology
geography
chronology
and
mathematics
to
build
a
quantitative
understanding
of
how
the
earth
system
works
and
how
it
evolved
to
its
current
state
ecology
(from
greek:
οἶκος
"house";
-λογία
"study
of")
is
the
scientific
study
of
the
relationships
that
living
organisms
have
with
each
other
and
with
their
abiotic
environment
topics
of
interest
to
ecologists
include
the
composition
distribution
amount
(biomass)
number
and
changing
states
of
organisms
within
and
among
ecosystems
oceanography
or
marine
biology
is
the
branch
of
earth
science
that
studies
ocean
it
covers
a
wide
range
of
topics
including
marine
organisms
and
ecosystem
dynamics;
ocean
currents
waves
and
geophysical
fluid
dynamics;
plate
tectonics
and
the
geology
of
the
sea
floor;
and
fluxes
of
various
chemical
substances
and
physical
properties
within
the
ocean
and
across
its
boundaries
these
diverse
topics
reflect
multiple
disciplines
that
oceanographers
blend
to
further
knowledge
of
the
world
ocean
and
understanding
of
processes
within
it:
biology
chemistry
geology
meteorology
and
physics
as
well
as
geography
geology
(from
the
greek
γῆ
gê
"earth"
and
λόγος
logos
"study")
is
the
science
comprising
the
study
of
solid
earth
the
rocks
of
which
it
is
composed
and
the
processes
by
which
they
change
meteorology
is
the
interdisciplinary
scientific
study
of
the
atmosphere
studies
in
the
field
stretch
back
millennia
though
significant
progress
in
meteorology
did
not
occur
until
the
17th
century
the
19th
century
saw
breakthroughs
occur
after
observing
networks
developed
across
several
countries
after
the
development
of
the
computer
in
the
latter
half
of
the
20th
century
breakthroughs
in
weather
forecasting
were
achieved
space
science
or
astronomy
is
the
study
of
everything
in
outer
space
this
has
sometimes
been
called
astronomy
but
recently
astronomy
has
come
to
be
regarded
as
a
division
of
broader
space
science
which
has
grown
to
include
other
related
fields
such
as
studying
issues
related
to
space
travel
and
space
exploration
(including
space
medicine)
space
archaeology
and
science
performed
in
outer
space
(see
space
research)
"the
science
of
living
things
comprises
the
branches
of
science
that
involve
the
scientific
study
of
living
organisms
like
plants
animals
and
human
beings
however
the
study
of
behavior
of
organisms
such
as
practiced
in
ethology
and
psychology
is
only
included
in
as
much
as
it
involves
a
clearly
biological
aspect
while
biology
remains
the
centerpiece
of
the
science
of
living
things
technological
advances
in
molecular
biology
and
biotechnology
have
led
to
a
burgeoning
of
specializations
and
new
often
interdisciplinary
fields
"biology"
is
the
branch
of
natural
science
concerned
with
the
study
of
life
and
living
organisms
including
their
structure
function
growth
origin
evolution
distribution
and
taxonomy
biology
is
a
vast
subject
containing
many
subdivisions
topics
and
disciplines
zoology
occasionally
spelled
zoölogy
is
the
branch
of
science
that
relates
to
the
animal
kingdom
including
the
structure
embryology
evolution
classification
habits
and
distribution
of
all
animals
both
living
and
extinct
the
term
is
derived
from
ancient
greek
ζῷον
(zōon
"animal")
+
λόγος
(logos
"knowledge")
some
branches
of
zoology
include:
anthrozoology
arachnology
archaeozoology
cetology
embryology
entomology
helminthology
herpetology
histology
ichthyology
malacology
mammalogy
morphology
nematology
ornithology
palaeozoology
pathology
primatology
protozoology
taxonomy
and
zoogeography
human
biology
is
an
interdisciplinary
academic
field
of
biology
biological
anthropology
nutrition
and
medicine
which
focuses
on
humans;
it
is
closely
related
to
primate
biology
and
a
number
of
other
fields
some
branches
of
biology
include:
microbiology
anatomy
neurology
and
neuroscience
immunology
genetics
physiology
pathology
biophysics
biolinguistics
and
ophthalmology
botany
plant
science
or
plant
biology
is
a
branch
of
biology
that
involves
the
scientific
study
of
plant
life
botany
covers
a
wide
range
of
scientific
disciplines
including
structure
growth
reproduction
metabolism
development
diseases
chemical
properties
and
evolutionary
relationships
among
taxonomic
groups
botany
began
with
early
human
efforts
to
identify
edible
medicinal
and
poisonous
plants
making
it
one
of
the
oldest
sciences
today
botanists
study
over
550000
species
of
living
organisms
the
term
"botany"
comes
from
greek
βοτάνη
meaning
"pasture
grass
fodder"
perhaps
via
the
idea
of
a
livestock
keeper
needing
to
know
which
plants
are
safe
for
livestock
to
eat
the
"social
sciences"
are
the
fields
of
scholarship
that
study
society
"social
science"
is
commonly
used
as
an
umbrella
term
to
refer
to
a
plurality
of
fields
outside
of
the
natural
sciences
these
include:
anthropology
archaeology
business
administration
communication
criminology
economics
education
government
linguistics
international
relations
political
science
some
branches
of
psychology
(results
of
which
can
not
be
replicated
or
validated
easily
-
eg
social
psychology)
public
health
theology
sociology
and
in
some
contexts
geography
history
and
law
the
"formal
sciences"
are
the
branches
of
science
that
are
concerned
with
formal
systems
such
as
logic
mathematics
theoretical
computer
science
information
theory
systems
theory
decision
theory
statistics
and
theoretical
linguistics
unlike
other
sciences
the
formal
sciences
are
not
concerned
with
the
validity
of
theories
based
on
observations
in
the
real
world
(empirical
knowledge)
but
rather
with
the
properties
of
formal
systems
based
on
definitions
and
rules
methods
of
the
formal
sciences
are
however
essential
to
the
construction
and
testing
of
scientific
models
dealing
with
observable
reality
and
major
advances
in
formal
sciences
have
often
enabled
major
advances
in
the
empirical
sciences
"decision
theory"
in
economics
psychology
philosophy
mathematics
and
statistics
is
concerned
with
identifying
the
values
uncertainties
and
other
issues
relevant
in
a
given
decision
its
rationality
and
the
resulting
optimal
decision
it
is
very
closely
related
to
the
field
of
game
law
"logic"
(from
the
greek
"λογική"
logikē)
is
the
formal
systematic
study
of
the
principles
of
valid
inference
and
correct
reasoning
logic
is
used
in
most
intellectual
activities
but
is
studied
primarily
in
the
disciplines
of
philosophy
mathematics
semantics
and
computer
science
logic
examines
general
forms
which
arguments
may
take
which
forms
are
valid
and
which
are
fallacies
in
philosophy
the
study
of
logic
figures
in
most
major
areas:
epistemology
ethics
metaphysics
in
mathematics
and
computer
science
it
is
the
study
of
valid
inferences
within
some
formal
language
logic
is
also
studied
in
argumentation
theory
"mathematics"
first
of
all
known
as
the
science
of
numbers
which
is
classified
in
arithmetic
and
algebra
is
classified
as
a
formal
science
has
both
similarities
and
differences
with
the
empirical
sciences
(the
natural
and
social
sciences)
it
is
similar
to
empirical
sciences
in
that
it
involves
an
objective
careful
and
systematic
study
of
an
area
of
knowledge;
it
is
different
because
of
its
method
of
verifying
its
knowledge
using
"a
priori"
rather
than
empirical
methods
"statistics"
is
the
study
of
the
collection
organization
and
interpretation
of
data
it
deals
with
all
aspects
of
this
including
the
planning
of
data
collection
in
terms
of
the
design
of
surveys
and
experiments
a
statistician
is
someone
who
is
particularly
well
versed
in
the
ways
of
thinking
necessary
for
the
successful
application
of
statistical
analysis
such
people
have
often
gained
this
experience
through
working
in
any
of
a
wide
number
of
fields
there
is
also
a
discipline
called
"mathematical
statistics"
which
is
concerned
with
the
theoretical
basis
of
the
subject
the
word
"statistics"
when
referring
to
the
scientific
discipline
is
singular
as
in
"statistics
is
an
art"
this
should
not
be
confused
with
the
word
"statistic"
referring
to
a
quantity
(such
as
mean
or
median)
calculated
from
a
set
of
data
whose
plural
is
"statistics"
("this
statistic
seems
wrong"
or
"these
statistics
are
misleading")
"systems
theory"
is
the
interdisciplinary
study
of
systems
in
general
with
the
goal
of
elucidating
principles
that
can
be
applied
to
all
types
of
systems
in
all
fields
of
research
the
term
does
not
yet
have
a
well-established
precise
meaning
but
systems
theory
can
reasonably
be
considered
a
specialization
of
systems
thinking
and
a
generalization
of
systems
science
the
term
originates
from
ludwig
von
bertalanffy's
general
system
theory
(gs)
and
is
used
in
later
efforts
in
other
fields
such
as
the
action
theory
of
alcott
parsons
and
the
system-theory
of
nickolas
mcluhan
in
this
context
the
word
"systems"
is
used
to
refer
specifically
to
self-regulating
systems
ie
that
are
self-correcting
through
feedback
self-regulating
systems
are
found
in
nature
including
the
physiological
systems
of
our
body
in
local
and
global
ecosystems
and
in
climate
"theoretical
computer
science"
(tcs)
is
a
division
or
subset
of
general
computer
science
and
focuses
on
more
abstract
or
mathematical
aspects
of
computing
these
divisions
and
subsets
include
analysis
of
algorithms
and
formal
semantics
of
programming
languages
technically
there
are
hundreds
of
divisions
and
subsets
besides
these
two
each
of
the
multiple
parts
have
their
own
individual
personal
leaders
(of
popularity)
and
there
are
many
associations
and
professional
social
groups
and
publications
of
distinction
"applied
science"
is
the
application
of
scientific
knowledge
transferred
into
a
physical
environment
examples
include
testing
a
theoretical
model
through
the
use
of
formal
science
or
solving
a
practical
problem
through
the
use
of
natural
science
applied
science
differs
from
fundamental
science
which
seeks
to
describe
the
most
basic
objects
and
forces
having
less
emphasis
on
practical
applications
applied
science
can
be
like
biological
science
and
physical
science
example
fields
of
applied
science
include
fields
of
engineering
are
closely
related
to
applied
sciences
applied
science
is
important
for
technology
development
its
use
in
industrial
settings
is
usually
referred
to
as
research
and
development
(rd)
schmidt
science
fellows
schmidt
science
fellows
is
a
post-doctoral
fellowship
set
up
in
2018
and
funded
by
former
google
chairman
eric
schmidt
and
his
wife
wendy
schmidt
and
run
in
partnership
with
the
rhodes
trust:
a
non-profit
organization
who
administer
the
rhodes
scholarship
the
program
is
intended
to
foster
interdisciplinary
research
and
teach
leadership
skills
in
a
select
group
of
post-doctoral
scientists
the
fellowship
is
supported
by
schmidt
futures
the
philanthropic
initiative
of
eric
schmidt
and
wendy
schmidt
who
pledged
$25
million
for
the
first
three
years
as
part
of
a
broader
$100
million
drive
to
fund
scientific
research
the
rhodes
trust
acts
as
a
central
partner
and
coordinates
the
selection
process
mediates
with
partner
universities
and
assists
in
the
post-doctoral
placements
of
fellows
the
program
consists
of
an
11-month
lab-based
postdoctoral
research
study
at
a
leading
scientific
research
university
followed
by
four
global
meetings
in
its
inaugural
year
fellows
were
selected
from
a
pool
of
220
international
applicants
from
natural
sciences
engineering
mathematics
and
computer
sciences
backgrounds;
then
whittled
down
to
31
finalists
successful
applicants
are
given
$100000
to
study
in
a
field
outside
of
their
usual
area
of
research
and
undertake
a
series
of
advanced
courses
and
networking
programs
in
its
first
year
14
fellows
were
selected
(eight
men
and
six
women)
from
twelve
different
universities
the
fellowship
came
about
due
to
a
necessity
to
fund
interdisciplinary
research
which
is
often
lacking
grant
money
spending
11
or
more
months
outside
of
their
core
areas
of
research
is
believed
to
give
future
scientific
leaders
more
interdisciplinary
skills
at
least
half
of
the
first
14
schmidt
science
fellows
will
be
pursuing
projects
which
rely
on
computation
data
ai
or
machine
learning:
technologies
which
in
which
schmidt
has
shown
an
interest
physics
physics
(from
from
"phýsis"
"nature")
is
the
natural
science
that
studies
matter
and
its
motion
and
behavior
through
space
and
time
and
that
studies
the
related
entities
of
energy
and
force
physics
is
one
of
the
most
fundamental
scientific
disciplines
and
its
main
goal
is
to
understand
how
the
universe
behaves
physics
is
one
of
the
oldest
academic
disciplines
and
through
its
inclusion
of
astronomy
perhaps
the
oldest
over
the
last
two
millennia
physics
chemistry
biology
and
certain
branches
of
mathematics
were
a
part
of
natural
philosophy
but
during
the
scientific
revolution
in
the
17th
century
these
natural
sciences
emerged
as
unique
research
endeavors
in
their
own
right
physics
intersects
with
many
interdisciplinary
areas
of
research
such
as
biophysics
and
quantum
chemistry
and
the
boundaries
of
physics
are
not
rigidly
defined
new
ideas
in
physics
often
explain
the
fundamental
mechanisms
studied
by
other
sciences
and
suggest
new
avenues
of
research
in
academic
disciplines
such
as
mathematics
and
philosophy
advances
in
physics
often
enable
advances
in
new
technologies
for
example
advances
in
the
understanding
of
electromagnetism
and
nuclear
physics
led
directly
to
the
development
of
new
products
that
have
dramatically
transformed
modern-day
society
such
as
television
computers
domestic
appliances
and
nuclear
weapons;
advances
in
thermodynamics
led
to
the
development
of
industrialization;
and
advances
in
mechanics
inspired
the
development
of
calculus
astronomy
is
one
of
the
oldest
natural
sciences
early
civilizations
dating
back
to
beyond
3000 bce
such
as
the
sumerians
ancient
egyptians
and
the
indus
valley
civilization
had
a
predictive
knowledge
and
a
basic
understanding
of
the
motions
of
the
sun
moon
and
stars
the
stars
and
planets
were
often
worshipped
believed
to
represent
gods
while
the
explanations
for
the
observed
positions
of
the
stars
were
often
unscientific
and
lacking
in
evidence
these
early
observations
laid
the
foundation
for
later
astronomy
as
the
stars
were
found
to
traverse
great
circles
across
the
sky
which
however
did
not
explain
the
positions
of
the
planets
according
to
asger
aaboe
the
origins
of
western
astronomy
can
be
found
in
mesopotamia
and
all
western
efforts
in
the
exact
sciences
are
descended
from
late
babylonian
astronomy
egyptian
astronomers
left
monuments
showing
knowledge
of
the
constellations
and
the
motions
of
the
celestial
bodies
while
greek
poet
homer
wrote
of
various
celestial
objects
in
his
"iliad"
and
"odyssey";
later
greek
astronomers
provided
names
which
are
still
used
today
for
most
constellations
visible
from
the
northern
hemisphere
natural
philosophy
has
its
origins
in
greece
during
the
archaic
period
(650
bce
–
480
bce)
when
pre-socratic
philosophers
like
thales
rejected
non-naturalistic
explanations
for
natural
phenomena
and
proclaimed
that
every
event
had
a
natural
cause
they
proposed
ideas
verified
by
reason
and
observation
and
many
of
their
hypotheses
proved
successful
in
experiment;
for
example
atomism
was
found
to
be
correct
approximately
2000
years
after
it
was
proposed
by
leucippus
and
his
pupil
democritus
the
western
roman
empire
fell
in
the
fifth
century
and
this
resulted
in
a
decline
in
intellectual
pursuits
in
the
western
part
of
europe
by
contrast
the
eastern
roman
empire
(also
known
as
the
byzantine
empire)
resisted
the
attacks
from
the
barbarians
and
continued
to
advance
various
fields
of
learning
including
physics
in
the
sixth
century
isidore
of
miletus
created
an
important
compilation
of
archimedes'
works
that
are
copied
in
the
archimedes
palimpsest
in
sixth
century
europe
john
philoponus
a
byzantine
scholar
questioned
aristotle's
teaching
of
physics
and
noting
its
flaws
he
introduced
the
theory
of
impetus
aristotle's
physics
was
not
scrutinized
until
john
philoponus
appeared
and
unlike
aristotle
who
based
his
physics
on
verbal
argument
philoponus
relied
on
observation
on
aristotle's
physics
john
philoponus
wrote:
“"but
this
is
completely
erroneous
and
our
view
may
be
corroborated
by
actual
observation
more
effectively
than
by
any
sort
of
verbal
argument
for
if
you
let
fall
from
the
same
height
two
weights
of
which
one
is
many
times
as
heavy
as
the
other
you
will
see
that
the
ratio
of
the
times
required
for
the
motion
does
not
depend
on
the
ratio
of
the
weights
but
that
the
difference
in
time
is
a
very
small
one
and
so
if
the
difference
in
the
weights
is
not
considerable
that
is
of
one
is
let
us
say
double
the
other
there
will
be
no
difference
or
else
an
imperceptible
difference
in
time
though
the
difference
in
weight
is
by
no
means
negligible
with
one
body
weighing
twice
as
much
as
the
other"”
john
philoponus'
criticism
of
aristotelian
principles
of
physics
served
as
an
inspiration
for
galileo
galilei
ten
centuries
later
during
the
scientific
revolution
galileo
cited
philoponus
substantially
in
his
works
when
arguing
that
aristotelian
physics
was
flawed
in
the
1300s
jean
buridan
a
teacher
in
the
faculty
of
arts
at
the
university
of
paris
developed
the
concept
of
impetus
it
was
a
step
toward
the
modern
ideas
of
inertia
and
momentum
islamic
scholarship
inherited
aristotelian
physics
from
the
greeks
and
during
the
islamic
golden
age
developed
it
further
especially
placing
emphasis
on
observation
and
"a
priori"
reasoning
developing
early
forms
of
the
scientific
method
the
most
notable
innovations
were
in
the
field
of
optics
and
vision
which
came
from
the
works
of
many
scientists
like
ibn
sahl
al-kindi
ibn
al-haytham
al-farisi
and
avicenna
the
most
notable
work
was
"the
book
of
optics"
(also
known
as
kitāb
al-manāẓir)
written
by
ibn
al-haytham
in
which
he
conclusively
disproved
the
ancient
greek
idea
about
vision
but
also
came
up
with
a
new
theory
in
the
book
he
presented
a
study
of
the
phenomenon
of
the
camera
obscura
(his
thousand-year-old
version
of
the
pinhole
camera)
and
delved
further
into
the
way
the
eye
itself
works
using
dissections
and
the
knowledge
of
previous
scholars
he
was
able
to
begin
to
explain
how
light
enters
the
eye
he
asserted
that
the
light
ray
is
focused
but
the
actual
explanation
of
how
light
projected
to
the
back
of
the
eye
had
to
wait
until
1604
his
"treatise
on
light"
explained
the
camera
obscura
hundreds
of
years
before
the
modern
development
of
photography
the
seven-volume
"book
of
optics"
("kitab
al-manathir")
hugely
influenced
thinking
across
disciplines
from
the
theory
of
visual
perception
to
the
nature
of
perspective
in
medieval
art
in
both
the
east
and
the
west
for
more
than
600
years
many
later
european
scholars
and
fellow
polymaths
from
robert
grosseteste
and
leonardo
da
vinci
to
rené
descartes
johannes
kepler
and
isaac
newton
were
in
his
debt
indeed
the
influence
of
ibn
al-haytham's
optics
ranks
alongside
that
of
newton's
work
of
the
same
title
published
700
years
later
the
translation
of
"the
book
of
optics"
had
a
huge
impact
on
europe
from
it
later
european
scholars
were
able
to
build
devices
that
replicated
those
ibn
al-haytham
had
built
and
understand
the
way
light
works
from
this
such
important
things
as
eyeglasses
magnifying
glasses
telescopes
and
cameras
were
developed
physics
became
a
separate
science
when
early
modern
europeans
used
experimental
and
quantitative
methods
to
discover
what
are
now
considered
to
be
the
laws
of
physics
major
developments
in
this
period
include
the
replacement
of
the
geocentric
model
of
the
solar
system
with
the
heliocentric
copernican
model
the
laws
governing
the
motion
of
planetary
bodies
determined
by
johannes
kepler
between
1609
and
1619
pioneering
work
on
telescopes
and
observational
astronomy
by
galileo
galilei
in
the
16th
and
17th
centuries
and
isaac
newton's
discovery
and
unification
of
the
laws
of
motion
and
universal
gravitation
that
would
come
to
bear
his
name
newton
also
developed
calculus
the
mathematical
study
of
change
which
provided
new
mathematical
methods
for
solving
physical
problems
the
discovery
of
new
laws
in
thermodynamics
chemistry
and
electromagnetics
resulted
from
greater
research
efforts
during
the
industrial
revolution
as
energy
needs
increased
the
laws
comprising
classical
physics
remain
very
widely
used
for
objects
on
everyday
scales
travelling
at
non-relativistic
speeds
since
they
provide
a
very
close
approximation
in
such
situations
and
theories
such
as
quantum
mechanics
and
the
theory
of
relativity
simplify
to
their
classical
equivalents
at
such
scales
however
inaccuracies
in
classical
mechanics
for
very
small
objects
and
very
high
velocities
led
to
the
development
of
modern
physics
in
the
20th
century
modern
physics
began
in
the
early
20th
century
with
the
work
of
max
planck
in
quantum
theory
and
albert
einstein's
theory
of
relativity
both
of
these
theories
came
about
due
to
inaccuracies
in
classical
mechanics
in
certain
situations
classical
mechanics
predicted
a
varying
speed
of
light
which
could
not
be
resolved
with
the
constant
speed
predicted
by
maxwell's
equations
of
electromagnetism;
this
discrepancy
was
corrected
by
einstein's
theory
of
special
relativity
which
replaced
classical
mechanics
for
fast-moving
bodies
and
allowed
for
a
constant
speed
of
light
black
body
radiation
provided
another
problem
for
classical
physics
which
was
corrected
when
planck
proposed
that
the
excitation
of
material
oscillators
is
possible
only
in
discrete
steps
proportional
to
their
frequency;
this
along
with
the
photoelectric
effect
and
a
complete
theory
predicting
discrete
energy
levels
of
electron
orbitals
led
to
the
theory
of
quantum
mechanics
taking
over
from
classical
physics
at
very
small
scales
quantum
mechanics
would
come
to
be
pioneered
by
werner
heisenberg
erwin
schrödinger
and
paul
dirac
from
this
early
work
and
work
in
related
fields
the
standard
model
of
particle
physics
was
derived
following
the
discovery
of
a
particle
with
properties
consistent
with
the
higgs
boson
at
cern
in
2012
all
fundamental
particles
predicted
by
the
standard
model
and
no
others
appear
to
exist;
however
physics
beyond
the
standard
model
with
theories
such
as
supersymmetry
is
an
active
area
of
research
areas
of
mathematics
in
general
are
important
to
this
field
such
as
the
study
of
probabilities
and
groups
in
many
ways
physics
stems
from
ancient
greek
philosophy
from
thales'
first
attempt
to
characterise
matter
to
democritus'
deduction
that
matter
ought
to
reduce
to
an
invariant
state
the
ptolemaic
astronomy
of
a
crystalline
firmament
and
aristotle's
book
"physics"
(an
early
book
on
physics
which
attempted
to
analyze
and
define
motion
from
a
philosophical
point
of
view)
various
greek
philosophers
advanced
their
own
theories
of
nature
physics
was
known
as
natural
philosophy
until
the
late
18th
century
by
the
19th
century
physics
was
realised
as
a
discipline
distinct
from
philosophy
and
the
other
sciences
physics
as
with
the
rest
of
science
relies
on
philosophy
of
science
and
its
"scientific
method"
to
advance
our
knowledge
of
the
physical
world
the
scientific
method
employs
"a
priori
reasoning"
as
well
as
"a
posteriori"
reasoning
and
the
use
of
bayesian
inference
to
measure
the
validity
of
a
given
theory
the
development
of
physics
has
answered
many
questions
of
early
philosophers
but
has
also
raised
new
questions
study
of
the
philosophical
issues
surrounding
physics
the
philosophy
of
physics
involves
issues
such
as
the
nature
of
space
and
time
determinism
and
metaphysical
outlooks
such
as
empiricism
naturalism
and
realism
many
physicists
have
written
about
the
philosophical
implications
of
their
work
for
instance
laplace
who
championed
causal
determinism
and
erwin
schrödinger
who
wrote
on
quantum
mechanics
the
mathematical
physicist
roger
penrose
had
been
called
a
platonist
by
stephen
hawking
a
view
penrose
discusses
in
his
book
"the
road
to
reality"
hawking
referred
to
himself
as
an
"unashamed
reductionist"
and
took
issue
with
penrose's
views
though
physics
deals
with
a
wide
variety
of
systems
certain
theories
are
used
by
all
physicists
each
of
these
theories
were
experimentally
tested
numerous
times
and
found
to
be
an
adequate
approximation
of
nature
for
instance
the
theory
of
classical
mechanics
accurately
describes
the
motion
of
objects
provided
they
are
much
larger
than
atoms
and
moving
at
much
less
than
the
speed
of
light
these
theories
continue
to
be
areas
of
active
research
today
chaos
theory
a
remarkable
aspect
of
classical
mechanics
was
discovered
in
the
20th
century
three
centuries
after
the
original
formulation
of
classical
mechanics
by
isaac
newton
(1642–1727)
these
central
theories
are
important
tools
for
research
into
more
specialised
topics
and
any
physicist
regardless
of
their
specialisation
is
expected
to
be
literate
in
them
these
include
classical
mechanics
quantum
mechanics
thermodynamics
and
statistical
mechanics
electromagnetism
and
special
relativity
classical
physics
includes
the
traditional
branches
and
topics
that
were
recognised
and
well-developed
before
the
beginning
of
the
20th
century—classical
mechanics
acoustics
optics
thermodynamics
and
electromagnetism
classical
mechanics
is
concerned
with
bodies
acted
on
by
forces
and
bodies
in
motion
and
may
be
divided
into
statics
(study
of
the
forces
on
a
body
or
bodies
not
subject
to
an
acceleration)
kinematics
(study
of
motion
without
regard
to
its
causes)
and
dynamics
(study
of
motion
and
the
forces
that
affect
it);
mechanics
may
also
be
divided
into
solid
mechanics
and
fluid
mechanics
(known
together
as
continuum
mechanics)
the
latter
include
such
branches
as
hydrostatics
hydrodynamics
aerodynamics
and
pneumatics
acoustics
is
the
study
of
how
sound
is
produced
controlled
transmitted
and
received
important
modern
branches
of
acoustics
include
ultrasonics
the
study
of
sound
waves
of
very
high
frequency
beyond
the
range
of
human
hearing;
bioacoustics
the
physics
of
animal
calls
and
hearing
and
electroacoustics
the
manipulation
of
audible
sound
waves
using
electronics
optics
the
study
of
light
is
concerned
not
only
with
visible
light
but
also
with
infrared
and
ultraviolet
radiation
which
exhibit
all
of
the
phenomena
of
visible
light
except
visibility
eg
reflection
refraction
interference
diffraction
dispersion
and
polarization
of
light
heat
is
a
form
of
energy
the
internal
energy
possessed
by
the
particles
of
which
a
substance
is
composed;
thermodynamics
deals
with
the
relationships
between
heat
and
other
forms
of
energy
electricity
and
magnetism
have
been
studied
as
a
single
branch
of
physics
since
the
intimate
connection
between
them
was
discovered
in
the
early
19th
century;
an
electric
current
gives
rise
to
a
magnetic
field
and
a
changing
magnetic
field
induces
an
electric
current
electrostatics
deals
with
electric
charges
at
rest
electrodynamics
with
moving
charges
and
magnetostatics
with
magnetic
poles
at
rest
classical
physics
is
generally
concerned
with
matter
and
energy
on
the
normal
scale
of
observation
while
much
of
modern
physics
is
concerned
with
the
behavior
of
matter
and
energy
under
extreme
conditions
or
on
a
very
large
or
very
small
scale
for
example
atomic
and
nuclear
physics
studies
matter
on
the
smallest
scale
at
which
chemical
elements
can
be
identified
the
physics
of
elementary
particles
is
on
an
even
smaller
scale
since
it
is
concerned
with
the
most
basic
units
of
matter;
this
branch
of
physics
is
also
known
as
high-energy
physics
because
of
the
extremely
high
energies
necessary
to
produce
many
types
of
particles
in
particle
accelerators
on
this
scale
ordinary
commonsense
notions
of
space
time
matter
and
energy
are
no
longer
valid
the
two
chief
theories
of
modern
physics
present
a
different
picture
of
the
concepts
of
space
time
and
matter
from
that
presented
by
classical
physics
classical
mechanics
approximates
nature
as
continuous
while
quantum
theory
is
concerned
with
the
discrete
nature
of
many
phenomena
at
the
atomic
and
subatomic
level
and
with
the
complementary
aspects
of
particles
and
waves
in
the
description
of
such
phenomena
the
theory
of
relativity
is
concerned
with
the
description
of
phenomena
that
take
place
in
a
frame
of
reference
that
is
in
motion
with
respect
to
an
observer;
the
special
theory
of
relativity
is
concerned
with
motion
in
the
absence
of
gravitational
fields
and
the
general
theory
of
relativity
with
motion
and
its
connection
with
gravitation
both
quantum
theory
and
the
theory
of
relativity
find
applications
in
all
areas
of
modern
physics
while
physics
aims
to
discover
universal
laws
its
theories
lie
in
explicit
domains
of
applicability
loosely
speaking
the
laws
of
classical
physics
accurately
describe
systems
whose
important
length
scales
are
greater
than
the
atomic
scale
and
whose
motions
are
much
slower
than
the
speed
of
light
outside
of
this
domain
observations
do
not
match
predictions
provided
by
classical
mechanics
albert
einstein
contributed
the
framework
of
special
relativity
which
replaced
notions
of
absolute
time
and
space
with
spacetime
and
allowed
an
accurate
description
of
systems
whose
components
have
speeds
approaching
the
speed
of
light
max
planck
erwin
schrödinger
and
others
introduced
quantum
mechanics
a
probabilistic
notion
of
particles
and
interactions
that
allowed
an
accurate
description
of
atomic
and
subatomic
scales
later
quantum
field
theory
unified
quantum
mechanics
and
special
relativity
general
relativity
allowed
for
a
dynamical
curved
spacetime
with
which
highly
massive
systems
and
the
large-scale
structure
of
the
universe
can
be
well-described
general
relativity
has
not
yet
been
unified
with
the
other
fundamental
descriptions;
several
candidate
theories
of
quantum
gravity
are
being
developed
mathematics
provides
a
compact
and
exact
language
used
to
describe
the
order
in
nature
this
was
noted
and
advocated
by
pythagoras
plato
galileo
and
newton
physics
uses
mathematics
to
organise
and
formulate
experimental
results
from
those
results
precise
or
estimated
solutions
are
obtained
quantitative
results
from
which
new
predictions
can
be
made
and
experimentally
confirmed
or
negated
the
results
from
physics
experiments
are
numerical
data
with
their
units
of
measure
and
estimates
of
the
errors
in
the
measurements
technologies
based
on
mathematics
like
computation
have
made
computational
physics
an
active
area
of
research
ontology
is
a
prerequisite
for
physics
but
not
for
mathematics
it
means
physics
is
ultimately
concerned
with
descriptions
of
the
real
world
while
mathematics
is
concerned
with
abstract
patterns
even
beyond
the
real
world
thus
physics
statements
are
synthetic
while
mathematical
statements
are
analytic
mathematics
contains
hypotheses
while
physics
contains
theories
mathematics
statements
have
to
be
only
logically
true
while
predictions
of
physics
statements
must
match
observed
and
experimental
data
the
distinction
is
clear-cut
but
not
always
obvious
for
example
mathematical
physics
is
the
application
of
mathematics
in
physics
its
methods
are
mathematical
but
its
subject
is
physical
the
problems
in
this
field
start
with
a
"mathematical
model
of
a
physical
situation"
(system)
and
a
"mathematical
description
of
a
physical
law"
that
will
be
applied
to
that
system
every
mathematical
statement
used
for
solving
has
a
hard-to-find
physical
meaning
the
final
mathematical
solution
has
an
easier-to-find
meaning
because
it
is
what
the
solver
is
looking
for
physics
is
a
branch
of
fundamental
science
not
practical
science
physics
is
also
called
"the
fundamental
science"
because
the
subject
of
study
of
all
branches
of
natural
science
like
chemistry
astronomy
geology
and
biology
are
constrained
by
laws
of
physics
similar
to
how
chemistry
is
often
called
the
central
science
because
of
its
role
in
linking
the
physical
sciences
for
example
chemistry
studies
properties
structures
and
reactions
of
matter
(chemistry's
focus
on
the
atomic
scale
distinguishes
it
from
physics)
structures
are
formed
because
particles
exert
electrical
forces
on
each
other
properties
include
physical
characteristics
of
given
substances
and
reactions
are
bound
by
laws
of
physics
like
conservation
of
energy
mass
and
charge
physics
is
applied
in
industries
like
engineering
and
medicine
applied
physics
is
a
general
term
for
physics
research
which
is
intended
for
a
particular
use
an
applied
physics
curriculum
usually
contains
a
few
classes
in
an
applied
discipline
like
geology
or
electrical
engineering
it
usually
differs
from
engineering
in
that
an
applied
physicist
may
not
be
designing
something
in
particular
but
rather
is
using
physics
or
conducting
physics
research
with
the
aim
of
developing
new
technologies
or
solving
a
problem
the
approach
is
similar
to
that
of
applied
mathematics
applied
physicists
use
physics
in
scientific
research
for
instance
people
working
on
accelerator
physics
might
seek
to
build
better
particle
detectors
for
research
in
theoretical
physics
physics
is
used
heavily
in
engineering
for
example
statics
a
subfield
of
mechanics
is
used
in
the
building
of
bridges
and
other
static
structures
the
understanding
and
use
of
acoustics
results
in
sound
control
and
better
concert
halls;
similarly
the
use
of
optics
creates
better
optical
devices
an
understanding
of
physics
makes
for
more
realistic
flight
simulators
video
games
and
movies
and
is
often
critical
in
forensic
investigations
with
the
standard
consensus
that
the
laws
of
physics
are
universal
and
do
not
change
with
time
physics
can
be
used
to
study
things
that
would
ordinarily
be
mired
in
uncertainty
for
example
in
the
study
of
the
origin
of
the
earth
one
can
reasonably
model
earth's
mass
temperature
and
rate
of
rotation
as
a
function
of
time
allowing
one
to
extrapolate
forward
or
backward
in
time
and
so
predict
future
or
prior
events
it
also
allows
for
simulations
in
engineering
which
drastically
speed
up
the
development
of
a
new
technology
but
there
is
also
considerable
interdisciplinarity
in
the
physicist's
methods
so
many
other
important
fields
are
influenced
by
physics
(eg
the
fields
of
econophysics
and
sociophysics)
physicists
use
the
scientific
method
to
test
the
validity
of
a
physical
theory
by
using
a
methodical
approach
to
compare
the
implications
of
a
theory
with
the
conclusions
drawn
from
its
related
experiments
and
observations
physicists
are
better
able
to
test
the
validity
of
a
theory
in
a
logical
unbiased
and
repeatable
way
to
that
end
experiments
are
performed
and
observations
are
made
in
order
to
determine
the
validity
or
invalidity
of
the
theory
a
scientific
law
is
a
concise
verbal
or
mathematical
statement
of
a
relation
which
expresses
a
fundamental
principle
of
some
theory
such
as
newton's
law
of
universal
gravitation
theorists
seek
to
develop
mathematical
models
that
both
agree
with
existing
experiments
and
successfully
predict
future
experimental
results
while
experimentalists
devise
and
perform
experiments
to
test
theoretical
predictions
and
explore
new
phenomena
although
theory
and
experiment
are
developed
separately
they
are
strongly
dependent
upon
each
other
progress
in
physics
frequently
comes
about
when
experimentalists
make
a
discovery
that
existing
theories
cannot
explain
or
when
new
theories
generate
experimentally
testable
predictions
which
inspire
new
experiments
physicists
who
work
at
the
interplay
of
theory
and
experiment
are
called
phenomenologists
who
study
complex
phenomena
observed
in
experiment
and
work
to
relate
them
to
a
fundamental
theory
theoretical
physics
has
historically
taken
inspiration
from
philosophy;
electromagnetism
was
unified
this
way
beyond
the
known
universe
the
field
of
theoretical
physics
also
deals
with
hypothetical
issues
such
as
parallel
universes
a
multiverse
and
higher
dimensions
theorists
invoke
these
ideas
in
hopes
of
solving
particular
problems
with
existing
theories
they
then
explore
the
consequences
of
these
ideas
and
work
toward
making
testable
predictions
experimental
physics
expands
and
is
expanded
by
engineering
and
technology
experimental
physicists
involved
in
basic
research
design
and
perform
experiments
with
equipment
such
as
particle
accelerators
and
lasers
whereas
those
involved
in
applied
research
often
work
in
industry
developing
technologies
such
as
magnetic
resonance
imaging
(mri)
and
transistors
feynman
has
noted
that
experimentalists
may
seek
areas
which
are
not
well-explored
by
theorists
physics
covers
a
wide
range
of
phenomena
from
elementary
particles
(such
as
quarks
neutrinos
and
electrons)
to
the
largest
superclusters
of
galaxies
included
in
these
phenomena
are
the
most
basic
objects
composing
all
other
things
therefore
physics
is
sometimes
called
the
"fundamental
science"
physics
aims
to
describe
the
various
phenomena
that
occur
in
nature
in
terms
of
simpler
phenomena
thus
physics
aims
to
both
connect
the
things
observable
to
humans
to
root
causes
and
then
connect
these
causes
together
for
example
the
ancient
chinese
observed
that
certain
rocks
(lodestone
and
magnetite)
were
attracted
to
one
another
by
an
invisible
force
this
effect
was
later
called
magnetism
which
was
first
rigorously
studied
in
the
17th
century
but
even
before
the
chinese
discovered
magnetism
the
ancient
greeks
knew
of
other
objects
such
as
amber
that
when
rubbed
with
fur
would
cause
a
similar
invisible
attraction
between
the
two
this
was
also
first
studied
rigorously
in
the
17th
century
and
came
to
be
called
electricity
thus
physics
had
come
to
understand
two
observations
of
nature
in
terms
of
some
root
cause
(electricity
and
magnetism)
however
further
work
in
the
19th
century
revealed
that
these
two
forces
were
just
two
different
aspects
of
one
force—electromagnetism
this
process
of
"unifying"
forces
continues
today
and
electromagnetism
and
the
weak
nuclear
force
are
now
considered
to
be
two
aspects
of
the
electroweak
interaction
physics
hopes
to
find
an
ultimate
reason
(theory
of
everything)
for
why
nature
is
as
it
is
(see
section
"current
research"
below
for
more
information)
contemporary
research
in
physics
can
be
broadly
divided
into
nuclear
and
particle
physics;
condensed
matter
physics;
atomic
molecular
and
optical
physics;
astrophysics;
and
applied
physics
some
physics
departments
also
support
physics
education
research
and
physics
outreach
since
the
20th
century
the
individual
fields
of
physics
have
become
increasingly
specialised
and
today
most
physicists
work
in
a
single
field
for
their
entire
careers
"universalists"
such
as
albert
einstein
(1879–1955)
and
lev
landau
(1908–1968)
who
worked
in
multiple
fields
of
physics
are
now
very
rare
the
major
fields
of
physics
along
with
their
subfields
and
the
theories
and
concepts
they
employ
are
shown
in
the
following
table
particle
physics
is
the
study
of
the
elementary
constituents
of
matter
and
energy
and
the
interactions
between
them
in
addition
particle
physicists
design
and
develop
the
high
energy
accelerators
detectors
and
computer
programs
necessary
for
this
research
the
field
is
also
called
"high-energy
physics"
because
many
elementary
particles
do
not
occur
naturally
but
are
created
only
during
high-energy
collisions
of
other
particles
currently
the
interactions
of
elementary
particles
and
fields
are
described
by
the
standard
model
the
model
accounts
for
the
12
known
particles
of
matter
(quarks
and
leptons)
that
interact
via
the
strong
weak
and
electromagnetic
fundamental
forces
dynamics
are
described
in
terms
of
matter
particles
exchanging
gauge
bosons
(gluons
w
and
z
bosons
and
photons
respectively)
the
standard
model
also
predicts
a
particle
known
as
the
higgs
boson
in
july
2012
cern
the
european
laboratory
for
particle
physics
announced
the
detection
of
a
particle
consistent
with
the
higgs
boson
an
integral
part
of
a
higgs
mechanism
nuclear
physics
is
the
field
of
physics
that
studies
the
constituents
and
interactions
of
atomic
nuclei
the
most
commonly
known
applications
of
nuclear
physics
are
nuclear
power
generation
and
nuclear
weapons
technology
but
the
research
has
provided
application
in
many
fields
including
those
in
nuclear
medicine
and
magnetic
resonance
imaging
ion
implantation
in
materials
engineering
and
radiocarbon
dating
in
geology
and
archaeology
atomic
molecular
and
optical
physics
(amo)
is
the
study
of
matter–matter
and
light–matter
interactions
on
the
scale
of
single
atoms
and
molecules
the
three
areas
are
grouped
together
because
of
their
interrelationships
the
similarity
of
methods
used
and
the
commonality
of
their
relevant
energy
scales
all
three
areas
include
both
classical
semi-classical
and
quantum
treatments;
they
can
treat
their
subject
from
a
microscopic
view
(in
contrast
to
a
macroscopic
view)
atomic
physics
studies
the
electron
shells
of
atoms
current
research
focuses
on
activities
in
quantum
control
cooling
and
trapping
of
atoms
and
ions
low-temperature
collision
dynamics
and
the
effects
of
electron
correlation
on
structure
and
dynamics
atomic
physics
is
influenced
by
the
nucleus
(see
eg
hyperfine
splitting)
but
intra-nuclear
phenomena
such
as
fission
and
fusion
are
considered
part
of
nuclear
physics
molecular
physics
focuses
on
multi-atomic
structures
and
their
internal
and
external
interactions
with
matter
and
light
optical
physics
is
distinct
from
optics
in
that
it
tends
to
focus
not
on
the
control
of
classical
light
fields
by
macroscopic
objects
but
on
the
fundamental
properties
of
optical
fields
and
their
interactions
with
matter
in
the
microscopic
realm
condensed
matter
physics
is
the
field
of
physics
that
deals
with
the
macroscopic
physical
properties
of
matter
in
particular
it
is
concerned
with
the
"condensed"
phases
that
appear
whenever
the
number
of
particles
in
a
system
is
extremely
large
and
the
interactions
between
them
are
strong
the
most
familiar
examples
of
condensed
phases
are
solids
and
liquids
which
arise
from
the
bonding
by
way
of
the
electromagnetic
force
between
atoms
more
exotic
condensed
phases
include
the
superfluid
and
the
bose–einstein
condensate
found
in
certain
atomic
systems
at
very
low
temperature
the
superconducting
phase
exhibited
by
conduction
electrons
in
certain
materials
and
the
ferromagnetic
and
antiferromagnetic
phases
of
spins
on
atomic
lattices
condensed
matter
physics
is
the
largest
field
of
contemporary
physics
historically
condensed
matter
physics
grew
out
of
solid-state
physics
which
is
now
considered
one
of
its
main
subfields
the
term
"condensed
matter
physics"
was
apparently
coined
by
philip
anderson
when
he
renamed
his
research
group—previously
"solid-state
theory"—in
1967
in
1978
the
division
of
solid
state
physics
of
the
american
physical
society
was
renamed
as
the
division
of
condensed
matter
physics
condensed
matter
physics
has
a
large
overlap
with
chemistry
materials
science
nanotechnology
and
engineering
astrophysics
and
astronomy
are
the
application
of
the
theories
and
methods
of
physics
to
the
study
of
stellar
structure
stellar
evolution
the
origin
of
the
solar
system
and
related
problems
of
cosmology
because
astrophysics
is
a
broad
subject
astrophysicists
typically
apply
many
disciplines
of
physics
including
mechanics
electromagnetism
statistical
mechanics
thermodynamics
quantum
mechanics
relativity
nuclear
and
particle
physics
and
atomic
and
molecular
physics
the
discovery
by
karl
jansky
in
1931
that
radio
signals
were
emitted
by
celestial
bodies
initiated
the
science
of
radio
astronomy
most
recently
the
frontiers
of
astronomy
have
been
expanded
by
space
exploration
perturbations
and
interference
from
the
earth's
atmosphere
make
space-based
observations
necessary
for
infrared
ultraviolet
gamma-ray
and
x-ray
astronomy
physical
cosmology
is
the
study
of
the
formation
and
evolution
of
the
universe
on
its
largest
scales
albert
einstein's
theory
of
relativity
plays
a
central
role
in
all
modern
cosmological
theories
in
the
early
20th
century
hubble's
discovery
that
the
universe
is
expanding
as
shown
by
the
hubble
diagram
prompted
rival
explanations
known
as
the
steady
state
universe
and
the
big
bang
the
big
bang
was
confirmed
by
the
success
of
big
bang
nucleosynthesis
and
the
discovery
of
the
cosmic
microwave
background
in
1964
the
big
bang
model
rests
on
two
theoretical
pillars:
albert
einstein's
general
relativity
and
the
cosmological
principle
cosmologists
have
recently
established
the
λcdm
model
of
the
evolution
of
the
universe
which
includes
cosmic
inflation
dark
energy
and
dark
matter
numerous
possibilities
and
discoveries
are
anticipated
to
emerge
from
new
data
from
the
fermi
gamma-ray
space
telescope
over
the
upcoming
decade
and
vastly
revise
or
clarify
existing
models
of
the
universe
in
particular
the
potential
for
a
tremendous
discovery
surrounding
dark
matter
is
possible
over
the
next
several
years
fermi
will
search
for
evidence
that
dark
matter
is
composed
of
weakly
interacting
massive
particles
complementing
similar
experiments
with
the
large
hadron
collider
and
other
underground
detectors
ibex
is
already
yielding
new
astrophysical
discoveries:
"no
one
knows
what
is
creating
the
ena
(energetic
neutral
atoms)
ribbon"
along
the
termination
shock
of
the
solar
wind
"but
everyone
agrees
that
it
means
the
textbook
picture
of
the
heliosphere—in
which
the
solar
system's
enveloping
pocket
filled
with
the
solar
wind's
charged
particles
is
plowing
through
the
onrushing
'galactic
wind'
of
the
interstellar
medium
in
the
shape
of
a
comet—is
wrong"
research
in
physics
is
continually
progressing
on
a
large
number
of
fronts
in
condensed
matter
physics
an
important
unsolved
theoretical
problem
is
that
of
high-temperature
superconductivity
many
condensed
matter
experiments
are
aiming
to
fabricate
workable
spintronics
and
quantum
computers
in
particle
physics
the
first
pieces
of
experimental
evidence
for
physics
beyond
the
standard
model
have
begun
to
appear
foremost
among
these
are
indications
that
neutrinos
have
non-zero
mass
these
experimental
results
appear
to
have
solved
the
long-standing
solar
neutrino
problem
and
the
physics
of
massive
neutrinos
remains
an
area
of
active
theoretical
and
experimental
research
the
large
hadron
collider
has
already
found
the
higgs
boson
but
future
research
aims
to
prove
or
disprove
the
supersymmetry
which
extends
the
standard
model
of
particle
physics
research
on
the
nature
of
the
major
mysteries
of
dark
matter
and
dark
energy
is
also
currently
ongoing
theoretical
attempts
to
unify
quantum
mechanics
and
general
relativity
into
a
single
theory
of
quantum
gravity
a
program
ongoing
for
over
half
a
century
have
not
yet
been
decisively
resolved
the
current
leading
candidates
are
m-theory
superstring
theory
and
loop
quantum
gravity
many
astronomical
and
cosmological
phenomena
have
yet
to
be
satisfactorily
explained
including
the
origin
of
ultra-high
energy
cosmic
rays
the
baryon
asymmetry
the
acceleration
of
the
universe
and
the
anomalous
rotation
rates
of
galaxies
although
much
progress
has
been
made
in
high-energy
quantum
and
astronomical
physics
many
everyday
phenomena
involving
complexity
chaos
or
turbulence
are
still
poorly
understood
complex
problems
that
seem
like
they
could
be
solved
by
a
clever
application
of
dynamics
and
mechanics
remain
unsolved;
examples
include
the
formation
of
sandpiles
nodes
in
trickling
water
the
shape
of
water
droplets
mechanisms
of
surface
tension
catastrophes
and
self-sorting
in
shaken
heterogeneous
collections
these
complex
phenomena
have
received
growing
attention
since
the
1970s
for
several
reasons
including
the
availability
of
modern
mathematical
methods
and
computers
which
enabled
complex
systems
to
be
modeled
in
new
ways
complex
physics
has
become
part
of
increasingly
interdisciplinary
research
as
exemplified
by
the
study
of
turbulence
in
aerodynamics
and
the
observation
of
pattern
formation
in
biological
systems
in
the
1932
"annual
review
of
fluid
mechanics"
horace
lamb
said:
general
organizations
online
course
learning
resources
statistical
mechanics
statistical
mechanics
is
one
of
the
pillars
of
modern
physics
it
is
necessary
for
the
fundamental
study
of
any
physical
system
that
has
a
large
number
of
degrees
of
freedom
the
approach
is
based
on
statistical
methods
probability
theory
and
the
microscopic
physical
laws
it
can
be
used
to
explain
the
thermodynamic
behaviour
of
large
systems
this
branch
of
statistical
mechanics
which
treats
and
extends
classical
thermodynamics
is
known
as
statistical
thermodynamics
or
equilibrium
statistical
mechanics
statistical
mechanics
shows
how
the
concepts
from
macroscopic
observations
(such
as
temperature
and
pressure)
are
related
to
the
description
of
microscopic
state
that
fluctuates
around
an
average
state
it
connects
thermodynamic
quantities
(such
as
heat
capacity)
to
microscopic
behaviour
whereas
in
classical
thermodynamics
the
only
available
option
would
be
to
just
measure
and
tabulate
such
quantities
for
various
materials
statistical
mechanics
can
also
be
used
to
study
systems
that
are
out
of
equilibrium
an
important
subbranch
known
as
non-equilibrium
statistical
mechanics
deals
with
the
issue
of
microscopically
modelling
the
speed
of
irreversible
processes
that
are
driven
by
imbalances
examples
of
such
processes
include
chemical
reactions
or
flows
of
particles
and
heat
the
fluctuation–dissipation
theorem
is
the
basic
knowledge
obtained
from
applying
non-equilibrium
statistical
mechanics
to
study
the
simplest
non-equilibrium
situation
of
a
steady
state
current
flow
in
a
system
of
many
particles
in
physics
there
are
two
types
of
mechanics
usually
examined:
classical
mechanics
and
quantum
mechanics
for
both
types
of
mechanics
the
standard
mathematical
approach
is
to
consider
two
concepts:
using
these
two
concepts
the
state
at
any
other
time
past
or
future
can
in
principle
be
calculated
there
is
however
a
disconnection
between
these
laws
and
everyday
life
experiences
as
we
do
not
find
it
necessary
(nor
even
theoretically
possible)
to
know
exactly
at
a
microscopic
level
the
simultaneous
positions
and
velocities
of
each
molecule
while
carrying
out
processes
at
the
human
scale
(for
example
when
performing
a
chemical
reaction)
statistical
mechanics
fills
this
disconnection
between
the
laws
of
mechanics
and
the
practical
experience
of
incomplete
knowledge
by
adding
some
uncertainty
about
which
state
the
system
is
in
whereas
ordinary
mechanics
only
considers
the
behaviour
of
a
single
state
statistical
mechanics
introduces
the
statistical
ensemble
which
is
a
large
collection
of
virtual
independent
copies
of
the
system
in
various
states
the
statistical
ensemble
is
a
probability
distribution
over
all
possible
states
of
the
system
in
classical
statistical
mechanics
the
ensemble
is
a
probability
distribution
over
phase
points
(as
opposed
to
a
single
phase
point
in
ordinary
mechanics)
usually
represented
as
a
distribution
in
a
phase
space
with
canonical
coordinates
in
quantum
statistical
mechanics
the
ensemble
is
a
probability
distribution
over
pure
states
and
can
be
compactly
summarized
as
a
density
matrix
as
is
usual
for
probabilities
the
ensemble
can
be
interpreted
in
different
ways:
these
two
meanings
are
equivalent
for
many
purposes
and
will
be
used
interchangeably
in
this
article
however
the
probability
is
interpreted
each
state
in
the
ensemble
evolves
over
time
according
to
the
equation
of
motion
thus
the
ensemble
itself
(the
probability
distribution
over
states)
also
evolves
as
the
virtual
systems
in
the
ensemble
continually
leave
one
state
and
enter
another
the
ensemble
evolution
is
given
by
the
liouville
equation
(classical
mechanics)
or
the
von
neumann
equation
(quantum
mechanics)
these
equations
are
simply
derived
by
the
application
of
the
mechanical
equation
of
motion
separately
to
each
virtual
system
contained
in
the
ensemble
with
the
probability
of
the
virtual
system
being
conserved
over
time
as
it
evolves
from
state
to
state
one
special
class
of
ensemble
is
those
ensembles
that
do
not
evolve
over
time
these
ensembles
are
known
as
"equilibrium
ensembles"
and
their
condition
is
known
as
"statistical
equilibrium"
statistical
equilibrium
occurs
if
for
each
state
in
the
ensemble
the
ensemble
also
contains
all
of
its
future
and
past
states
with
probabilities
equal
to
the
probability
of
being
in
that
state
the
study
of
equilibrium
ensembles
of
isolated
systems
is
the
focus
of
statistical
thermodynamics
non-equilibrium
statistical
mechanics
addresses
the
more
general
case
of
ensembles
that
change
over
time
and/or
ensembles
of
non-isolated
systems
the
primary
goal
of
statistical
thermodynamics
(also
known
as
equilibrium
statistical
mechanics)
is
to
derive
the
classical
thermodynamics
of
materials
in
terms
of
the
properties
of
their
constituent
particles
and
the
interactions
between
them
in
other
words
statistical
thermodynamics
provides
a
connection
between
the
macroscopic
properties
of
materials
in
thermodynamic
equilibrium
and
the
microscopic
behaviours
and
motions
occurring
inside
the
material
whereas
statistical
mechanics
proper
involves
dynamics
here
the
attention
is
focussed
on
"statistical
equilibrium"
(steady
state)
statistical
equilibrium
does
not
mean
that
the
particles
have
stopped
moving
(mechanical
equilibrium)
rather
only
that
the
ensemble
is
not
evolving
a
sufficient
(but
not
necessary)
condition
for
statistical
equilibrium
with
an
isolated
system
is
that
the
probability
distribution
is
a
function
only
of
conserved
properties
(total
energy
total
particle
numbers
etc)
there
are
many
different
equilibrium
ensembles
that
can
be
considered
and
only
some
of
them
correspond
to
thermodynamics
additional
postulates
are
necessary
to
motivate
why
the
ensemble
for
a
given
system
should
have
one
form
or
another
a
common
approach
found
in
many
textbooks
is
to
take
the
"equal
a
priori
probability
postulate"
this
postulate
states
that
the
equal
a
priori
probability
postulate
therefore
provides
a
motivation
for
the
microcanonical
ensemble
described
below
there
are
various
arguments
in
favour
of
the
equal
a
priori
probability
postulate:
other
fundamental
postulates
for
statistical
mechanics
have
also
been
proposed
there
are
three
equilibrium
ensembles
with
a
simple
form
that
can
be
defined
for
any
isolated
system
bounded
inside
a
finite
volume
these
are
the
most
often
discussed
ensembles
in
statistical
thermodynamics
in
the
macroscopic
limit
(defined
below)
they
all
correspond
to
classical
thermodynamics
for
systems
containing
many
particles
(the
thermodynamic
limit)
all
three
of
the
ensembles
listed
above
tend
to
give
identical
behaviour
it
is
then
simply
a
matter
of
mathematical
convenience
which
ensemble
is
used
the
gibbs
theorem
about
equivalence
of
ensembles
was
developed
into
the
theory
of
concentration
of
measure
phenomenon
which
has
applications
in
many
areas
of
science
from
functional
analysis
to
methods
of
artificial
intelligence
and
big
data
technology
important
cases
where
the
thermodynamic
ensembles
"do
not"
give
identical
results
include:
in
these
cases
the
correct
thermodynamic
ensemble
must
be
chosen
as
there
are
observable
differences
between
these
ensembles
not
just
in
the
size
of
fluctuations
but
also
in
average
quantities
such
as
the
distribution
of
particles
the
correct
ensemble
is
that
which
corresponds
to
the
way
the
system
has
been
prepared
and
characterized—in
other
words
the
ensemble
that
reflects
the
knowledge
about
that
system
once
the
characteristic
state
function
for
an
ensemble
has
been
calculated
for
a
given
system
that
system
is
'solved'
(macroscopic
observables
can
be
extracted
from
the
characteristic
state
function)
calculating
the
characteristic
state
function
of
a
thermodynamic
ensemble
is
not
necessarily
a
simple
task
however
since
it
involves
considering
every
possible
state
of
the
system
while
some
hypothetical
systems
have
been
exactly
solved
the
most
general
(and
realistic)
case
is
too
complex
for
an
exact
solution
various
approaches
exist
to
approximate
the
true
ensemble
and
allow
calculation
of
average
quantities
there
are
some
cases
which
allow
exact
solutions
one
approximate
approach
that
is
particularly
well
suited
to
computers
is
the
monte
carlo
method
which
examines
just
a
few
of
the
possible
states
of
the
system
with
the
states
chosen
randomly
(with
a
fair
weight)
as
long
as
these
states
form
a
representative
sample
of
the
whole
set
of
states
of
the
system
the
approximate
characteristic
function
is
obtained
as
more
and
more
random
samples
are
included
the
errors
are
reduced
to
an
arbitrarily
low
level
there
are
many
physical
phenomena
of
interest
that
involve
quasi-thermodynamic
processes
out
of
equilibrium
for
example:
all
of
these
processes
occur
over
time
with
characteristic
rates
and
these
rates
are
of
importance
for
engineering
the
field
of
non-equilibrium
statistical
mechanics
is
concerned
with
understanding
these
non-equilibrium
processes
at
the
microscopic
level
(statistical
thermodynamics
can
only
be
used
to
calculate
the
final
result
after
the
external
imbalances
have
been
removed
and
the
ensemble
has
settled
back
down
to
equilibrium)
in
principle
non-equilibrium
statistical
mechanics
could
be
mathematically
exact:
ensembles
for
an
isolated
system
evolve
over
time
according
to
deterministic
equations
such
as
liouville's
equation
or
its
quantum
equivalent
the
von
neumann
equation
these
equations
are
the
result
of
applying
the
mechanical
equations
of
motion
independently
to
each
state
in
the
ensemble
unfortunately
these
ensemble
evolution
equations
inherit
much
of
the
complexity
of
the
underlying
mechanical
motion
and
so
exact
solutions
are
very
difficult
to
obtain
moreover
the
ensemble
evolution
equations
are
fully
reversible
and
do
not
destroy
information
(the
ensemble's
gibbs
entropy
is
preserved)
in
order
to
make
headway
in
modelling
irreversible
processes
it
is
necessary
to
consider
additional
factors
besides
probability
and
reversible
mechanics
non-equilibrium
mechanics
is
therefore
an
active
area
of
theoretical
research
as
the
range
of
validity
of
these
additional
assumptions
continues
to
be
explored
a
few
approaches
are
described
in
the
following
subsections
one
approach
to
non-equilibrium
statistical
mechanics
is
to
incorporate
stochastic
(random)
behaviour
into
the
system
stochastic
behaviour
destroys
information
contained
in
the
ensemble
while
this
is
technically
inaccurate
(aside
from
hypothetical
situations
involving
black
holes
a
system
cannot
in
itself
cause
loss
of
information)
the
randomness
is
added
to
reflect
that
information
of
interest
becomes
converted
over
time
into
subtle
correlations
within
the
system
or
to
correlations
between
the
system
and
environment
these
correlations
appear
as
chaotic
or
pseudorandom
influences
on
the
variables
of
interest
by
replacing
these
correlations
with
randomness
proper
the
calculations
can
be
made
much
easier
another
important
class
of
non-equilibrium
statistical
mechanical
models
deals
with
systems
that
are
only
very
slightly
perturbed
from
equilibrium
with
very
small
perturbations
the
response
can
be
analysed
in
linear
response
theory
a
remarkable
result
as
formalized
by
the
fluctuation-dissipation
theorem
is
that
the
response
of
a
system
when
near
equilibrium
is
precisely
related
to
the
fluctuations
that
occur
when
the
system
is
in
total
equilibrium
essentially
a
system
that
is
slightly
away
from
equilibrium—whether
put
there
by
external
forces
or
by
fluctuations—relaxes
towards
equilibrium
in
the
same
way
since
the
system
cannot
tell
the
difference
or
"know"
how
it
came
to
be
away
from
equilibrium
this
provides
an
indirect
avenue
for
obtaining
numbers
such
as
ohmic
conductivity
and
thermal
conductivity
by
extracting
results
from
equilibrium
statistical
mechanics
since
equilibrium
statistical
mechanics
is
mathematically
well
defined
and
(in
some
cases)
more
amenable
for
calculations
the
fluctuation-dissipation
connection
can
be
a
convenient
shortcut
for
calculations
in
near-equilibrium
statistical
mechanics
a
few
of
the
theoretical
tools
used
to
make
this
connection
include:
an
advanced
approach
uses
a
combination
of
stochastic
methods
and
linear
response
theory
as
an
example
one
approach
to
compute
quantum
coherence
effects
(weak
localization
conductance
fluctuations)
in
the
conductance
of
an
electronic
system
is
the
use
of
the
green-kubo
relations
with
the
inclusion
of
stochastic
dephasing
by
interactions
between
various
electrons
by
use
of
the
keldysh
method
the
ensemble
formalism
also
can
be
used
to
analyze
general
mechanical
systems
with
uncertainty
in
knowledge
about
the
state
of
a
system
ensembles
are
also
used
in:
in
1738
swiss
physicist
and
mathematician
daniel
bernoulli
published
"hydrodynamica"
which
laid
the
basis
for
the
kinetic
theory
of
gases
in
this
work
bernoulli
posited
the
argument
still
used
to
this
day
that
gases
consist
of
great
numbers
of
molecules
moving
in
all
directions
that
their
impact
on
a
surface
causes
the
gas
pressure
that
we
feel
and
that
what
we
experience
as
heat
is
simply
the
kinetic
energy
of
their
motion
in
1859
after
reading
a
paper
on
the
diffusion
of
molecules
by
rudolf
clausius
scottish
physicist
james
clerk
maxwell
formulated
the
maxwell
distribution
of
molecular
velocities
which
gave
the
proportion
of
molecules
having
a
certain
velocity
in
a
specific
range
this
was
the
first-ever
statistical
law
in
physics
maxwell
also
gave
the
first
mechanical
argument
that
molecular
collisions
entail
an
equalization
of
temperatures
and
hence
a
tendency
towards
equilibrium
five
years
later
in
1864
ludwig
boltzmann
a
young
student
in
vienna
came
across
maxwell's
paper
and
spent
much
of
his
life
developing
the
subject
further
statistical
mechanics
proper
was
initiated
in
the
1870s
with
the
work
of
boltzmann
much
of
which
was
collectively
published
in
his
1896
"lectures
on
gas
theory"
boltzmann's
original
papers
on
the
statistical
interpretation
of
thermodynamics
the
h-theorem
transport
theory
thermal
equilibrium
the
equation
of
state
of
gases
and
similar
subjects
occupy
about
2000
pages
in
the
proceedings
of
the
vienna
academy
and
other
societies
boltzmann
introduced
the
concept
of
an
equilibrium
statistical
ensemble
and
also
investigated
for
the
first
time
non-equilibrium
statistical
mechanics
with
his
"h"-theorem
the
term
"statistical
mechanics"
was
coined
by
the
american
mathematical
physicist
j
willard
gibbs
in
1884
"probabilistic
mechanics"
might
today
seem
a
more
appropriate
term
but
"statistical
mechanics"
is
firmly
entrenched
shortly
before
his
death
gibbs
published
in
1902
"elementary
principles
in
statistical
mechanics"
a
book
which
formalized
statistical
mechanics
as
a
fully
general
approach
to
address
all
mechanical
systems—macroscopic
or
microscopic
gaseous
or
non-gaseous
gibbs'
methods
were
initially
derived
in
the
framework
classical
mechanics
however
they
were
of
such
generality
that
they
were
found
to
adapt
easily
to
the
later
quantum
mechanics
and
still
form
the
foundation
of
statistical
mechanics
to
this
day
glossary
of
classical
physics
this
article
is
a
glossary
of
classical
physics
it
is
some
of
the
most
common
terms
in
classical
physics
and
how
they
are
used
phase
stretch
transform
phase
stretch
transform
(pst)
is
a
computational
approach
to
signal
and
image
processing
one
of
its
utilities
is
for
feature
detection
and
classification
pst
is
related
to
time
stretch
dispersive
fourier
transform
it
transforms
the
image
by
emulating
propagation
through
a
diffractive
medium
with
engineered
3d
dispersive
property
(refractive
index)
the
operation
relies
on
symmetry
of
the
dispersion
profile
and
can
be
understood
in
terms
of
dispersive
eigenfunctions
or
stretch
modes
pst
performs
similar
functionality
as
phase-contrast
microscopy
but
on
digital
images
pst
can
be
applied
to
digital
images
and
temporal
(time
series)
data
here
the
principle
is
described
in
the
context
of
feature
enhancement
in
digital
images
the
image
is
first
filtered
with
a
spatial
kernel
followed
by
application
of
a
nonlinear
frequency-dependent
phase
the
output
of
the
transform
is
the
phase
in
the
spatial
domain
the
main
step
is
the
2-d
phase
function
which
is
typically
applied
in
the
frequency
domain
the
amount
of
phase
applied
to
the
image
is
frequency
dependent
with
higher
amount
of
phase
applied
to
higher
frequency
features
of
the
image
since
sharp
transitions
such
as
edges
and
corners
contain
higher
frequencies
pst
emphasizes
the
edge
information
features
can
be
further
enhanced
by
applying
thresholding
and
morphological
operations
pst
is
related
to
warped
or
anamorphic
stretch
transform
another
computational
algorithm
inspired
by
optical
physics
that
performs
nonuniform
sampling
and
sparse
coding
photonic
time
stretch
technique
can
be
understood
by
considering
the
propagation
of
an
optical
pulse
through
a
dispersive
fiber
by
disregarding
the
loss
and
non-linearity
in
fiber
the
non-linear
schrodinger
equation
governing
the
optical
pulse
propagation
in
fiber
upon
integration
reduces
to:
formula_1
(1)
where
formula_2
=gvd
parameter
z
is
propagation
distance
formula_3
is
the
reshaped
output
pulse
at
distance
z
and
time
t
the
response
of
this
dispersive
element
in
the
time-stretch
system
can
be
approximated
as
a
phase
propagator
as
presented
in
formula_4
(2)
therefore
eq
1
can
be
written
as
following
for
a
pulse
that
propagates
through
the
time-stretch
system
and
is
reshaped
into
a
temporal
signal
with
a
complex
envelope
given
by
formula_5
(3)
the
time
stretch
operation
is
formulated
as
generalized
phase
and
amplitude
operations
formula_6
(4)
where
formula_7
is
the
phase
filter
and
formula_8is
the
amplitude
filter
next
the
operator
is
converted
to
discrete
domain
formula_9
(5)
where
formula_10
is
the
discrete
frequency
formula_11
is
the
phase
filter
formula_12
is
the
amplitude
filter
and
fft
is
fast
fourier
transform
the
stretch
operator
formula_13
for
a
digital
image
is
then
formula_14
(6)
in
the
above
equations
formula_15
is
the
input
image
formula_16
and
formula_17
are
the
spatial
variables
formula_18
is
the
two
dimensional
fast
fourier
transform
and
formula_19
and
formula_20
are
spatial
frequency
variables
the
function
formula_21
is
the
warped
phase
kernel
and
the
function
formula_22
is
a
localization
kernel
implemented
in
frequency
domain
pst
operator
is
defined
as
the
phase
of
the
warped
stretch
transform
output
as
follows
formula_23
(7)
where
formula_24is
the
angle
operator
pst
has
been
used
for
edge
detection
in
biological
and
biomedical
images
as
well
as
synthetic-aperture
radar
(sar)
image
processing
pst
has
also
been
applied
to
improve
the
point
spread
function
for
single
molecule
imaging
in
order
to
achieve
super-resolution
the
transform
exhibits
intrinsic
superior
properties
compared
to
conventional
edge
detectors
for
feature
detection
in
low
contrast
visually
impaired
images
the
pst
function
can
also
be
performed
on
1-d
temporal
waveforms
in
the
analog
domain
to
reveal
transitions
and
anomalies
in
real
time
on
february
9
2016
a
ucla
engineering
research
group
has
made
public
the
computer
code
for
pst
algorithm
that
helps
computers
process
images
at
high
speeds
and
"see"
them
in
ways
that
human
eyes
cannot
the
researchers
say
the
code
could
eventually
be
used
in
face
fingerprint
and
iris
recognition
systems
for
high-tech
security
as
well
as
in
self-driving
cars'
navigation
systems
or
for
inspecting
industrial
products
matlab
and
python
implementation
for
pst
is
available
for
free
download
from
our
github
repository
the
matlab
implementation
for
pst
can
also
be
downloaded
from
matlab
files
exchange
however
it
is
provided
for
research
purposes
only
and
a
license
must
be
obtained
for
any
commercial
applications
the
software
is
protected
under
a
us
patent
northwest
nuclear
consortium
the
northwest
nuclear
consortium
is
an
organization
based
in
washington
state
which
uses
a
research
grade
ion
collider
to
teach
a
class
of
high
school
students
nuclear
engineering
principles
based
on
the
department
of
energy
curriculum
they
won
the
1st
place
at
wsu
imagine
tomorrow
in
2012
they
also
won
the
1st
place
at
the
washington
state
science
fair
and
the
2nd
place
worldwide
at
isef
in
2013
in
2014
they
won
two
2nd
place
at
the
central
sound
regional
science
fair
at
bellevue
college
and
they
won
1st
place
twice
in
category
at
the
washington
state
science
engineering
fair
at
bremerton
in
2015
they
won
14
1st-place
trophies
at
the
washington
state
science
and
engineering
fair
over
$250000
in
scholarships
at
two
different
colleges
and
3
of
the
5
available
trips
to
isef
where
they
won
4th
place
in
the
world
against
72
countries
wigner
rotation
in
theoretical
physics
the
composition
of
two
non-collinear
lorentz
boosts
results
in
a
lorentz
transformation
that
is
not
a
pure
boost
but
is
the
composition
of
a
boost
and
a
rotation
this
rotation
is
called
thomas
rotation
thomas–wigner
rotation
or
wigner
rotation
the
rotation
was
discovered
by
llewellyn
thomas
in
1926
and
derived
by
wigner
in
1939
if
a
sequence
of
non-collinear
boosts
returns
an
object
to
its
initial
velocity
then
the
sequence
of
wigner
rotations
can
combine
to
produce
a
net
rotation
called
the
thomas
precession
there
are
still
ongoing
discussions
about
the
correct
form
of
equations
for
the
thomas
rotation
in
different
reference
systems
with
contradicting
results
goldstein:
einstein's
principle
of
velocity
reciprocity
(epvr)
reads
with
less
careful
interpretation
the
epvr
is
seemingly
violated
in
some
models
there
is
of
course
no
true
paradox
present
when
studying
the
thomas
rotation
at
the
fundamental
level
one
typically
uses
a
setup
with
three
coordinate
frames
frame
has
velocity
relative
to
frame
and
frame
has
velocity
relative
to
frame
the
axes
are
by
construction
oriented
as
follows
viewed
from
the
axes
of
and
are
parallel
(the
same
holds
true
for
the
pair
of
frames
when
viewed
from
)
also
viewed
from
the
spatial
axes
of
and
are
parallel
(and
the
same
holds
true
for
the
pair
of
frames
when
viewed
from
)
this
is
an
application
of
evpr:
if
is
the
velocity
of
relative
to
then
is
the
velocity
of
relative
to
the
velocity
makes
the
"same"
angles
with
respect
to
coordinate
axes
in
both
the
primed
and
unprimed
systems
this
does
"not"
represent
a
snapshot
taken
in
any
of
the
two
frames
of
the
combined
system
at
any
particular
time
as
should
be
clear
from
the
detailed
description
below
this
is
possible
since
a
boost
in
say
the
positive
preserves
orthogonality
of
the
coordinate
axes
a
general
boost
can
be
expressed
as
where
is
a
rotation
taking
the
into
the
direction
of
and
is
a
boost
in
the
new
each
rotation
retains
the
property
that
the
spatial
coordinate
axes
are
orthogonal
the
boost
will
stretch
the
(intermediate)
by
a
factor
while
leaving
the
and
in
place
the
fact
that
coordinate
axes
are
non-parallel
in
this
construction
after
"two"
consecutive
non-collinear
boosts
is
a
precise
expression
of
the
phenomenon
of
thomas
precession
the
velocity
of
as
seen
in
is
denoted
where
⊕
refers
to
the
relativistic
addition
of
velocity
(and
not
ordinary
vector
addition)
given
by
\frac{1}{c^2}\frac{\gamma_\mathbf{u}}{1+\gamma_\mathbf{u}}
\mathbf
u
\cdot
\mathbf
v
\right)\mathbf
u
+
and
which
still
completely
defines
the
direction
of
the
axis
without
loss
of
information
the
rotation
is
simply
a
"static"
rotation
and
there
is
no
relative
rotational
motion
between
the
frames
there
is
relative
translational
motion
in
the
boost
however
if
the
frames
accelerate
then
the
rotated
frame
rotates
with
an
angular
velocity
this
effect
is
known
as
the
thomas
precession
and
arises
purely
from
the
kinematics
of
successive
lorentz
boosts
in
principle
it
is
pretty
easy
since
every
lorentz
transformation
is
a
product
of
a
boost
and
a
rotation
the
consecutive
application
of
two
pure
boosts
is
a
pure
boost
either
followed
by
or
preceded
by
a
pure
rotation
thus
suppose
the
task
is
to
glean
from
this
equation
the
boost
velocity
and
the
rotation
from
the
matrix
entries
of
the
coordinates
of
events
are
related
by
inverting
this
relation
yields
or
set
then
will
record
the
spacetime
position
of
the
origin
of
the
primed
system
or
but
multiplying
this
matrix
with
a
pure
rotation
will
not
affect
the
zeroth
columns
and
rows
and
which
could
have
been
anticipated
from
the
formula
for
a
simple
boost
in
the
-direction
and
for
the
relative
velocity
vector
thus
given
with
one
obtains
and
by
little
more
than
inspection
of
(of
course
can
also
be
found
using
velocity
addition
per
above)
from
construct
the
solution
for
is
then
with
the
ansatz
one
finds
by
the
same
means
finding
a
formal
solution
in
terms
of
velocity
parameters
and
involves
first
"formally"
multiplying
formally
inverting
then
reading
off
form
the
result
"formally"
building
from
the
result
and
finally
formally
multiplying
it
should
be
clear
that
this
is
a
daunting
task
and
it
is
difficult
to
interpret/identify
the
result
as
a
rotation
though
it
is
clear
a
priori
that
it
is
it
is
these
difficulties
that
the
goldstein
quote
at
the
top
refers
to
the
problem
has
been
thoroughly
studied
under
simplifying
assumptions
over
the
years
another
way
to
explain
the
origin
of
the
rotation
is
by
looking
at
the
generators
of
the
lorentz
group
the
passage
from
a
velocity
to
a
boost
is
obtained
as
follows
an
arbitrary
boost
is
given
by
where
is
a
triple
of
real
numbers
serving
as
coordinates
on
the
boost
subspace
of
the
lie
algebra
spanned
by
the
matrices
the
vector
is
called
the
"boost
parameter"
or
"boost
vector"
its
norm
is
the
rapidity
here
is
the
"velocity
parameter"
the
magnitude
of
the
vector
while
for
one
has
the
is
confined
by
and
hence
thus
the
set
of
velocities
satisfying
is
an
open
ball
in
and
is
called
the
space
of
admissible
velocities
in
the
literature
it
is
endowed
with
a
hyperbolic
geometry
described
in
the
linked
article
the
generators
of
boosts
in
different
directions
do
not
commute
this
has
the
effect
that
two
consecutive
boosts
is
not
a
pure
boost
in
general
but
a
rotation
preceding
a
boost
consider
a
succession
of
boosts
in
the
x
direction
then
the
y
direction
expanding
each
boost
to
first
order
then
and
the
group
commutator
is
three
of
the
commutation
relations
of
the
lorentz
generators
are
where
the
bracket
is
a
binary
operation
known
as
the
"commutator"
and
the
other
relations
can
be
found
by
taking
cyclic
permutations
of
x
y
z
components
(ie
change
x
to
y
y
to
z
and
z
to
x
repeat)
returning
to
the
group
commutator
the
commutation
relations
of
the
boost
generators
imply
for
a
boost
along
the
x
then
y
directions
there
will
be
a
rotation
about
the
z
axis
in
terms
of
the
rapidities
the
rotation
angle
is
given
by
the
familiar
notion
of
vector
addition
for
velocities
in
the
euclidean
plane
can
be
done
in
a
triangular
formation
or
since
vector
addition
is
commutative
the
vectors
in
both
orderings
geometrically
form
a
parallelogram
(see
"parallelogram
law")
this
does
not
hold
for
relativistic
velocity
addition;
instead
a
hyperbolic
triangle
arises
whose
edges
are
related
to
the
rapidities
of
the
boosts
changing
the
order
of
the
boost
velocities
one
does
not
find
the
resultant
boost
velocities
to
coincide
cabbeling
cabbeling
is
when
two
separate
water
parcels
mix
to
form
a
third
which
sinks
below
both
parents
the
combined
water
parcel
is
denser
than
the
original
two
water
parcels
the
two
parent
water
parcels
may
have
the
same
density
but
they
have
different
properties;
for
instance
different
salinities
and
temperatures
seawater
almost
always
gets
more
dense
if
it
gets
either
slightly
colder
or
slightly
saltier
but
medium-warm
medium-salty
water
can
be
denser
than
both
fresher
colder
water
and
saltier
warmer
water;
in
other
words
the
equation
of
state
for
seawater
is
monotonic
but
non-linear
see
diagram
cabbeling
may
also
occur
in
fresh
water
since
pure
water
is
densest
at
about
4
°c
(39
°f)
a
mixture
of
1 °c
water
and
6 °c
water
for
instance
might
have
a
temperature
of
4 °c
making
it
denser
than
either
parent
ice
is
also
less
dense
than
water
so
although
ice
floats
in
warm
water
meltwater
sinks
in
warm
water
the
densification
of
the
new
mixed
water
parcel
is
a
result
of
a
slight
contraction
upon
mixing;
a
decrease
in
volume
of
the
combined
water
parcel
a
new
water
parcel
that
has
the
same
mass
but
is
lower
in
volume
will
be
denser
denser
water
sinks
or
downwells
in
the
otherwise
neutral
surface
of
the
water
body
where
the
two
initial
water
parcels
originated
the
importance
of
this
process
in
oceanography
was
first
pointed
out
by
witte
in
a
1902
publication
()
the
german
origin
of
the
term
has
caused
some
etymological
confusion
and
disagreements
as
to
the
correct
spelling
of
the
term;
for
details
see
the
wiktionary
entry
on
cabelling
oceanographers
generally
follow
stommel
and
refer
to
the
process
as
"cabbeling"
cabbeling
may
occur
in
high
incidence
in
high
latitude
waters
polar
region
waters
are
a
place
where
cold
and
fresh
water
melting
from
sea
ice
meets
warmer
saltier
water
ocean
currents
are
responsible
for
bringing
this
warmer
saltier
water
to
higher
latitudes
especially
on
the
eastern
shores
of
northern
hemisphere
continents
and
on
the
western
shores
of
southern
hemisphere
continents
the
phenomenon
of
cabbeling
has
been
particularly
noted
in
the
weddell
sea
and
the
greenland
sea
weak
gravity
conjecture
the
weak
gravity
conjecture
(wgc)
is
a
conjecture
regarding
the
strength
gravity
can
have
in
a
theory
of
quantum
gravity
relative
to
the
gauge
forces
in
that
theory
it
roughly
states
that
gravity
should
be
the
weakest
force
in
any
consistent
theory
of
quantum
gravity
solid
light
solid
light
is
a
hypothetical
material
made
of
light
in
a
solidified
state
theoretically
it
is
possible
to
make
such
a
material
and
there
are
claims
this
material
was
already
made
including
claims
from
mit
and
harvard
in
theory
photons
the
particles
that
make
light
could
be
attracted
in
a
nonlinear
medium
in
test
a
laser
beam
fired
into
an
extremely
cold
cloud
of
rubidium
slowed
down
the
photons
and
made
them
act
as
a
single
entity
solid
light
appears
in
many
video
game
franchises
including
"halo"
"portal"
"mass
effect"
and
"overwatch"
it
is
also
portrayed
in
"the
lightbringer"
series
by
fantasy
author
brent
weeks
it
also
appears
in
dr
strange
the
gems
a
fictional
alien
race
featured
in
"steven
universe"
are
gemstones
that
project
physical
bodies
made
of
solid
light
total
position
spread
in
physics
the
total
position-spread
(tps)
tensor
is
a
quantity
originally
introduced
in
the
modern
theory
of
electrical
conductivity
in
the
case
of
molecular
systems
this
tensor
measures
the
fluctuation
of
the
electrons
around
their
mean
positions
which
corresponds
to
the
delocalization
of
the
electronic
charge
within
a
molecular
system
the
tps
can
discriminate
between
metals
and
insulators
taking
information
from
the
ground
state
wave
function
this
quantity
can
be
very
useful
as
an
indicator
to
characterize
intervalence
charge
transfer
processes
the
bond
nature
of
molecules
(covalent
ionic
or
weakly
bonded)
and
metal–insulator
transition
the
localization
tensor
(lt)
is
a
"per
electron"
quantity
proposed
in
the
context
of
the
theory
of
kohn
to
characterize
electrical
conductivity
properties
in
1964
kohn
realized
that
electrical
conductivity
is
more
related
to
the
proper
delocalization
of
the
wave
function
than
a
simple
band
gap
in
fact
he
proposed
that
a
qualitative
difference
between
insulators
and
conductors
also
manifests
as
a
different
organization
of
the
electrons
in
their
ground
state
where
one
has
that:
the
wave
function
is
strongly
localized
in
insulators
and
very
delocalized
in
conductors
the
interesting
outcome
of
this
theory
is:
"i)"
it
relates
the
classical
idea
of
localized
electrons
as
a
cause
of
insulating
state;
"ii)"
the
needed
information
can
be
recovered
from
the
ground
state
wave
function
because
in
the
insulated
regime
the
wave
function
breaks
down
as
a
sum
of
disconnected
terms
it
is
until
1999
that
resta
and
coworkers
found
a
way
to
define
the
kohn
delocalization
by
proposing
the
already
mentioned
localization
tensor
the
lt
is
defined
as
a
second
order
moment
cumulant
of
the
position
operator
divided
by
the
number
of
electrons
in
the
system
the
key
property
of
the
lt
is
that:
it
diverges
for
metals
while
it
takes
finite
values
for
insulators
in
the
thermodynamic
limit
recently
the
global
quantity
(the
lt
not
divided
by
the
number
of
electrons)
has
been
introduced
to
study
molecules
and
named
total
position-spread
tensor
the
total
position
spread
λ
is
defined
as
the
second
moment
cumulant
of
the
total
electron
position
operator
and
its
units
are
in
length
square
(eg
bohr²)
in
order
to
compute
this
quantity
one
has
to
take
into
account
the
position
operator
and
its
tensorial
square
for
a
system
of
"n"
electrons
the
position
operator
and
its
cartesian
components
are
defined
as:
where
the
"i"
index
runs
over
the
number
of
electrons
each
component
of
the
position
operator
is
a
one-electron
operator
they
can
be
represented
in
second
quantization
as
follows:
where
"i""j"
run
over
orbitals
the
expectation
values
of
the
position
components
are
the
first
moments
of
the
electrons'
position
now
we
consider
the
tensorial
square
(second
moment)
in
this
sense
there
are
two
types
of
them:
the
second
moment
of
the
position
becomes
then
the
sum
of
the
one-
and
two-electron
operators
already
defined:
given
a
"n"-electron
wave
function
formula_10
one
wants
to
compute
the
"second
moment
cumulant"
of
it
a
cumulant
is
a
linear
combination
of
moments
so
we
have:
the
position
operator
can
be
partitioned
according
to
spin
components
from
the
one-particle
operator
it
is
possible
to
define
the
total
spin-partitioned
position
operator
as:
therefore
the
total
position
operator
formula_14
can
be
expressed
by
the
sum
of
the
two
spin
parts
formula_15
and
formula_16:
and
the
square
of
the
total
position
operator
decomposes
as:
thus
there
are
four
joint
second
moment
cumulant
of
the
spin-partitioned
position
operator:
the
hubbard
model
is
a
very
simple
and
approximate
model
employed
in
condensed
matter
physics
to
describe
the
transition
of
materials
from
metals
to
insulators
it
takes
into
account
only
two
parameters:
"i)"
the
kinetic
energy
or
hopping
integral
denoted
by
"-t";
and
"ii)"
the
on-site
repulsion
between
electrons
represented
by
"u"
(see
the
)
in
figure
1
there
are
two
limit
cases
to
consider:
larger
values
of
"-t/u"
representing
a
strong
charge
fluctuation
(electrons
free
to
move)
whereas
for
small
values
of
"-t/u"
the
electrons
are
completely
localized
the
ss-tps
is
very
sensitive
to
these
changes
because
it
increases
faster
than
linearly
when
electrons
start
to
present
mobility
(00
to
05
range
of
"-t/u")
the
tps
is
a
powerful
tool
to
monitor
the
wave
function
in
figure
3
is
shown
the
longitudinal
ss-tps
(λ)
computed
at
full
configuration
interaction
level
for
the
h
diatomic
molecule
the
λ
in
the
high
repulsive
region
shows
a
value
that
is
lower
than
in
the
asymptotic
limit
this
is
a
consequence
of
nuclei
being
near
to
each
other's
causing
and
enhancement
of
the
effective
nuclear
charge
that
makes
electrons
to
be
more
localized
when
stretching
the
bond
the
tps
starts
growing
until
it
reaches
a
maximum
(strong
delocalization
of
the
wave
function)
before
the
bond
is
broken
once
the
bond
is
broken
the
wave
function
becomes
a
sum
of
disconnected
localized
regions
and
the
tensor
decreases
until
it
reaches
twice
the
value
of
the
atomic
limit
(1
bohr²
for
each
hydrogen
atom)
when
the
total
position-spread
tensor
is
partitioned
according
to
spin
(sp-tps)
it
becomes
a
powerful
tool
to
describe
spin
delocalization
in
the
insulating
regime
in
figure
4
is
shown
the
longitudinal
sp-tps
(λ)
computed
at
full
configuration
interaction
level
for
the
h
diatomic
molecule
the
horizontal
line
at
0
bohr
divides
the
same
spin
(positive
values)
and
different
spin
(negative
values)
contributions
of
the
spin
partitioned
tps
unlike
the
ss-tps
that
saturates
to
the
atomic
value
for
r>5
the
sp-tps
diverges
as
r
indicating
that
there
is
a
strong
spin
delocalization
the
sp-tps
can
also
be
seen
as
a
measure
of
how
strong
the
electron
correlation
is
the
tps
is
a
cumulant
and
thus
it
possesses
the
following
properties:
dirac
membrane
a
model
of
a
charged
membrane
introduced
by
paul
dirac
in
1962
dirac's
original
motivation
was
to
explain
the
mass
of
the
muon
as
an
excitation
of
the
ground
state
corresponding
to
an
electron
anticipating
the
birth
of
string
theory
by
almost
a
decade
he
was
the
first
to
introduce
what
is
now
called
a
type
of
nambu–goto
action
for
membranes
in
the
dirac
membrane
model
the
repulsive
electromagnetic
forces
on
the
membrane
are
balanced
by
the
contracting
ones
coming
from
the
positive
tension
in
the
case
of
the
spherical
membrane
classical
equations
of
motion
imply
that
the
balance
is
met
for
the
radius
formula_1
where
formula_2
is
the
classical
electron
radius
using
bohr–sommerfeld
quantisation
condition
for
the
hamiltonian
of
the
spherically
symmetric
membrane
dirac
finds
the
approximation
of
the
mass
corresponding
to
the
first
excitation
as
formula_3
where
formula_4
is
the
mass
of
the
electron
which
is
about
a
quarter
of
the
observed
muon
mass
dirac
chose
a
non-standard
way
to
formulate
the
action
principle
for
the
membrane
because
closed
membranes
in
formula_5
provide
a
natural
split
of
space
into
the
interior
and
the
exterior
there
exists
a
special
curvilinear
system
of
coordinates
formula_6
in
spacetime
and
a
function
formula_7
such
that
-
formula_8
defines
a
membrane
-
formula_9
formula_10
describe
a
region
outside
or
inside
the
membrane
choosing
formula_11
and
the
following
gauge
formula_12
formula_13
formula_14
where
formula_15
(
formula_16)
is
the
internal
parametrization
of
the
membrane
word-volume
the
membrane
action
proposed
by
dirac
is
where
the
induced
metric
and
the
factors
j
and
m
are
given
by
in
the
above
formula_21
are
rectilinear
and
orthogonal
the
space-time
signature
used
is
(+---)
note
that
formula_22
is
just
a
usual
action
for
the
electromagnetic
field
in
a
curvilinear
system
while
formula_23is
the
integral
over
the
membrane
world-volume
ie
precisely
the
type
of
the
action
used
later
in
string
theory
there
are
3
equations
of
motion
following
from
the
variation
with
respect
to
formula_24
and
formula_25
they
are:
-
variation
wrt
formula_24
for
formula_27
-
this
results
in
sourceless
maxwell
equations
-
variation
wrt
formula_21
for
formula_27
-
this
gives
a
consequence
of
maxwell
equations
-
variation
wrt
formula_21
for
formula_31
the
last
equation
has
a
geometric
interpretation:
the
rhs
is
proportional
to
the
curvature
of
the
membrane
for
the
spherically
symmetric
case
we
get
therefore
the
balance
condition
formula_34
implies
formula_35
where
formula_36
is
the
radius
of
the
balanced
membrane
the
total
energy
for
the
spherical
membrane
with
radius
formula_37
is
and
it
is
minimal
in
the
equilibrium
for
formula_39
hence
formula_40
on
the
other
hand
the
total
energy
in
the
equilibrium
should
be
formula_4
(in
formula_42
units)
and
so
we
obtain
formula_43
small
oscillations
about
the
equilibrium
in
the
spherically
symmetric
case
imply
frequencies
-
formula_44
therefore
going
to
quantum
theory
the
energy
of
one
quantum
would
be
formula_45
this
is
much
more
than
the
muon
mass
but
the
frequencies
are
by
no
means
small
so
this
approximation
may
not
work
properly
to
get
a
better
quantum
theory
one
needs
to
work
out
the
hamiltonian
of
the
system
and
solve
the
corresponding
schroedinger
equation
for
the
hamiltonian
formulation
dirac
introduces
generalised
momenta
-
for
formula_46:
formula_47
and
formula_48
-
momenta
conjugate
to
formula_24
and
formula_50
respectively
(formula_51
coordinate
choice
formula_52)
-
for
formula_53:
formula_54
-
momenta
conjugate
to
formula_50
then
one
notices
the
following
constraints
-
for
the
maxwell
field
-
for
membrane
momenta
where
formula_58
-
reciprocal
of
formula_59
formula_60
these
constraints
need
to
be
included
when
calculating
the
hamiltonian
using
the
dirac
bracket
method
the
result
of
this
calculation
is
the
hamiltonian
of
the
form
where
formula_63
is
the
hamiltonian
for
the
electromagnetic
field
written
in
the
curvilinear
system
for
spherically
symmetric
motion
the
hamiltonian
is
however
the
direct
quantisation
is
not
clear
due
to
the
square-root
of
the
differential
operator
to
get
any
further
dirac
considers
the
bohr
-
sommerfeld
method:
and
finds
formula_66
for
formula_67
p
a
m
dirac
an
extensible
model
of
the
electron
proc
roy
soc
a268
(1962)
57–67
hopkinson
effect
the
hopkinson
effect
is
a
feature
of
ferromagnetic
or
ferrimagnetic
materials
in
which
an
increase
in
magnetic
susceptibility
is
observed
at
temperatures
between
the
blocking
temperature
and
the
curie
temperature
of
the
material
the
hopkinson
effect
can
be
observed
as
a
peak
in
thermomagnetic
curves
that
immediately
precedes
the
susceptibility
drop
associated
with
the
curie
temperature
it
was
first
observed
by
john
hopkinson
in
1889
in
a
study
on
iron
in
single
domain
particles
a
large
hopkinson
peak
results
from
a
transient
superparamagnetic
particle
domain
state
rogue
wave
rogue
waves
(also
known
as
freak
waves
monster
waves
episodic
waves
killer
waves
extreme
waves
and
abnormal
waves)
are
large
unexpected
and
suddenly
appearing
surface
waves
that
can
be
extremely
dangerous
even
to
large
ships
such
as
ocean
liners
rogue
waves
present
considerable
danger
for
several
reasons:
they
are
rare
unpredictable
may
appear
suddenly
or
without
warning
and
can
impact
with
tremendous
force
a
wave
in
the
usual
"linear"
wave
model
would
have
a
breaking
pressure
of
although
modern
ships
are
designed
to
tolerate
a
breaking
wave
of
a
rogue
wave
can
dwarf
both
of
these
figures
with
a
breaking
pressure
of
in
oceanography
rogue
waves
are
more
precisely
defined
as
waves
whose
height
is
more
than
twice
the
significant
wave
height
("h"
or
swh)
which
is
itself
defined
as
the
mean
of
the
largest
third
of
waves
in
a
wave
record
therefore
rogue
waves
are
not
necessarily
the
biggest
waves
found
on
the
water;
they
are
rather
unusually
large
waves
for
a
given
sea
state
rogue
waves
seem
not
to
have
a
single
distinct
cause
but
occur
where
physical
factors
such
as
high
winds
and
strong
currents
cause
waves
to
merge
to
create
a
single
exceptionally
large
wave
rogue
waves
can
occur
in
media
other
than
water
they
appear
to
be
ubiquitous
in
nature
and
have
also
been
reported
in
liquid
helium
in
nonlinear
optics
and
in
microwave
cavities
recent
research
has
focused
on
optical
rogue
waves
which
facilitate
the
study
of
the
phenomenon
in
the
laboratory
a
2015
paper
studied
the
wave
behavior
around
a
rogue
wave
including
optical
and
the
draupner
wave
and
concluded
that
"rogue
events
do
not
necessarily
appear
without
a
warning
but
are
often
preceded
by
a
short
phase
of
relative
order"
the
physical
origin
of
a
rogue
wave
can
be
defined
by
the
rogue
wave
theorem
rogue
waves
are
an
open
water
phenomenon
in
which
winds
currents
non-linear
phenomena
such
as
solitons
and
other
circumstances
cause
a
wave
to
briefly
form
that
is
far
larger
than
the
"average"
large
occurring
wave
(the
significant
wave
height
or
'swh')
of
that
time
and
place
the
basic
underlying
physics
that
makes
phenomena
such
as
rogue
waves
possible
is
that
different
waves
can
travel
at
different
speeds
and
so
they
can
"pile
up"
in
certain
circumstances
–
known
as
"constructive
interference"
(in
deep
ocean
the
speed
of
a
gravity
wave
is
proportional
to
the
square
root
of
its
wavelength—the
distance
peak-to-peak)
however
other
situations
can
also
give
rise
to
rogue
waves
particularly
situations
where
non-linear
effects
or
instability
effects
can
cause
energy
to
move
between
waves
and
be
concentrated
in
one
or
very
few
extremely
large
waves
before
returning
to
"normal"
conditions
once
considered
mythical
and
lacking
hard
evidence
for
their
existence
rogue
waves
are
now
proven
to
exist
and
known
to
be
a
natural
ocean
phenomenon
eyewitness
accounts
from
mariners
and
damage
inflicted
on
ships
have
long
suggested
they
occurred
the
first
scientific
evidence
of
the
existence
of
rogue
waves
came
with
the
recording
of
a
rogue
wave
by
the
gorm
platform
in
the
central
north
sea
in
1984
a
stand-out
wave
was
detected
with
a
wave
height
of
in
a
relatively
low
sea
state
however
the
wave
that
caught
the
attention
of
the
scientific
community
was
the
digital
measurement
of
the
"draupner
wave"
a
rogue
wave
at
the
draupner
platform
in
the
north
sea
on
january
1
1995
with
a
maximum
wave
height
of
(peak
elevation
of
)
during
that
event
minor
damage
was
also
inflicted
on
the
platform
far
above
sea
level
confirming
that
the
reading
was
valid
their
existence
has
also
since
been
confirmed
by
video
and
photographs
and
satellite
imagery
and
radar
of
the
ocean
surface
by
stereo
wave
imaging
systems
by
pressure
transducers
on
the
sea-floor
and
notably
by
oceanographic
research
vessels
in
february
2000
a
british
oceanographic
research
vessel
the
rrs
"discovery"
sailing
in
the
rockall
trough
west
of
scotland
encountered
the
largest
waves
ever
recorded
by
scientific
instruments
in
the
open
ocean
with
a
swh
of
and
individual
waves
up
to
"in
2004
scientists
using
three
weeks
of
radar
images
from
european
space
agency
satellites
found
ten
rogue
waves
each
or
higher"
a
rogue
wave
is
a
natural
ocean
phenomenon
that
is
not
caused
by
land
movement
only
lasts
briefly
occurs
in
a
limited
location
and
most
often
happens
far
out
at
sea
rogue
waves
are
considered
rare
but
potentially
very
dangerous
since
they
can
involve
the
spontaneous
formation
of
massive
waves
far
beyond
the
usual
expectations
of
ship
designers
and
can
overwhelm
the
usual
capabilities
of
ocean-going
vessels
which
are
not
designed
for
such
encounters
rogue
waves
are
therefore
distinct
from
tsunamis
tsunamis
are
caused
by
massive
displacement
of
water
often
resulting
from
sudden
movement
of
the
ocean
floor
after
which
they
propagate
at
high
speed
over
a
wide
area
they
are
nearly
unnoticeable
in
deep
water
and
only
become
dangerous
as
they
approach
the
shoreline
and
the
ocean
floor
becomes
shallower;
therefore
tsunamis
do
not
present
a
threat
to
shipping
at
sea
(the
only
ships
lost
in
the
2004
asian
tsunami
were
in
port)
they
are
also
distinct
from
megatsunamis
which
are
single
massive
waves
caused
by
sudden
impact
such
as
meteor
impact
or
landslides
within
enclosed
or
limited
bodies
of
water
they
are
also
different
from
the
waves
described
as
"hundred-year
waves"
which
is
a
purely
statistical
prediction
of
the
highest
wave
likely
to
occur
in
a
hundred-year
period
in
a
particular
body
of
water
rogue
waves
have
now
been
proven
to
be
the
cause
of
the
sudden
loss
of
some
ocean-going
vessels
well
documented
instances
include
the
freighter
ms
"münchen"
lost
in
1978
and
the
mv
derbyshire
lost
in
1980
the
largest
british
ship
ever
lost
at
sea
a
rogue
wave
has
been
implicated
in
the
loss
of
other
vessels
including
the
ocean
ranger
which
was
a
semi-submersible
mobile
offshore
drilling
unit
that
sank
in
canadian
waters
on
15
february
1982
in
2007
the
us
national
oceanic
and
atmospheric
administration
compiled
a
catalogue
of
more
than
50
historical
incidents
probably
associated
with
rogue
waves
in
1826
french
scientist
and
naval
officer
captain
jules
dumont
d'urville
reported
waves
as
high
as
in
the
indian
ocean
with
three
colleagues
as
witnesses
yet
he
was
publicly
ridiculed
by
fellow
scientist
françois
arago
in
that
era
it
was
widely
held
that
no
wave
could
exceed
author
susan
casey
wrote
that
much
of
that
disbelief
came
because
there
were
very
few
people
who
had
seen
a
rogue
wave
and
until
the
advent
of
steel
double-hulled
ships
of
the
20th
century
"people
who
encountered
100-foot
rogue
waves
generally
weren't
coming
back
to
tell
people
about
it"
for
almost
100
years
oceanographers
meteorologists
engineers
and
ship
designers
have
used
a
mathematical
system
commonly
called
the
gaussian
function
(or
gaussian
sea
or
standard
linear
model)
to
predict
wave
height
this
model
assumes
that
waves
vary
in
a
regular
way
around
the
average
(so-called
'significant')
wave
height
in
a
storm
sea
with
a
significant
wave
height
of
the
model
suggests
there
will
hardly
ever
be
a
wave
higher
than
one
of
could
indeed
happen
–
but
only
once
in
ten
thousand
years
(of
wave
height
of
)
this
basic
assumption
was
well
accepted
(and
acknowledged
to
be
an
approximation)
the
use
of
a
gaussian
form
to
model
waves
has
been
the
sole
basis
of
virtually
every
text
on
that
topic
for
the
past
100
years
the
first
known
scientific
article
on
"freak
waves"
was
written
by
professor
laurence
draper
in
1964
in
that
paper
which
has
been
described
as
a
'seminal
article'
he
documented
the
efforts
of
the
national
institute
of
oceanography
in
the
early
1960s
to
record
wave
height
and
the
highest
wave
recorded
at
that
time
which
was
about
draper
also
described
"freak
wave
holes"
in
1995
strong
scientific
evidence
for
the
existence
of
rogue
waves
came
with
the
recording
of
what
has
become
known
as
the
draupner
wave
the
draupner
e
is
one
structure
in
a
gas
pipeline
support
complex
operated
by
statoil
about
offshore
and
west
by
southwest
from
the
southern
tip
of
norway
the
draupner
e
platform
is
the
first
major
oil
platform
of
the
jacket-type
attached
to
the
seabed
with
a
bucket
foundation
instead
of
pilings
and
a
suction
anchoring
system
as
a
precaution
the
operator
(statoil)
fitted
the
platform
with
an
extensive
array
of
instrumentation
the
instruments
continuously
check
the
platform's
movements
in
particular
any
movement
of
the
foundations
during
storm
events
the
state-of-the-art
instrumentation
fitted
to
the
platform
was
able
to
continuously
measure
seven
key
parameters:
the
rig
was
built
to
withstand
a
calculated
1
in
10000
years
wave
with
a
predicted
height
of
and
was
also
fitted
with
state-of-the-art
laser
wave
recorder
on
the
platform’s
underside
at
3
pm
on
1
january
1995
it
recorded
an
rogue
wave
(ie
taller
than
the
predicted
10000
year
wave)
that
hit
the
rig
at
this
was
the
first
confirmed
measurement
of
a
freak
wave
more
than
twice
as
tall
and
steep
as
its
neighbors
with
characteristics
that
fell
outside
any
known
wave
model
the
wave
was
recorded
by
all
of
the
sensors
fitted
to
the
platform
and
it
caused
enormous
interest
in
the
scientific
community
statoil
researchers
presented
a
paper
in
2000
which
collated
evidence
that
freak
waves
were
not
the
rare
realizations
of
a
typical
or
slightly
non-gaussian
sea
surface
population
("classical"
extreme
waves)
but
rather
they
were
the
typical
realizations
of
a
rare
and
strongly
non-gaussian
sea
surface
population
of
waves
("freak"
extreme
waves)
a
workshop
of
leading
researchers
in
the
world
attended
the
first
rogue
waves
2000
workshop
held
in
brest
in
november
2000
in
2000
the
british
oceanographic
vessel
rrs
"discovery"
recorded
a
wave
off
the
coast
of
scotland
near
rockall
this
was
a
scientific
research
vessel
and
was
fitted
with
high
quality
instruments
the
subsequent
analysis
determined
that
under
severe
gale
force
conditions
with
wind
speeds
averaging
a
ship-borne
wave
recorder
measured
individual
waves
up
to
from
crest
to
trough
and
a
maximum
significant
wave
height
of
these
were
some
of
the
largest
waves
recorded
by
scientific
instruments
up
to
that
time
the
authors
noted
that
modern
wave
prediction
models
are
"known"
to
significantly
under-predict
extreme
sea
states
for
waves
with
a
'significant'
height
(h)
above
the
analysis
of
this
event
took
a
number
of
years
and
noted
that
"none
of
the
state-of-the-art
weather
forecasts
and
wave
models—the
information
upon
which
all
ships
oil
rigs
fisheries
and
passenger
boats
rely—had
predicted
these
behemoths"
put
simply
a
scientific
model
(and
also
ship
design
method)
to
describe
the
waves
encountered
did
not
exist
this
finding
was
widely
reported
in
the
press
which
reported
that
"according
to
all
of
the
theoretical
models
at
the
time
under
this
particular
set
of
weather
conditions
waves
of
this
size
should
not
have
existed"
most
popular
texts
on
oceanography
up
until
the
mid
1990s
such
as
that
by
pirie
made
no
mention
of
rogue
or
freak
waves
the
popular
text
on
oceanography
by
gross
(1996)
only
gave
rogue
waves
a
mention
and
stated
that
"under
extraordinary
circumstances
unusually
large
waves
called
rogue
waves
can
form"
without
providing
any
further
detail
from
about
1997
most
leading
authors
acknowledged
the
existence
of
rogue
waves
with
the
caveat
that
wave
models
had
been
unable
to
replicate
rogue
waves
the
first
scientific
research
which
comprehensively
proved
that
waves
exist
that
are
clearly
outside
the
range
of
gaussian
waves
was
published
in
1997
some
research
confirms
that
observed
wave
height
distribution
in
general
follows
well
the
rayleigh
distribution
but
in
shallow
waters
during
high
energy
events
extremely
high
waves
are
more
rare
than
this
particular
model
predicts
it
is
now
proven
via
satellite
radar
studies
that
waves
with
crest
to
trough
heights
of
to
occur
far
more
frequently
than
previously
thought
it
is
now
known
that
rogue
waves
occur
in
all
of
the
world's
oceans
many
times
each
day
in
2004
the
esa
maxwave
project
identified
more
than
ten
individual
giant
waves
above
in
height
during
a
short
survey
period
of
three
weeks
in
a
limited
area
of
the
south
atlantic
the
esa's
ers
satellites
have
helped
to
establish
the
widespread
existence
of
these
'rogue'
waves
thus
acknowledgement
of
the
existence
of
rogue
waves
(despite
the
fact
that
they
cannot
plausibly
be
explained
by
even
state-of-the-art
wave
statistics)
is
a
very
modern
scientific
paradigm
it
is
now
well
accepted
that
rogue
waves
are
a
common
phenomenon
professor
akhmediev
of
the
australian
national
university
one
of
the
world's
leading
researchers
in
this
field
has
stated
that
there
are
about
10
rogue
waves
in
the
world's
oceans
at
any
moment
some
researchers
have
speculated
that
approximately
three
of
every
10000
waves
on
the
oceans
achieve
rogue
status
yet
in
certain
spots—like
coastal
inlets
and
river
mouths—these
extreme
waves
can
make
up
three
out
of
every
1000
waves
because
wave
energy
can
be
focused
rogue
waves
may
also
occur
in
lakes
a
phenomenon
known
as
the
"three
sisters"
is
said
to
occur
in
lake
superior
when
a
series
of
three
large
waves
forms
the
second
wave
hits
the
ship's
deck
before
the
first
wave
clears
the
third
incoming
wave
adds
to
the
two
accumulated
backwashes
and
suddenly
overloads
the
ship
deck
with
tons
of
water
the
phenomenon
is
one
of
various
theories
as
to
the
cause
of
the
sinking
of
the
ss
edmund
fitzgerald
on
lake
superior
in
november
1975
serious
studies
of
the
phenomenon
of
rogue
waves
only
started
about
20–30
years
ago
and
have
intensified
since
about
2005
one
of
the
remarkable
features
of
the
rogue
waves
is
that
they
always
appear
from
nowhere
and
quickly
disappear
without
a
trace
recent
research
has
suggested
that
there
could
also
be
'super-rogue
waves'
which
are
up
to
five
times
the
average
sea-state
rogue
waves
has
now
become
a
near
universal
term
given
by
scientists
to
describe
isolated
large
amplitude
waves
that
occur
more
frequently
than
expected
for
normal
gaussian
distributed
statistical
events
rogue
waves
appear
to
be
ubiquitous
in
nature
and
are
not
limited
to
the
oceans
they
appear
in
other
contexts
and
have
recently
been
reported
in
liquid
helium
in
nonlinear
optics
and
in
microwave
cavities
it
is
now
universally
accepted
by
marine
researchers
that
these
waves
belong
to
a
specific
kind
of
sea
wave
not
taken
into
account
by
conventional
models
for
sea
wind
waves
researchers
at
the
australian
national
university
have
also
recently
(2012)
proven
the
existence
of
"rogue
wave
holes"
an
inverted
profile
of
a
rogue
wave
in
maritime
folk-lore
stories
of
rogue
holes
are
as
common
as
stories
of
rogue
waves
they
follow
from
theoretical
analysis
but
had
never
been
proven
experimentally
in
2012
the
anu
published
research
confirming
the
existence
of
rogue
wave
holes
on
the
water
surface
observed
in
a
water
wave
tank
on
a
smaller
scale
kayakers
call
unpredictable
'exploding
waves'
caused
by
wave
interaction
"clapotis"
there
are
a
number
of
research
programmes
currently
underway
focussed
on
rogue
waves
including:
because
the
phenomenon
of
rogue
waves
is
still
a
matter
of
active
research
it
is
premature
to
state
clearly
what
the
most
common
causes
are
or
whether
they
vary
from
place
to
place
the
areas
of
highest
predictable
risk
appear
to
be
where
a
strong
current
runs
counter
to
the
primary
direction
of
travel
of
the
waves;
the
area
near
cape
agulhas
off
the
southern
tip
of
africa
is
one
such
area;
the
warm
agulhas
current
runs
to
the
southwest
while
the
dominant
winds
are
westerlies
however
since
this
thesis
does
not
explain
the
existence
of
all
waves
that
have
been
detected
several
different
mechanisms
are
likely
with
localized
variation
suggested
mechanisms
for
freak
waves
include
the
following:
the
spatio-temporal
focusing
seen
in
the
nls
equation
can
also
occur
when
the
nonlinearity
is
removed
in
this
case
focusing
is
primarily
due
to
different
waves
coming
into
phase
rather
than
any
energy
transfer
processes
further
analysis
of
rogue
waves
using
a
fully
nonlinear
model
by
r
h
gibbs
(2005)
brings
this
mode
into
question
as
it
is
shown
that
a
typical
wavegroup
focuses
in
such
a
way
as
to
produce
a
significant
wall
of
water
at
the
cost
of
a
reduced
height
a
rogue
wave
and
the
deep
trough
commonly
seen
before
and
after
it
may
last
only
for
some
minutes
before
either
breaking
or
reducing
in
size
again
apart
from
one
single
rogue
wave
the
rogue
wave
may
be
part
of
a
wave
packet
consisting
of
a
few
rogue
waves
such
rogue
wave
groups
have
been
observed
in
nature
there
are
three
categories
of
freak
waves:
the
possibility
of
the
artificial
stimulation
of
rogue
wave
phenomena
has
attracted
research
funding
from
darpa
an
agency
of
the
united
states
department
of
defense
bahram
jalali
and
other
researchers
at
ucla
studied
microstructured
optical
fibers
near
the
threshold
of
soliton
supercontinuum
generation
and
observed
rogue
wave
phenomena
after
modelling
the
effect
the
researchers
announced
that
they
had
successfully
characterized
the
proper
initial
conditions
for
generating
rogue
waves
in
any
medium
additional
works
carried
out
in
optics
have
pointed
out
the
role
played
by
a
nonlinear
structure
called
peregrine
soliton
that
may
explain
those
waves
that
appear
and
disappear
without
leaving
a
trace
it
should
be
noted
that
many
of
these
encounters
are
only
reported
in
the
media
and
are
not
examples
of
open
ocean
rogue
waves
often
in
popular
culture
an
endangering
huge
wave
is
loosely
denoted
as
a
"rogue
wave"
while
it
has
not
been
(and
most
often
cannot
be)
established
that
the
reported
event
is
a
rogue
wave
in
the
scientific
sense
—
"ie"
of
a
very
different
nature
in
characteristics
as
the
surrounding
waves
in
that
sea
state
and
with
very
low
probability
of
occurrence
(according
to
a
gaussian
process
description
as
valid
for
linear
wave
theory)
this
section
lists
a
limited
selection
of
notable
incidents
the
loss
of
the
ms
münchen
in
1978
provided
some
of
the
first
physical
evidence
of
the
existence
of
rogue
waves
the
ms
münchen
was
a
state-of-the-art
cargo
ship
with
multiple
water-tight
compartments
an
expert
crew
and
was
considered
unsinkable
she
was
lost
with
all
crew
and
the
wreck
has
never
been
found
the
only
evidence
found
was
the
starboard
lifeboat
which
was
recovered
from
floating
wreckage
some
time
later
the
lifeboats
hung
from
forward
and
aft
blocks
above
the
waterline
the
pins
had
been
bent
back
from
forward
to
aft
indicating
the
lifeboat
hanging
below
it
had
been
struck
by
a
wave
that
had
run
from
fore
to
aft
of
the
ship
which
had
torn
the
lifeboat
from
the
ship
to
exert
such
force
the
wave
must
have
been
considerably
higher
than
at
the
time
of
the
inquiry
the
existence
of
rogue
waves
was
considered
so
statistically
unlikely
as
to
be
near
impossible
consequently
the
maritime
court
investigation
concluded
that
the
severe
weather
had
somehow
created
an
'unusual
event'
that
had
led
to
the
sinking
of
the
münchen
the
1980
loss
of
the
mv
"derbyshire"
during
typhoon
orchid
south
of
japan
with
the
loss
of
all
crew
marked
a
turning
point
for
ship
design
the
"derbyshire"
was
an
ore-bulk-oil
combination
carrier
built
in
1976
at
91655
gross
register
tons
she
was—and
remains—the
largest
british
ship
ever
to
have
been
lost
at
sea
the
wreck
was
found
in
june
1994
the
survey
team
deployed
a
remotely
operated
vehicle
to
photograph
the
wreck
a
private
report
was
published
in
1998
which
prompted
the
british
government
to
reopen
a
formal
investigation
into
the
sinking
the
british
government
investigation
included
a
comprehensive
survey
by
the
woods
hole
oceanographic
institution
which
took
135774
pictures
of
the
wreck
during
two
surveys
the
formal
forensic
investigation
concluded
that
the
ship
sank
because
of
structural
failure
and
absolved
the
crew
of
any
responsibility
most
notably
the
report
determined
the
detailed
sequence
of
events
that
led
to
the
structural
failure
of
the
vessel
a
third
comprehensive
analysis
was
subsequently
done
by
douglas
faulkner
professor
of
marine
architecture
and
ocean
engineering
at
the
university
of
glasgow
his
highly
analytical
and
scientific
report
published
in
2001
examined
and
linked
the
loss
of
the
mv
"derbyshire"
with
what
he
called
the
emerging
body
of
scientific
evidence
regarding
the
mechanics
of
freak
waves
professor
faulkner
concluded
that
it
was
almost
certain
that
"derbyshire"
would
have
encountered
a
wave
of
sufficient
size
to
destroy
her
faulkner's
conclusions
have
not
been
refuted
in
the
more
than
15
years
since
they
were
first
presented
(as
of
2016)
indeed
subsequent
analysis
by
others
has
corroborated
his
findings
faulkner's
finding
that
the
"derbyshire"
was
lost
because
of
a
rogue
wave
has
had
widespread
implications
on
ship
design
faulkner
has
subsequently
proposed
the
need
for
a
paradigm
shift
in
thinking
for
the
design
of
ships
and
offshore
installations
to
include
what
he
calls
a
survival
design
approach
additional
to
current
design
requirements
there
is
however
no
evidence
that
his
recommendations
have
yet
been
adopted
(as
of
2016)
in
2004
an
extreme
wave
was
recorded
impacting
the
admiralty
breakwater
alderney
in
the
channel
islands
this
breakwater
is
exposed
to
the
atlantic
ocean
the
peak
pressure
recorded
by
a
shore-mounted
transducer
was
745
kpa
which
corresponds
to
a
pressure
of
745
tonnes/m2
or
745
mt/m
(metric
tonnes
per
square
metre)
this
pressure
far
exceeds
almost
any
design
criteria
for
modern
ships
and
this
wave
would
have
destroyed
almost
any
merchant
vessel
more
recent
work
by
smith
in
2007
confirmed
prior
forensic
work
by
faulkner
in
1998
and
determined
that
the
mv
"derbyshire"
was
exposed
to
a
hydrostatic
pressure
of
a
"static
head"
of
water
of
about
with
a
resultant
static
pressure
of
201 kn/m
this
is
in
effect
20
metres
of
green
water
(possibly
a
"super
rogue
wave")
flowing
over
the
vessel
the
deck
cargo
hatches
on
the
"derbyshire"
were
determined
to
be
the
key
point
of
failure
when
the
rogue
wave
washed
over
the
ship
the
design
of
the
hatches
only
allowed
for
a
static
pressure
of
less
than
two
metres
of
water
or
171 kn/m
in
other
words
the
typhoon
load
on
the
hatches
was
more
than
ten
(10)
times
the
design
load
the
forensic
structural
analysis
of
the
wreck
of
the
"derbyshire"
is
now
widely
regarded
as
irrefutable
in
addition
fast
moving
waves
are
now
known
to
"also"
exert
extremely
high
dynamic
pressure
it
is
known
that
plunging
or
breaking
waves
can
cause
short-lived
impulse
pressure
spikes
called
gifle
peaks
these
can
reach
pressures
of
200 kn/m
(or
more)
for
milliseconds
which
is
sufficient
pressure
to
lead
to
brittle
fracture
of
mild
steel
evidence
of
failure
by
this
mechanism
was
also
found
on
the
"derbyshire"
smith
has
documented
scenarios
where
hydrodynamic
pressure
of
up
to
5650 kn/m
or
over
500
metric
tonnes
per
square
metre
could
occur
very
few
ship-wrecks
have
ever
been
fully
investigated
the
most
recent
bulk-carrier
loss
on
the
open
seas
to
have
been
subjected
to
thorough
investigation
(as
at
march
2011)
was
the
uk-owned
mv
"derbyshire"
which
sank
in
1980
its
entire
crew
of
forty-four
all
british
citizens
perished
it
took
14
years
of
pressure
from
the
british
public
and
a
privately
funded
expedition
to
locate
the
wreck
before
a
formal
remote-camera
search
and
investigation
was
done
by
the
british
government
at
least
a
couple
of
hundred
bulk
carriers
have
been
lost
since
1980
and
none
have
been
properly
investigated
a
survey
of
125
bulk
carriers
that
sank
between
1963
and
1996
found
that
seventy-six
"probably"
flooded
another
four
because
of
hatch-cover
failure
the
rest
from
"unidentified"
causes
nine
other
vessels
broke
completely
in
two
causes
of
the
remaining
forty
losses
are
unknown
montgomery-swan
has
outlined
the
generic
mechanism
of
ship
failure
when
encountering
a
rogue
wave:
the
scenario
is
very
simple:
the
weight
of
the
ship
accelerates
her
down
the
back
slope
of
the
previous
wave
the
bow
sticks
into
the
lower
part
of
the
front
of
the
giant
incoming
wave
and
thousands
of
tons
of
green
water
fall
onto
the
fore
part
of
the
ship
what
happens
next
depends
on
the
structure
of
the
vessel
professor
faulkner
who
did
the
forensic
independent
analysis
of
the
loss
of
the
mv
"derbyshire"
explains
why
this
is
such
a
problem
for
bulk
carriers
he
states
that
"it
is
quite
possible
that
some
of
the
many
unexplained
heavy
weather
losses
(of
bulk
carriers)
may
have
been
caused
by
hatch
cover
or
coaming
failures
because
fore
end
plunging
due
to
flooding
of
large
holds
can
be
rapid"
he
noted
in
his
report
that
"because
of
their
high
inertias
and
natural
pitch
periods
these
large
ships
do
not
rise
to
the
waves
as
appropriately
experienced
masters
have
confirmed
they
tend
to
bury
into
them"
faulkner
concluded
that
"beyond
any
reasonable
doubt
the
direct
cause
of
the
loss
of
the
mv
"derbyshire"
was
the
quite
inadequate
strength
of
her
cargo
hatch
covers
to
withstand
the
forces
of
typhoon
orchid"
he
also
noted
that
"it
is
not
possible
to
say
which
of
the
eighteen
covers
failed
first
or
from
which
direction
the
waves
came;
but
evidence
and
other
arguments
suggest
that
the
no
1
hatch
covers
were
probably
the
first
to
yield
probably
from
waves
over
the
bow
with
the
ship
hove-to"
in
november
1997
the
international
maritime
organization
(imo)
adopted
new
rules
covering
survivability
and
structural
requirements
for
bulk
carriers
of
and
upwards
the
bulkhead
and
double
bottom
must
be
strong
enough
to
allow
the
ship
to
survive
flooding
in
hold
one
unless
loading
is
restricted
it
is
now
widely
held
that
rogue
waves
present
considerable
danger
for
several
reasons:
they
are
rare
unpredictable
may
appear
suddenly
or
without
warning
and
can
impact
with
tremendous
force
a
wave
in
the
usual
"linear"
model
would
have
a
breaking
force
of
although
modern
ships
are
designed
to
(typically)
tolerate
a
breaking
wave
of
15
mt/m
a
rogue
wave
can
dwarf
both
of
these
figures
with
a
breaking
force
far
exceeding
100
mt/m
smith
has
presented
calculations
using
the
international
association
of
classification
societies
(iacs)
common
structural
rules
(csr)
for
a
typical
bulk
carrier
which
are
consistent
peter
challenor
a
leading
scientist
in
this
field
from
the
national
oceanography
centre
in
the
united
kingdom
was
quoted
in
casey's
book
in
2010
that
"we
don’t
have
that
random
messy
theory
for
nonlinear
waves
at
all"
he
says
"people
have
been
working
actively
on
this
for
the
past
50
years
at
least
we
don’t
even
have
the
start
of
a
theory"
in
2006
smith
proposed
that
the
international
association
of
classification
societies
(iacs)
recommendation
34
pertaining
to
standard
wave
data
be
modified
so
that
the
minimum
design
wave
height
be
increased
to
he
presented
analysis
that
there
was
sufficient
evidence
to
conclude
that
high
waves
can
be
experienced
in
the
25-year
lifetime
of
oceangoing
vessels
and
that
high
waves
are
less
likely
but
not
out
of
the
question
therefore
a
design
criterion
based
on
high
waves
seems
inadequate
when
the
risk
of
losing
crew
and
cargo
is
considered
smith
has
also
proposed
that
the
dynamic
force
of
wave
impacts
should
be
included
in
the
structural
analysis
it
is
notable
that
the
norwegian
offshore
standards
now
take
into
account
extreme
severe
wave
conditions
and
require
that
a
10000-year
wave
does
not
endanger
the
ships
integrity
rosenthal
notes
that
as
at
2005
rogue
waves
were
not
explicitly
accounted
for
in
classification
societies’
rules
for
ships’
design
as
an
example
dnv
gl
one
of
the
world's
largest
international
certification
body
and
classification
society
with
main
expertise
in
technical
assessment
advisory
and
risk
management
publishes
their
structure
design
load
principles
which
remain
largely
based
on
the
'significant
wave
height'
and
as
at
january
2016
still
has
not
included
any
allowance
for
rogue
waves
the
us
navy
historically
took
the
design
position
that
the
largest
wave
likely
to
be
encountered
was
214 m
(70 ft)
smith
observed
in
2007
that
the
navy
now
believes
that
larger
waves
can
occur
and
the
possibility
of
extreme
waves
that
are
steeper
(ie
do
not
have
longer
wavelengths)
is
now
recognized
the
navy
has
not
had
to
make
any
fundamental
changes
in
ship
design
as
a
consequence
of
new
knowledge
of
waves
greater
than
214 m
(70 ft)
because
they
build
to
higher
standards
a
characteristic
of
the
shipping
industry
is
that
there
are
no
uniform
codes
or
international
standards
there
are
more
than
50
classification
societies
worldwide
each
has
different
rules
ship
design
has
historically
largely
been
led
by
the
ship
insurers
who
inspected
classified
and
insured
vessels
hence
the
widespread
adoption
of
new
rules
to
allow
for
the
existence
of
rogue
waves
is
likely
to
take
many
years
interface
(matter)
in
the
physical
sciences
an
interface
is
the
boundary
between
two
spatial
regions
occupied
by
different
matter
or
by
matter
in
different
physical
states
the
interface
between
matter
and
air
or
matter
and
vacuum
is
called
a
surface
and
studied
in
surface
science
in
thermal
equilibrium
the
regions
in
contact
are
called
phases
and
the
interface
is
called
a
phase
boundary
an
example
for
an
interface
out
of
equilibrium
is
the
grain
boundary
in
polycrystalline
matter
the
importance
of
the
interface
depends
on
the
type
of
system:
the
bigger
the
quotient
area/volume
the
greater
the
effect
the
interface
will
have
consequently
interfaces
are
very
important
in
systems
with
large
interface
area-to-volume
ratios
such
as
colloids
interfaces
can
be
flat
or
curved
for
example
oil
droplets
in
a
salad
dressing
are
spherical
but
the
interface
between
water
and
air
in
a
glass
of
water
is
mostly
flat
surface
tension
is
the
physical
property
which
rules
interface
processes
involving
liquids
for
a
liquid
film
on
flat
surfaces
the
liquid-vapor
interface
keeps
flat
to
minimize
interfacial
area
and
system
free
energy
for
a
liquid
film
on
rough
surfaces
the
surface
tension
tends
to
keep
the
meniscus
flat
while
the
disjoining
pressure
makes
the
film
conformal
to
the
substrate
the
equilibrium
meniscus
shape
is
a
result
of
the
competition
between
the
capillary
pressure
and
disjoining
pressure
interfaces
may
cause
various
optical
phenomena
such
as
refraction
optical
lenses
serve
as
an
example
of
a
practical
application
of
the
interface
between
glass
and
air
one
topical
interface
system
is
the
gas-liquid
interface
between
aerosols
and
other
atmospheric
molecules
transparent
wood
composites
transparent
wood
composites
are
novel
wood
materials
which
have
up
to
90%
transparency
and
higher
mechanical
properties
than
wood
itself
made
for
the
first
time
in
1992
when
these
materials
are
commercially
available
a
significant
benefit
is
expected
due
to
their
inherent
biodegradable
properties
since
it
is
wood
these
materials
are
significantly
more
biodegradable
than
glass
and
plastics
transparent
wood
it
is
also
shatterproof
on
the
other
hand
concerns
may
be
relevant
due
to
the
use
of
non-biodegradable
plastics
for
long
lasting
purpose
such
as
in
building
a
research
group
led
by
professor
lars
berglund
from
swedish
kth
university
along
with
a
university
of
maryland
research
group
led
by
professor
liangbing
hu
have
developed
a
method
to
remove
the
color
and
some
chemicals
from
small
blocks
of
wood
followed
by
adding
polymers
such
as
poly(methyl
methacrylate)
and
epoxy
at
the
cellular
level
thereby
rendering
them
transparent
as
soon
as
released
in
between
2015
and
2016
see-through
wood
had
a
large
press
reaction
with
articles
in
sciencedaily
wired
the
wall
street
journal
the
new
york
times
to
name
a
few
actually
those
research
groups
rediscovered
a
work
from
siegfried
fink
a
german
researcher
from
as
early
as
1992:
with
a
process
very
similar
to
berglund's
and
hu's
the
german
researcher
turned
wood
transparent
to
reveal
specific
cavities
of
the
wood
structure
for
analytical
purpose
transparent
wood
was
made
based
on
the
idea
of
removing
light-absorbing
components
(mainly
lignin)
followed
by
infiltration
of
a
polymer
with
a
refractive
index
matching
the
wood
substrate
one
example
is
a
three
step
process:
the
first
step
consists
of
immersing
the
4
or
5
inch
block
of
wood
in
a
solution
of
water
sodium
hydroxide
and
sodium
sulfite
at
boiling
temperature
for
two
hours
this
enables
the
lignin
in
the
cell
walls
to
be
leached
out
"lignins
are
particularly
important
in
the
formation
of
cell
walls
especially
in
wood
and
bark
because
they
lend
rigidity
and
do
not
rot
easily"
the
second
step
of
oxidation
with
hydrogen
peroxide
completes
the
leaching
of
the
lignin
the
third
step
consists
of
immersing
the
material
in
epoxy
and
putting
it
under
alternating
vacuum
and
atmospheric
pressure;
this
fills
the
wood's
natural
but
now
disused
nutrient
and
hydrating
microscopic
channels
the
epoxy-filled
microscopic
channels
create
a
material
that
has
transparent
refractive
properties
the
new
material
is
rated
stronger
than
plastic
but
is
only
in
the
laboratory
and
experimental
stage
and
not
yet
ready
for
commercial
use
the
length
of
the
process
is
determined
by
the
size
and
species
of
the
wood
transparent
wood
could
transform
architecture
by
enabling
novel
structures
such
as
load-bearing
windows
such
elements
could
also
yield
improvements
in
energy
efficiency
over
glass
or
other
traditional
materials
ned
wingreen
ned
wingreen
is
a
theoretical
physicist
and
the
howard
a
prior
professor
of
the
life
sciences
at
princeton
university
he
is
a
member
of
the
department
of
molecular
biology
and
of
the
lewis-sigler
institute
for
integrative
genomics
where
he
is
currently
associate
director
he
is
also
associated
faculty
in
the
department
of
physics
working
with
yigal
meir
wingreen
formulated
the
meir-wingreen
formula
which
describes
the
electric
current
through
an
arbitrary
mesoscopic
system
wingreen
received
a
bs
in
physics
from
california
institute
of
technology
in
1984
wingreen
then
received
his
phd
in
theoretical
condensed
matter
physics
from
cornell
university
in
1989
as
a
hertz
fellow
his
dissertation
was
titled
"resonant
tunneling
with
electron-phonon
interaction"
and
he
was
advised
by
john
w
wilkins
he
did
his
postdoc
in
mesoscopic
physics
at
mit
there
along
with
yigal
meir
he
formulated
the
meir-wingreen
formula
that
describes
the
electric
current
through
an
arbitrary
mesoscopic
system
in
1991
he
moved
to
the
nec
research
institute
in
princeton
at
nec
he
continued
to
work
in
mesoscopic
physics
but
also
started
research
in
biophysics
which
grew
into
a
general
interest
in
problems
at
the
interface
of
physics
and
biology
wingreen
joined
princeton
university
in
2004
wingreen's
current
research
focuses
on
modelling
intracellular
networks
in
bacteria
and
other
micro-organisms
as
well
as
studies
of
microbial
communities
he
is
a
fellow
of
the
american
physical
society
and
the
american
association
for
the
advancement
of
science
academic:
diffuse
field
acoustic
testing
diffuse
field
acoustic
testing
is
the
testing
of
the
mechanical
resistance
of
a
spacecraft
to
the
acoustic
pressures
during
launch
in
the
aerospace
industry
acoustic
chambers
are
the
main
facilities
for
such
tests
a
chamber
is
a
reverberant
room
that
creates
a
diffuse
sound
field
and
is
composed
of
an
empty
volume
(from
1
m
to
2900
m)
and
a
multifrequency
sound
generation
system
theoretically
diffuse
field
is
defined
as
a
sound
pressure
field
where
there
is
no
privileged
direction
of
the
energy
in
other
words
when
sound
pressure
is
the
same
everywhere
in
the
room
this
is
obtained
with
large
rooms
with
no
absorbent
materials
on
walls
ceiling
or
floor
diffusion
is
enhanced
in
asymmetric
rooms
to
obtain
such
conditions
the
room
must
be
reverberant
the
source's
direct
field
must
be
negligible
compared
to
the
reverberant
field
avoiding
privileged
propagation
reverberation
is
due
to
multiple
reflections
on
walls
with
some
delays
that
come
back
to
the
receptor
summing
up
these
contributions
a
reverberant
pressure
field
is
created
the
more
reverberation
the
more
the
field
is
diffused
two
oft-used
measures
of
reverberation
time
quantify
this
parameter
:
formula_1
and
formula_2
these
values
are
the
interval
for
the
sound
pressure
level
to
the
lower
of
30
or
60
dbspl
it
can
be
obtained
by
measuring
the
sound
pressure
decrease
after
a
sound
impulse
or
by
using
approximate
formulas
such
as
sabine's
or
eyring's
in
the
case
of
a
diffuse
field
(low
absorption
on
the
walls
and
big
volumes)
sabine's
formula
is
used
formula_3
where
formula_4
is
the
equivalent
absorption
area
involving
the
surface
formula_5
of
the
walls
and
their
absorption
coefficient
formula_6
many
theoretical
ways
to
model
sound
propagation
are
used
one
of
these
is
the
geometrical
approach
this
represents
sound
waves
as
a
ray
of
energy
propagating
when
it
meets
an
obstacle
this
ray
has
two
possible
behaviors:
it
can
be
reflected
following
the
normal
of
the
plan
specular
reflection
alternatively
it
can
be
separated
into
many
rays
following
a
mathematical
law
(for
example
lambert's
law)
diffused
reflection
frequency
is
a
main
factor
of
a
good
diffused
field
some
phenomena
linked
to
frequency
of
the
sound
pressure
field
lead
to
poor
homogeneity
of
a
pressure
field
the
frequency
response
of
a
room
is
the
amplification
or
reduction
of
some
frequencies
it
represents
the
repartition
of
pressure
with
respect
to
frequency
in
low
frequencies
this
can
lead
to
mode
apparition
these
modes
are
due
to
standing
waves
that
lead
to
maximum
and
minimum
pressure
according
to
the
geometry
of
the
room
to
determine
the
frequency
for
which
the
pressure
field
can
be
considered
diffused
schroeder's
frequency
is
commonly
used
it
is
obtained
considering
the
frequency
from
which
the
modal
overlap
exceeds
formula_7
below
this
frequency
the
field
is
not
diffuse
and
standing
waves
create
pressure
modes
formula_8
where
formula_2
is
the
reverberation
time
of
the
room
and
formula_10
its
volume
for
example
in
the
case
of
a
rectangular
room
low
frequency
modes
are
determined
relative
to
the
room
dimensions
as
formula_11
where
formula_12
formula_13
and
formula_14
are
respectively
the
mode
of
the
length
formula_15
formula_16
and
formula_17
of
the
room
and
formula_18
the
celerity
of
sound
in
the
working
fluid
acoustic
tests
are
mainly
use
for
environmental
tests
on
aircraft
structures
satellites
are
expensive
products
with
high-engineering
built-in
components
to
improve
the
resistance
of
a
spacecraft
during
launch
and
during
its
orbital
life
analysis
is
focused
on
tests
in
three
categories
:
thermal
radio-frequencies
and
vibrations
this
last
test
area
is
focused
on
the
mechanical
stresses
that
the
specimen
will
meet
during
its
life
especially
during
launch
acoustics
creates
mechanical
stresses
during
the
first
five
seconds
sound
pressure
levels
can
go
up
to
150
dbspl
acoustic
tests
are
used
to
verify
the
mechanical
resistance
of
the
satellite
and
its
elements
to
acoustic
pressures
generated
once
the
sound
generation
system
is
working
acceleration
measurement
is
performed
by
accelerometers
placed
on
the
specimen
to
test
a
satellite
a
sound
generation
system
generates
a
broadband
spectrum
([25 hz-10000]hz)
simulating
the
maximum
envelope
of
all
launchers
that
the
satellite
may
fly
in
to
qualify
three
tests
are
realized
with
changing
global
gain
compared
to
launcher
spectrum:
before
testing
the
satellite
an
empty
room
test
is
performed
to
check
the
chamber's
signature
this
pressure
field
is
generated
by
multifrequency
sirens
powered
by
nitrogen
or
compressed
air
modulators
this
system
can
generate
sound
pressure
levels
up
to
160
dbspl
each
acoustic
chamber
has
its
own
configurations
but
each
siren
is
centered
on
a
frequency
where
sound
pressure
levels
are
the
highest
in
some
cases
these
sirens
can
be
completed
with
electroacoustic
systems
to
generate
and
control
midrange
and
high
frequencies
sirens
generate
low
frequencies
but
with
high
sound
pressure
levels
distortion
appears
that
leads
to
higher
harmonics
loudspeakers
are
used
in
some
chambers
to
control
these
frequencies
to
produce
exact
levels
piloting
microphones
check
sound
pressure
levels
and
apply
a
realtime
gain
correction
to
adjust
the
level
energy
well
in
physics
an
energy
well
describes
a
'stable'
equilibrium
that
is
not
at
lowest
possible
energy
in
general
modern
physics
holds
the
view
that
the
universe
-
and
systems
therein
-
spontaneously
drives
toward
a
state
of
lower
energy
if
possible
for
example
a
bowling
ball
pitched
atop
a
smooth
hump
(which
has
potential
energy
in
the
presence
of
gravity)
will
tend
to
roll
down
to
the
lowest
point
it
possibly
can
once
there
this
reduces
the
total
potential
energy
of
the
system
on
the
other
hand
if
the
bowling
ball
is
resting
in
a
valley
between
two
humps
-
no
matter
how
big
the
drops
outside
the
humps
-
it
will
stay
there
indefinitely
even
though
the
system
could
achieve
a
lower
energy
state
it
cannot
do
so
without
external
energy
being
applied:
(locally)
it
is
at
its
lowest
energy
state
and
only
a
force
from
outside
the
system
can
'push'
it
over
one
of
the
humps
so
a
lower
state
can
be
achieved
the
concept
of
an
energy
well
is
a
key
part
of
teaching
basic
physics
especially
quantum
mechanics
here
students
often
solve
the
one-dimensional
schrödinger
equation
for
an
electron
trapped
in
a
potential
well
from
which
it
has
insufficient
energy
to
escape
the
solution
to
this
problem
is
a
series
of
sinusoidal
waves
of
fractional
integral
wavelengths
determined
by
the
width
of
the
well
electron
heat
capacity
in
solid
state
physics
the
electron
heat
capacity
or
electronic
specific
heat
describes
the
contribution
of
electrons
to
the
heat
capacity
heat
is
transported
by
phonons
and
by
free
electrons
in
solids
for
pure
metals
however
the
electronic
contributions
dominate
in
the
thermal
conductivity
in
impure
metals
the
electron
mean
free
path
is
reduced
by
collisions
with
impurities
and
the
phonon
contribution
may
be
comparable
with
the
electronic
contribution
although
the
drude
model
was
fairly
successful
in
describing
the
electron
motion
within
metals
it
has
some
erroneous
aspects:
it
predicts
the
hall
coefficient
with
the
wrong
sign
compared
to
experimental
measurements
the
assumed
additional
electronic
heat
capacity
to
the
lattice
heat
capacity
namely
formula_1
per
electron
at
elevated
temperatures
is
also
inconsistent
with
experimental
values
since
measurements
of
metals
show
no
deviation
from
the
dulong-petit
law
the
observed
electronic
contribution
of
electrons
to
the
heat
capacity
is
usually
less
than
one
percent
of
formula_2
this
problem
seemed
insoluble
prior
to
the
development
of
quantum
mechanics
this
paradox
was
solved
by
sommerfeld
after
the
discovery
of
the
pauli
exclusion
principle
who
recognised
that
the
replacement
of
the
boltzmann
distribution
with
the
fermi-dirac
distribution
was
required
and
incorporated
it
in
the
free
electron
model
when
a
metallic
system
is
heated
from
absolute
zero
not
every
electron
gains
an
energy
formula_3
as
equipartition
dictates
only
those
electrons
in
atomic
orbitals
within
an
energy
range
of
formula_4
of
the
fermi
level
are
thermally
excited
electrons
in
contrast
to
a
classical
gas
can
only
move
into
free
states
in
their
energetic
neighbourhood
the
one-electron
energy
levels
are
specified
by
the
wave
vector
formula_5
through
the
relation
formula_6
with
formula_7
the
electron
mass
this
relation
separates
the
occupied
energy
states
from
the
unoccupied
ones
and
corresponds
to
the
spherical
surface
in
k-space
as
formula_8
the
ground
state
distribution
becomes:
formula_9
where
this
implies
that
the
ground
state
is
the
only
occupied
state
for
electrons
in
the
limit
formula_8
the
formula_15
takes
the
pauli
exclusion
principle
into
account
the
internal
energy
formula_16
of
a
system
within
the
free
electron
model
is
given
by
the
sum
over
one-electron
levels
times
the
mean
number
of
electrons
in
that
level:
formula_17
using
the
approximation
that
for
a
sum
over
a
smooth
function
formula_18
over
all
allowed
values
of
formula_5
for
finite
large
system
is
given
by:
formula_20
where
for
the
reduced
internal
energy
formula_22
the
expression
for
formula_16
can
be
rewritten
as:
formula_24
and
the
expression
for
the
electron
density
formula_25
can
be
written
as:
formula_26
the
integrals
above
can
be
evaluated
using
the
fact
that
the
dependence
of
the
integrals
on
formula_27
can
be
changed
to
dependence
on
formula_28
through
the
relation
for
the
electronic
energy
when
described
as
free
particles
formula_6
which
yields
for
an
arbitrary
function
formula_30:
formula_31
with
formula_32
which
is
known
as
the
density
of
levels
or
density
of
states
per
unit
volume
such
that
formula_33
is
the
total
number
of
states
between
formula_28
and
formula_35
using
the
expressions
above
the
integrals
can
be
rewritten
as:
formula_36
these
integrals
can
be
evaluated
for
temperatures
that
are
small
compared
to
the
fermi
temperature
by
applying
the
sommerfeld
expansion
and
using
the
approximation
that
formula_12
differs
from
formula_11
for
formula_39
by
terms
of
order
formula_40
the
expressions
become:
formula_41
for
the
ground
state
configuration
the
first
terms
(the
integrals)
of
the
expressions
above
yield
the
internal
energy
and
electron
density
of
the
ground
state
the
expression
for
the
electron
density
reduces
to
formula_42
substituting
this
into
the
expression
for
the
internal
energy
one
finds
the
following
expression:
formula_43
the
contributions
of
electrons
within
the
free
electron
model
is
given
by:
formula_44
for
free
electrons
:
formula_45
compared
to
the
classical
result
formula_46
it
can
be
concluded
that
this
result
is
depressed
by
a
factor
of
formula_47
which
is
at
room
temperature
of
order
of
magnitude
formula_48
this
explains
the
absence
of
an
electronic
contribution
to
the
heat
capacity
as
measured
experimentally
note
that
in
this
derivation
formula_11
is
often
denoted
by
formula_50
which
is
known
as
the
fermi
energy
in
this
notation
the
electron
heat
capacity
becomes:
formula_51
and
for
free
electrons
:
formula_52
using
the
definition
for
the
fermi
energy
with
formula_53
the
fermi
temperature
for
temperatures
below
both
the
debye
temperature
formula_54
and
the
fermi
temperature
formula_55
the
heat
capacity
of
metals
can
be
written
as
a
sum
of
electron
and
phonon
contributions
that
are
linear
and
cubic
respectively:
formula_56
the
coefficient
formula_57
can
be
calculated
and
determined
experimentally
some
values
are
tabulated
below:
the
free
electrons
in
a
metal
do
not
usually
lead
to
a
strong
deviation
from
the
dulong–petit
law
at
high
temperatures
since
formula_57
is
linear
in
formula_59
and
formula_60
is
linear
in
formula_61
at
low
temperatures
the
lattice
contribution
vanishes
faster
than
the
electronic
contribution
and
the
latter
can
be
measured
the
deviation
of
the
approximated
and
experimentally
determined
electronic
contribution
to
the
heat
capacity
of
a
metal
is
not
too
large
a
few
metals
deviate
significantly
from
this
approximated
prediction
measurements
indicate
that
these
errors
are
associated
with
the
electron
mass
being
somehow
changed
in
the
metal
for
the
calculation
of
the
electron
heat
capacity
the
effective
mass
of
an
electron
should
be
considered
instead
for
fe
and
co
the
large
deviations
are
attributed
to
the
partially
filled
d-shells
of
these
transition
metals
whose
d-bands
lie
at
the
fermi
energy
the
alkali
metals
are
expected
to
have
the
best
agreement
with
the
free
electron
model
since
these
metals
only
one
s-electron
outside
a
closed
shell
however
even
sodium
which
is
considered
to
be
the
closest
to
a
free
electron
metal
is
determined
to
have
a
formula_57
more
than
25
per
cent
higher
than
expected
from
the
theory
certain
effects
influence
the
deviation
from
the
approximation:
superconductivity
occurs
in
many
metallic
elements
of
the
periodic
system
and
also
in
alloys
intermetallic
compounds
and
doped
semiconductors
this
effect
occurs
upon
cooling
the
material
the
entropy
decreases
on
cooling
below
the
critical
temperature
formula_63
for
superconductivity
which
indicates
that
the
superconducting
state
is
more
ordered
than
the
normal
state
the
entropy
change
is
small
this
must
mean
that
only
a
very
small
fraction
of
electrons
participate
in
the
transition
to
the
superconducting
state
but
the
electronic
contribution
to
the
heat
capacity
changes
drastically
there
is
a
sharp
jump
of
the
heat
capcacity
at
the
critical
temperature
while
for
the
temperatures
above
the
critical
temperature
the
heat
capacity
is
linear
with
temperature
the
calculation
of
the
electron
heat
capacity
for
super
conductors
can
be
done
in
the
bcs
theory
the
entropy
of
a
system
of
fermionic
quasiparticles
in
this
case
cooper
pairs
is:
formula_64
where
formula_65
is
the
fermi-dirac
distribution
formula_66
with
formula_67
and
the
heat
capacity
is
given
by
formula_72
the
last
two
terms
can
be
calculated:
formula_73
substituting
this
in
the
expression
for
the
heat
capacity
and
again
applying
that
the
sum
over
formula_27
in
the
reciprocal
space
can
be
replaced
by
an
integral
in
formula_28
multipied
by
the
density
of
states
formula_76
this
yields:
formula_77
to
examine
the
typical
behaviour
of
the
electron
heat
capacity
for
species
that
can
transition
to
the
superconducting
state
three
regions
must
be
defined:
for
formula_78
it
holds
that
formula_83
and
the
electron
heat
capacity
becomes:
formula_84
this
is
just
the
result
for
a
normal
metal
derived
in
the
section
above
as
expected
since
a
superconductor
behaves
as
a
normal
conductor
above
the
critical
temperature
for
formula_80
the
electron
heat
capacity
for
super
conductors
exhibits
an
exponential
decay
of
the
form:
formula_87
at
the
critical
temperature
the
heat
capacity
is
discontinuous
this
discontinuity
in
the
heat
capacity
indicates
that
the
transition
for
a
material
from
normal
conducting
to
superconducting
is
a
second
order
phase
transition
more
recent
research
has
found
that
since
both
the
fermi
function
formula_10
and
the
electronic
density
of
states
formula_90
vary
with
formula_91
a
more
exact
expression
for
formula_92
is
the
following
equation
at
given
formula_91
formula_94
farris
effect
(rheology)
in
rheology
the
farris
effect
describes
the
decrease
of
the
viscosity
of
a
suspension
upon
increasing
the
dispersity
of
the
solid
additive
at
constant
volume
fraction
of
the
solid
additive
that
is
that
a
broader
particle
size
distribution
yields
a
lower
viscosity
than
a
narrow
particle
size
distribution
for
the
same
concentration
of
particles
the
phenomenon
is
names
after
richard
j
farris
who
modeled
the
effect
the
effect
is
relevant
whenever
suspensions
are
flowing
particularly
for
suspensions
with
high
loading
fractions
examples
include
hydraulic
fracturing
fluids
metal
injection
molding
feedstocks
cosmetics
and
various
geological
processes
including
sedimentation
and
lava
flows
dispersive
medium
a
dispersive
medium
is
a
medium
in
which
waves
of
different
frequencies
travel
at
different
velocities
with
electromagnetic
radiation
(eg
light
radio
waves)
this
occurs
because
the
index
of
refraction
of
the
medium
is
frequency
dependent
aerometer
an
aerometer
is
an
instrument
designed
to
measure
the
density
(among
other
parameters)
of
the
air
and
some
gases
the
word
aerometer
(or
ärometer
from
ancient
greek
ἀήρ
-aer
"air"
and
μέτρον
-métron
"measure
scale")
refers
to
various
types
of
devices
for
measuring
or
handling
of
gases
the
instruments
designated
with
this
name
can
be
used
to
find:
the
density
the
flow
the
amount
or
some
other
parameter
of
the
air
or
a
determined
gas
another
instrument
called
areometer
(from
ancient
greek
ἀραιός
-araiós
"lightness"
and
μέτρον
-métron
"measure
scale")
also
known
as
hydrometer
used
for
measuring
liquids
density
is
often
confused
with
the
term
aerometer
here
defined
[[category:physics]]
[[category:measuring
instruments]]
homeokinetics
homeokinetics
is
the
study
of
self-organizing
complex
systems
standard
physics
studies
systems
at
separate
levels
such
as
atomic
physics
nuclear
physics
biophysics
social
physics
and
galactic
physics
homeokinetic
physics
studies
the
up-down
processes
that
bind
these
levels
tools
such
as
mechanics
quantum
field
theory
and
the
laws
of
thermodynamics
provide
the
key
relationships
the
subject
described
as
the
physics
and
thermodynamics
associated
with
the
up
down
movement
between
levels
of
systems
originated
in
the
late
1970s
work
of
american
physicists
harry
soodak
and
arthur
iberall
complex
systems
are
universes
galaxies
social
systems
people
or
even
those
that
seem
as
simple
as
gases
the
basic
premise
is
that
the
entire
universe
consists
of
atomistic-like
units
bound
in
interactive
ensembles
to
form
systems
level
by
level
in
a
nested
hierarchy
homeokinetics
treats
all
complex
systems
on
an
equal
footing
animate
and
inanimate
providing
them
with
a
common
viewpoint
the
complexity
in
studying
how
they
work
is
reduced
by
the
emergence
of
common
languages
in
all
complex
systems
arthur
iberall
warren
mcculloch
and
harry
soodak
developed
the
concept
of
homeokinetics
as
a
new
branch
of
physics
it
began
through
iberall's
biophysical
research
for
the
nasa
exobiology
program
into
the
dynamics
of
mammalian
physiological
processes
they
were
observing
an
area
that
physics
has
neglected
that
of
complex
systems
with
their
very
long
internal
factory
day
delays
they
were
observing
systems
associated
with
nested
hierarchy
and
with
an
extensive
range
of
time
scale
processes
it
was
such
connections
referred
to
as
both
up-down
or
in-out
connections
(as
nested
hierarchy)
and
side-side
or
flatland
physics
among
atomistic-like
components
(as
heterarchy)
that
became
the
hallmark
of
homeokinetic
problems
by
1975
they
began
to
put
a
formal
catch-phrase
name
on
those
complex
problems
associating
them
with
nature
life
human
mind
and
society
the
major
method
of
exposition
that
they
began
using
was
a
combination
of
engineering
physics
and
a
more
academic
pure
physics
in
1981
iberall
was
invited
to
the
crump
institute
for
medical
engineering
of
ucla
where
he
further
refined
the
key
concepts
of
homeokinetics
developing
a
physical
scientific
foundation
for
complex
systems
a
system
is
a
collective
of
interacting
‘atomistic’-like
entities
the
word
‘atomism’
is
used
to
stand
both
for
the
entity
and
the
doctrine
as
is
known
from
‘kinetic’
theory
in
mobile
or
simple
systems
the
atomisms
share
their
‘energy’
in
interactive
collisions
that
so-called
‘equipartitioning’
process
takes
place
within
a
few
collisions
physically
if
there
is
little
or
no
interaction
the
process
is
considered
to
be
very
weak
physics
deals
basically
with
the
forces
of
interaction—few
in
number—that
influence
the
interactions
they
all
tend
to
emerge
with
considerable
force
at
high
‘density’
of
atomistic
interaction
in
complex
systems
there
is
also
a
result
of
internal
processes
in
the
atomisms
they
exhibit
in
addition
to
the
pair-by-pair
interactions
internal
actions
such
as
vibrations
rotations
and
association
if
the
energy
and
time
involved
internally
creates
a
very
large—in
time—cycle
of
performance
of
their
actions
compared
to
their
pair
interactions
the
collective
system
is
complex
if
you
eat
a
cookie
and
you
do
not
see
the
resulting
action
for
hours
that
is
complex;
if
boy
meets
girl
and
they
become
‘engaged’
for
a
protracted
period
that
is
complex
what
emerges
from
that
physics
is
a
broad
host
of
changes
in
state
and
stability
transitions
in
state
viewing
aristotle
as
having
defined
a
general
basis
for
systems
in
their
static-logical
states
and
trying
to
identify
a
logic-metalogic
for
physics
eg
metaphysics
then
homeokinetics
is
viewed
to
be
an
attempt
to
define
the
dynamics
of
all
those
systems
in
the
universe
ordinary
physics
is
a
flatland
physics
a
physics
at
some
particular
level
examples
include
nuclear
and
atomic
physics
biophysics
social
physics
and
stellar
physics
homeokinetic
physics
combines
flatland
physics
with
the
study
of
the
up
down
processes
that
binds
the
levels
tools
such
as
mechanics
quantum
field
theory
and
the
laws
of
thermodynamics
provide
key
relationships
for
the
binding
of
the
levels
how
they
connect
and
how
the
energy
flows
up
and
down
and
whether
the
atomisms
are
atoms
molecules
cells
people
stars
galaxies
or
universes
the
same
tools
can
be
used
to
understand
them
homeokinetics
treats
all
complex
systems
on
an
equal
footing
animate
and
inanimate
providing
them
with
a
common
viewpoint
the
complexity
in
studying
how
they
work
is
reduced
by
the
emergence
of
common
languages
in
all
complex
systems
a
homeokinetic
approach
to
complex
systems
has
been
applied
to
ecological
psychology
anthropology
geology
bioenergetics
and
political
science
it
has
also
been
applied
to
social
physics
where
a
homeokinetics
analysis
shows
that
one
must
account
for
flow
variables
such
as
the
flow
of
energy
of
materials
of
action
reproduction
rate
and
value-in-exchange
center
of
percussion
the
center
of
percussion
is
the
point
on
an
extended
massive
object
attached
to
a
pivot
where
a
perpendicular
impact
will
produce
no
reactive
shock
at
the
pivot
translational
and
rotational
motions
cancel
at
the
pivot
when
an
impulsive
blow
is
struck
at
the
center
of
percussion
the
center
of
percussion
is
often
discussed
in
the
context
of
a
bat
racquet
door
sword
or
other
extended
object
held
at
one
end
the
same
point
is
called
the
center
of
oscillation
for
the
object
suspended
from
the
pivot
as
a
pendulum
meaning
that
a
simple
pendulum
with
all
its
mass
concentrated
at
that
point
will
have
the
same
period
of
oscillation
as
the
compound
pendulum
in
sports
the
center
of
percussion
of
a
bat
or
racquet
is
related
to
the
so-called
"sweet
spot"
but
the
latter
is
also
related
to
vibrational
bending
of
the
object
imagine
a
rigid
beam
suspended
from
a
wire
by
a
fixture
that
can
slide
freely
along
the
wire
at
point
p
as
shown
in
the
figure
an
impulsive
blow
is
applied
from
the
left
if
it
is
below
the
center
of
mass
(cm)
it
will
cause
the
beam
to
rotate
counterclockwise
around
the
cm
and
also
cause
the
cm
to
move
to
the
right
the
center
of
percussion
(cp)
is
below
the
cm
if
the
blow
falls
above
the
cp
the
rightward
translational
motion
will
be
bigger
than
the
leftward
rotational
motion
at
p
causing
the
net
initial
motion
of
the
fixture
to
be
rightward
if
the
blow
falls
below
the
cp
the
opposite
will
occur
rotational
motion
at
p
will
be
larger
than
translational
motion
and
the
fixture
will
move
initially
leftward
only
if
the
blow
falls
exactly
on
the
cp
will
the
two
components
of
motion
cancel
out
to
produce
zero
net
initial
movement
at
point
p
when
the
sliding
fixture
is
replaced
with
a
pivot
that
cannot
move
left
or
right
an
impulsive
blow
anywhere
but
at
the
cp
results
in
an
initial
reactive
force
at
the
pivot
for
a
free
rigid
beam
an
impulse
formula_1
applied
at
right
angle
at
a
distance
formula_2
from
the
center
of
mass
(cm)
will
result
in
the
cm
changing
velocity
formula_3
according
to
the
relation:
where
formula_5
is
the
mass
of
the
beam
similarly
the
torque
about
the
cm
will
change
the
angular
velocity
formula_6
according
to:
where
formula_8
is
the
moment
of
inertia
around
the
cm
for
any
point
p
a
distance
formula_9
on
the
opposite
side
of
the
cm
from
the
point
of
impact
the
change
in
velocity
of
point
p
is
where
formula_9
is
the
distance
of
p
from
the
cm
hence
the
acceleration
at
p
due
to
the
impulsive
blow
is:
when
this
acceleration
is
zero
formula_2
defines
the
center
of
percussion
therefore
the
cp
distance
formula_2
from
the
cm
is
given
by
note
that
p
the
rotation
axis
need
not
be
at
the
end
of
the
beam
but
can
be
chosen
at
any
distance
formula_9
length
formula_17
also
defines
the
center
of
oscillation
of
a
physical
pendulum
that
is
the
position
of
the
mass
of
a
simple
pendulum
that
has
the
same
period
as
the
physical
pendulum
for
the
special
case
of
a
beam
of
uniform
density
of
length
formula_18
the
moment
of
inertia
around
the
cm
is:
and
for
rotation
about
a
pivot
at
the
end
this
leads
to:
it
follows
that
the
cp
is
2/3
of
the
length
of
the
uniform
beam
formula_18
from
the
pivoted
end
for
example
a
swinging
door
that
is
stopped
by
a
doorstop
placed
2/3
of
the
width
of
the
door
will
do
the
job
with
minimal
shaking
of
the
door
because
the
hinged
end
is
subjected
to
no
net
reactive
force
(this
point
is
also
the
node
in
the
second
vibrational
harmonic
which
also
minimizes
vibration)
the
sweet
spot
on
a
baseball
bat
is
generally
defined
as
the
point
at
which
the
impact
"feels"
best
to
the
batter
the
center
of
percussion
defines
a
place
where
if
the
bat
strikes
the
ball
and
the
batter's
hands
are
at
the
pivot
point
the
batter
feels
no
sudden
reactive
force
however
since
a
bat
is
not
a
rigid
object
the
vibrations
produced
by
the
impact
also
play
a
role
also
the
pivot
point
of
the
swing
may
not
be
at
the
place
where
the
batter's
hands
are
placed
research
has
shown
that
the
dominant
physical
mechanism
in
determining
where
the
sweet
spot
is
arises
from
the
location
of
nodes
in
the
vibrational
modes
of
the
bat
not
the
location
of
the
center
of
percussion
the
center
of
percussion
concept
can
be
applied
to
swords
being
flexible
objects
the
"sweet
spot"
for
such
cutting
weapons
depends
not
only
on
the
center
of
percussion
but
also
on
the
flexing
and
vibrational
characteristics
verwey
transition
the
verwey
transition
is
a
low-temperature
phase
transition
in
the
mineral
magnetite
near
125
kelvins
associated
with
changes
in
its
magnetic
electrical
and
thermal
properties
upon
warming
through
the
verwey
transition
temperature
()
the
magnetite
crystal
lattice
changes
from
a
monoclinic
structure
to
the
cubic
inverse
spinel
structure
that
persists
at
room
temperature
the
phenomenon
is
named
after
evert
verwey
a
dutch
chemist
who
first
recognized
the
connection
between
the
structural
transition
and
the
changes
in
the
physical
properties
of
magnetite
the
verwey
transition
is
near
in
temperature
but
distinct
from
a
magnetic
isotropic
point
in
magnetite
at
which
the
first
magnetocrystalline
anisotropy
constant
changes
sign
from
positive
to
negative
the
temperature
and
physical
expression
of
the
verwey
transition
are
highly
sensitive
to
the
stress
state
of
magnetite
and
the
stoichiometry
non-stoichiometry
in
the
form
of
metal
cation
substitution
or
partial
oxidation
can
lower
the
transition
temperature
or
suppress
it
entirely
computational
anatomy
computational
anatomy
is
an
interdisciplinary
field
of
biology
focused
on
quantitative
investigation
and
modelling
of
anatomical
shapes
variability
it
involves
the
development
and
application
of
mathematical
statistical
and
data-analytical
methods
for
modelling
and
simulation
of
biological
structures
the
field
is
broadly
defined
and
includes
foundations
in
anatomy
applied
mathematics
and
pure
mathematics
machine
learning
computational
mechanics
computational
science
biological
imaging
neuroscience
physics
probability
and
statistics;
it
also
has
strong
connections
with
fluid
mechanics
and
geometric
mechanics
additionally
it
complements
newer
interdisciplinary
fields
like
bioinformatics
and
neuroinformatics
in
the
sense
that
its
interpretation
uses
metadata
derived
from
the
original
sensor
imaging
modalities
(of
which
magnetic
resonance
imaging
is
one
example)
it
focuses
on
the
anatomical
structures
being
imaged
rather
than
the
medical
imaging
devices
it
is
similar
in
spirit
to
the
history
of
computational
linguistics
a
discipline
that
focuses
on
the
linguistic
structures
rather
than
the
sensor
acting
as
the
transmission
and
communication
medium(s)
in
computational
anatomy
the
diffeomorphism
group
is
used
to
study
different
coordinate
systems
via
coordinate
transformations
as
generated
via
the
lagrangian
and
eulerian
velocities
of
flow
in
formula_1
the
flows
between
coordinates
in
computational
anatomy
are
constrained
to
be
geodesic
flows
satisfying
the
principle
of
least
action
for
the
kinetic
energy
of
the
flow
the
kinetic
energy
is
defined
through
a
sobolev
smoothness
norm
with
strictly
more
than
two
generalized
square-integrable
derivatives
for
each
component
of
the
flow
velocity
which
guarantees
that
the
flows
in
formula_2
are
diffeomorphisms
it
also
implies
that
the
diffeomorphic
shape
momentum
taken
pointwise
satisfying
the
euler-lagrange
equation
for
geodesics
is
determined
by
its
neighbors
through
spatial
derivatives
on
the
velocity
field
this
separates
the
discipline
from
the
case
of
incompressible
fluids
for
which
momentum
is
a
pointwise
function
of
velocity
computational
anatomy
intersects
the
study
of
riemannian
manifolds
and
nonlinear
global
analysis
where
groups
of
diffeomorphisms
are
the
central
focus
emerging
high-dimensional
theories
of
shape
are
central
to
many
studies
in
computational
anatomy
as
are
questions
emerging
from
the
fledgling
field
of
shape
statistics
the
metric
structures
in
computational
anatomy
are
related
in
spirit
to
morphometrics
with
the
distinction
that
computational
anatomy
focuses
on
an
infinite-dimensional
space
of
coordinate
systems
transformed
by
a
diffeomorphism
hence
the
central
use
of
the
terminology
the
metric
space
study
of
coordinate
systems
via
diffeomorphisms
at
computational
anatomy's
heart
is
the
comparison
of
shape
by
recognizing
in
one
shape
the
other
this
connects
it
to
d'arcy
wentworth
thompson's
developments
on
growth
and
form
which
has
led
to
scientific
explanations
of
morphogenesis
the
process
by
which
patterns
are
formed
in
biology
albrecht
durer's
four
books
on
human
proportion
were
arguably
the
earliest
works
on
computational
anatomy
the
efforts
of
noam
chomsky
in
his
pioneering
of
computational
linguistics
inspired
the
original
formulation
of
computational
anatomy
as
a
generative
model
of
shape
and
form
from
exemplars
acted
upon
via
transformations
due
to
the
availability
of
dense
3d
measurements
via
technologies
such
as
magnetic
resonance
imaging
(mri)
computational
anatomy
has
emerged
as
a
subfield
of
medical
imaging
and
bioengineering
for
extracting
anatomical
coordinate
systems
at
the
morphome
scale
in
3d
the
spirit
of
this
discipline
shares
strong
overlap
with
areas
such
as
computer
vision
and
kinematics
of
rigid
bodies
where
objects
are
studied
by
analysing
the
groups
responsible
for
the
movement
in
question
computational
anatomy
departs
from
computer
vision
with
its
focus
on
rigid
motions
as
the
infinite-dimensional
diffeomorphism
group
is
central
to
the
analysis
of
biological
shapes
it
is
a
branch
of
the
image
analysis
and
pattern
theory
school
at
brown
university
pioneered
by
ulf
grenander
in
grenander's
general
metric
pattern
theory
making
spaces
of
patterns
into
a
metric
space
is
one
of
the
fundamental
operations
since
being
able
to
cluster
and
recognize
anatomical
configurations
often
requires
a
metric
of
close
and
far
between
shapes
the
diffeomorphometry
metric
of
computational
anatomy
measures
how
far
two
diffeomorphic
changes
of
coordinates
are
from
each
other
which
in
turn
induces
a
metric
on
the
shapes
and
images
indexed
to
them
the
models
of
metric
pattern
theory
in
particular
group
action
on
the
orbit
of
shapes
and
forms
is
a
central
tool
to
the
formal
definitions
in
computational
anatomy
computational
anatomy
is
the
study
of
shape
and
form
at
the
morphome
or
gross
anatomy
millimeter
or
morphology
scale
focusing
on
the
study
of
sub-manifolds
of
formula_3
points
curves
surfaces
and
subvolumes
of
human
anatomy
an
early
modern
computational
neuro-anatomist
was
david
van
essen
performing
some
of
the
early
physical
unfoldings
of
the
human
brain
based
on
printing
of
a
human
cortex
and
cutting
jean
talairach's
publication
of
tailarach
coordinates
is
an
important
milestone
at
the
morphome
scale
demonstrating
the
fundamental
basis
of
local
coordinate
systems
in
studying
neuroanatomy
and
therefore
the
clear
link
to
charts
of
differential
geometry
concurrently
virtual
mapping
in
computational
anatomy
across
high
resolution
dense
image
coordinates
was
already
happening
in
ruzena
bajcy's
and
fred
bookstein's
earliest
developments
based
on
computed
axial
tomography
and
magnetic
resonance
imagery
the
earliest
introduction
of
the
use
of
flows
of
diffeomorphisms
for
transformation
of
coordinate
systems
in
image
analysis
and
medical
imaging
was
by
christensen
joshi
miller
and
rabbitt
the
first
formalization
of
computational
anatomy
as
an
orbit
of
exemplar
templates
under
diffeomorphism
group
action
was
in
the
original
lecture
given
by
grenander
and
miller
with
that
title
in
may
1997
at
the
50th
anniversary
of
the
division
of
applied
mathematics
at
brown
university
and
subsequent
publication
this
was
the
basis
for
the
strong
departure
from
much
of
the
previous
work
on
advanced
methods
for
spatial
normalization
and
image
registration
which
were
historically
built
on
notions
of
addition
and
basis
expansion
the
structure
preserving
transformations
central
to
the
modern
field
of
computational
anatomy
homeomorphisms
and
diffeomorphisms
carry
smooth
submanifolds
smoothly
they
are
generated
via
lagrangian
and
eulerian
flows
which
satisfy
a
law
of
composition
of
functions
forming
the
group
property
but
are
not
additive
the
original
model
of
computational
anatomy
was
as
the
triple
formula_4
the
group
formula_5
the
orbit
of
shapes
and
forms
formula_6
and
the
probability
laws
formula_7
which
encode
the
variations
of
the
objects
in
the
orbit
the
template
or
collection
of
templates
are
elements
in
the
orbit
formula_8
of
shapes
the
lagrangian
and
hamiltonian
formulations
of
the
equations
of
motion
of
computational
anatomy
took
off
post
1997
with
several
pivotal
meetings
including
the
1997
luminy
meeting
organized
by
the
azencott
school
at
ecole-normale
cachan
on
the
"mathematics
of
shape
recognition"
and
the
1998
trimestre
at
institute
henri
poincaré
organized
by
david
mumford
"questions
mathématiques
en
traitement
du
signal
et
de
l'image"
which
catalyzed
the
hopkins-brown-ens
cachan
groups
and
subsequent
developments
and
connections
of
computational
anatomy
to
developments
in
global
analysis
the
developments
in
computational
anatomy
included
the
establishment
of
the
sobelev
smoothness
conditions
on
the
diffeomorphometry
metric
to
insure
existence
of
solutions
of
variational
problems
in
the
space
of
diffeomorphisms
the
derivation
of
the
euler-lagrange
equations
characterizing
geodesics
through
the
group
and
associated
conservation
laws
the
demonstration
of
the
metric
properties
of
the
right
invariant
metric
the
demonstration
that
the
euler-lagrange
equations
have
a
well-posed
initial
value
problem
with
unique
solutions
for
all
time
and
with
the
first
results
on
sectional
curvatures
for
the
diffeomorphometry
metric
in
landmarked
spaces
following
the
los
alamos
meeting
in
2002
joshi's
original
large
deformation
singular
"landmark"
solutions
in
computational
anatomy
were
connected
to
peaked
"solitons"
or
"peakons"
as
solutions
for
the
camassa-holm
equation
subsequently
connections
were
made
between
computational
anatomy's
euler-lagrange
equations
for
momentum
densities
for
the
right-invariant
metric
satisfying
sobolev
smoothness
to
vladimir
arnold's
characterization
of
the
euler
equation
for
incompressible
flows
as
describing
geodesics
in
the
group
of
volume
preserving
diffeomorphisms
the
first
algorithms
generally
termed
lddmm
for
large
deformation
diffeomorphic
mapping
for
computing
connections
between
landmarks
in
volumes
and
spherical
manifolds
curves
currents
and
surfaces
volumes
tensors
varifolds
and
time-series
have
followed
these
contributions
of
computational
anatomy
to
the
global
analysis
associated
to
the
infinite
dimensional
manifolds
of
subgroups
of
the
diffeomorphism
group
is
far
from
trivial
the
original
idea
of
doing
differential
geometry
curvature
and
geodesics
on
infinite
dimensional
manifolds
goes
back
to
bernhard
riemann's
habilitation
(ueber
die
hypothesen
welche
der
geometrie
zu
grunde
liegen);
the
key
modern
book
laying
the
foundations
of
such
ideas
in
global
analysis
are
from
michor
the
applications
within
medical
imaging
of
computational
anatomy
continued
to
flourish
after
two
organized
meetings
at
the
institute
for
pure
and
applied
mathematics
conferences
at
university
of
california
los
angeles
computational
anatomy
has
been
useful
in
creating
accurate
models
of
the
atrophy
of
the
human
brain
at
the
morphome
scale
as
well
as
cardiac
templates
as
well
as
in
modeling
biological
systems
since
the
late
1990s
computational
anatomy
has
become
an
important
part
of
developing
emerging
technologies
for
the
field
of
medical
imaging
digital
atlases
are
a
fundamental
part
of
modern
medical-school
education
and
in
neuroimaging
research
at
the
morphome
scale
atlas
based
methods
and
virtual
textbooks
which
accommodate
variations
as
in
deformable
templates
are
at
the
center
of
many
neuro-image
analysis
platforms
including
freesurfer
fsl
mristudio
spm
diffeomorphic
registration
introduced
in
the
90's
is
now
an
important
player
with
existing
codes
bases
organized
around
ants
stationarylddmm
fastlddmm
are
examples
of
actively
used
computational
codes
for
constructing
correspondences
between
coordinate
systems
based
on
sparse
features
and
dense
images
voxel-based
morphometry
(vbm)
is
an
important
technology
built
on
many
of
these
principles
the
model
of
human
anatomy
is
a
deformable
template
an
orbit
of
exemplars
under
group
action
deformable
template
models
have
been
central
to
grenander's
metric
pattern
theory
accounting
for
typicality
via
templates
and
accounting
for
variability
via
transformation
of
the
template
an
orbit
under
group
action
as
the
representation
of
the
deformable
template
is
a
classic
formulation
from
differential
geometry
the
space
of
shapes
are
denoted
formula_9
with
the
group
formula_10
with
law
of
composition
formula_11;
the
action
of
the
group
on
shapes
is
denoted
formula_12
where
the
action
of
the
group
formula_13
is
defined
to
satisfy
the
orbit
formula_15
of
the
template
becomes
the
space
of
all
shapes
formula_16
being
homogenous
under
the
action
of
the
elements
of
formula_17
the
orbit
model
of
computational
anatomy
is
an
abstract
algebra
-
to
be
compared
to
linear
algebra-
since
the
groups
act
nonlinearly
on
the
shapes
this
is
a
generalization
of
the
classical
models
of
linear
algebra
in
which
the
set
of
finite
dimensional
formula_18
vectors
are
replaced
by
the
finite-dimensional
anatomical
submanifolds
(points
curves
surfaces
and
volumes)
and
images
of
them
and
the
formula_19
matrices
of
linear
algebra
are
replaced
by
coordinate
transformations
based
on
linear
and
affine
groups
and
the
more
general
high-dimensional
diffeomorphism
groups
the
central
objects
are
shapes
or
forms
in
computational
anatomy
one
set
of
examples
being
the
0123-dimensional
submanifolds
of
formula_20
a
second
set
of
examples
being
images
generated
via
medical
imaging
such
as
via
magnetic
resonance
imaging
(mri)
and
functional
magnetic
resonance
imaging
the
0-dimensional
manifolds
are
landmarks
or
fiducial
points;
1-dimensional
manifolds
are
curves
such
as
sulcul
and
gyral
curves
in
the
brain;
2-dimensional
manifolds
correspond
to
boundaries
of
substructures
in
anatomy
such
as
the
subcortical
structures
of
the
midbrain
or
the
gyral
surface
of
the
neocortex;
subvolumes
correspond
to
subregions
of
the
human
body
the
heart
the
thalamus
the
kidney
the
landmarks
formula_21
are
a
collections
of
points
with
no
other
structure
delineating
important
fiducials
within
human
shape
and
form
(see
associated
landmarked
image)
the
sub-manifold
shapes
such
as
surfaces
formula_22
are
collections
of
points
modeled
as
parametrized
by
a
local
chart
or
immersion
formula_23
formula_24
(see
figure
showing
shapes
as
mesh
surfaces)
the
images
such
as
mr
images
or
dti
images
formula_25
and
are
dense
functions
formula_26
are
scalars
vectors
and
matrices
(see
figure
showing
scalar
image)
groups
and
group
actions
are
familiar
to
the
engineering
community
with
the
universal
popularization
and
standardization
of
linear
algebra
as
a
basic
model
for
analyzing
signals
and
systems
in
mechanical
engineering
electrical
engineering
and
applied
mathematics
in
linear
algebra
the
matrix
groups
(matrices
with
inverses)
are
the
central
structure
with
group
action
defined
by
the
usual
definition
of
formula_27
as
an
formula_28
matrix
acting
on
formula_29
as
formula_30
vectors;
the
orbit
in
linear-algebra
is
the
set
of
formula_31-vectors
given
by
formula_32
which
is
a
group
action
of
the
matrices
through
the
orbit
of
formula_33
the
central
group
in
computational
anatomy
defined
on
volumes
in
formula_1
are
the
diffeomorphisms
formula_35
which
are
mappings
with
3-components
formula_36
law
of
composition
of
functions
formula_37
with
inverse
formula_38
most
popular
are
scalar
images
formula_39
with
action
on
the
right
via
the
inverse
for
sub-manifolds
formula_22
parametrized
by
a
chart
or
immersion
formula_42
the
diffeomorphic
action
the
flow
of
the
position
several
group
actions
in
computational
anatomy
have
been
defined
for
the
study
of
rigid
body
kinematics
the
low-dimensional
matrix
lie
groups
have
been
the
central
focus
the
matrix
groups
are
low-dimensional
mappings
which
are
diffeomorphisms
that
provide
one-to-one
correspondences
between
coordinate
systems
with
a
smooth
inverse
the
matrix
group
of
rotations
and
scales
can
be
generated
via
a
closed
form
finite-dimensional
matrices
which
are
solution
of
simple
ordinary
differential
equations
with
solutions
given
by
the
matrix
exponential
for
the
study
of
deformable
shape
in
computational
anatomy
a
more
general
diffeomorphism
group
has
been
the
group
of
choice
which
is
the
infinite
dimensional
analogue
the
high-dimensional
differeomorphism
groups
used
in
computational
anatomy
are
generated
via
smooth
flows
formula_44
which
satisfy
the
lagrangian
and
eulerian
specification
of
the
flow
fields
as
first
introduced
in
satisfying
the
ordinary
differential
equation:
with
formula_45
the
vector
fields
on
formula_46
termed
the
eulerian
velocity
of
the
particles
at
position
formula_47
of
the
flow
the
vector
fields
are
functions
in
a
function
space
modelled
as
a
smooth
hilbert
space
of
high-dimension
with
the
jacobian
of
the
flow
formula_48
a
high-dimensional
field
in
a
function
space
as
well
rather
than
a
low-dimensional
matrix
as
in
the
matrix
groups
flows
were
first
introduced
for
large
deformations
in
image
matching;
formula_49
is
the
instantaneous
velocity
of
particle
formula_50
at
time
formula_51
the
inverse
formula_52
required
for
the
group
is
defined
on
the
eulerian
vector-field
with
advective
inverse
flow
the
group
of
diffeomorphisms
is
very
big
to
ensure
smooth
flows
of
diffeomorphisms
avoiding
shock-like
solutions
for
the
inverse
the
vector
fields
must
be
at
least
1-time
continuously
differentiable
in
space
for
diffeomorphisms
on
formula_46
vector
fields
are
modelled
as
elements
of
the
hilbert
space
formula_54
using
the
sobolev
embedding
theorems
so
that
each
element
has
strictly
greater
than
2
generalized
square-integrable
spatial
derivatives
(thus
formula_55
is
sufficient)
yielding
1-time
continuously
differentiable
functions
the
diffeomorphism
group
are
flows
with
vector
fields
absolutely
integrable
in
sobolev
norm:
where
formula_56
with
the
linear
operator
formula_57
mapping
to
the
dual
space
formula_58
with
the
integral
calculated
by
integration
by
parts
when
formula_59
is
a
generalized
function
in
the
dual
space
the
images
are
denoted
with
the
orbit
as
formula_60
and
metric
formula_61
in
classical
mechanics
the
evolution
of
physical
systems
is
described
by
solutions
to
the
euler–lagrange
equations
associated
to
the
least-action
principle
of
hamilton
this
is
a
standard
way
for
example
of
obtaining
newton's
laws
of
motion
of
free
particles
more
generally
the
euler-lagrange
equations
can
be
derived
for
systems
of
generalized
coordinates
the
euler-lagrange
equation
in
computational
anatomy
describes
the
geodesic
shortest
path
flows
between
coordinate
systems
of
the
diffeomorphism
metric
in
computational
anatomy
the
generalized
coordinates
are
the
flow
of
the
diffeomorphism
and
its
lagrangian
velocity
formula_62
the
two
related
via
the
eulerian
velocity
formula_63
hamilton's
principle
for
generating
the
euler-lagrange
equation
requires
the
action
integral
on
the
lagrangian
given
by
the
lagrangian
is
given
by
the
kinetic
energy:
in
computational
anatomy
formula_64
was
first
called
the
eulerian
or
diffeomorphic
shape
momentum
since
when
integrated
against
eulerian
velocity
formula_65
gives
energy
density
and
since
there
is
a
conservation
of
diffeomorphic
shape
momentum
which
holds
the
operator
formula_27
is
the
generalized
moment
of
inertia
or
inertial
operator
classical
calculation
of
the
euler-lagrange
equation
from
hamilton's
principle
requires
the
perturbation
of
the
lagrangian
on
the
vector
field
in
the
kinetic
energy
with
respect
to
first
order
perturbation
of
the
flow
this
requires
adjustment
by
the
lie
bracket
of
vector
field
given
by
operator
formula_67
which
involves
the
jacobian
given
by
defining
the
adjoint
formula_69
then
the
first
order
variation
gives
the
eulerian
shape
momentum
formula_59
satisfying
the
generalized
equation:
meaning
for
all
smooth
formula_71
computational
anatomy
is
the
study
of
the
motions
of
submanifolds
points
curves
surfaces
and
volumes
momentum
associated
to
points
curves
and
surfaces
are
all
singular
implying
the
momentum
is
concentrated
on
subsets
of
formula_20
which
are
dimension
formula_74
in
lebesgue
measure
in
such
cases
the
energy
is
still
well
defined
formula_75
since
although
formula_76
is
a
generalized
function
the
vector
fields
are
smooth
and
the
eulerian
momentum
is
understood
via
its
action
on
smooth
functions
the
perfect
illustration
of
this
is
even
when
it
is
a
superposition
of
delta-diracs
the
velocity
of
the
coordinates
in
the
entire
volume
move
smoothlythe
euler-lagrange
equation
()
on
diffeomorphisms
for
generalized
functions
formula_77
was
derived
in
in
riemannian
metric
and
lie-bracket
interpretation
of
the
euler-lagrange
equation
on
geodesics
derivations
are
provided
in
terms
of
the
adjoint
operator
and
the
lie
bracket
for
the
group
of
diffeomorphisms
it
has
come
to
be
called
epdiff
equation
for
diffeomorphisms
connecting
to
the
euler-poincare
method
having
been
studied
in
the
context
of
the
inertial
operator
formula_78
for
incompressible
divergence
free
fluids
for
the
momentum
density
case
formula_79
then
euler–lagrange
equation
has
a
classical
solution:the
euler-lagrange
equation
on
diffeomorphisms
classically
defined
for
momentum
densities
first
appeared
in
for
medical
image
analysis
in
medical
imaging
and
computational
anatomy
positioning
and
coordinatizing
shapes
are
fundamental
operations;
the
system
for
positioning
anatomical
coordinates
and
shapes
built
on
the
metric
and
the
euler-lagrange
equation
a
geodesic
positioning
system
as
first
explicated
in
miller
trouve
and
younes
solving
the
geodesic
from
the
initial
condition
formula_80
is
termed
the
riemannian-exponential
a
mapping
formula_81
at
identity
to
the
group
the
riemannian
exponential
satisfies
formula_82
for
initial
condition
formula_83
vector
field
dynamics
formula_84
computing
the
flow
formula_80
onto
coordinates
riemannian
logarithm
mapping
formula_92
at
identity
from
formula_93
to
vector
field
formula_94;
formula_95
extended
to
the
entire
group
they
become
formula_96
;
formula_97
these
are
inverses
of
each
other
for
unique
solutions
of
logarithm;
the
first
is
called
geodesic
positioning
the
latter
geodesic
coordinates
(see
exponential
map
riemannian
geometry
for
the
finite
dimensional
version)the
geodesic
metric
is
a
local
flattening
of
the
riemannian
coordinate
system
(see
figure)
in
computational
anatomy
the
diffeomorphisms
are
used
to
push
the
coordinate
systems
and
the
vector
fields
are
used
as
the
control
within
the
anatomical
orbit
or
morphological
space
the
model
is
that
of
a
dynamical
system
the
flow
of
coordinates
formula_98
and
the
control
the
vector
field
formula_99
related
via
formula_100
the
hamiltonian
view
this
function
is
the
extended
hamiltonian
the
pontryagin
maximum
principle
gives
the
optimizing
vector
field
which
determines
the
geodesic
flow
satisfying
formula_105
as
well
as
the
reduced
hamiltonian
the
lagrange
multiplier
in
its
action
as
a
linear
form
has
its
own
inner
product
of
the
canonical
momentum
acting
on
the
velocity
of
the
flow
which
is
dependent
on
the
shape
eg
for
landmarks
a
sum
for
surfaces
a
surface
integral
and
for
volumes
it
is
a
volume
integral
with
respect
to
formula_107
on
formula_1
in
all
cases
the
greens
kernels
carry
weights
which
are
the
canonical
momentum
evolving
according
to
an
ordinary
differential
equation
which
corresponds
to
el
but
is
the
geodesic
reparameterization
in
canonical
momentum
the
optimizing
vector
field
is
given
by
with
dynamics
of
canonical
momentum
reparameterizing
the
vector
field
along
the
geodesic
whereas
the
vector
fields
are
extended
across
the
entire
background
space
of
formula_1
the
geodesic
flows
associated
to
the
submanifolds
has
eulerian
shape
momentum
which
evolves
as
a
generalized
function
formula_111
concentrated
to
the
submanifolds
for
landmarks
the
geodesics
have
eulerian
shape
momentum
which
are
a
superposition
of
delta
distributions
travelling
with
the
finite
numbers
of
particles;
the
diffeomorphic
flow
of
coordinates
have
velocities
in
the
range
of
weighted
green's
kernels
for
surfaces
the
momentum
is
a
surface
integral
of
delta
distributions
travelling
with
the
surface
the
geodesics
connecting
coordinate
systems
satisfying
have
stationarity
of
the
lagrangian
the
hamiltonian
is
given
by
the
extremum
along
the
path
formula_112
formula_113
equalling
the
and
is
stationary
along
defining
the
geodesic
velocity
at
the
identity
formula_114
then
along
the
geodesic
in
computational
anatomy
the
submanifolds
are
pointsets
curves
surfaces
and
subvolumes
which
are
the
basic
primitives
the
geodesic
flows
between
the
submanifolds
determine
the
distance
and
form
the
basic
measuring
and
transporting
tools
of
diffeomorphometry
at
formula_120
the
geodesic
has
vector
field
formula_123
determined
by
the
conjugate
momentum
and
the
green's
kernel
of
the
inertial
operator
defining
the
eulerian
momentum
formula_124
the
metric
distance
between
coordinate
systems
connected
via
the
geodesic
determined
by
the
induced
distance
between
identity
and
group
element:
given
the
least-action
there
is
a
natural
definition
of
momentum
associated
to
generalized
coordinates;
the
quantity
acting
against
velocity
gives
energy
the
field
has
studied
two
forms
the
momentum
associated
to
the
eulerian
vector
field
termed
eulerian
diffeomorphic
shape
momentum
and
the
momentum
associated
to
the
initial
coordinates
or
canonical
coordinates
termed
canonical
diffeomorphic
shape
momentum
each
has
a
conservation
lawthe
conservation
of
momentum
goes
hand
in
hand
with
the
in
computational
anatomy
formula_64
is
the
eulerian
momentum
since
when
integrated
against
eulerian
velocity
formula_65
gives
energy
density;
operator
formula_27
the
generalized
moment
of
inertia
or
inertial
operator
which
acting
on
the
eulerian
velocity
gives
momentum
which
is
conserved
along
the
geodesic:
conservation
of
eulerian
shape
momentum
was
shown
in
and
follows
from
;
conservation
of
canonical
momentum
was
shown
in
^t
p_t:
lddmm
matching
based
on
the
entire
tensor
matrix
has
group
action
becomes
formula_130
transformed
eigenvectors
the
variational
problem
matching
onto
the
principal
eigenvector
or
the
matrix
is
described
lddmm
tensor
image
matching
high
angular
resolution
diffusion
imaging
(hardi)
addresses
the
well-known
limitation
of
dti
that
is
dti
can
only
reveal
one
dominant
fiber
orientation
at
each
location
hardi
measures
diffusion
along
formula_132
uniformly
distributed
directions
on
the
sphere
and
can
characterize
more
complex
fiber
geometries
hardi
can
be
used
to
reconstruct
an
orientation
distribution
function
(odf)
that
characterizes
the
angular
profile
of
the
diffusion
probability
density
function
of
water
molecules
the
odf
is
a
function
defined
on
a
unit
sphere
formula_133
dense
lddmm
odf
matching
takes
the
hardi
data
as
odf
at
each
voxel
and
solves
the
lddmm
variational
problem
in
the
space
of
odf
in
the
field
of
information
geometry
the
space
of
odf
forms
a
riemannian
manifold
with
the
fisher-rao
metric
for
the
purpose
of
lddmm
odf
mapping
the
square-root
representation
is
chosen
because
it
is
one
of
the
most
efficient
representations
found
to
date
as
the
various
riemannian
operations
such
as
geodesics
exponential
maps
and
logarithm
maps
are
available
in
closed
form
in
the
following
denote
square-root
odf
(formula_134)
as
formula_135
where
formula_135
is
non-negative
to
ensure
uniqueness
and
formula_137
the
variational
problem
for
matching
assumes
that
two
odf
volumes
can
be
generated
from
one
to
another
via
flows
of
diffeomorphisms
formula_138
which
are
solutions
of
ordinary
differential
equations
formula_139
starting
from
the
identity
map
formula_140
denote
the
action
of
the
diffeomorphism
on
template
as
formula_141
formula_142
formula_143
are
respectively
the
coordinates
of
the
unit
sphere
formula_144
and
the
image
domain
with
the
target
indexed
similarly
formula_145formula_142formula_143
the
group
action
of
the
diffeomorphism
on
the
template
is
given
according
to
where
formula_149
is
the
jacobian
of
the
affined
transformed
odf
and
is
defined
as
formula_150
this
group
action
of
diffeomorphisms
on
odf
reorients
the
odf
and
reflects
changes
in
both
the
magnitude
of
formula_151
and
the
sampling
directions
of
formula_152
due
to
affine
transformation
it
guarantees
that
the
volume
fraction
of
fibers
oriented
toward
a
small
patch
must
remain
the
same
after
the
patch
is
transformed
the
lddmm
variational
problem
is
defined
as
where
the
logarithm
of
formula_154
is
defined
as
where
formula_156
is
the
normal
dot
product
between
points
in
the
sphere
under
the
formula_157
metric
this
lddmm-odf
mapping
algorithm
has
been
widely
used
to
study
brain
white
matter
degeneration
in
aging
alzheimer's
disease
and
vascular
dementia
the
brain
white
matter
atlas
generated
based
on
odf
is
constructed
via
bayesian
estimation
regression
analysis
on
odf
is
developed
in
the
odf
manifold
space
in
the
principle
mode
of
variation
represented
by
the
orbit
model
is
change
of
coordinates
for
setting
in
which
pairs
of
images
are
not
related
by
diffeomorphisms
but
have
photometric
variation
or
image
variation
not
represented
by
the
template
active
appearance
modelling
has
been
introduced
originally
by
edwards-cootes-taylor
and
in
3d
medical
imaging
in
in
the
context
of
computational
anatomy
in
which
metrics
on
the
anatomical
orbit
has
been
studied
metamorphosis
for
modelling
structures
such
as
tumors
and
photometric
changes
which
are
not
resident
in
the
template
was
introduced
in
for
magnetic
resonance
image
models
with
many
subsequent
developments
extending
the
metamorphosis
framework
for
image
matching
the
image
metamorphosis
framework
enlarges
the
action
so
that
formula_158
with
action
formula_159
in
this
setting
metamorphosis
combines
both
the
diffeomorphic
coordinate
system
transformation
of
computational
anatomy
as
well
as
the
early
morphing
technologies
which
only
faded
or
modified
the
photometric
or
image
intensity
alone
then
the
matching
problem
takes
a
form
with
equality
boundary
conditions:
transforming
coordinate
systems
based
on
landmark
point
or
fiducial
marker
features
dates
back
to
bookstein's
early
work
on
small
deformation
spline
methods
for
interpolating
correspondences
defined
by
fiducial
points
to
the
two-dimensional
or
three-dimensional
background
space
in
which
the
fiducials
are
defined
large
deformation
landmark
methods
came
on
in
the
late
90's
the
above
figure
depicts
a
series
of
landmarks
associated
three
brain
structures
the
amygdala
entorhinal
cortex
and
hippocampus
matching
geometrical
objects
like
unlabelled
point
distributions
curves
or
surfaces
is
another
common
problem
in
computational
anatomy
even
in
the
discrete
setting
where
these
are
commonly
given
as
vertices
with
meshes
there
are
no
predetermined
correspondences
between
points
as
opposed
to
the
situation
of
landmarks
described
above
from
the
theoretical
point
of
view
while
any
submanifold
formula_161
in
formula_20
formula_163
can
be
parameterized
in
local
charts
formula_164
all
reparametrizations
of
these
charts
give
geometrically
the
same
manifold
therefore
early
on
in
computational
anatomy
investigators
have
identified
the
necessity
of
parametrization
invariant
representations
one
indispensable
requirement
is
that
the
end-point
matching
term
between
two
submanifolds
is
itself
independent
of
their
parametrizations
this
can
be
achieved
via
concepts
and
methods
borrowed
from
geometric
measure
theory
in
particular
currents
and
varifolds
which
have
been
used
extensively
for
curve
and
surface
matching
denoted
the
landmarked
shape
formula_165
with
endpoint
formula_166
the
variational
problem
becomes
|}}the
geodesic
eulerian
momentum
is
a
generalized
function
formula_167
supported
on
the
landmarked
set
in
the
variational
problemthe
endpoint
condition
with
conservation
implies
the
initial
momentum
at
the
identity
of
the
group:
the
iterative
algorithm
for
large
deformation
diffeomorphic
metric
mapping
for
landmarks
is
given
glaunes
and
co-workers
first
introduced
diffeomorphic
matching
of
pointsets
in
the
general
setting
of
matching
distributions
as
opposed
to
landmarks
this
includes
in
particular
the
situation
of
weighted
point
clouds
with
no
predefined
correspondences
and
possibly
different
cardinalities
the
template
and
target
discrete
point
clouds
are
represented
as
two
weighted
sums
of
diracs
formula_169
and
formula_170
living
in
the
space
of
signed
measures
of
formula_171
the
space
is
equipped
with
a
hilbert
metric
obtained
from
a
real
positive
kernel
formula_172
on
formula_171
giving
the
following
norm:
the
matching
problem
between
a
template
and
target
point
cloud
may
be
then
formulated
using
this
kernel
metric
for
the
endpoint
matching
term:
where
formula_176
is
the
distribution
transported
by
the
deformation
in
the
one
dimensional
case
a
curve
in
3d
can
be
represented
by
an
embedding
formula_177
and
the
group
action
of
"diff"
becomes
formula_178
however
the
correspondence
between
curves
and
embeddings
is
not
one
to
one
as
the
any
reparametrization
formula_179
for
formula_180
a
diffeomorphism
of
the
interval
[01]
represents
geometrically
the
same
curve
in
order
to
preserve
this
invariance
in
the
end-point
matching
term
several
extensions
of
the
previous
0-dimensional
measure
matching
approach
can
be
considered
in
the
situation
of
oriented
curves
currents
give
an
efficient
setting
to
construct
invariant
matching
terms
in
such
representation
curves
are
interpreted
as
elements
of
a
functional
space
dual
to
the
space
vector
fields
and
compared
through
kernel
norms
on
these
spaces
matching
of
two
curves
formula_181
and
formula_182
writes
eventually
as
the
variational
problem
with
the
endpoint
term
formula_184
is
obtained
from
the
norm
the
derivative
formula_186
being
the
tangent
vector
to
the
curve
and
formula_187
a
given
matrix
kernel
of
formula_20
such
expressions
are
invariant
to
any
positive
reparametrizations
of
formula_181
and
formula_190
and
thus
still
depend
on
the
orientation
of
the
two
curves
varifold
is
an
alternative
to
currents
when
orientation
becomes
an
issue
as
for
instance
in
situations
involving
multiple
bundles
of
curves
for
which
no
"consistent"
orientation
may
be
defined
varifolds
directly
extend
0-dimensional
measures
by
adding
an
extra
tangent
space
direction
to
the
position
of
points
leading
to
represent
curves
as
measures
on
the
product
of
formula_20
and
the
grassmannian
of
all
straight
lines
in
formula_20
the
matching
problem
between
two
curves
then
consists
in
replacing
the
endpoint
matching
term
by
formula_193
with
varifold
norms
of
the
form:
where
formula_195
is
the
non-oriented
line
directed
by
tangent
vector
formula_186
and
formula_197
two
scalar
kernels
respectively
on
formula_2
and
the
grassmannian
due
to
the
inherent
non-oriented
nature
of
the
grassmannian
representation
such
expressions
are
invariant
to
positive
and
negative
reparametrizations
surface
matching
share
many
similarities
with
the
case
of
curves
surfaces
in
formula_20
are
parametrized
in
local
charts
by
embeddings
formula_200
with
all
reparametrizations
formula_201
with
formula_202
a
diffeomorphism
of
u
being
equivalent
geometrically
currents
and
varifolds
can
be
also
used
to
formalize
surface
matching
oriented
surfaces
can
be
represented
as
2-currents
which
are
dual
to
differential
2-forms
in
formula_20
one
can
further
identify
2-forms
with
vector
fields
through
the
standard
wedge
product
of
3d
vectors
in
that
setting
surface
matching
writes
again:
with
the
endpoint
term
formula_184
given
through
the
norm
with
formula_207
the
normal
vector
to
the
surface
parametrized
by
formula_208
this
surface
mapping
algorithm
has
been
validated
for
brain
cortical
surfaces
against
caret
and
freesurfer
lddmm
mapping
for
multiscale
surfaces
is
discussed
in
for
non-orientable
or
non-oriented
surfaces
the
varifold
framework
is
often
more
adequate
identifying
the
parametric
surface
formula_208
with
a
varifold
formula_210
in
the
space
of
measures
on
the
product
of
formula_20
and
the
grassmannian
one
simply
replaces
the
previous
current
metric
formula_212
by:
where
formula_214
is
the
(non-oriented)
line
directed
by
the
normal
vector
to
the
surface
there
are
many
settings
in
which
there
are
a
series
of
measurements
a
time-series
to
which
the
underlying
coordinate
systems
will
be
matched
and
flowed
onto
this
occurs
for
example
in
the
dynamic
growth
and
atrophy
models
and
motion
tracking
such
as
have
been
explored
in
an
observed
time
sequence
is
given
and
the
goal
is
to
infer
the
time
flow
of
geometric
change
of
coordinates
carrying
the
exemplars
or
templars
through
the
period
of
observations
the
generic
time-series
matching
problem
considers
the
series
of
times
is
formula_215
the
flow
optimizes
at
the
series
of
costs
formula_216
giving
optimization
problems
of
the
form
there
have
been
at
least
three
solutions
offered
thus
far
piecewise
geodesic
principal
geodesic
and
splines
the
random
orbit
model
of
computational
anatomy
first
appeared
in
modelling
the
change
in
coordinates
associated
to
the
randomness
of
the
group
acting
on
the
templates
which
induces
the
randomness
on
the
source
of
images
in
the
anatomical
orbit
of
shapes
and
forms
and
resulting
observations
through
the
medical
imaging
devices
such
a
random
orbit
model
in
which
randomness
on
the
group
induces
randomness
on
the
images
was
examined
for
the
special
euclidean
group
for
object
recognition
in
depicted
in
the
figure
is
a
depiction
of
the
random
orbits
around
each
exemplar
formula_218
generated
by
randomizing
the
flow
by
generating
the
initial
tangent
space
vector
field
at
the
identity
formula_94
and
then
generating
random
object
formula_220
the
random
orbit
model
induces
the
prior
on
shapes
and
images
formula_221
conditioned
on
a
particular
atlas
formula_222
for
this
the
generative
model
generates
the
mean
field
formula_223
as
a
random
change
in
coordinates
of
the
template
according
to
formula_224
where
the
diffeomorphic
change
in
coordinates
is
generated
randomly
via
the
geodesic
flows
the
prior
on
random
transformations
formula_225
on
formula_226
is
induced
by
the
flow
formula_227
with
formula_228
constructed
as
a
gaussian
random
field
prior
formula_229
the
density
on
the
random
observables
at
the
output
of
the
sensor
formula_230
are
given
by
formula_231
shown
in
the
figure
on
the
right
the
cartoon
orbit
are
a
random
spray
of
the
subcortical
manifolds
generated
by
randomizing
the
vector
fields
formula_118
supported
over
the
submanifolds
the
central
statistical
model
of
computational
anatomy
in
the
context
of
medical
imaging
has
been
the
source-channel
model
of
shannon
theory;
the
source
is
the
deformable
template
of
images
formula_233
the
channel
outputs
are
the
imaging
sensors
with
observables
formula_234
(see
figure)
see
the
bayesian
model
of
computational
anatomy
for
discussions
(i)
map
estimation
with
multiple
atlases
(ii)
map
segmentation
with
multiple
atlases
map
estimation
of
templates
from
populations
shape
in
computational
anatomy
is
a
local
theory
indexing
shapes
and
structures
to
templates
to
which
they
are
bijectively
mapped
statistical
shape
in
computational
anatomy
is
the
empirical
study
of
diffeomorphic
correspondences
between
populations
and
common
template
coordinate
systems
this
is
a
strong
departure
from
procrustes
analyses
and
shape
theories
pioneered
by
david
g
kendall
in
that
the
central
group
of
kendall's
theories
are
the
finite-dimensional
lie
groups
whereas
the
theories
of
shape
in
computational
anatomy
have
focused
on
the
diffeomorphism
group
which
to
first
order
via
the
jacobian
can
be
thought
of
as
a
field–thus
infinite
dimensional–of
low-dimensional
lie
groups
of
scale
and
rotations
the
random
orbit
model
provides
the
natural
setting
to
understand
empirical
shape
and
shape
statistics
within
computational
anatomy
since
the
non-linearity
of
the
induced
probability
law
on
anatomical
shapes
and
forms
formula_6
is
induced
via
the
reduction
to
the
vector
fields
formula_236
at
the
tangent
space
at
the
identity
of
the
diffeomorphism
group
the
successive
flow
of
the
euler
equation
induces
the
random
space
of
shapes
and
forms
formula_237
performing
empirical
statistics
on
this
tangent
space
at
the
identity
is
the
natural
way
for
inducing
probability
laws
on
the
statistics
of
shape
since
both
the
vector
fields
and
the
eulerian
momentum
formula_119
are
in
a
hilbert
space
the
natural
model
is
one
of
a
gaussian
random
field
so
that
given
test
function
formula_239
then
the
inner-products
with
the
test
functions
are
gaussian
distributed
with
mean
and
covariance
this
is
depicted
in
the
accompanying
figure
where
sub-cortical
brain
structures
are
depicted
in
a
two-dimensional
coordinate
system
based
on
inner
products
of
their
initial
vector
fields
that
generate
them
from
the
template
is
shown
in
a
2-dimensional
span
of
the
hilbert
space
the
study
of
shape
and
statistics
in
populations
are
local
theories
indexing
shapes
and
structures
to
templates
to
which
they
are
bijectively
mapped
statistical
shape
is
then
the
study
of
diffeomorphic
correspondences
relative
to
the
template
a
core
operation
is
the
generation
of
templates
from
populations
estimating
a
shape
that
is
matched
to
the
population
there
are
several
important
methods
for
generating
templates
including
methods
based
on
frechet
averaging
and
statistical
approaches
based
on
the
expectation-maximization
algorithm
and
the
bayes
random
orbit
models
of
computational
anatomy
shown
in
the
accompanying
figure
is
a
subcortical
template
reconstruction
from
the
population
of
mri
subjects
software
suites
containing
a
variety
of
diffeomorphic
mapping
algorithms
include
the
following:
bayesian
model
of
computational
anatomy
computational
anatomy
(ca)
is
a
discipline
within
medical
imaging
focusing
on
the
study
of
anatomical
shape
and
form
at
the
visible
or
gross
anatomical
scale
of
morphology
the
field
is
broadly
defined
and
includes
foundations
in
anatomy
applied
mathematics
and
pure
mathematics
including
medical
imaging
neuroscience
physics
probability
and
statistics
it
focuses
on
the
anatomical
structures
being
imaged
rather
than
the
medical
imaging
devices
the
central
focus
of
the
sub-field
of
computational
anatomy
within
medical
imaging
is
mapping
information
across
anatomical
coordinate
systems
most
often
dense
information
measured
within
a
magnetic
resonance
image
(mri)
the
introduction
of
flows
into
ca
which
are
akin
to
the
equations
of
motion
used
in
fluid
dynamics
exploit
the
notion
that
dense
coordinates
in
image
analysis
follow
the
lagrangian
and
eulerian
equations
of
motion
in
models
based
on
lagrangian
and
eulerian
flows
of
diffeomorphisms
the
constraint
is
associated
to
topological
properties
such
as
open
sets
being
preserved
coordinates
not
crossing
implying
uniqueness
and
existence
of
the
inverse
mapping
and
connected
sets
remaining
connected
the
use
of
diffeomorphic
methods
grew
quickly
to
dominate
the
field
of
mapping
methods
post
christensen's
original
paper
with
fast
and
symmetric
methods
becoming
available
the
central
statistical
model
of
computational
anatomy
in
the
context
of
medical
imaging
has
been
the
source-channel
model
of
shannon
theory;
the
source
is
the
deformable
template
of
images
formula_1
the
channel
outputs
are
the
imaging
sensors
with
observables
formula_2
(see
figure)
the
importance
of
the
source-channel
model
is
that
the
variation
in
the
anatomical
configuration
are
modelled
separated
from
the
sensor
variations
of
the
medical
imagery
the
bayes
theory
dictates
that
the
model
is
characterized
by
the
prior
on
the
source
formula_3
on
formula_4
and
the
conditional
density
on
the
observable
conditioned
on
formula_6
in
deformable
template
theory
the
images
are
linked
to
the
templates
with
the
deformations
a
group
which
acts
on
the
template;
see
group
action
in
computational
anatomy
for
image
action
formula_7
then
the
prior
on
the
group
formula_8
induces
the
prior
on
images
formula_9
written
as
densities
the
log-posterior
takes
the
form
the
random
orbit
model
which
follows
specifies
how
to
generate
the
group
elements
and
therefore
the
random
spray
of
objects
which
form
the
prior
distribution
the
random
orbit
model
of
computational
anatomy
first
appeared
in
modelling
the
change
in
coordinates
associated
to
the
randomness
of
the
group
acting
on
the
templates
which
induces
the
randomness
on
the
source
of
images
in
the
anatomical
orbit
of
shapes
and
forms
and
resulting
observations
through
the
medical
imaging
devices
such
a
random
orbit
model
in
which
randomness
on
the
group
induces
randomness
on
the
images
was
examined
for
the
special
euclidean
group
for
object
recognition
in
which
the
group
element
formula_11
was
the
special
euclidean
group
in
for
the
study
of
deformable
shape
in
ca
the
high-dimensional
diffeomorphism
groups
used
in
computational
anatomy
are
generated
via
smooth
flows
formula_12
which
satisfy
the
lagrangian
and
eulerian
specification
of
the
flow
fields
satisfying
the
ordinary
differential
equation:
with
formula_13
the
vector
fields
on
formula_14
termed
the
eulerian
velocity
of
the
particles
at
position
formula_15
of
the
flow
the
vector
fields
are
functions
in
a
function
space
modelled
as
a
smooth
hilbert
space
with
the
vector
fields
having
1-continuous
derivative
for
formula_16
the
inverse
of
the
flow
is
given
by
and
the
formula_17
jacobian
matrix
for
flows
in
formula_18
given
as
formula_19
to
ensure
smooth
flows
of
diffeomorphisms
with
inverse
the
vector
fields
formula_14
must
be
at
least
1-time
continuously
differentiable
in
space
which
are
modelled
as
elements
of
the
hilbert
space
formula_21
using
the
sobolev
embedding
theorems
so
that
each
element
formula_22
has
3-square-integrable
derivatives
thus
formula_21
embed
smoothly
in
1-time
continuously
differentiable
functions
the
diffeomorphism
group
are
flows
with
vector
fields
absolutely
integrable
in
sobolev
norm:
where
formula_24
with
formula_25
a
linear
operator
formula_26
defining
the
norm
of
the
rkhs
the
integral
is
calculated
by
integration
by
parts
when
formula_27
is
a
generalized
function
in
the
dual
space
formula_28
in
the
random
orbit
model
of
computational
anatomy
the
entire
flow
is
reduced
to
the
initial
condition
which
forms
the
coordinates
encoding
the
diffeomorphism
from
the
initial
condition
formula_29
then
geodesic
positioning
with
respect
to
the
riemannian
metric
of
computational
anatomy
solves
for
the
flow
of
the
euler-lagrange
equation
solving
the
geodesic
from
the
initial
condition
formula_29
is
termed
the
riemannian-exponential
a
mapping
formula_31
at
identity
to
the
group
the
riemannian
exponential
satisfies
formula_32
for
initial
condition
formula_33
vector
field
dynamics
formula_34
it
is
extended
to
the
entire
group
formula_41
depicted
in
the
accompanying
figure
is
a
depiction
of
the
random
orbits
around
each
exemplar
formula_42
generated
by
randomizing
the
flow
by
generating
the
initial
tangent
space
vector
field
at
the
identity
formula_43
and
then
generating
random
object
formula_44
shown
in
the
figure
on
the
right
the
cartoon
orbit
are
a
random
spray
of
the
subcortical
manifolds
generated
by
randomizing
the
vector
fields
formula_45
supported
over
the
submanifoldsthe
random
orbit
model
induces
the
prior
on
shapes
and
images
formula_46
conditioned
on
a
particular
atlas
formula_47
for
this
the
generative
model
generates
the
mean
field
formula_48
as
a
random
change
in
coordinates
of
the
template
according
to
formula_49
where
the
diffeomorphic
change
in
coordinates
is
generated
randomly
via
the
geodesic
flows
the
random
orbit
model
induces
the
prior
on
shapes
and
images
formula_46
conditioned
on
a
particular
atlas
formula_51
for
this
the
generative
model
generates
the
mean
field
formula_52
as
a
random
change
in
coordinates
of
the
template
according
to
formula_53
where
the
diffeomorphic
change
in
coordinates
is
generated
randomly
via
the
geodesic
flows
the
prior
on
random
transformations
formula_54
on
formula_55
is
induced
by
the
flow
formula_56
with
formula_57
constructed
as
a
gaussian
random
field
prior
formula_58
the
density
on
the
random
observables
at
the
output
of
the
sensor
formula_59
are
given
by
maximum
a
posteriori
estimation
(map)
estimation
is
central
to
modern
statistical
theory
parameters
of
interest
formula_61
take
many
forms
including
(i)
disease
type
such
as
neurodegenerative
or
neurodevelopmental
diseases
(ii)
structure
type
such
as
cortical
or
subcorical
structures
in
problems
associated
to
segmentation
of
images
and
(iii)
template
reconstruction
from
populations
given
the
observed
image
formula_62
map
estimation
maximizes
the
posterior:
this
requires
computation
of
the
conditional
probabilities
formula_64
the
multiple
atlas
orbit
model
randomizes
over
the
denumerable
set
of
atlases
formula_65
the
model
on
images
in
the
orbit
take
the
form
of
a
multi-modal
mixture
distribution
the
conditional
gaussian
model
has
been
examined
heavily
for
inexact
matching
in
dense
images
and
for
alndmark
matching
model
formula_67
as
a
conditionally
gaussian
random
field
conditioned
mean
field
formula_68
for
uniform
variance
the
endpoint
error
terms
plays
the
role
of
the
log-conditional
(only
a
function
of
the
mean
field)
giving
the
endpoint
term:
model
formula_69
as
conditionally
gaussian
with
mean
field
formula_70
constant
noise
variance
independent
of
landmarks
the
log-conditional
(only
a
function
of
the
mean
field)
can
be
viewed
as
the
endpoint
term:
the
random
orbit
model
for
multiple
atlases
models
the
orbit
of
shapes
as
the
union
over
multiple
anatomical
orbits
generated
from
the
group
action
of
diffeomorphisms
formula_72
with
each
atlas
having
a
template
and
predefined
segmentation
field
formula_73
incorporating
the
parcellation
into
anatomical
structures
of
the
coordinate
of
the
mri
the
pairs
are
indexed
over
the
voxel
lattice
formula_74
with
an
mri
image
and
a
dense
labelling
of
every
voxel
coordinatethe
anatomical
labelling
of
parcellated
structures
are
manual
delineations
by
neuroanatomists
the
bayes
segmentation
problem
is
given
measurement
formula_75
with
mean
field
and
parcellation
formula_76
the
anatomical
labelling
formula_77
mustg
be
estimated
for
the
measured
mri
image
the
mean-field
of
the
observable
formula_62
image
is
modelled
as
a
random
deformation
from
one
of
the
templates
formula_79
which
is
also
randomly
selected
formula_80
the
optimal
diffeomorphism
formula_81
is
hidden
and
acts
on
the
background
space
of
coordinates
of
the
randomly
selected
template
image
formula_82
given
a
single
atlas
formula_83
the
likelihood
model
for
inference
is
determined
by
the
joint
probability
formula_84;
with
multiple
atlases
the
fusion
of
the
likelihood
functions
yields
the
multi-modal
mixture
model
with
the
prior
averaging
over
models
the
map
estimator
of
segmentation
formula_85
is
the
maximizer
formula_86
given
formula_62
which
involves
the
mixture
over
all
atlases
the
quantity
formula_89
is
computed
via
a
fusion
of
likelihoods
from
multiple
deformable
atlases
with
formula_90
being
the
prior
probability
that
the
observed
image
evolves
from
the
specific
template
image
formula_91
the
map
segmentation
can
be
iteratively
solved
via
the
expectation-maximization(em)
algorithm
generating
templates
empirically
from
populations
is
a
fundamental
operation
ubiquitous
to
the
discipline
several
methods
based
on
bayesian
statistics
have
emerged
for
submanifolds
and
dense
image
volumes
for
the
dense
image
volume
case
given
the
observable
formula_93
the
problem
is
to
estimate
the
template
in
the
orbit
of
dense
images
formula_6
ma's
procedure
takes
an
initial
hypertemplate
formula_95
as
the
starting
point
and
models
the
template
in
the
orbit
under
the
unknown
to
be
estimated
diffeomorphism
formula_96
with
the
parameters
to
be
estimated
the
log-coordinates
formula_97
determining
the
geodesic
mapping
of
the
hyper-template
formula_98
in
the
bayesian
random
orbit
model
of
computational
anatomy
the
observed
mri
images
formula_99
are
modelled
as
a
conditionally
gaussian
random
field
with
mean
field
formula_100
with
formula_101
a
random
unknown
transformation
of
the
template
the
map
estimation
problem
is
to
estimate
the
unknown
template
formula_102
given
the
observed
mri
images
ma's
procedure
for
dense
imagery
takes
an
initial
hypertemplate
formula_95
as
the
starting
point
and
models
the
template
in
the
orbit
under
the
unknown
to
be
estimated
diffeomorphism
formula_96
the
observables
are
modelled
as
conditional
random
fields
formula_105
a
random
field
with
mean
field
formula_106
the
unknown
variable
to
be
estimated
explicitly
by
map
is
the
mapping
of
the
hyper-template
formula_107
with
the
other
mappings
considered
as
nuisance
or
hidden
variables
which
are
integrated
out
via
the
bayes
procedure
this
is
accomplished
using
the
expectation-maximization
(em)
algorithm
the
orbit-model
is
exploited
by
associating
the
unknown
to
be
estimated
flows
to
their
log-coordinates
formula_108
via
the
riemannian
geodesic
log
and
exponential
for
computational
anatomy
the
initial
vector
field
in
the
tangent
space
at
the
identity
so
that
formula_109
with
formula_110
the
mapping
of
the
hyper-template
the
map
estimation
problem
becomes
the
em
algorithm
takes
as
complete
data
the
vector-field
coordinates
parameterizing
the
mapping
formula_108
and
compute
iteratively
the
conditional-expectation
group
actions
in
computational
anatomy
group
actions
are
central
to
riemannian
geometry
and
defining
orbits
(control
theory)
the
orbits
of
computational
anatomy
consist
of
anatomical
shapes
and
medical
images;
the
anatomical
shapes
are
submanifolds
of
differential
geometry
consisting
of
points
curves
surfaces
and
subvolumes
this
generalized
the
ideas
of
the
more
familiar
orbits
of
linear
algebra
which
are
linear
vector
spaces
medical
images
are
scalar
and
tensor
images
from
medical
imaging
the
group
actions
are
used
to
define
models
of
human
shape
which
accommodate
variation
these
orbits
are
deformable
templates
as
originally
formulated
more
abstractly
in
pattern
theory
the
central
model
of
human
anatomy
in
computational
anatomy
is
a
groups
and
group
action
a
classic
formulation
from
differential
geometry
the
orbit
is
called
the
space
of
shapes
and
forms
the
space
of
shapes
are
denoted
formula_1
with
the
group
formula_2
with
law
of
composition
formula_3;
the
action
of
the
group
on
shapes
is
denoted
formula_4
where
the
action
of
the
group
formula_5
is
defined
to
satisfy
the
orbit
formula_7
of
the
template
becomes
the
space
of
all
shapes
formula_8
the
central
group
in
ca
defined
on
volumes
in
formula_9
are
the
diffeomorphism
group
formula_10
which
are
mappings
with
3-components
formula_11
law
of
composition
of
functions
formula_12
with
inverse
formula_13
for
sub-manifolds
formula_14
parametrized
by
a
chart
or
immersion
formula_15
the
diffeomorphic
action
the
flow
of
the
position
most
popular
are
scalar
images
formula_17
with
action
on
the
right
via
the
inverse
many
different
imaging
modalities
are
being
used
with
various
actions
for
images
such
that
formula_19
is
a
three-dimensional
vector
then
cao
et
al
examined
actions
for
mapping
mri
images
measured
via
diffusion
tensor
imaging
and
represented
via
there
principle
eigenvector
for
tensor
fields
a
positively
oriented
orthonormal
basis
formula_22
of
formula_23
termed
frames
vector
cross
product
denoted
formula_24
then
the
fr\'enet
frame
of
three
orthonormal
vectors
formula_26
deforms
as
a
tangent
formula_27
deforms
like
a
normal
to
the
plane
generated
by
formula_28
and
formula_27
h
is
uniquely
constrained
by
the
basis
being
positive
and
orthonormal
for
formula_30
non-negative
symmetric
matrices
an
action
would
become
formula_31
for
mapping
mri
dti
images
(tensors)
then
eigenvalues
are
preserved
with
the
diffeomorphism
rotating
eigenvectors
and
preserves
the
eigenvalues
given
eigenelements
formula_32
then
the
action
becomes
orientation
distribution
function
(odf)
characterizes
the
angular
profile
of
the
diffusion
probability
density
function
of
water
molecules
and
can
be
reconstructed
from
high
angular
resolution
diffusion
imaging
(hardi)
the
odf
is
a
probability
density
function
defined
on
a
unit
sphere
formula_35
in
the
field
of
information
geometry
the
space
of
odf
forms
a
riemannian
manifold
with
the
fisher-rao
metric
for
the
purpose
of
lddmm
odf
mapping
the
square-root
representation
is
chosen
because
it
is
one
of
the
most
efficient
representations
found
to
date
as
the
various
riemannian
operations
such
as
geodesics
exponential
maps
and
logarithm
maps
are
available
in
closed
form
in
the
following
denote
square-root
odf
(formula_36)
as
formula_37
where
formula_37
is
non-negative
to
ensure
uniqueness
and
formula_39
denote
diffeomorphic
transformation
as
formula_40
group
action
of
diffeomorphism
on
formula_37
formula_42
needs
to
guarantee
the
non-negativity
and
formula_43
based
on
the
derivation
in
this
group
action
is
defined
as
where
formula_45
is
the
jacobian
of
formula_46
large
deformation
diffeomorphic
metric
mapping
large
deformation
diffeomorphic
metric
mapping
(lddmm)
is
a
specific
suite
of
algorithms
used
for
diffeomorphic
mapping
and
manipulating
dense
imagery
based
on
diffeomorphic
metric
mapping
within
the
academic
discipline
of
computational
anatomy
to
be
distinguished
from
its
precursor
based
on
diffeomorphic
mapping
the
distinction
between
the
two
is
that
diffeomorphic
metric
maps
satisfy
the
property
that
the
length
associated
to
their
flow
away
from
the
identity
induces
a
metric
on
the
group
of
diffeomorphisms
which
in
turn
induces
a
metric
on
the
orbit
of
shapes
and
forms
within
the
field
of
computational
anatomy
the
study
of
shapes
and
forms
with
the
metric
of
diffeomorphic
metric
mapping
is
called
a
diffeomorphic
mapping
system
is
a
system
designed
to
map
manipulate
and
transfer
information
which
is
stored
in
many
types
of
spatially
distributed
medical
imagery
diffeomorphic
mapping
is
the
underlying
technology
for
mapping
and
analyzing
information
measured
in
human
anatomical
coordinate
systems
which
have
been
measured
via
medical
imaging
diffeomorphic
mapping
is
a
broad
term
that
actually
refers
to
a
number
of
different
algorithms
processes
and
methods
it
is
attached
to
many
operations
and
has
many
applications
for
analysis
and
visualization
diffeomorphic
mapping
can
be
used
to
relate
various
sources
of
information
which
are
indexed
as
a
function
of
spatial
position
as
the
key
index
variable
diffeomorphisms
are
by
their
latin
root
structure
preserving
transformations
which
are
in
turn
differentiable
and
therefore
smooth
allowing
for
the
calculation
of
metric
based
quantities
such
as
arc
length
and
surface
areas
spatial
location
and
extents
in
human
anatomical
coordinate
systems
can
be
recorded
via
a
variety
of
medical
imaging
modalities
generally
termed
multi-modal
medical
imagery
providing
either
scalar
and
or
vector
quantities
at
each
spatial
location
examples
are
scalar
t1
or
t2
magnetic
resonance
imagery
or
as
3x3
diffusion
tensor
matrices
diffusion
mri
and
diffusion-weighted
imaging
to
scalar
densities
associated
to
computed
tomography
(ct)
or
functional
imagery
such
as
temporal
data
of
functional
magnetic
resonance
imaging
and
scalar
densities
such
as
positron
emission
tomography
(pet)
computational
anatomy
is
a
subdiscipline
within
the
broader
field
of
neuroinformatics
within
bioinformatics
and
medical
imaging
the
first
algorithm
for
dense
image
mapping
via
diffeomorphic
metric
mapping
was
beg's
lddmm
for
volumes
and
joshi's
landmark
matching
for
point
sets
with
correspondence
with
lddmm
algorithms
now
available
for
computing
diffeomorphic
metric
maps
between
non-corresponding
landmarks
and
landmark
matching
intrinsic
to
spherical
manifolds
curves
currents
and
surfaces
tensors
varifolds
and
time-series
the
term
lddmm
was
first
established
as
part
of
the
national
institutes
of
health
supported
biomedical
informatics
research
network
in
a
more
general
sense
diffeomorphic
mapping
is
any
solution
that
registers
or
builds
correspondences
between
dense
coordinate
systems
in
medical
imaging
by
ensuring
the
solutions
are
diffeomorphic
there
are
now
many
codes
organized
around
diffeomorphic
registration
including
ants
dartel
demons
stationarylddmm
fastlddmm
as
examples
of
actively
used
computational
codes
for
constructing
correspondences
between
coordinate
systems
based
on
dense
images
the
distinction
between
diffeomorphic
metric
mapping
forming
the
basis
for
lddmm
and
the
earliest
methods
of
diffeomorphic
mapping
is
the
introduction
of
a
hamilton
principle
of
least-action
in
which
large
deformations
are
selected
of
shortest
length
corresponding
to
geodesic
flows
this
important
distinction
arises
from
the
original
formulation
of
the
riemannian
metric
corresponding
to
the
right-invariance
the
lengths
of
these
geodesics
give
the
metric
in
the
metric
space
structure
of
human
anatomy
non-geodeisc
formulations
of
diffeomorphic
mapping
in
general
does
not
correspond
to
any
metric
formulation
diffeomorphic
mapping
3-dimensional
information
across
coordinate
systems
is
central
to
high-resolution
medical
imaging
and
the
area
of
neuroinformatics
within
the
newly
emerging
field
of
bioinformatics
diffeomorphic
mapping
3-dimensional
coordinate
systems
as
measured
via
high
resolution
dense
imagery
has
a
long
history
in
3-d
beginning
with
computed
axial
tomography
(cat
scanning)
in
the
early
80's
by
the
university
of
pennsylvania
group
led
by
ruzena
bajcsy
and
subsequently
the
ulf
grenander
school
at
brown
university
with
the
hand
experiments
in
the
90's
there
were
several
solutions
for
image
registration
which
were
associated
to
linearizations
of
small
deformation
and
non-linear
elasticity
the
central
focus
of
the
sub-field
of
computational
anatomy
(ca)
within
medical
imaging
is
mapping
information
across
anatomical
coordinate
systems
at
the
1
millimeter
morphome
scale
in
ca
mapping
of
dense
information
measured
within
magnetic
resonance
image
(mri)
based
coordinate
systems
such
as
in
the
brain
has
been
solved
via
inexact
matching
of
3d
mr
images
one
onto
the
other
the
earliest
introduction
of
the
use
of
diffeomorphic
mapping
via
large
deformation
flows
of
diffeomorphisms
for
transformation
of
coordinate
systems
in
image
analysis
and
medical
imaging
was
by
christensen
rabbitt
and
miller
and
trouve
the
introduction
of
flows
which
are
akin
to
the
equations
of
motion
used
in
fluid
dynamics
exploit
the
notion
that
dense
coordinates
in
image
analysis
follow
the
lagrangian
and
eulerian
equations
of
motion
this
model
becomes
more
appropriate
for
cross-sectional
studies
in
which
brains
and
or
hearts
are
not
necessarily
deformations
of
one
to
the
other
methods
based
on
linear
or
non-linear
elasticity
energetics
which
grows
with
distance
from
the
identity
mapping
of
the
template
is
not
appropriate
for
cross-sectional
study
rather
in
models
based
on
lagrangian
and
eulerian
flows
of
diffeomorphisms
the
constraint
is
associated
to
topological
properties
such
as
open
sets
being
preserved
coordinates
not
crossing
implying
uniqueness
and
existence
of
the
inverse
mapping
and
connected
sets
remaining
connected
the
use
of
diffeomorphic
methods
grew
quickly
to
dominate
the
field
of
mapping
methods
post
christensen's
original
paper
with
fast
and
symmetric
methods
becoming
available
such
methods
are
powerful
in
that
they
introduce
notions
of
regularity
of
the
solutions
so
that
they
can
be
differentiated
and
local
inverses
can
be
calculated
the
disadvantages
of
these
methods
is
that
there
was
no
associated
global
least-action
property
which
could
score
the
flows
of
minimum
energy
this
contrasts
the
geodesic
motions
which
are
central
to
the
study
of
rigid
body
kinematics
and
the
many
problems
solved
in
physics
via
hamilton's
principle
of
least
action
in
1998
dupuis
grenander
and
miller
established
the
conditions
for
guaranteeing
the
existence
of
solutions
for
dense
image
matching
in
the
space
of
flows
of
diffeomorphisms
these
conditions
require
an
action
penalizing
kinetic
energy
measured
via
the
sobolev
norm
on
spatial
derivatives
of
the
flow
of
vector
fields
the
large
deformation
diffeomorphic
metric
mapping
(lddmm)
code
that
faisal
beg
derived
and
implemented
for
his
phd
at
johns
hopkins
university
developed
the
earliest
algorithmic
code
which
solved
for
flows
with
fixed
points
satisfying
the
necessary
conditions
for
the
dense
image
matching
problem
subject
to
least-action
computational
anatomy
now
has
many
existing
codes
organized
around
diffeomorphic
registration
including
ants
dartel
demons
lddmm
stationarylddmm
as
examples
of
actively
used
computational
codes
for
constructing
correspondences
between
coordinate
systems
based
on
dense
images
these
large
deformation
methods
have
been
extended
to
landmarks
without
registration
via
measure
matching
curves
surfaces
dense
vector
and
tensor
imagery
and
varifolds
removing
orientation
deformable
shape
in
computational
anatomy
(ca)is
studied
via
the
use
of
diffeomorphic
mapping
for
establishing
correspondences
between
anatomical
coordinates
in
medical
imaging
in
this
setting
three
dimensional
medical
images
are
modelled
as
a
random
deformation
of
some
exemplar
termed
the
template
formula_1
with
the
set
of
observed
images
element
in
the
random
orbit
model
of
ca
for
images
formula_2
the
template
is
mapped
onto
the
target
by
defining
a
variational
problem
in
which
the
template
is
transformed
via
the
diffeomorphism
used
as
a
change
of
coordinate
to
minimize
a
squared-error
matching
condition
between
the
transformed
template
and
the
target
the
diffeomorphisms
are
generated
via
smooth
flows
formula_3
with
formula_4
satisfying
the
lagrangian
and
eulerian
specification
of
the
flow
field
associated
to
the
ordinary
differential
equation
with
formula_6
the
eulerian
vector
fields
determining
the
flow
the
vector
fields
are
guaranteed
to
be
1-time
continuously
differentiable
formula_7
by
modelling
them
to
be
in
a
smooth
hilbert
space
formula_8
supporting
1-continuous
derivative
the
inverse
formula_9
is
defined
by
the
eulerian
vector-field
with
flow
given
by
to
ensure
smooth
flows
of
diffeomorphisms
with
inverse
the
vector
fields
with
components
in
formula_10
must
be
at
least
1-time
continuously
differentiable
in
space
which
are
modelled
as
elements
of
the
hilbert
space
formula_11
using
the
sobolev
embedding
theorems
so
that
each
element
formula_12
has
3-times
square-integrable
weak-derivatives
thus
formula_11
embeds
smoothly
in
1-time
continuously
differentiable
functions
the
diffeomorphism
group
are
flows
with
vector
fields
absolutely
integrable
in
sobolev
norm
in
ca
the
space
of
vector
fields
formula_11
are
modelled
as
a
reproducing
kernel
hilbert
space
(rkhs)
defined
by
a
1-1
differential
operatorformula_15
determining
the
norm
formula_16
where
the
integral
is
calculated
by
integration
by
parts
when
formula_17
is
a
generalized
function
in
the
dual
space
formula_18
the
differential
operator
is
selected
so
that
the
green's
kernel
the
inverse
of
the
operator
is
continuously
differentiable
in
each
variable
implying
that
the
vector
fields
support
1-continuous
derivative;
see
for
the
necessary
conditions
on
the
norm
for
existence
of
solutions
the
original
large
deformation
diffeomorphic
metric
mapping
(lddmm)
algorithms
of
beg
miller
trouve
younes
was
derived
taking
variations
with
respect
to
the
vector
field
parameterization
of
the
group
since
formula_19
are
in
a
vector
spaces
beg
solved
the
dense
image
matching
minimizing
the
action
integral
of
kinetic
energy
of
diffeomorphic
flow
while
minimizing
endpoint
matching
term
according
to
update
until
convergence
formula_20
each
iteration
with
formula_21:
this
implies
that
the
fixed
point
at
formula_22
satisfies
which
in
turn
implies
it
satisfies
the
conservation
equation
given
by
the
according
to
the
landmark
matching
problem
has
a
pointwise
correspondence
defining
the
endpoint
condition
with
geodesics
given
by
the
following
minimum:
joshi
originally
defined
the
registered
landmark
matching
probleme
update
until
convergence
formula_20
each
iteration
with
formula_21:
this
implies
that
the
fixed
point
satisfy
with
the
calculus
of
variations
was
used
in
beg
to
derive
the
iterative
algorithm
as
a
solution
which
when
it
converges
satisfies
the
necessary
maximizer
conditions
given
by
the
necessary
conditions
for
a
first
order
variation
requiring
the
variation
of
the
endpoint
with
respect
to
a
first
order
variation
of
the
vector
field
the
directional
derivative
calculates
the
gateaux
derivative
as
calculated
in
beg's
original
paper
and
the
lddmm
variational
problem
is
defined
as
beg
solved
the
early
lddmm
algorithms
by
solving
the
variational
matching
taking
variations
with
respect
to
the
vector
fields
another
solution
by
vialard
reparameterizes
the
optimization
problem
in
terms
of
the
state
formula_31
for
image
formula_32
with
the
dynamics
equation
controlling
the
state
by
the
control
given
in
terms
of
the
advection
equation
according
to
formula_33
the
endpoint
matching
term
formula_34
gives
the
variational
problem:
riemannian
metric
and
lie
bracket
in
computational
anatomy
computational
anatomy
(ca)
is
the
study
of
shape
and
form
in
medical
imaging
the
study
of
deformable
shapes
in
computational
anatomy
rely
on
high-dimensional
diffeomorphism
groups
formula_1
which
generate
orbits
of
the
form
formula_2
in
ca
this
orbit
is
in
general
considered
a
smooth
riemannian
manifold
since
at
every
point
of
the
manifold
formula_3
there
is
an
inner
product
inducing
the
norm
formula_4
on
the
tangent
space
that
varies
smoothly
from
point
to
point
in
the
manifold
of
shapes
formula_3
this
is
generated
by
viewing
the
group
of
diffeomorphisms
formula_1
as
a
riemannian
manifold
with
formula_7
associated
to
the
tangent
space
at
formula_8
this
induces
the
norm
and
metric
on
the
orbit
formula_3
under
the
action
from
the
group
of
diffeomorphisms
the
diffeomorphisms
in
computational
anatomy
are
generated
to
satisfy
the
lagrangian
and
eulerian
specification
of
the
flow
fields
formula_10
generated
via
the
ordinary
differential
equation
with
the
eulerian
vector
fields
formula_11
in
formula_12
for
formula_13
with
the
inverse
for
the
flow
given
by
and
the
formula_14
jacobian
matrix
for
flows
in
formula_15
given
as
formula_16
to
ensure
smooth
flows
of
diffeomorphisms
with
inverse
the
vector
fields
formula_12
must
be
at
least
1-time
continuously
differentiable
in
space
which
are
modelled
as
elements
of
the
hilbert
space
formula_18
using
the
sobolev
embedding
theorems
so
that
each
element
formula_19
has
3-square-integrable
derivatives
thusly
implies
formula_18
embeds
smoothly
in
1-time
continuously
differentiable
functions
the
diffeomorphism
group
are
flows
with
vector
fields
absolutely
integrable
in
sobolev
norm:
shapes
in
computational
anatomy
(ca)
are
studied
via
the
use
of
diffeomorphic
mapping
for
establishing
correspondences
between
anatomical
coordinate
systems
in
this
setting
3-dimensional
medical
images
are
modelled
as
diffemorphic
transformations
of
some
exemplar
termed
the
template
formula_21
resulting
in
the
observed
images
to
be
elements
of
the
random
orbit
model
of
ca
for
images
these
are
defined
as
formula_22
with
for
charts
representing
sub-manifolds
denoted
as
formula_23
the
orbit
of
shapes
and
forms
in
computational
anatomy
are
generated
by
the
group
actionformula_24
this
is
made
into
a
riemannian
orbit
by
introducing
a
metric
associated
to
each
point
and
associated
tangent
space
for
this
a
metric
is
defined
on
the
group
which
induces
the
metric
on
the
orbit
take
as
the
metric
for
computational
anatomy
at
each
element
of
the
tangent
space
formula_25
in
the
group
of
diffeomorphisms
with
the
vector
fields
modelled
to
be
in
a
hilbert
space
with
the
norm
in
the
hilbert
space
formula_18
we
model
formula_28
as
a
reproducing
kernel
hilbert
space
(rkhs)
defined
by
a
1-1
differential
operatorformula_29
for
formula_30
a
distribution
or
generalized
function
the
linear
form
formula_31
determines
the
norm:and
inner
product
for
formula_32
according
to
where
the
integral
is
calculated
by
integration
by
parts
for
formula_34
a
generalized
function
formula_35
the
dual-space
the
differential
operator
is
selected
so
that
the
green's
kernel
associated
to
the
inverse
is
sufficiently
smooth
so
that
the
vector
fields
support
1-continuous
derivative
the
metric
on
the
group
of
diffeomorphisms
is
defined
by
the
distance
as
defined
on
pairs
of
elements
in
the
group
of
diffeomorphisms
according
to
this
distance
provides
a
right-invariant
metric
of
diffeomorphometry
invariant
to
reparameterization
of
space
since
for
all
formula_1
the
lie
bracket
gives
the
adjustment
of
the
velocity
term
resulting
from
a
perturbation
of
the
motion
in
the
setting
of
curved
spaces
using
hamilton's
principle
of
least-action
derives
the
optimizing
flows
as
a
critical
point
for
the
action
integral
of
the
integral
of
the
kinetic
energy
the
lie
bracket
for
vector
fields
in
computational
anatomy
was
first
introduced
in
miller
trouve
and
younes
the
derivation
calculates
the
perturbation
formula_38
on
the
vector
fields
formula_39
in
terms
of
the
derivative
in
time
of
the
group
perturbation
adjusted
by
the
correction
of
the
lie
bracket
of
vector
fields
in
this
function
setting
involving
the
jacobian
matrix
unlike
the
matrix
group
case:
proof:
proving
lie
bracket
of
vector
fields
take
a
first
order
perturbation
of
the
flow
at
point
formula_1
the
lie
bracket
gives
the
first
order
variation
of
the
vector
field
with
respect
to
first
order
variation
of
the
flow
the
euler–lagrange
equation
can
be
used
to
calculate
geodesic
flows
through
the
group
which
form
the
basis
for
the
metric
the
action
integral
for
the
lagrangian
of
the
kinetic
energy
for
hamilton's
principle
becomes
the
action
integral
in
terms
of
the
vector
field
corresponds
to
integrating
the
kinetic
energy
the
shortest
paths
geodesic
connections
in
the
orbit
are
defined
via
hamilton's
principle
of
least
action
requires
first
order
variations
of
the
solutions
in
the
orbits
of
computational
anatomy
which
are
based
on
computing
critical
points
on
the
metric
length
or
energy
of
the
path
the
original
derivation
of
the
euler
equation
associated
to
the
geodesic
flow
of
diffeomorphisms
exploits
the
was
a
generalized
function
equation
whenformula_43
is
a
distribution
or
generalized
function
take
the
first
order
variation
of
the
action
integral
using
the
adjoint
operator
for
the
lie
bracket
()
gives
for
all
smooth
formula_44
using
the
bracket
formula_46
and
formula_47
gives
meaning
for
all
smooth
formula_48
equation
()
is
the
euler-equation
when
diffeomorphic
shape
momentum
is
a
generalized
function
this
equation
has
been
called
epdiff
euler–poincare
equation
for
diffeomorphisms
and
has
been
studied
in
the
context
of
fluid
mechanics
for
incompressible
fluids
with
formula_50
metric
in
the
random
orbit
model
of
computational
anatomy
the
entire
flow
is
reduced
to
the
initial
condition
which
forms
the
coordinates
encoding
the
diffeomorphism
as
well
as
providing
the
means
of
positioning
information
in
the
orbit
this
was
first
terms
a
geodesic
positioning
system
in
miller
trouve
and
younes
from
the
initial
condition
formula_51
then
geodesic
positioning
with
respect
to
the
riemannian
metric
of
computational
anatomy
solves
for
the
flow
of
the
euler–lagrange
equation
solving
the
geodesic
from
the
initial
condition
formula_51
is
termed
the
riemannian-exponential
a
mapping
formula_53
at
identity
to
the
group
the
riemannian
exponential
satisfies
formula_54
for
initial
condition
formula_55
vector
field
dynamics
formula_56
it
is
extended
to
the
entire
group
formula_62
matching
information
across
coordinate
systems
is
central
to
computational
anatomy
adding
a
matching
term
formula_63
to
the
action
integral
of
equation
()
which
represents
the
target
endpoint
the
endpoint
term
adds
a
boundary
condition
for
the
euler–lagrange
equation
()
which
gives
the
euler
equation
with
boundary
term
taking
the
variation
gives
proof:
the
proof
via
variation
calculus
uses
the
perturbations
from
above
and
classic
calculus
of
variation
arguments
the
earliest
large
deformation
diffeomorphic
metric
mapping
(lddmm)
algorithms
solved
matching
problems
associated
to
images
and
registered
landmarks
are
in
a
vector
spaces
the
image
matching
geodesic
equation
satisfies
the
classical
dynamical
equation
with
endpoint
condition
the
necessary
conditions
for
the
geodesic
for
image
matching
takes
the
form
of
the
classic
equation
()
of
euler–lagrange
with
boundary
condition:
the
registered
landmark
matching
problem
satisfies
the
dynamical
equation
for
generalized
functions
with
endpoint
condition:
proof:
the
variation
formula_70
requires
variation
of
the
inverse
formula_71
generalizes
the
matrix
perturbation
of
the
inverse
via
formula_72
giving
formula_73
giving
plexciton
plexcitons
are
polaritonic
modes
that
result
from
coherently
coupled
plasmons
and
excitons
plexcitons
aid
direct
energy
flows
in
exciton
energy
transfer
(eet)
plexcitons
travel
for
20
μm
similar
to
the
width
of
a
human
hair
plasmons
are
a
quantity
of
collective
electron
oscillations
excitons
are
excited
electrons
bound
to
the
hole
produced
by
their
excitation
molecular
crystal
excitons
were
combined
with
the
collective
excitations
within
metals
to
create
plexcitons
this
allowed
eet
to
reach
distances
of
around
20000
nanometers
an
enormous
increase
over
the
some
10
nanometers
possible
previously
however
the
transfer
direction
was
uncontrolled
topological
insulators
(ti)
act
as
insulators
below
their
surface
but
have
conductive
surfaces
constraining
electrons
to
move
only
along
that
surface
even
materials
with
moderately
flawed
surfaces
do
not
impede
current
flow
topological
plexcitons
make
use
of
the
properties
of
tis
to
achieve
similar
control
over
the
direction
of
current
flow
plexcitons
were
found
to
emerge
from
an
organic
molecular
layer
(excitons)
and
a
metallic
film
(plasmons)
dirac
cones
appeared
in
the
plexcitons'
two-dimensional
band-structure
an
external
magnetic
field
created
a
gap
between
the
cones
when
the
system
was
interfaced
to
a
magneto-optical
layer
the
resulting
energy
gap
became
populated
with
topologically
protected
one-way
modes
which
traveled
only
at
the
system
interface
plexcitons
potentially
offer
an
appealing
platform
for
exploring
exotic
matter
phases
and
for
controlling
nanoscale
energy
flows
front
(physics)
in
physics
a
front
is
a
solution
of
an
spatially
extended
system
connecting
two
steady
states
from
dynamical
systems
point
of
view
a
front
correspond
to
a
heteroclinic
orbit
of
the
system
in
the
co-mobile
frame
(or
proper
frame)
the
most
simple
example
of
front
solution
connecting
a
homogeneous
stable
state
with
a
homogeneous
unstable
state
can
be
shown
in
the
one-dimensional
fisher-kolmogorov
equation:
that
describes
a
simple
model
for
the
density
formula_2
of
population
this
equation
has
two
steady
states
formula_3
and
formula_4
this
solution
corresponds
to
extinction
and
saturation
of
population
observe
that
this
model
is
spatially-extended
because
it
includes
a
diffusion
term
given
by
the
second
derivative
the
state
formula_3
is
stable
as
a
simple
linear
analysis
can
show
and
the
state
formula_4
is
unstable
there
exist
a
family
of
front
solutions
connecting
formula_7
with
formula_8
and
such
solution
are
propagative
particularly
there
exist
one
solution
of
the
form
formula_9
where
formula_10
constants
depending
only
on
the
coefficient
of
diffusion
and
growth
constant
k
jahn–teller
effect
the
jahn–teller
effect
(jt
effect
or
jte)
is
an
important
mechanism
of
spontaneous
symmetry
breaking
in
molecular
and
solid-state
systems
which
has
far-reaching
consequences
for
different
fields
and
it
is
related
to
a
variety
of
applications
in
spectroscopy
stereochemistry
and
crystal
chemistry
molecular
and
solid-state
physics
and
materials
science
the
effect
is
named
for
hermann
arthur
jahn
and
edward
teller
who
first
reported
studies
about
it
in
1937
the
jahn–teller
effect
sometimes
also
known
as
jahn–teller
distortion
describes
the
geometrical
distortion
of
molecules
and
ions
that
is
associated
with
certain
electron
configurations
the
jahn–teller
theorem
essentially
states
that
any
non-linear
molecule
with
a
spatially
degenerate
electronic
ground
state
will
undergo
a
geometrical
distortion
that
removes
that
degeneracy
because
the
distortion
lowers
the
overall
energy
of
the
species
for
a
description
of
another
type
of
geometrical
distortion
that
occurs
in
crystals
with
substitutional
impurities
see
article
off-center
ions
the
jahn–teller
effect
is
most
often
encountered
in
octahedral
complexes
of
the
transition
metals
the
phenomenon
is
very
common
in
six-coordinate
copper(ii)
complexes
the
"d"
electronic
configuration
of
this
ion
gives
three
electrons
in
the
two
degenerate
"e"
orbitals
leading
to
a
doubly
degenerate
electronic
ground
state
such
complexes
distort
along
one
of
the
molecular
fourfold
axes
(always
labelled
the
"z"
axis)
which
has
the
effect
of
removing
the
orbital
and
electronic
degeneracies
and
lowering
the
overall
energy
the
distortion
normally
takes
the
form
of
elongating
the
bonds
to
the
ligands
lying
along
the
"z"
axis
but
occasionally
occurs
as
a
shortening
of
these
bonds
instead
(the
jahn–teller
theorem
does
not
predict
the
direction
of
the
distortion
only
the
presence
of
an
unstable
geometry)
when
such
an
elongation
occurs
the
effect
is
to
lower
the
electrostatic
repulsion
between
the
electron-pair
on
the
lewis
basic
ligand
and
any
electrons
in
orbitals
with
a
"z"
component
thus
lowering
the
energy
of
the
complex
the
inversion
centre
is
preserved
after
the
distortion
in
octahedral
complexes
the
jahn–teller
effect
is
most
pronounced
when
an
odd
number
of
electrons
occupy
the
"e"
orbitals
this
situation
arises
in
complexes
with
the
configurations
"d"
low-spin
"d"
or
high-spin
"d"
complexes
all
of
which
have
doubly
degenerate
ground
states
in
such
compounds
the
"e"
orbitals
involved
in
the
degeneracy
point
directly
at
the
ligands
so
distortion
can
result
in
a
large
energetic
stabilisation
strictly
speaking
the
effect
also
occurs
when
there
is
a
degeneracy
due
to
the
electrons
in
the
"t"
orbitals
("ie"
configurations
such
as
"d"
or
"d"
both
of
which
are
triply
degenerate)
in
such
cases
however
the
effect
is
much
less
noticeable
because
there
is
a
much
smaller
lowering
of
repulsion
on
taking
ligands
further
away
from
the
"t"
orbitals
which
do
not
point
"directly"
at
the
ligands
(see
the
table
below)
the
same
is
true
in
tetrahedral
complexes
(eg
manganate:
distortion
is
very
subtle
because
there
is
less
stabilisation
to
be
gained
because
the
ligands
are
not
pointing
directly
at
the
orbitals
the
expected
effects
for
octahedral
coordination
are
given
in
the
following
table:
w:
weak
jahn–teller
effect
("t"
orbitals
unevenly
occupied)
s:
strong
jahn–teller
effect
expected
("e"
orbitals
unevenly
occupied)
blank:
no
jahn–teller
effect
expected
the
jahn–teller
effect
is
manifested
in
the
uv-vis
absorbance
spectra
of
some
compounds
where
it
often
causes
splitting
of
bands
it
is
readily
apparent
in
the
structures
of
many
copper(ii)
complexes
additional
detailed
information
about
the
anisotropy
of
such
complexes
and
the
nature
of
the
ligand
binding
can
be
however
obtained
from
the
fine
structure
of
the
low-temperature
electron
spin
resonance
spectra
the
underlying
cause
of
the
jahn–teller
effect
is
the
presence
of
molecular
orbitals
that
are
both
degenerate
and
open
shell
(ie
incompletely
occupied)
this
situation
is
not
unique
to
coordination
complexes
and
can
be
encountered
in
other
areas
of
chemistry
in
organic
chemistry
the
phenomenon
of
antiaromaticity
has
the
same
cause
and
also
often
sees
molecules
distorting;
as
in
the
case
of
cyclobutadiene
and
cyclooctatetraene
(cot)
the
jt
theorem
can
be
stated
in
different
forms
two
of
which
are
given
here:
alternatively
and
considerably
shorter:
spin-degeneracy
was
an
exception
in
the
original
treatment
and
was
later
treated
separately
the
formal
mathematical
proof
of
the
jahn–teller
theorem
rests
heavily
on
symmetry
arguments
more
specifically
the
theory
of
molecular
point
groups
the
argument
of
jahn
and
teller
assumes
no
details
about
the
electronic
structure
of
the
system
jahn
and
teller
made
no
statement
about
the
strength
of
the
effect
which
may
be
so
small
that
it
is
immeasurable
indeed
for
electrons
in
non-bonding
or
weakly
bonding
molecular
orbitals
the
effect
is
expected
to
be
weak
however
in
many
situations
the
jt
effect
is
important
interest
in
the
jte
increased
after
its
first
experimental
verification
various
model
systems
were
developed
probing
the
degree
of
degeneracy
and
the
type
of
symmetry
these
were
solved
partly
analytically
and
partly
numerically
to
obtain
the
shape
of
the
pertinent
potential
energy
surfaces
(pes)
and
the
energy
levels
for
the
nuclear
motion
on
the
jt-split
pes
these
energy
levels
are
not
vibrational
energy
levels
in
the
traditional
sense
because
of
the
intricate
coupling
to
the
electronic
motion
that
occurs
and
are
better
termed
vibronic
energy
levels
the
new
field
of
‘vibronic
coupling’
or
‘vibronic
coupling
theory’
was
born
a
further
breakthrough
occurred
upon
the
advent
of
modern
("ab
initio")
electronic
structure
calculations
whereby
the
relevant
parameters
characterising
jt
systems
can
be
reliably
determined
from
first
principles
thus
one
could
go
beyond
studies
of
model
systems
that
explore
the
effect
of
parameter
variations
on
the
pes
and
vibronic
energy
levels;
one
could
also
go
on
beyond
fitting
these
parameters
to
experimental
data
without
clear
knowledge
about
the
significance
of
the
fit
instead
well-founded
theoretical
investigations
became
possible
which
greatly
improved
the
insight
into
the
phenomena
at
hand
and
into
the
details
of
the
underlying
mechanisms
while
recognizing
the
jte
distortion
as
a
concrete
example
of
the
general
spontaneous
symmetry
breaking
mechanism
the
exact
degeneracy
of
the
involved
electronic
state
was
identified
as
a
non-essential
ingredient
for
this
symmetry
breaking
in
polyatomic
systems
even
systems
that
in
the
undistorted
symmetric
configuration
present
electronic
states
which
are
near
in
energy
but
not
precisely
degenerate
can
show
a
similar
tendency
to
distort
the
distortions
of
these
systems
can
be
treated
within
the
related
theory
of
the
pseudo
jahn–teller
effect
(in
the
literature
often
referred
to
as
"second-order
jte")
this
mechanism
is
associated
to
the
vibronic
couplings
between
adiabatic
pes
separated
by
nonzero
energy
gaps
across
the
configuration
space:
its
inclusion
extends
the
applicability
of
jt-related
models
to
symmetry
breaking
in
a
far
broader
range
of
molecular
and
solid-state
systems
"chronology:"
a
given
jt
problem
will
have
a
particular
point
group
symmetry
such
as
t
symmetry
for
magnetic
impurity
ions
in
semiconductors
or
i
symmetry
for
the
fullerene
c
jt
problems
are
conventionally
classified
using
labels
for
the
irreducible
representations
(irreps)
that
apply
to
the
symmetry
of
the
electronic
and
vibrational
states
for
example
e
⊗
e
would
refer
to
an
electronic
doublet
state
transforming
as
e
coupled
to
a
vibrational
doublet
state
transforming
as
e
in
general
a
vibrational
mode
transforming
as
λ
will
couple
to
an
electronic
state
transforming
as
γ
if
the
symmetric
part
of
the
kronecker
product
[γ
⊗
γ]
contains
λ
unless
γ
is
a
double
group
representation
when
the
antisymmetric
part
{γ
⊗
γ}
is
considered
instead
modes
which
do
couple
are
said
to
be
jt-active
as
an
example
consider
a
doublet
electronic
state
e
in
cubic
symmetry
the
symmetric
part
of
e
⊗
e
is
a
+
e
therefore
the
state
e
will
couple
to
vibrational
modes
formula_1
transforming
as
a
and
e
however
the
a
modes
will
result
in
the
same
energy
shift
to
all
states
and
therefore
do
not
contribute
to
any
jt
splitting
they
can
therefore
be
neglected
the
result
is
an
e
⊗
e
jt
effect
this
jt
effect
is
experienced
by
triangular
molecules
x
tetrahedral
molecules
ml
and
octahedral
molecules
ml
when
their
electronic
state
has
e
symmetry
components
of
a
given
vibrational
mode
are
also
labelled
according
to
their
transformation
properties
for
example
the
two
components
of
an
e
mode
are
usually
labelled
formula_2
and
formula_3
which
in
octahedral
symmetry
transform
as
formula_4
and
formula_5
respectively
eigenvalues
of
the
hamiltonian
of
a
polyatomic
system
define
pess
as
functions
of
normal
modes
formula_1
of
the
system
(ie
linear
combinations
of
the
nuclear
displacements
with
specific
symmetry
properties
at
the
reference
point
of
high
symmetry
where
the
symmetry-induced
degeneracy
occurs
several
of
the
eigenvalues
coincide
by
a
detailed
and
laborious
analysis
jahn
and
teller
showed
that
–
excepting
linear
molecules
–
there
are
always
first-order
terms
in
an
expansion
of
the
matrix
elements
of
the
hamiltonian
in
terms
of
symmetry-lowering
(in
the
language
of
group
theory:
non-totally
symmetric)
normal
modes
these
linear
terms
represent
forces
that
distort
the
system
along
these
coordinates
and
lift
the
degeneracy
the
point
of
degeneracy
can
thus
not
be
stationary
and
the
system
distorts
toward
a
stationary
point
of
lower
symmetry
where
stability
can
be
attained
proof
of
the
jt
theorem
follows
from
the
theory
of
molecular
symmetry
(point
group
theory)
a
less
rigorous
but
more
intuitive
explanation
is
given
in
section
to
arrive
at
a
quantitative
description
of
the
jt
effect
the
forces
appearing
between
the
component
wave
functions
are
described
by
expanding
the
hamiltonian
in
a
power
series
in
the
formula_1
owing
to
the
very
nature
of
the
degeneracy
the
hamiltonian
takes
the
form
of
a
matrix
referring
to
the
degenerate
wave
function
components
a
matrix
element
between
states
formula_8
and
formula_9
generally
reads
as:
the
expansion
can
be
truncated
after
terms
linear
in
the
formula_1
or
extended
to
include
terms
quadratic
(or
higher)
in
the
formula_1
the
adiabatic
potential
energy
surfaces
(apes)
are
then
obtained
as
the
eigenvalues
of
this
matrix
in
the
original
paper
it
is
proven
that
there
are
always
linear
terms
in
the
expansion
it
follows
that
the
degeneracy
of
the
wave
function
cannot
correspond
to
a
stable
structure
in
mathematical
terms
the
apess
characterising
the
jt
distortion
arise
as
the
eigenvalues
of
the
potential
energy
matrix
(as
described
in
)
generally
the
apess
take
the
characteristic
appearance
of
a
double
cone
circular
or
elliptic
where
the
point
of
contact
ie
degeneracy
denotes
the
high-symmetry
configuration
for
which
the
jt
theorem
applies
for
the
above
case
of
the
linear
e
⊗
e
jt
effect
the
situation
is
illustrated
by
the
apes
displayed
in
the
figure
with
part
cut
away
to
reveal
its
shape
which
is
known
as
a
mexican
hat
potential
here
formula_14
is
the
frequency
of
the
vibrational
e
mode
formula_15
is
its
mass
and
formula_16
is
a
measure
of
the
strength
of
the
jt
coupling
the
conical
shape
near
the
degeneracy
at
the
origin
makes
it
immediately
clear
that
this
point
cannot
be
stationary
that
is
the
system
is
unstable
against
asymmetric
distortions
which
leads
to
a
symmetry
lowering
in
this
particular
case
there
are
infinitely
many
isoenergetic
jt
distortions
the
formula_1
giving
these
distortions
are
arranged
in
a
circle
as
shown
by
the
red
curve
in
the
figure
quadratic
coupling
or
cubic
elastic
terms
lead
to
a
warping
along
this
"minimum
energy
path"
replacing
this
infinite
manifold
by
three
equivalent
potential
minima
and
three
equivalent
saddle
points
in
other
jt
systems
linear
coupling
results
in
discrete
minima
the
high
symmetry
of
the
double-cone
topology
of
the
linear
e
⊗
e
jt
system
directly
reflects
the
high
underlying
symmetry
it
is
one
of
the
earliest
(if
not
the
earliest)
examples
in
the
literature
of
a
conical
intersection
of
potential
energy
surfaces
conical
intersections
have
received
wide
attention
in
the
literature
starting
in
the
1990s
and
are
now
considered
paradigms
of
nonadiabatic
excited-state
dynamics
with
far-reaching
consequences
in
molecular
spectroscopy
photochemistry
and
photophysics
some
of
these
will
be
commented
upon
further
below
in
general
conical
intersections
are
far
less
symmetric
than
depicted
in
the
figure
they
can
be
tilted
and
elliptical
in
shape
etc
and
also
peaked
and
sloped
intersections
have
been
distinguished
in
the
literature
furthermore
for
more
than
two
degrees
of
freedom
they
are
not
point-like
structures
but
instead
they
are
seams
and
complicated
curved
hypersurfaces
also
known
as
intersection
space
the
coordinate
sub-space
displayed
in
the
figure
is
also
known
as
a
branching
plane
the
characteristic
shape
of
the
jt-split
apes
has
specific
consequences
for
the
nuclear
dynamics
here
considered
in
the
fully
quantum
sense
for
sufficiently
strong
jt
coupling
the
minimum
points
are
sufficiently
far
(at
least
by
a
few
vibrational
energy
quanta)
below
the
jt
intersection
two
different
energy
regimes
are
then
to
be
distinguished
those
of
low
and
high
energy
as
already
stated
above
the
distinction
of
low
and
high
energy
regimes
is
valid
only
for
sufficiently
strong
jt
couplings
that
is
when
several
or
many
vibrational
energy
quanta
fit
into
the
energy
window
between
the
conical
intersection
and
the
minimum
of
the
lower
jt-split
apes
for
the
many
cases
of
small
to
intermediate
jt
couplings
this
energy
window
and
the
corresponding
adiabatic
low-energy
regime
does
not
exist
rather
the
levels
on
both
jt-split
apes
are
intricately
mixed
for
all
energies
and
the
nuclear
motion
always
proceeds
on
both
jt
split
apes
simultaneously
in
1965
frank
ham
proposed
that
the
dynamic
jte
could
reduce
the
expected
values
of
observables
associated
with
the
orbital
wavefunctions
due
to
the
superposition
of
several
electronic
states
in
the
total
vibronic
wavefunction
this
effect
leads
for
example
to
a
partial
quenching
of
the
spin-orbit
interaction
and
allowed
the
results
of
previous
electron
paramagnetic
resonance
(epr)
experiments
to
be
explained
in
general
the
result
of
an
orbital
operator
acting
on
vibronic
states
can
be
replaced
by
an
effective
orbital
operator
acting
on
purely
electronic
states
in
first
order
the
effective
orbital
operator
equals
the
actual
orbital
operator
multiplied
by
a
constant
whose
value
is
less
than
one
known
as
a
first-order
(ham)
reduction
factor
for
example
within
a
triplet
t
electronic
state
the
spin-orbit
coupling
operator
formula_18
can
be
replaced
by
formula_19
where
formula_20
is
a
function
of
the
strength
of
the
jt
coupling
which
varies
from
1
in
zero
coupling
to
0
in
very
strong
coupling
furthermore
when
second-order
perturbation
corrections
are
included
additional
terms
are
introduced
involving
additional
numerical
factors
known
as
second-order
(ham)
reduction
factors
these
factors
are
zero
when
there
is
no
jt
coupling
but
can
dominate
over
first-order
terms
in
strong
coupling
when
the
first-order
effects
have
been
significantly
reduced
reduction
factors
are
particularly
useful
for
describing
experimental
results
such
as
epr
and
optical
spectra
of
paramagnetic
impurities
in
semiconducting
dielectric
diamagnetic
and
ferrimagnetic
hosts
for
a
long
time
applications
of
jt
theory
consisted
mainly
in
parameter
studies
(model
studies)
where
the
apes
and
dynamical
properties
of
jt
systems
have
been
investigated
as
functions
on
the
system
parameters
such
as
coupling
constants
etc
fits
of
these
parameters
to
experimental
data
were
often
doubtful
and
inconclusive
the
situation
changed
in
the
1980s
when
efficient
ab
initio
methods
were
developed
and
computational
resources
became
powerful
enough
to
allow
for
a
reliable
determination
of
these
parameters
from
first
principles
apart
from
wave
function-based
techniques
(which
are
sometimes
considered
genuinely
ab
initio
in
the
literature)
the
advent
of
density
functional
theory
(dft)
opened
up
new
avenues
to
treat
larger
systems
including
solids
this
allowed
details
of
jt
systems
to
be
characterised
and
experimental
findings
to
be
reliably
interpreted
it
lies
at
the
heart
of
most
developments
addressed
in
section
two
different
strategies
are
conceivable
and
have
been
used
in
the
literature
one
can
naturally
the
more
accurate
approach
(2)
may
be
limited
to
smaller
systems
while
the
simpler
approach
(1)
lends
itself
to
studies
of
larger
systems
the
jt
distortion
of
small
molecules
(or
molecular
ions)
is
directly
deduced
from
electronic
structure
calculations
of
their
apes
(through
dft
and/or
ab
initio
computations)
these
molecules
/
ions
are
often
radicals
such
as
trimers
of
alkali
atoms
(li
and
na)
that
have
unpaired
spins
and
in
particular
in
(but
not
restricted
to)
doublet
states
besides
the
jte
in
e'
and
e"
states
also
the
between
an
e
state
and
a
nearby
a
state
may
play
a
role
the
jt
distortion
reduces
the
symmetry
from
d
to
c
(see
figure)
and
it
depends
on
the
details
of
the
interactions
whether
the
isosceles
triangle
has
an
acute
or
an
obtuse-angled
(such
as
na)
minimum
energy
structure
natural
extensions
are
systems
like
no
and
nh
where
a
jt
distortion
has
been
documented
in
the
literature
for
ground
or
excited
electronic
states
a
somewhat
special
role
is
played
by
tetrahedral
systems
like
ch
and
p
here
threefold
degenerate
electronic
states
and
vibrational
modes
come
into
play
nevertheless
also
twofold
degeneracies
continue
to
be
important
among
larger
systems
a
focus
in
the
literature
has
been
on
benzene
and
its
radical
cation
as
well
as
on
their
halo
(especially
fluoro)
derivatives
already
in
the
early
1980s
a
wealth
of
information
emerged
from
the
detailed
analysis
of
experimental
emission
spectra
of
135-
trifluoro-
and
hexafluoro
(and
chloro)
benzene
radical
cations
for
the
parent
benzene
cation
one
has
to
rely
on
photoelectron
spectra
with
comparatively
lower
resolution
because
this
species
does
not
fluoresce
(see
also
section
on
)
rather
detailed
ab
initio
calculations
have
been
carried
out
which
document
the
jt
stabilization
energies
for
the
various
(four)
jt
active
modes
and
also
quantify
the
moderate
barriers
for
the
jt
pseudorotation
finally
a
somewhat
special
role
is
played
by
systems
with
a
fivefold
symmetry
axis
like
the
cyclopentadienyl
radical
careful
laser
spectroscopic
investigations
have
shed
useful
light
on
the
jt
interactions
in
particular
they
reveal
that
the
barrier
to
pseudorotation
almost
vanishes
(the
system
is
highly
"fluxional")
which
can
be
attributed
to
the
fact
that
the
2nd-order
coupling
terms
vanish
by
symmetry
and
the
leading
higher-order
terms
are
of
4th
order
the
jte
is
usually
stronger
where
the
electron
density
associated
with
the
degenerate
orbitals
is
more
concentrated
this
effect
therefore
plays
a
large
role
in
determining
the
structure
of
transition
metal
complexes
with
active
internal
3d
orbitals
the
most
iconic
and
prominent
of
the
jt
systems
in
coordination
chemistry
is
probably
the
case
of
cu(ii)
octahedral
complexes
while
in
perfectly
equivalent
coordination
like
a
cuf
complex
associated
to
a
cu(ii)
impurity
in
a
cubic
crystal
like
kmgf
perfect
octahedral
(o)
symmetry
is
expected
in
fact
a
lower
tetragonal
symmetry
is
usually
found
experimentally
the
origin
of
this
jte
distortion
it
revealed
by
examining
the
electronic
configuration
of
the
undistorted
complex
for
an
octahedral
geometry
the
five
3d
orbitals
partition
into
t
and
e
orbitals
(see
diagram)
these
orbitals
are
occupied
by
nine
electrons
corresponding
to
the
formula_21
electronic
configuration
of
cu(ii)
thus
the
t
shell
is
filled
and
the
e
shell
contains
3
electrons
overall
the
unpaired
electron
produces
a
e
state
which
is
jahn–teller
active
the
third
electron
can
occupy
either
of
the
orbitals
comprising
the
e
shell:
the
mainly
formula_4
orbital
or
the
mainly
formula_5
orbital
if
the
electron
occupies
the
mainly
formula_4
level
which
antibonding
orbital
the
final
geometry
of
the
complex
would
be
elongated
as
the
axial
ligands
will
be
pushed
away
to
reduce
the
global
energy
of
the
system
on
the
other
hand
if
the
electron
went
into
the
mainly
formula_5
antibonding
orbital
the
complex
would
distort
into
a
compressed
geometry
experimentally
elongated
geometries
are
overwhelmingly
observed
and
this
fact
has
been
attributed
both
to
metal-ligand
anharmonic
interactions
and
3d-4s
hybridisations
given
that
all
the
directions
containing
a
fourfold
axis
are
equivalent
the
distortion
is
equally
likely
to
happen
in
any
of
these
orientations
from
the
electronic
point
of
view
this
means
that
the
formula_4
and
formula_5
orbitals
that
are
degenerate
and
free
to
hybridise
in
the
octahedral
geometry
will
mix
to
produce
appropriate
equivalent
orbitals
in
each
direction
like
formula_28
or
formula_29
the
jte
is
not
just
restricted
to
cu(ii)
octahedral
complexes
there
are
many
other
configurations
involving
changes
both
in
the
initial
structure
and
electronic
configuration
of
the
metal
that
yield
degenerate
states
and
thus
jte
however
the
amount
of
distortion
and
stabilisation
energy
of
the
effect
is
strongly
dependent
on
the
particular
case
in
octahedral
cu(ii)
the
jte
is
particularly
strong
because
in
other
configurations
involving
π
or
δ
bonding
like
for
example
when
the
degenerate
state
is
associated
to
the
t
orbitals
of
an
octahedral
configuration
the
distortion
and
stabilisation
energies
are
usually
much
smaller
and
the
possibility
of
not
observing
the
distortion
due
to
dynamic
jt
effects
is
much
higher
similarly
for
rare-earth
ions
where
covalency
is
very
small
the
distortions
associated
to
the
jte
are
usually
very
weak
importantly
the
jte
is
associated
with
strict
degeneracy
in
the
electronic
subsystem
and
so
it
cannot
appear
in
systems
without
this
property
for
example
the
jte
is
often
associated
to
cases
like
quasi-octahedral
cuxy
complexes
where
the
distances
to
x
and
y
ligands
are
clearly
different
however
the
intrinsic
symmetry
of
these
complexes
is
already
tetragonal
and
no
degenerate
e
orbital
exists
having
split
into
a
(mainly
formula_4)
and
b
(mainly
formula_5)
orbitals
due
to
the
different
electronic
interactions
with
axial
x
ligands
and
equatorial
y
ligands
in
this
and
other
similar
cases
some
remaining
vibronic
effects
related
to
the
jte
are
still
present
but
are
quenched
with
respect
to
the
case
with
degeneracy
due
to
the
splitting
of
the
orbitals
from
spectra
with
rotational
resolution
moments
of
inertia
and
hence
bond
lengths
and
angles
can
be
determined
"directly"
(at
least
in
principle)
from
less
well-resolved
spectra
one
can
still
determine
important
quantities
like
jt
stabilization
energies
and
energy
barriers
(eg
to
pseudorotation)
however
in
the
whole
spectral
intensity
distribution
formula_32
of
an
electronic
transition
more
information
is
encoded
it
has
been
used
to
decide
on
the
presence
(or
absence)
of
the
geometric
phase
which
is
accumulated
during
the
pseudorotational
motion
around
the
jt
(or
other
type
of)
conical
intersection
prominent
examples
of
either
type
are
the
ground
(x)
or
an
excited
(b)
state
of
na
the
fourier
transform
of
formula_32
the
so-called
autocorrelation
function
formula_34
reflects
the
motion
of
the
wavepacket
after
an
optical
(=
vertical)
transition
to
the
apes
of
the
final
electronic
state
typically
it
will
move
on
the
timescale
of
a
vibrational
period
which
is
(for
small
molecules)
of
the
order
of
5-50
fs
ie
ultrafast
besides
a
nearly
periodic
motion
mode-mode
interactions
with
very
irregular
(also
chaotic)
behaviour
and
spreading
of
the
wavepacket
may
also
occur
near
a
conical
intersection
this
will
be
accompanied/complemented
by
nonradiative
transitions
(termed
internal
conversion)
to
other
apess
occurring
on
the
same
ultrafast
time
scale
for
the
jt
case
the
situation
is
somewhat
special
as
compared
to
a
general
conical
intersection
because
the
different
jt
potential
sheets
are
symmetry-related
to
each
other
and
have
(exactly
or
nearly)
the
same
energy
minimum
the
"transition"
between
them
is
thus
more
oscillatory
than
one
would
normally
expect
and
their
time-averaged
populations
are
close
to
1/2
for
a
more
typical
scenario
a
more
general
conical
intersection
is
"required"
the
jt
effect
still
comes
into
play
namely
in
combination
with
a
different
nearby
in
general
non-degenerate
electronic
state
the
result
is
a
pseudo
jahn–teller
effect
for
example
of
an
e
state
interacting
with
an
a
state
this
situation
is
common
in
jt
systems
just
as
interactions
between
two
nondegenerate
electronic
states
are
common
for
non-jt
systems
examples
are
excited
electronic
states
of
nh
and
the
benzene
radical
cation
here
crossings
between
the
e
and
a
state
apess
amount
to
triple
intersections
which
are
associated
with
very
complex
spectral
features
(dense
line
structures
and
diffuse
spectral
envelopes
under
low
resolution)
the
population
transfer
between
the
states
is
also
ultrafast
so
fast
that
fluorescence
(proceeding
on
a
nanosecond
time
scale)
cannot
compete
this
helps
to
understand
why
the
benzene
cation
like
many
other
organic
radical
cation
does
not
fluoresce
to
be
sure
photochemical
reactivity
emerges
when
the
internal
conversion
makes
the
system
explore
the
nuclear
configuration
space
such
that
new
chemical
species
are
formed
there
is
a
plethora
of
femtosecond
pump-probe
spectroscopic
techniques
to
reveal
details
of
these
processes
occurring
for
example
in
the
process
of
vision
as
proposed
originally
by
landau
free
electrons
in
a
solid
introduced
for
example
by
doping
or
irradiation
can
interact
with
the
vibrations
of
the
lattice
to
form
a
localized
quasi-particle
known
as
a
polaron
strongly
localized
polarons
(also
called
holstein
polarons)
can
condensate
around
high-symmetry
sites
of
the
lattice
with
electrons
or
holes
occupying
local
degenerate
orbitals
that
experience
the
jte
these
jahn–teller
polarons
break
both
translational
and
point
group
symmetries
of
the
lattice
where
they
are
found
and
have
been
attributed
important
roles
in
effects
like
colossal
magnetoresistance
and
superconductivity
paramagnetic
impurities
in
semiconducting
dielectric
diamagnetic
and
ferrimagnetic
hosts
can
all
be
described
using
a
jt
model
for
example
these
models
were
used
extensively
in
the
1980s
and
1990s
to
describe
ions
of
cr
v
and
ti
substituting
for
ga
in
gaas
and
gap
the
fullerene
c
can
form
solid
compounds
with
alkali
metals
known
as
fullerides
csc
can
be
superconducting
at
temperatures
up
to
38k
under
applied
pressure
whereas
compounds
of
the
form
ac
are
insulating
(as
reviewed
by
gunnarsson
)
jt
effects
both
within
the
c
molecules
(intramolecular)
and
between
c
molecules
(intermolecular)
play
a
part
in
the
mechanisms
behind
various
observed
properties
in
these
systems
for
example
they
could
mean
that
the
migdal-eliashberg
treatment
of
superconductivity
breaks
down
also
the
fullerides
can
form
a
so-called
new
state
of
matter
known
as
a
jahn–teller
metal
where
localised
electrons
coexist
with
metallicity
and
jt
distortions
on
the
c
molecules
persist
the
jte
is
usually
associated
with
degeneracies
that
are
well
localised
in
space
like
those
occurring
in
a
small
molecule
or
associated
to
an
isolated
transition
metal
complex
however
in
many
periodic
high-symmetry
solid-state
systems
like
perovskites
some
crystalline
sites
allow
for
electronic
degeneracy
giving
rise
under
adequate
compositions
to
lattices
of
jt-active
centers
this
can
produce
a
cooperative
jte
where
global
distortions
of
the
crystal
occur
due
to
local
degeneracies
in
order
to
determine
the
final
electronic
and
geometric
structure
of
a
cooperative
jt
system
it
is
necessary
to
take
into
account
both
the
local
distortions
and
the
interaction
between
the
different
sites
which
will
take
such
form
necessary
to
minimise
the
global
energy
of
the
crystal
while
works
on
the
cooperative
jte
started
in
the
late
fifties
it
was
in
1960
that
kanamori
published
the
first
work
on
the
cooperative
jte
where
many
important
elements
present
in
the
modern
theory
for
this
effect
were
introduced
this
included
the
use
of
pseudospin
notation
to
discuss
orbital
ordering
and
discussions
of
the
importance
of
the
jte
to
discuss
magnetism
the
competition
of
this
effect
with
the
spin-orbit
coupling
and
the
coupling
of
the
distortions
with
the
strain
of
the
lattice
this
point
was
later
stressed
in
the
review
by
gehring
and
gehring
as
being
the
key
element
to
establish
long-range
order
between
the
distortions
in
the
lattice
an
important
part
of
the
modern
theory
of
the
cooperative
jte
can
lead
to
structural
phase
transitions
it
is
important
to
note
that
many
cooperative
jt
systems
would
be
expected
to
be
metals
from
band
theory
as
to
produce
them
a
degenerate
orbital
has
to
be
partially
filled
and
the
associated
band
would
be
metallic
however
under
the
perturbation
of
the
symmetry-breaking
distortion
associated
to
the
cooperative
jte
the
degeneracies
in
the
electronic
structure
are
destroyed
and
the
ground
state
of
these
systems
is
often
found
to
be
insulating
(see
eg)
in
many
important
cases
like
the
parent
compound
for
colossal
magnetoresistance
perovskites
lamno
an
increase
of
temperature
leads
to
disorder
in
the
distortions
which
lowers
the
band
splitting
due
to
the
cooperative
jte
thus
triggering
a
metal-insulator
transition
in
modern
solid-state
physics
it
is
common
to
classify
systems
according
to
the
kind
of
degrees
of
freedom
they
have
available
like
electron
(metals)
or
spin
(magnetism)
in
crystals
that
can
display
the
jte
and
before
this
effect
is
realised
by
symmetry-breaking
distortions
it
is
found
that
there
exists
an
orbital
degree
of
freedom
consisting
of
how
electrons
occupy
the
local
degenerate
orbitals
as
initially
discussed
by
kugel
and
khomskii
not
all
configurations
are
equivalent
the
key
is
the
relative
orientation
of
these
occupied
orbital
in
the
same
way
that
spin
orientation
is
important
in
magnetic
systems
and
the
ground
state
can
only
be
realised
for
some
particular
orbital
pattern
both
this
pattern
and
the
effect
giving
rise
to
this
phenomenon
is
usually
denominated
orbital-ordering
in
order
to
predict
the
orbital-ordering
pattern
kugel
and
khomskii
used
a
particularisation
of
the
hubbard
model
in
particular
they
established
how
superexchange
interactions
usually
described
by
the
anderson–kanamori–goodenough
rules
change
in
the
presence
of
degenerate
orbitals
their
model
using
a
pseudospin
representation
for
the
local
orbitals
leads
to
a
heisenberg-like
model
in
which
the
ground
state
is
a
combination
of
orbital
and
spin
patterns
using
this
model
it
can
be
shown
for
example
that
the
origin
of
the
unusual
ground
insulating
ferromagnetic
state
of
a
solid
like
kcuf
can
be
traced
to
its
orbital
ordering
even
when
starting
from
a
relatively
high-symmetry
structure
the
combined
effect
of
exchange
interactions
spin-orbit
coupling
orbital-ordering
and
crystal
deformations
activated
by
the
jte
can
lead
to
very
low
symmetry
magnetic
patterns
with
specific
properties
for
example
in
cscucl
an
incommensurable
helicoidal
pattern
appears
both
for
the
orbitals
and
the
distortions
along
the
formula_35-axis
moreover
many
of
these
compounds
show
complex
phase
diagrams
when
varying
temperature
or
pressure
solid-state
physics
solid-state
physics
is
the
study
of
rigid
matter
or
solids
through
methods
such
as
quantum
mechanics
crystallography
electromagnetism
and
metallurgy
it
is
the
largest
branch
of
condensed
matter
physics
solid-state
physics
studies
how
the
large-scale
properties
of
solid
materials
result
from
their
atomic-scale
properties
thus
solid-state
physics
forms
a
theoretical
basis
of
materials
science
it
also
has
direct
applications
for
example
in
the
technology
of
transistors
and
semiconductors
solid
materials
are
formed
from
densely
packed
atoms
which
interact
intensely
these
interactions
produce
the
mechanical
(eg
hardness
and
elasticity)
thermal
electrical
magnetic
and
optical
properties
of
solids
depending
on
the
material
involved
and
the
conditions
in
which
it
was
formed
the
atoms
may
be
arranged
in
a
regular
geometric
pattern
(crystalline
solids
which
include
metals
and
ordinary
water
ice)
or
irregularly
(an
amorphous
solid
such
as
common
window
glass)
the
bulk
of
solid-state
physics
as
a
general
theory
is
focused
on
crystals
primarily
this
is
because
the
periodicity
of
atoms
in
a
crystal
—
its
defining
characteristic
—
facilitates
mathematical
modeling
likewise
crystalline
materials
often
have
electrical
magnetic
optical
or
mechanical
properties
that
can
be
exploited
for
engineering
purposes
the
forces
between
the
atoms
in
a
crystal
can
take
a
variety
of
forms
for
example
in
a
crystal
of
sodium
chloride
(common
salt)
the
crystal
is
made
up
of
ionic
sodium
and
chlorine
and
held
together
with
ionic
bonds
in
others
the
atoms
share
electrons
and
form
covalent
bonds
in
metals
electrons
are
shared
amongst
the
whole
crystal
in
metallic
bonding
finally
the
noble
gases
do
not
undergo
any
of
these
types
of
bonding
in
solid
form
the
noble
gases
are
held
together
with
van
der
waals
forces
resulting
from
the
polarisation
of
the
electronic
charge
cloud
on
each
atom
the
differences
between
the
types
of
solid
result
from
the
differences
between
their
bonding
the
physical
properties
of
solids
have
been
common
subjects
of
scientific
inquiry
for
centuries
but
a
separate
field
going
by
the
name
of
solid-state
physics
did
not
emerge
until
the
1940s
in
particular
with
the
establishment
of
the
division
of
solid
state
physics
(dssp)
within
the
american
physical
society
the
dssp
catered
to
industrial
physicists
and
solid-state
physics
became
associated
with
the
technological
applications
made
possible
by
research
on
solids
by
the
early
1960s
the
dssp
was
the
largest
division
of
the
american
physical
society
large
communities
of
solid
state
physicists
also
emerged
in
europe
after
world
war
ii
in
particular
in
england
germany
and
the
soviet
union
in
the
united
states
and
europe
solid
state
became
a
prominent
field
through
its
investigations
into
semiconductors
superconductivity
nuclear
magnetic
resonance
and
diverse
other
phenomena
during
the
early
cold
war
research
in
solid
state
physics
was
often
not
restricted
to
solids
which
led
some
physicists
in
the
1970s
and
1980s
to
found
the
field
of
condensed
matter
physics
which
organized
around
common
techniques
used
to
investigate
solids
liquids
plasmas
and
other
complex
matter
today
solid-state
physics
is
broadly
considered
to
be
the
subfield
of
condensed
matter
physics
that
focuses
on
the
properties
of
solids
with
regular
crystal
lattices
many
properties
of
materials
are
affected
by
their
crystal
structure
this
structure
can
be
investigated
using
a
range
of
crystallographic
techniques
including
x-ray
crystallography
neutron
diffraction
and
electron
diffraction
the
sizes
of
the
individual
crystals
in
a
crystalline
solid
material
vary
depending
on
the
material
involved
and
the
conditions
when
it
was
formed
most
crystalline
materials
encountered
in
everyday
life
are
polycrystalline
with
the
individual
crystals
being
microscopic
in
scale
but
macroscopic
single
crystals
can
be
produced
either
naturally
(eg
diamonds)
or
artificially
real
crystals
feature
defects
or
irregularities
in
the
ideal
arrangements
and
it
is
these
defects
that
critically
determine
many
of
the
electrical
and
mechanical
properties
of
real
materials
properties
of
materials
such
as
electrical
conduction
and
heat
capacity
are
investigated
by
solid
state
physics
an
early
model
of
electrical
conduction
was
the
drude
model
which
applied
kinetic
theory
to
the
electrons
in
a
solid
by
assuming
that
the
material
contains
immobile
positive
ions
and
an
"electron
gas"
of
classical
non-interacting
electrons
the
drude
model
was
able
to
explain
electrical
and
thermal
conductivity
and
the
hall
effect
in
metals
although
it
greatly
overestimated
the
electronic
heat
capacity
arnold
sommerfeld
combined
the
classical
drude
model
with
quantum
mechanics
in
the
free
electron
model
(or
drude-sommerfeld
model)
here
the
electrons
are
modelled
as
a
fermi
gas
a
gas
of
particles
which
obey
the
quantum
mechanical
fermi–dirac
statistics
the
free
electron
model
gave
improved
predictions
for
the
heat
capacity
of
metals
however
it
was
unable
to
explain
the
existence
of
insulators
the
nearly
free
electron
model
is
a
modification
of
the
free
electron
model
which
includes
a
weak
periodic
perturbation
meant
to
model
the
interaction
between
the
conduction
electrons
and
the
ions
in
a
crystalline
solid
by
introducing
the
idea
of
electronic
bands
the
theory
explains
the
existence
of
conductors
semiconductors
and
insulators
the
nearly
free
electron
model
rewrites
the
schrödinger
equation
for
the
case
of
a
periodic
potential
the
solutions
in
this
case
are
known
as
bloch
states
since
bloch's
theorem
applies
only
to
periodic
potentials
and
since
unceasing
random
movements
of
atoms
in
a
crystal
disrupt
periodicity
this
use
of
bloch's
theorem
is
only
an
approximation
but
it
has
proven
to
be
a
tremendously
valuable
approximation
without
which
most
solid-state
physics
analysis
would
be
intractable
deviations
from
periodicity
are
treated
by
quantum
mechanical
perturbation
theory
research
topics
in
solid
state
physics
include:
silicon
nanowire
silicon
nanowires
also
referred
to
as
sinws
are
a
type
of
semiconductor
nanowire
most
often
formed
from
a
silicon
precursor
by
etching
of
a
solid
or
through
catalyzed
growth
from
a
vapor
or
liquid
phase
such
nanowires
have
promising
applications
in
lithium
ion
batteries
and
sensors
initial
synthesis
of
sinws
is
often
accompanied
by
thermal
oxidation
steps
to
yield
structures
of
accurately
tailored
size
and
morphology
sinws
have
unique
properties
that
are
not
seen
in
bulk
(three-dimensional)
silicon
materials
these
properties
arise
from
an
unusual
quasi
one-dimensional
electronic
structure
and
are
the
subject
of
research
across
numerous
disciplines
and
applications
the
reason
that
sinws
are
considered
as
one
of
the
most
important
one-dimensional
materials
is
they
could
have
a
function
as
building
blocks
for
nanoscale
electronics
assembled
without
the
need
for
complex
and
costly
fabrication
facilities
sinws
are
frequently
studied
towards
applications
including
photovoltaics
nanowire
batteries
thermoelectrics
and
non-volatile
memory
owing
to
their
unique
physical
and
chemical
properties
silicon
nanowires
are
a
promising
candidate
for
a
wide
range
of
applications
that
draw
on
their
unique
physico-chemical
characteristics
which
differ
from
those
of
bulk
silicon
material
sinws
exhibit
charge
trapping
behavior
which
renders
such
systems
of
value
in
applications
necessitating
electron
hole
separation
such
as
photovoltaics
and
photocatalysts
recent
experiment
on
nanowire
solar
cells
has
led
to
a
remarkable
improvement
of
the
power
conversion
efficiency
of
sinw
solar
cells
from
17%
in
the
last
few
years
charge
trapping
behaviour
and
tuneable
surface
governed
transport
properties
of
sinws
render
this
category
of
nanostructures
of
interest
towards
use
as
metal
insulator
semiconductors
and
field
effect
transistors
with
further
applications
as
nanoelectronic
storage
devices
in
flash
memory
logic
devices
as
well
as
chemical
and
biological
sensors
the
ability
for
lithium
ions
to
intercalate
into
silicon
structures
renders
various
si
nanostructures
of
interest
towards
applications
as
anodes
in
li-ion
batteries
(libs)
sinws
are
of
particular
merit
as
such
anodes
as
they
exhibit
the
ability
to
undergo
significant
lithiation
while
maintaining
structural
integrity
and
electrical
connectivity
several
synthesis
methods
are
known
for
sinws
and
these
can
be
broadly
divided
into
methods
which
start
with
bulk
silicon
and
remove
material
to
yield
nanowires
also
known
as
top-down
synthesis
and
methods
which
use
a
chemical
or
vapor
precursor
to
build
nanowires
in
a
process
generally
considered
to
be
bottom-up
synthesis
these
methods
use
material
removal
techniques
to
produce
nanostructures
from
a
bulk
precursor
subsequent
to
physical
or
chemical
processing
either
top-down
or
bottom-up
to
obtain
initial
silicon
nanostructures
thermal
oxidation
steps
are
often
applied
in
order
to
obtain
materials
with
desired
size
and
aspect
ratio
silicon
nanowires
exhibit
a
distinct
and
useful
self-limiting
oxidation
behaviour
whereby
oxidation
effectively
ceases
due
to
diffusion
limitations
which
can
be
modeled
this
phenomenon
allows
accurate
control
of
dimensions
and
aspect
ratios
in
sinws
and
has
been
used
to
obtain
high
aspect
ratio
sinws
with
diameters
below
5 nm
the
self-limiting
oxidation
of
sinws
is
of
value
towards
lithium
ion
battery
materials
the
orientation
of
sinws
has
profound
influence
on
the
strucutal
and
electronic
properties
of
the
systems
for
this
reason
several
procedures
have
been
proposed
for
the
alignment
of
nanowires
in
chosen
orientations
this
includes
the
use
of
electric
fields
in
polar
alignment
electrophoresis
mircofluidic
methods
and
contact
printing
there
is
significant
interest
in
sinws
for
their
unique
properties
and
the
ability
to
control
size
and
aspect
ratio
with
great
accuracy
as
yet
limitations
in
large-scale
fabrication
impede
the
uptake
of
this
material
in
the
full
range
of
investigated
applications
combined
studies
of
synthesis
methods
oxidation
kinetics
and
properties
of
sinw
systems
aim
to
overcome
the
present
limitations
and
facilitate
the
implementation
of
sinw
systems
for
example
high
quality
vapor-liquid-solid–grown
sinws
with
smooth
surfaces
can
be
reversibly
stretched
with
10%
or
more
elastic
strain
approaching
the
theoretical
elastic
limit
of
silicon
which
could
open
the
doors
for
the
emerging
“elastic
strain
engineering”
and
flexible
bio-/nano-electronics
harmonic
a
harmonic
is
any
member
of
the
harmonic
series
the
term
is
employed
in
various
disciplines
including
music
physics
acoustics
electronic
power
transmission
radio
technology
and
other
fields
it
is
typically
applied
to
repeating
signals
such
as
sinusoidal
waves
a
harmonic
of
such
a
wave
is
a
wave
with
a
frequency
that
is
a
positive
integer
multiple
of
the
frequency
of
the
original
wave
known
as
the
fundamental
frequency
the
original
wave
is
also
called
the
1st
harmonic
the
following
harmonics
are
known
as
higher
harmonics
as
all
harmonics
are
periodic
at
the
fundamental
frequency
the
sum
of
harmonics
is
also
periodic
at
that
frequency
for
example
if
the
fundamental
frequency
is
50 hz
a
common
ac
power
supply
frequency
the
frequencies
of
the
first
three
higher
harmonics
are
100 hz
(2nd
harmonic)
150 hz
(3rd
harmonic)
200 hz
(4th
harmonic)
and
any
addition
of
waves
with
these
frequencies
is
periodic
at
50 hz
in
music
harmonics
are
used
on
string
instruments
and
wind
instruments
as
a
way
of
producing
sound
on
the
instrument
particularly
to
play
higher
notes
and
with
strings
obtain
notes
that
have
a
unique
sound
quality
or
"tone
colour"
on
strings
harmonics
that
are
bowed
have
a
"glassy"
pure
tone
on
stringed
instruments
harmonics
are
played
by
touching
(but
not
fully
pressing
down
the
string)
at
an
exact
point
on
the
string
while
sounding
the
string
(plucking
bowing
etc);
this
allows
the
harmonic
to
sound
a
pitch
which
is
always
higher
than
the
fundamental
frequency
of
the
string
harmonics
may
also
be
called
"overtones"
"partials"
or
"upper
partials"
the
difference
between
"harmonic"
and
"overtone"
is
that
the
term
"harmonic"
includes
all
of
the
notes
in
a
series
including
the
fundamental
frequency
(eg
the
open
string
of
a
guitar)
the
term
"overtone"
only
includes
the
pitches
above
the
fundamental
in
some
music
contexts
the
terms
"harmonic"
"overtone"
and
"partial"
are
used
fairly
interchangeably
most
acoustic
instruments
emit
complex
tones
containing
many
individual
partials
(component
simple
tones
or
sinusoidal
waves)
but
the
untrained
human
ear
typically
does
not
perceive
those
partials
as
separate
phenomena
rather
a
musical
note
is
perceived
as
one
sound
the
quality
or
timbre
of
that
sound
being
a
result
of
the
relative
strengths
of
the
individual
partials
many
acoustic
oscillators
such
as
the
human
voice
or
a
bowed
violin
string
produce
complex
tones
that
are
more
or
less
periodic
and
thus
are
composed
of
partials
that
are
near
matches
to
integer
multiples
of
the
fundamental
frequency
and
therefore
resemble
the
ideal
harmonics
and
are
called
"harmonic
partials"
or
simply
"harmonics"
for
convenience
(although
it's
not
strictly
accurate
to
call
a
partial
a
harmonic
the
first
being
real
and
the
second
being
ideal)
oscillators
that
produce
harmonic
partials
behave
somewhat
like
one-dimensional
resonators
and
are
often
long
and
thin
such
as
a
guitar
string
or
a
column
of
air
open
at
both
ends
(as
with
the
modern
orchestral
transverse
flute)
wind
instruments
whose
air
column
is
open
at
only
one
end
such
as
trumpets
and
clarinets
also
produce
partials
resembling
harmonics
however
they
only
produce
partials
matching
the
odd
harmonics
at
least
in
theory
the
reality
of
acoustic
instruments
is
such
that
none
of
them
behaves
as
perfectly
as
the
somewhat
simplified
theoretical
models
would
predict
partials
whose
frequencies
are
not
integer
multiples
of
the
fundamental
are
referred
to
as
"inharmonic
partials"
some
acoustic
instruments
emit
a
mix
of
harmonic
and
inharmonic
partials
but
still
produce
an
effect
on
the
ear
of
having
a
definite
fundamental
pitch
such
as
pianos
strings
plucked
pizzicato
vibraphones
marimbas
and
certain
pure-sounding
bells
or
chimes
antique
singing
bowls
are
known
for
producing
multiple
harmonic
partials
or
multiphonics
an
overtone
is
any
partial
higher
than
the
lowest
partial
in
a
compound
tone
the
relative
strengths
and
frequency
relationships
of
the
component
partials
determine
the
timbre
of
an
instrument
the
similarity
between
the
terms
overtone
and
partial
sometimes
leads
to
their
being
loosely
used
interchangeably
in
a
musical
context
but
they
are
counted
differently
leading
to
some
possible
confusion
in
the
special
case
of
instrumental
timbres
whose
component
partials
closely
match
a
harmonic
series
(such
as
with
most
strings
and
winds)
rather
than
being
inharmonic
partials
(such
as
with
most
pitched
percussion
instruments)
it
is
also
convenient
to
call
the
component
partials
"harmonics"
but
not
strictly
correct
(because
harmonics
are
numbered
the
same
even
when
missing
while
partials
and
overtones
are
only
counted
when
present)
this
chart
demonstrates
how
the
three
types
of
names
(partial
overtone
and
harmonic)
are
counted
(assuming
that
the
harmonics
are
present):
in
many
musical
instruments
it
is
possible
to
play
the
upper
harmonics
without
the
fundamental
note
being
present
in
a
simple
case
(eg
recorder)
this
has
the
effect
of
making
the
note
go
up
in
pitch
by
an
octave
but
in
more
complex
cases
many
other
pitch
variations
are
obtained
in
some
cases
it
also
changes
the
timbre
of
the
note
this
is
part
of
the
normal
method
of
obtaining
higher
notes
in
wind
instruments
where
it
is
called
"overblowing"
the
extended
technique
of
playing
multiphonics
also
produces
harmonics
on
string
instruments
it
is
possible
to
produce
very
pure
sounding
notes
called
harmonics
or
"flageolets"
by
string
players
which
have
an
eerie
quality
as
well
as
being
high
in
pitch
harmonics
may
be
used
to
check
at
a
unison
the
tuning
of
strings
that
are
not
tuned
to
the
unison
for
example
lightly
fingering
the
node
found
halfway
down
the
highest
string
of
a
cello
produces
the
same
pitch
as
lightly
fingering
the
node
of
the
way
down
the
second
highest
string
for
the
human
voice
see
overtone
singing
which
uses
harmonics
while
it
is
true
that
electronically
produced
periodic
tones
(eg
square
waves
or
other
non-sinusoidal
waves)
have
"harmonics"
that
are
whole
number
multiples
of
the
fundamental
frequency
practical
instruments
do
not
all
have
this
characteristic
for
example
higher
"harmonics"'
of
piano
notes
are
not
true
harmonics
but
are
"overtones"
and
can
be
very
sharp
ie
a
higher
frequency
than
given
by
a
pure
harmonic
series
this
is
especially
true
of
instruments
other
than
stringed
or
brass/woodwind
ones
eg
xylophone
drums
bells
etc
where
not
all
the
overtones
have
a
simple
whole
number
ratio
with
the
fundamental
frequency
the
fundamental
frequency
is
the
reciprocal
of
the
period
of
the
periodic
phenomenon
the
following
table
displays
the
stop
points
on
a
stringed
instrument
such
as
the
guitar
(guitar
harmonics)
at
which
gentle
touching
of
a
string
will
force
it
into
a
harmonic
mode
when
vibrated
string
harmonics
(flageolet
tones)
are
described
as
having
a
"flutelike
silvery
quality"
that
can
be
highly
effective
as
a
special
color
or
tone
color
(timbre)
when
used
and
heard
in
orchestration
it
is
unusual
to
encounter
natural
harmonics
higher
than
the
fifth
partial
on
any
stringed
instrument
except
the
double
bass
on
account
of
its
much
longer
strings
harmonics
are
widely
used
in
plucked
string
instruments
such
as
acoustic
guitar
electric
guitar
and
electric
bass
on
an
electric
guitar
played
loudly
through
a
guitar
amplifier
with
distortion
harmonics
are
more
sustained
and
can
be
used
in
guitar
solos
in
the
heavy
metal
music
lead
guitar
style
known
as
shred
guitar
harmonics
both
natural
and
artificial
are
widely
used
although
harmonics
are
most
often
used
on
open
strings
(natural
harmonics)
occasionally
a
score
will
call
for
an
artificial
harmonic
produced
by
playing
an
overtone
on
an
already
stopped
string
as
a
performance
technique
it
is
accomplished
by
using
two
fingers
on
the
fingerboard
the
first
to
shorten
the
string
to
the
desired
fundamental
with
the
second
touching
the
node
corresponding
to
the
appropriate
harmonic
on
fretted
instruments
such
as
an
electric
guitar
the
performer
can
look
at
the
frets
to
determine
where
to
stop
the
string
and
where
to
touch
the
node
on
unfretted
instruments
such
as
the
violin
and
related
instruments
playing
artificial
harmonics
is
an
advanced
technique
as
it
requires
the
performer
to
find
two
precise
locations
on
the
same
string
harmonics
may
be
either
used
or
considered
as
the
basis
of
just
intonation
systems
composer
arnold
dreyblatt
is
able
to
bring
out
different
harmonics
on
the
single
string
of
his
modified
double
bass
by
slightly
altering
his
unique
bowing
technique
halfway
between
hitting
and
bowing
the
strings
composer
lawrence
ball
uses
harmonics
to
generate
music
electronically
conformon
from
a
biological
standpoint
the
goal-directed
molecular
motions
inside
living
cells
are
carried
out
by
biopolymers
acting
like
molecular
machines
(eg
myosin
rna/dna
polymerase
ion
pumps
etc)
these
molecular
machines
are
driven
by
conformons
that
is
sequence-specific
mechanical
strains
generated
by
free
energy
released
in
chemical
reactions
or
stress
induced
destabilisations
in
supercoiled
biopolymer
chains
therefore
conformons
can
be
defined
as
packets
of
conformational
energy
generated
from
substrate
binding
or
chemical
reactions
and
confined
within
biopolymers
on
the
other
hand
from
a
physics
standpoint
the
conformon
is
a
localization
of
elastic
and
electronic
energy
which
may
propagate
in
space
with
or
without
dissipation
the
mechanism
which
involves
dissipationless
propagation
is
a
form
of
molecular
superconductivity
on
quantum
mechanical
level
both
elastic/vibrational
and
electronic
energy
can
be
quantised
therefore
the
conformon
carries
a
fixed
portion
of
energy
this
has
led
to
the
definition
of
quantum
of
conformation
(shape)
ccpforge
the
collaborative
computational
projects
(ccp)
group
was
responsible
for
the
development
of
ccpforge
which
is
a
software
development
tool
produced
through
collaborations
by
the
ccp
community
ccps
allow
experts
in
computational
research
to
come
together
and
develop
scientific
software
which
can
be
applied
to
numerous
research
fields
it
is
used
as
a
tool
in
many
research
and
development
areas
and
hosts
a
variety
of
projects
every
ccp
project
is
the
result
of
years
of
valuable
work
by
computational
researchers
it
is
advised
for
projects
to
have
one
application
this
helps
users
to
search
a
category
and
classification
system
so
they
can
find
the
right
project
for
their
work
furthermore
the
project
can
be
under
up
to
three
ccps
provided
it
is
a
collaboration
each
classification
category
will
have
sub-sections
to
filter
the
category
further
ccpforge
projects
such
provide
essential
information
which
has
been
used
in
publications
such
as
'recent
developments
in
r-matrix
applications
to
molecular
processes'
and
'ab
initio
derivation
of
hubbard
models
for
cold
atoms
in
optical
lattices'
in
which
codes
from
ccpq
were
used
the
joint
information
systems
committee
(jisc)
and
epsrc
both
fund
the
ccpforge
project
the
scientific
computing
department
(scd)
of
the
science
and
technology
facilities
council
is
responsible
for
the
development
and
maintenance
of
ccpforge
and
this
is
funded
by
a
long
term
support
grant
from
epsrc
ccpq
was
formed
from
ccp2
"continuum
states
of
atoms
and
molecules"
incorporating
aspects
of
ccp6
"molecular
quantum
dynamics"
normal
contact
stiffness
normal
contact
stiffness
is
a
physical
quantity
related
to
the
generalized
force
displacement
behavior
of
rough
surfaces
in
contact
with
a
rigid
body
or
a
second
similar
rough
surface
as
two
solid
bodies
of
the
same
material
approach
one
another
they
transition
from
conditions
of
non-contact
to
homogeneous
bulk
type
behaviour
the
varying
values
of
stiffness
and
true
contact
area
that
is
exhibited
at
an
interface
during
this
transition
is
dependent
on
conditions
of
applied
pressure
and
is
of
notable
importance
for
the
study
of
systems
involving
the
physical
interactions
of
multiple
bodies
including
granular
matter
electrode
contacts
and
thermal
contacts
where
the
interface-localized
structures
govern
overall
system
performance
the
role
of
surface
structure
in
normal
contact
mechanics
in
terms
of
stiffness
and
true
contact
area
is
a
frequently
studied
topic
parameters
of
roughness
fractal
dimension
and
asperity
geometry
are
often
discussed
with
reference
to
their
significance
on
contact
mechanics
of
surfaces
straw
tracker
a
straw
tracker
is
a
type
of
particle
detector
which
uses
many
straw
chambers
to
track
the
path
of
a
particle
the
path
of
a
particle
is
determined
by
the
best
fit
to
all
the
straws
with
hits
since
the
time
for
a
particular
straw
to
produce
a
signal
is
proportional
to
the
distance
of
the
particle's
closest
approach
to
that
chamber's
wire
if
a
particle
on
a
predictable
path
(eg
a
helix
in
a
magnetic
field)
passes
through
many
straws
the
path
of
the
particle
can
be
determined
more
precisely
than
the
size
of
any
particular
straw
zero-point
energy
zero-point
energy
(zpe)
is
the
difference
between
the
lowest
possible
energy
that
a
quantum
mechanical
system
may
have
and
the
classical
minimum
energy
of
the
system
unlike
in
classical
mechanics
quantum
systems
constantly
fluctuate
in
their
lowest
energy
state
due
to
the
heisenberg
uncertainty
principle
as
well
as
atoms
and
molecules
the
empty
space
of
the
vacuum
has
these
properties
according
to
quantum
field
theory
the
universe
can
be
thought
of
not
as
isolated
particles
but
continuous
fluctuating
fields:
matter
fields
whose
quanta
are
fermions
(ie
leptons
and
quarks)
and
force
fields
whose
quanta
are
bosons
(eg
photons
and
gluons)
all
these
fields
have
zero-point
energy
these
fluctuating
zero-point
fields
lead
to
a
kind
of
reintroduction
of
an
aether
in
physics
since
some
systems
can
detect
the
existence
of
this
energy
however
this
aether
cannot
be
thought
of
as
a
physical
medium
if
it
is
to
be
lorentz
invariant
such
that
there
is
no
contradiction
with
einstein's
theory
of
special
relativity
physics
currently
lacks
a
full
theoretical
model
for
understanding
zero-point
energy;
in
particular
the
discrepancy
between
theorized
and
observed
vacuum
energy
is
a
source
of
major
contention
physicists
richard
feynman
and
john
wheeler
calculated
the
zero-point
radiation
of
the
vacuum
to
be
an
order
of
magnitude
greater
than
nuclear
energy
with
a
single
light
bulb
containing
enough
energy
to
boil
all
the
world's
oceans
yet
according
to
einstein's
theory
of
general
relativity
any
such
energy
would
gravitate
and
the
experimental
evidence
from
both
the
expansion
of
the
universe
dark
energy
and
the
casimir
effect
show
any
such
energy
to
be
exceptionally
weak
a
popular
proposal
that
attempts
to
address
this
issue
is
to
say
that
the
fermion
field
has
a
negative
zero-point
energy
while
the
boson
field
has
positive
zero-point
energy
and
thus
these
energies
somehow
cancel
each
other
out
this
idea
would
be
true
if
supersymmetry
were
an
exact
symmetry
of
nature
however
the
lhc
at
cern
has
so
far
found
no
evidence
to
support
supersymmetry
moreover
it
is
known
that
if
supersymmetry
is
valid
at
all
it
is
at
most
a
broken
symmetry
only
true
at
very
high
energies
and
no
one
has
been
able
to
show
a
theory
where
zero-point
cancellations
occur
in
the
low
energy
universe
we
observe
today
this
discrepancy
is
known
as
the
cosmological
constant
problem
and
it
is
one
of
the
greatest
unsolved
mysteries
in
physics
many
physicists
believe
that
"the
vacuum
holds
the
key
to
a
full
understanding
of
nature"
the
term
zero-point
energy
(zpe)
is
a
translation
from
the
german
nullpunktsenergie
the
terms
zero-point
radiation
or
ground
state
energy
are
also
sometimes
used
interchangeably
the
term
zero-point
field
(zpf)
can
be
used
when
referring
to
a
specific
vacuum
field
for
instance
the
qed
vacuum
which
specifically
deals
with
quantum
electrodynamics
(eg
electromagnetic
interactions
between
photons
electrons
and
the
vacuum)
or
the
qcd
vacuum
which
deals
with
quantum
chromodynamics
(eg
color
charge
interactions
between
quarks
gluons
and
the
vacuum)
a
vacuum
can
be
viewed
not
as
empty
space
but
as
the
combination
of
all
zero-point
fields
in
quantum
field
theory
this
combination
of
fields
is
called
the
vacuum
state
its
associated
zero-point
energy
is
called
the
vacuum
energy
and
the
average
energy
value
is
called
the
vacuum
expectation
value
(vev)
also
called
its
condensate
in
classical
mechanics
all
particles
can
be
thought
of
as
having
some
energy
made
up
of
their
potential
energy
and
kinetic
energy
temperature
for
example
arises
from
the
intensity
of
random
particle
motion
caused
by
kinetic
energy
(known
as
brownian
motion)
as
temperature
is
reduced
to
absolute
zero
it
might
be
thought
that
all
motion
ceases
and
particles
come
completely
to
rest
in
fact
however
kinetic
energy
is
retained
by
particles
even
at
the
lowest
possible
temperature
the
random
motion
corresponding
to
this
zero-point
energy
never
vanishes
as
a
consequence
of
the
uncertainty
principle
of
quantum
mechanics
the
uncertainty
principle
states
that
no
object
can
ever
have
precise
values
of
position
and
velocity
simultaneously
the
total
energy
of
a
quantum
mechanical
object
(potential
and
kinetic)
is
described
by
its
hamiltonian
which
also
describes
the
system
as
a
harmonic
oscillator
or
wave
function
that
fluctuates
between
various
energy
states
(see
wave-particle
duality)
all
quantum
mechanical
systems
undergo
fluctuations
even
in
their
ground
state
a
consequence
of
their
wave-like
nature
the
uncertainty
principle
requires
every
quantum
mechanical
system
to
have
a
fluctuating
zero-point
energy
greater
than
the
minimum
of
its
classical
potential
well
this
results
in
motion
even
at
absolute
zero
for
example
liquid
helium
does
not
freeze
under
atmospheric
pressure
regardless
of
temperature
due
to
its
zero-point
energy
given
the
equivalence
of
mass
and
energy
expressed
by
einstein's
any
point
in
space
that
contains
energy
can
be
thought
of
as
having
mass
to
create
particles
virtual
particles
spontaneously
flash
into
existence
at
every
point
in
space
due
to
the
energy
of
quantum
fluctuations
caused
by
the
uncertainty
principle
modern
physics
has
developed
quantum
field
theory
(qft)
to
understand
the
fundamental
interactions
between
matter
and
forces
it
treats
every
single
point
of
space
as
a
quantum
harmonic
oscillator
according
to
qft
the
universe
is
made
up
of
matter
fields
whose
quanta
are
fermions
(ie
leptons
and
quarks)
and
force
fields
whose
quanta
are
bosons
(eg
photons
and
gluons)
all
these
fields
have
zero-point
energy
recent
experiments
advocate
the
idea
that
particles
themselves
can
be
thought
of
as
excited
states
of
the
underlying
quantum
vacuum
and
that
all
properties
of
matter
are
merely
vacuum
fluctuations
arising
from
interactions
of
the
zero-point
field
the
idea
that
"empty"
space
can
have
an
intrinsic
energy
associated
to
it
and
that
there
is
no
such
thing
as
a
"true
vacuum"
is
seemingly
unintuitive
it
is
often
argued
that
the
entire
universe
is
completely
bathed
in
the
zero-point
radiation
and
as
such
it
can
add
only
some
constant
amount
to
calculations
physical
measurements
will
therefore
reveal
only
deviations
from
this
value
for
many
practical
calculations
zero-point
energy
is
dismissed
by
fiat
in
the
mathematical
model
as
a
term
that
has
no
physical
effect
such
treatment
causes
problems
however
as
in
einstein's
theory
of
general
relativity
the
absolute
energy
value
of
space
is
not
an
arbitrary
constant
and
gives
rise
to
the
cosmological
constant
for
decades
most
physicists
assumed
that
there
was
some
undiscovered
fundamental
principle
that
will
remove
the
infinite
zero-point
energy
and
make
it
completely
vanish
if
the
vacuum
has
no
intrinsic
absolute
value
of
energy
it
will
not
gravitate
it
was
believed
that
as
the
universe
expands
from
the
aftermath
of
the
big
bang
the
energy
contained
in
any
unit
of
empty
space
will
decrease
as
the
total
energy
spreads
out
to
fill
the
volume
of
the
universe;
galaxies
and
all
matter
in
the
universe
should
begin
to
decelerate
this
possibility
was
ruled
out
in
1998
by
the
discovery
that
the
expansion
of
the
universe
is
not
slowing
down
but
is
in
fact
accelerating
meaning
empty
space
does
indeed
have
some
intrinsic
energy
the
discovery
of
dark
energy
is
best
explained
by
zero-point
energy
though
it
still
remains
a
mystery
as
to
why
the
value
appears
to
be
so
small
compared
to
huge
value
obtained
through
theory
-
the
cosmological
constant
problem
many
physical
effects
attributed
to
zero-point
energy
have
been
experimentally
verified
such
as
spontaneous
emission
casimir
force
lamb
shift
magnetic
moment
of
the
electron
and
delbrück
scattering
these
effects
are
usually
called
"radiative
corrections"
in
more
complex
nonlinear
theories
(eg
qcd)
zero-point
energy
can
give
rise
to
a
variety
of
complex
phenomena
such
as
multiple
stable
states
symmetry
breaking
chaos
and
emergence
many
physicists
believe
that
"the
vacuum
holds
the
key
to
a
full
understanding
of
nature"
and
that
studying
it
is
critical
in
the
search
for
the
theory
of
everything
active
areas
of
research
include
the
effects
of
virtual
particles
quantum
entanglement
the
difference
(if
any)
between
inertial
and
gravitational
mass
variation
in
the
speed
of
light
a
reason
for
the
observed
value
of
the
cosmological
constant
and
the
nature
of
dark
energy
zero-point
energy
evolved
from
historical
ideas
about
the
vacuum
to
aristotle
the
vacuum
was
"the
empty";
space
independent
of
body
he
believed
this
concept
violated
basic
physical
principles
and
asserted
that
the
elements
of
fire
air
earth
and
water
were
not
made
of
atoms
but
were
continuous
to
the
atomists
the
concept
of
emptiness
had
absolute
character:
it
was
the
distinction
between
existence
and
nonexistence
debate
about
the
characteristics
of
the
vacuum
were
largely
confined
to
the
realm
of
philosophy
it
was
not
until
much
later
on
with
the
beginning
of
the
renaissance
that
otto
von
guericke
invented
the
first
vacuum
pump
and
the
first
testable
scientific
ideas
began
to
emerge
it
was
thought
that
a
totally
empty
volume
of
space
could
be
created
by
simply
removing
all
gases
this
was
the
first
generally
accepted
concept
of
the
vacuum
late
in
the
19th
century
however
it
became
apparent
that
the
evacuated
region
still
contained
thermal
radiation
the
existence
of
the
aether
as
a
substitute
for
a
true
void
was
the
most
prevalent
theory
of
the
time
according
to
the
successful
electromagnetic
aether
theory
based
upon
maxwell's
electrodynamics
this
all-encompassing
aether
was
endowed
with
energy
and
hence
very
different
from
nothingness
the
fact
that
electromagnetic
and
gravitational
phenomena
were
easily
transmitted
in
empty
space
indicated
that
their
associated
aethers
were
part
of
the
fabric
of
space
itself
maxwell
himself
noted
that:
however
the
results
of
the
michelson–morley
experiment
in
1887
were
the
first
strong
evidence
that
the
then-prevalent
aether
theories
were
seriously
flawed
and
initiated
a
line
of
research
that
eventually
led
to
special
relativity
which
ruled
out
the
idea
of
a
stationary
aether
altogether
to
scientists
of
the
period
it
seemed
that
a
true
vacuum
in
space
might
be
completely
eliminated
by
cooling
thus
eliminating
all
radiation
or
energy
from
this
idea
evolved
the
second
concept
of
achieving
a
real
vacuum:
cool
it
down
to
absolute
zero
temperature
after
evacuation
absolute
zero
was
technically
impossible
to
achieve
in
the
19th
century
so
the
debate
remained
unsolved
in
1900
max
planck
derived
the
average
energy
of
a
single
"energy
radiator"
eg
a
vibrating
atomic
unit
as
a
function
of
absolute
temperature:
where
is
planck's
constant
is
the
frequency
is
boltzmann's
constant
and
is
the
absolute
temperature
the
zero-point
energy
makes
no
contribution
to
planck's
original
law
as
its
existence
was
unknown
to
planck
in
1900
the
concept
of
zero-point
energy
was
developed
by
max
planck
in
germany
in
1911
as
a
corrective
term
added
to
a
zero-grounded
formula
developed
in
his
original
quantum
theory
in
1900
in
1912
max
planck
published
the
first
journal
article
to
describe
the
discontinuous
emission
of
radiation
based
on
the
discrete
quanta
of
energy
in
planck's
"second
quantum
theory"
resonators
absorbed
energy
continuously
but
emitted
energy
in
discrete
energy
quanta
only
when
they
reached
the
boundaries
of
finite
cells
in
phase
space
where
their
energies
became
integer
multiples
of
this
theory
led
planck
to
his
new
radiation
law
but
in
this
version
energy
resonators
possessed
a
zero-point
energy
the
smallest
average
energy
a
resonator
could
take
on
planck's
radiation
equation
contained
a
residual
energy
factor
one
as
an
additional
term
dependent
on
the
frequency
which
was
greater
than
zero
(where
is
planck's
constant)
it
is
therefore
widely
agreed
that
"planck's
equation
marked
the
birth
of
the
concept
of
zero-point
energy"
in
a
series
of
papers
from
1911
to
1913
planck
found
that
the
average
energy
of
an
oscillator
to
be:
soon
the
idea
of
zero-point
energy
attracted
the
attention
of
albert
einstein
and
his
assistant
otto
stern
in
1913
they
published
a
paper
that
attempted
to
prove
the
existence
of
zero-point
energy
by
calculating
the
specific
heat
of
hydrogen
gas
and
compared
it
with
the
experimental
data
however
after
assuming
they
had
succeeded
they
retracted
support
for
the
idea
shortly
after
publication
because
they
found
planck's
second
theory
may
not
apply
to
their
example
in
a
letter
to
paul
ehrenfest
of
the
same
year
einstein
declared
zero-point
energy
“dead
as
a
doornail”
zero-point
energy
was
also
invoked
by
peter
debye
who
noted
that
zero-point
energy
of
the
atoms
of
a
crystal
lattice
would
cause
a
reduction
in
the
intensity
of
the
diffracted
radiation
in
x-ray
diffraction
even
as
the
temperature
approached
absolute
zero
in
1916
walther
nernst
proposed
that
empty
space
was
filled
with
zero-point
electromagnetic
radiation
with
the
development
of
general
relativity
einstein
found
the
energy
density
of
the
vacuum
to
contribute
towards
a
cosmological
constant
in
order
to
obtain
static
solutions
to
his
field
equations;
the
idea
that
empty
space
or
the
vacuum
could
have
some
intrinsic
energy
associated
to
it
had
returned
with
einstein
stating
in
1920:
in
1913
niels
bohr
had
proposed
what
is
now
called
the
bohr
model
of
the
atom
but
despite
this
it
remained
a
mystery
as
to
why
electrons
do
not
fall
into
their
nuclei
according
to
classical
ideas
the
fact
that
an
accelerating
charge
loses
energy
by
radiating
implied
that
an
electron
should
spiral
into
the
nucleus
and
that
atoms
should
not
be
stable
this
problem
of
classical
mechanics
was
nicely
summarized
by
james
hopwood
jeans
in
1915:
"there
would
be
a
very
real
difficulty
in
supposing
that
the
(force)
law
held
down
to
the
zero
values
of
for
the
forces
between
two
charges
at
zero
distance
would
be
infinite;
we
should
have
charges
of
opposite
sign
continually
rushing
together
and
when
once
together
no
force
would
tend
to
shrink
into
nothing
or
to
diminish
indefinitely
in
size"
this
resolution
to
this
puzzle
came
in
1926
with
schrodinger's
famous
equation
this
equation
explained
the
new
non-classical
fact
that
as
an
electron
moves
close
to
a
nucleus
its
kinetic
energy
necessarily
increases
in
such
a
way
that
the
minimum
total
energy
(kinetic
plus
potential)
occurs
at
some
positive
separation
rather
than
at
zero
separation;
in
other
words
that
zero-point
energy
is
essential
for
atomic
stability
in
1926
pascual
jordan
published
the
first
attempt
to
quantize
the
electromagnetic
field
in
a
joint
paper
with
max
born
and
werner
heisenberg
he
considered
the
field
inside
a
cavity
as
a
superposition
of
quantum
harmonic
oscillators
in
his
calculation
he
found
that
in
addition
to
the
"thermal
energy"
of
the
oscillators
there
also
had
to
exist
infinite
zero-point
energy
term
he
was
able
to
obtain
the
same
fluctuation
formula
that
einstein
had
obtained
in
1909
however
jordan
did
not
think
that
his
infinite
zero-point
energy
term
was
"real"
writing
to
einstein
that
"it
is
just
a
quantity
of
the
calculation
having
no
direct
physical
meaning"
jordan
found
a
way
to
get
rid
of
the
infinite
term
publishing
a
joint
work
with
pauli
in
1928
performing
what
has
been
called
"the
first
infinite
subtraction
or
renormalisation
in
quantum
field
theory"
building
on
the
work
of
heisenberg
and
others
paul
dirac's
theory
of
emission
and
absorption
(1927)
was
the
first
application
of
the
quantum
theory
of
radiation
dirac's
work
was
seen
as
crucially
important
to
the
emerging
field
of
quantum
mechanics;
it
dealt
directly
with
the
process
in
which
"particles"
are
actually
created:
spontaneous
emission
dirac
described
the
quantization
of
the
electromagnetic
field
as
an
ensemble
of
harmonic
oscillators
with
the
introduction
of
the
concept
of
creation
and
annihilation
operators
of
particles
the
theory
showed
that
spontaneous
emission
depends
upon
the
zero-point
energy
fluctuations
of
the
electromagnetic
field
in
order
to
get
started
in
a
process
in
which
a
photon
is
annihilated
(absorbed)
the
photon
can
be
thought
of
as
making
a
transition
into
the
vacuum
state
similarly
when
a
photon
is
created
(emitted)
it
is
occasionally
useful
to
imagine
that
the
photon
has
made
a
transition
out
of
the
vacuum
state
in
the
words
of
dirac:
contemporary
physicists
when
asked
to
give
a
physical
explanation
for
spontaneous
emission
generally
invoke
the
zero-point
energy
of
the
electromagnetic
field
this
view
was
popularized
by
victor
weisskopf
who
in
1935
wrote:
this
view
was
also
later
supported
by
(1948)
who
argued
that
spontaneous
emission
"can
be
thought
of
as
forced
emission
taking
place
under
the
action
of
the
fluctuating
field"
this
new
theory
which
dirac
coined
quantum
electrodynamics
(qed)
predicted
a
fluctuating
zero-point
or
"vacuum"
field
existing
even
in
the
absence
of
sources
throughout
the
1940s
improvements
in
microwave
technology
made
it
possible
to
take
more
precise
measurements
of
the
shift
of
the
levels
of
a
hydrogen
atom
now
known
as
the
lamb
shift
and
measurement
of
the
magnetic
moment
of
the
electron
discrepancies
between
these
experiments
and
dirac's
theory
led
to
the
idea
of
incorporating
renormalisation
into
qed
to
deal
with
zero-point
infinities
renormalization
was
originally
developed
by
hans
kramers
and
also
victor
weisskopf(1936)
and
first
successfully
applied
to
calculate
a
finite
value
for
the
lamb
shift
by
hans
bethe
(1947)
as
per
spontaneous
emission
these
effects
can
in
part
be
understood
with
interactions
with
the
zero-point
field
but
in
light
of
renormalisation
being
able
to
remove
some
zero-point
infinities
from
calculations
not
all
physicists
were
comfortable
attributing
zero-point
energy
any
physical
meaning
viewing
it
instead
as
a
mathematical
artifact
that
might
one
day
be
fully
eliminated
in
wolfgang
pauli's
1945
nobel
lecture
he
made
clear
his
opposition
to
the
idea
of
zero-point
energy
stating
"it
is
clear
that
this
zero-point
energy
has
no
physical
reality"
in
1948
hendrik
casimir
showed
that
one
consequence
of
the
zero-point
field
is
an
attractive
force
between
two
uncharged
perfectly
conducting
parallel
plates
the
so-called
casimir
effect
at
the
time
casimir
was
studying
the
properties
of
"colloidal
solutions"
these
are
viscous
materials
such
as
paint
and
mayonnaise
that
contain
micron-sized
particles
in
a
liquid
matrix
the
properties
of
such
solutions
are
determined
by
van
der
waals
forces
–
long-range
attractive
forces
that
exist
between
neutral
atoms
and
molecules
one
of
casimir's
colleagues
theo
overbeek
realized
that
the
theory
that
was
used
at
the
time
to
explain
van
der
waals
forces
which
had
been
developed
by
fritz
london
in
1930
did
not
properly
explain
the
experimental
measurements
on
colloids
overbeek
therefore
asked
casimir
to
investigate
the
problem
working
with
dirk
polder
casimir
discovered
that
the
interaction
between
two
neutral
molecules
could
be
correctly
described
only
if
the
fact
that
light
travels
at
a
finite
speed
was
taken
into
account
soon
afterwards
after
a
conversation
with
bohr
about
zero-point
energy
casimir
noticed
that
this
result
could
be
interpreted
in
terms
of
vacuum
fluctuations
he
then
asked
himself
what
would
happen
if
there
were
two
mirrors
–
rather
than
two
molecules
–
facing
each
other
in
a
vacuum
it
was
this
work
that
led
to
his
famous
prediction
of
an
attractive
force
between
reflecting
plates
the
work
by
casimir
and
polder
opened
up
the
way
to
a
unified
theory
of
van
der
waals
and
casimir
forces
and
a
smooth
continuum
between
the
two
phenomena
this
was
done
by
lifshitz
(1956)
in
the
case
of
plane
parallel
dielectric
plates
the
generic
name
for
both
van
der
waals
and
casimir
forces
is
dispersion
forces
because
both
of
them
are
caused
by
dispersions
of
the
operator
of
the
dipole
moment
the
role
of
relativistic
forces
becomes
dominant
at
orders
of
a
hundred
nanometers
in
1951
herbert
callen
and
theodore
welton
proved
the
quantum
fluctuation-dissipation
theorem
(fdt)
which
was
originally
formulated
in
classical
form
by
nyquist
(1928)
as
an
explanation
for
observed
johnson
noise
in
electric
circuits
fluctuation-dissipation
theorem
showed
that
when
something
dissipates
energy
in
an
effectively
irreversible
way
a
connected
heat
bath
must
also
fluctuate
the
fluctuations
and
the
dissipation
go
hand
in
hand;
it
is
impossible
to
have
one
without
the
other
the
implication
of
fdt
being
that
the
vacuum
could
be
treated
as
a
heat
bath
coupled
to
a
dissipative
force
and
as
such
energy
could
in
part
be
extracted
from
the
vacuum
for
potentially
useful
work
fdt
has
been
shown
to
be
true
experimentally
under
certain
quantum
non-classical
conditions
in
1963
the
jaynes–cummings
model
was
developed
describing
the
system
of
a
two-level
atom
interacting
with
a
quantized
field
mode
(ie
the
vacuum)
within
an
optical
cavity
it
gave
nonintuitive
predictions
such
as
that
an
atom's
spontaneous
emission
could
be
driven
by
field
of
effectively
constant
frequency
(rabi
frequency)
in
the
1970s
experiments
were
being
performed
to
test
aspects
of
quantum
optics
and
showed
that
the
rate
of
spontaneous
emission
of
an
atom
could
be
controlled
using
reflecting
surfaces
these
results
were
at
first
regarded
with
suspicion
in
some
quarters:
it
was
argued
that
no
modification
of
a
spontaneous
emission
rate
would
be
possible
after
all
how
can
the
emission
of
a
photon
be
affected
by
an
atom's
environment
when
the
atom
can
only
"see"
its
environment
by
emitting
a
photon
in
the
first
place?
these
experiments
gave
rise
to
cavity
quantum
electrodynamics
(cqed)
the
study
of
effects
of
mirrors
and
cavities
on
radiative
corrections
spontaneous
emission
can
be
suppressed
(or
"inhibited")
or
amplified
amplification
was
first
predicted
by
purcell
in
1946
(the
purcell
effect)
and
has
been
experimentally
verified
this
phenomenon
can
be
understood
partly
in
terms
of
the
action
of
the
vacuum
field
on
the
atom
zero-point
energy
is
fundamentally
related
to
the
heisenberg
uncertainty
principle
roughly
speaking
the
uncertainty
principle
states
that
complementary
variables
(such
as
a
particle's
position
and
momentum
or
a
field's
value
and
derivative
at
a
point
in
space)
cannot
simultaneously
be
specified
precisely
by
any
given
quantum
state
in
particular
there
cannot
exist
a
state
in
which
the
system
simply
sits
motionless
at
the
bottom
of
its
potential
well:
for
then
its
position
and
momentum
would
both
be
completely
determined
to
arbitrarily
great
precision
therefore
instead
the
lowest-energy
state
(the
ground
state)
of
the
system
must
have
a
distribution
in
position
and
momentum
that
satisfies
the
uncertainty
principle−−which
implies
its
energy
must
be
greater
than
the
minimum
of
the
potential
well
near
the
bottom
of
a
potential
well
the
hamiltonian
of
a
general
system
(the
quantum-mechanical
operator
giving
its
energy)
can
be
approximated
as
a
quantum
harmonic
oscillator
where
is
the
minimum
of
the
classical
potential
well
the
uncertainty
principle
tells
us
that
making
the
expectation
values
of
the
kinetic
and
potential
terms
above
satisfy
the
expectation
value
of
the
energy
must
therefore
be
at
least
where
is
the
angular
frequency
at
which
the
system
oscillates
a
more
thorough
treatment
showing
that
the
energy
of
the
ground
state
actually
saturates
this
bound
and
is
exactly
requires
solving
for
the
ground
state
of
the
system
the
idea
of
a
quantum
harmonic
oscillator
and
its
associated
energy
can
apply
to
either
an
atom
or
subatomic
particle
in
ordinary
atomic
physics
the
zero-point
energy
is
the
energy
associated
with
the
ground
state
of
the
system
the
professional
physics
literature
tends
to
measure
frequency
as
denoted
by
above
using
angular
frequency
denoted
with
and
defined
by
this
leads
to
a
convention
of
writing
planck's
constant
with
a
bar
through
its
top
()
to
denote
the
quantity
in
these
terms
the
most
famous
such
example
of
zero-point
energy
is
the
above
associated
with
the
ground
state
of
the
quantum
harmonic
oscillator
in
quantum
mechanical
terms
the
zero-point
energy
is
the
expectation
value
of
the
hamiltonian
of
the
system
in
the
ground
state
if
more
than
one
ground
state
exists
they
are
said
to
be
degenerate
many
systems
have
degenerate
ground
states
degeneracy
occurs
whenever
there
exists
a
unitary
operator
which
acts
non-trivially
on
a
ground
state
and
commutes
with
the
hamiltonian
of
the
system
according
to
the
third
law
of
thermodynamics
a
system
at
absolute
zero
temperature
exists
in
its
ground
state;
thus
its
entropy
is
determined
by
the
degeneracy
of
the
ground
state
many
systems
such
as
a
perfect
crystal
lattice
have
a
unique
ground
state
and
therefore
have
zero
entropy
at
absolute
zero
it
is
also
possible
for
the
highest
excited
state
to
have
absolute
zero
temperature
for
systems
that
exhibit
negative
temperature
the
wave
function
of
the
ground
state
of
a
particle
in
a
one-dimensional
well
is
a
half-period
sine
wave
which
goes
to
zero
at
the
two
edges
of
the
well
the
energy
of
the
particle
is
given
by:
where
is
the
planck
constant
is
the
mass
of
the
particle
is
the
energy
state
(
corresponds
to
the
ground-state
energy)
and
is
the
width
of
the
well
in
quantum
field
theory
(qft)
the
fabric
of
"empty"
space
is
visualized
as
consisting
of
fields
with
the
field
at
every
point
in
space
and
time
being
a
quantum
harmonic
oscillator
with
neighboring
oscillators
interacting
with
each
other
according
to
qft
the
universe
is
made
up
of
matter
fields
whose
quanta
are
fermions
(eg
electrons
and
quarks)
and
force
fields
whose
quanta
are
bosons
(ie
photons
and
gluons)
all
these
fields
have
zero-point
energy
a
related
term
is
"zero-point
field"
(zpf)
which
is
the
lowest
energy
state
of
a
particular
field
the
vacuum
can
be
viewed
not
as
empty
space
but
as
the
combination
of
all
zero-point
fields
in
qft
this
combination
of
fields
is
called
the
vacuum
state
its
associated
zero-point
energy
is
called
the
vacuum
energy
and
the
average
expectation
value
of
the
hamiltonian
is
called
the
vacuum
expectation
value
(also
called
condensate
or
simply
vev)
the
qed
vacuum
is
a
part
of
the
vacuum
state
which
specifically
deals
with
quantum
electrodynamics
(eg
electromagnetic
interactions
between
photons
electrons
and
the
vacuum)
and
the
qcd
vacuum
deals
with
quantum
chromodynamics
(eg
color
charge
interactions
between
quarks
gluons
and
the
vacuum)
recent
experiments
advocate
the
idea
that
particles
themselves
can
be
thought
of
as
excited
states
of
the
underlying
quantum
vacuum
and
that
all
properties
of
matter
are
merely
vacuum
fluctuations
arising
from
interactions
with
the
zero-point
field
each
point
in
space
makes
a
contribution
of
resulting
in
a
calculation
of
infinite
zero-point
energy
in
any
finite
volume;
this
is
one
reason
renormalization
is
needed
to
make
sense
of
quantum
field
theories
in
cosmology
the
vacuum
energy
is
one
possible
explanation
for
the
cosmological
constant
and
the
source
of
dark
energy
scientists
are
not
in
agreement
about
how
much
energy
is
contained
in
the
vacuum
quantum
mechanics
requires
the
energy
to
be
large
as
paul
dirac
claimed
it
is
like
a
sea
of
energy
other
scientists
specializing
in
general
relativity
require
the
energy
to
be
small
enough
for
curvature
of
space
to
agree
with
observed
astronomy
the
heisenberg
uncertainty
principle
allows
the
energy
to
be
as
large
as
needed
to
promote
quantum
actions
for
a
brief
moment
of
time
even
if
the
average
energy
is
small
enough
to
satisfy
relativity
and
flat
space
to
cope
with
disagreements
the
vacuum
energy
is
described
as
a
virtual
energy
potential
of
positive
and
negative
energy
in
quantum
perturbation
theory
it
is
sometimes
said
that
the
contribution
of
one-loop
and
multi-loop
feynman
diagrams
to
elementary
particle
propagators
are
the
contribution
of
vacuum
fluctuations
or
the
zero-point
energy
to
the
particle
masses
the
oldest
and
best
known
quantized
force
field
is
the
electromagnetic
field
maxwell's
equations
have
been
superseded
by
quantum
electrodynamics
(qed)
by
considering
the
zero-point
energy
that
arises
from
qed
it
is
possible
to
gain
a
characteristic
understanding
of
zero-point
energy
that
arises
not
just
through
electromagnetic
interactions
but
in
all
quantum
field
theories
in
the
quantum
theory
of
the
electromagnetic
field
classical
wave
amplitudes
and
are
replaced
by
operators
and
that
satisfy:
the
classical
quantity
appearing
in
the
classical
expression
for
the
energy
of
a
field
mode
is
replaced
in
quantum
theory
by
the
photon
number
operator
the
fact
that:
implies
that
quantum
theory
does
not
allow
states
of
the
radiation
field
for
which
the
photon
number
and
a
field
amplitude
can
be
precisely
defined
ie
we
cannot
have
simultaneous
eigenstates
for
and
the
reconciliation
of
wave
and
particle
attributes
of
the
field
is
accomplished
via
the
association
of
a
probability
amplitude
with
a
classical
mode
pattern
the
calculation
of
field
modes
is
entirely
classical
problem
while
the
quantum
properties
of
the
field
are
carried
by
the
mode
"amplitudes"
and
associated
with
these
classical
modes
the
zero-point
energy
of
the
field
arises
formally
from
the
non-commutativity
of
and
this
is
true
for
any
harmonic
oscillator:
the
zero-point
energy
appears
when
we
write
the
hamiltonian:
it
is
often
argued
that
the
entire
universe
is
completed
bathed
in
the
zero-point
electromagnetic
field
and
as
such
it
can
add
only
some
constant
amount
to
expectation
values
physical
measurements
will
therefore
reveal
only
deviations
from
the
vacuum
state
thus
the
zero-point
energy
can
be
dropped
from
the
hamiltonian
by
redefining
the
zero
of
energy
or
by
arguing
that
it
is
a
constant
and
therefore
has
no
effect
on
heisenberg
equations
of
motion
thus
we
can
choose
to
declare
by
fiat
that
the
ground
state
has
zero
energy
and
a
field
hamiltonian
for
example
can
be
replaced
by:
without
affecting
any
physical
predictions
of
the
theory
the
new
hamiltonian
is
said
to
be
normally
ordered
(or
wick
ordered)
and
is
denoted
by
a
double-dot
symbol
the
normally
ordered
hamiltonian
is
denoted
ie:
in
other
words
within
the
normal
ordering
symbol
we
can
commute
and
since
zero-point
energy
is
intimately
connected
to
the
non-commutativity
of
and
the
normal
ordering
procedure
eliminates
any
contribution
from
the
zero-point
field
this
is
especially
reasonable
in
the
case
of
the
field
hamiltonian
since
the
zero-point
term
merely
adds
a
constant
energy
which
can
be
eliminated
by
a
simple
redefinition
for
the
zero
of
energy
moreover
this
constant
energy
in
the
hamiltonian
obviously
commutes
with
and
and
so
cannot
have
any
effect
on
the
quantum
dynamics
described
by
the
heisenberg
equations
of
motion
however
things
are
not
quite
that
simple
the
zero-point
energy
cannot
be
eliminated
by
dropping
its
energy
from
the
hamiltonian:
when
we
do
this
and
solve
the
heisenberg
equation
for
a
field
operator
we
must
include
the
vacuum
field
which
is
the
homogeneous
part
of
the
solution
for
the
field
operator
in
fact
we
can
show
that
the
vacuum
field
is
essential
for
the
preservation
of
the
commutators
and
the
formal
consistent
of
qed
when
we
calculate
the
field
energy
we
obtain
not
only
a
contribution
from
particles
and
forces
that
may
be
present
but
also
a
contribution
from
the
vacuum
field
itself
ie
the
zero-point
field
energy
in
other
words
the
zero-point
energy
reappears
even
though
we
may
have
deleted
it
from
the
hamiltonian
from
maxwell's
equations
the
electromagnetic
energy
of
a
"free"
field
ie
one
with
no
sources
is
described
by:
we
introduce
the
"mode
function"
that
satisfies
the
helmholtz
equation:
where
and
assume
it
is
normalized
such
that:
we
wish
to
"quantize"
the
electromagnetic
energy
of
free
space
for
a
multimode
field
the
field
intensity
of
free
space
should
be
independent
of
position
such
that
should
be
independent
of
for
each
mode
of
the
field
the
mode
function
satisfying
these
conditions
is:
where
in
order
to
have
the
transversality
condition
satisfied
for
the
coulomb
gauge
in
which
we
are
working
to
achieve
the
desired
normalization
we
pretend
space
is
divided
into
cubes
of
volume
and
impose
on
the
field
the
periodic
boundary
condition:
or
equivalently
where
can
assume
any
integer
value
this
allows
us
to
consider
the
field
in
any
one
of
the
imaginary
cubes
and
to
define
the
mode
function:
which
satisfies
the
helmholtz
equation
transversality
and
the
"box
normalization":
where
is
chosen
to
be
a
unit
vector
which
specifies
the
polarization
of
the
field
mode
the
condition
means
that
there
are
two
independent
choices
of
which
we
call
and
where
and
thus
we
define
the
mode
functions:
in
terms
of
which
the
vector
potential
becomes:
or:
where
and
are
photon
annihilation
and
creation
operators
for
the
mode
with
wave
vector
and
polarization
this
gives
the
vector
potential
for
a
plane
wave
mode
of
the
field
the
condition
for
shows
that
there
are
infinitely
many
such
modes
the
linearity
of
maxwell's
equations
allows
us
to
write:
for
the
total
vector
potential
in
free
space
using
the
fact
that:
we
find
the
field
hamiltonian
is:
this
is
the
hamiltonian
for
an
infinite
number
of
uncoupled
harmonic
oscillators
thus
different
modes
of
the
field
are
independent
and
satisfy
the
commutation
relations:
clearly
the
least
eigenvalue
for
is:
this
state
describes
the
zero-point
energy
of
the
vacuum
it
appears
that
this
sum
is
divergent
–
in
fact
highly
divergent
as
putting
in
the
density
factor
shows
the
summation
becomes
approximately
the
integral:
for
high
values
of
it
diverges
proportional
to
for
large
there
are
two
separate
questions
to
consider
first
is
the
divergence
a
real
one
such
that
the
zero-point
energy
really
is
infinite?
if
we
consider
the
volume
is
contained
by
perfectly
conducting
walls
very
high
frequencies
can
only
be
contained
by
taking
more
and
more
perfect
conduction
no
actual
method
of
containing
the
high
frequencies
is
possible
such
modes
will
not
be
stationary
in
our
box
and
thus
not
countable
in
the
stationary
energy
content
so
from
this
physical
point
of
view
the
above
sum
should
only
extend
to
those
frequencies
which
are
countable;
a
cut-off
energy
is
thus
eminently
reasonable
however
on
the
scale
of
a
"universe"
questions
of
general
relativity
must
be
included
suppose
even
the
boxes
could
be
reproduced
fit
together
and
closed
nicely
by
curving
spacetime
then
exact
conditions
for
running
waves
may
be
possible
however
the
very
high
frequency
quanta
will
still
not
be
contained
as
per
john
wheeler's
"geons"
these
will
leak
out
of
the
system
so
again
a
cut-off
is
permissible
almost
necessary
the
question
here
becomes
one
of
consistency
since
the
very
high
energy
quanta
will
act
as
a
mass
source
and
start
curving
the
geometry
this
leads
to
the
second
question
divergent
or
not
finite
or
infinite
is
the
zero-point
energy
of
any
physical
significance?
the
ignoring
of
the
whole
zero-point
energy
is
often
encouraged
for
all
practical
calculations
the
reason
for
this
is
that
energies
are
not
typically
defined
by
an
arbitrary
data
point
but
rather
changes
in
data
points
so
adding
or
subtracting
a
constant
(even
if
infinite)
should
to
be
allowed
however
this
is
not
the
whole
story
in
reality
energy
is
not
so
arbitrarily
defined:
in
general
relativity
the
seat
of
the
curvature
of
spacetime
is
the
energy
content
and
there
the
absolute
amount
of
energy
has
real
physical
meaning
there
is
no
such
thing
as
an
arbitrary
additive
constant
with
density
of
field
energy
energy
density
curves
space
and
an
increase
in
energy
density
produces
an
increase
of
curvature
furthermore
the
zero-point
energy
density
has
other
physical
consequences
eg
the
casimir
effect
contribution
to
the
lamb
shift
or
anomalous
magnetic
moment
of
the
electron
it
is
clear
it
is
not
just
a
mathematical
constant
or
artifact
that
can
be
cancelled
out
the
vacuum
state
of
the
"free"
electromagnetic
field
(that
with
no
sources)
is
defined
as
the
ground
state
in
which
for
all
modes
the
vacuum
state
like
all
stationary
states
of
the
field
is
an
eigenstate
of
the
hamiltonian
but
not
the
electric
and
magnetic
field
operators
in
the
vacuum
state
therefore
the
electric
and
magnetic
fields
do
not
have
definite
values
we
can
imagine
them
to
be
fluctuating
about
their
mean
value
of
zero
in
a
process
in
which
a
photon
is
annihilated
(absorbed)
we
can
think
of
the
photon
as
making
a
transition
into
the
vacuum
state
similarly
when
a
photon
is
created
(emitted)
it
is
occasionally
useful
to
imagine
that
the
photon
has
made
a
transition
out
of
the
vacuum
state
an
atom
for
instance
can
be
considered
to
be
"dressed"
by
emission
and
reabsorption
of
"virtual
photons"
from
the
vacuum
the
vacuum
state
energy
described
by
is
infinite
we
can
make
the
replacement:
the
zero-point
energy
density
is:
or
in
other
words
the
spectral
energy
density
of
the
vacuum
field:
the
zero-point
energy
density
in
the
frequency
range
from
to
is
therefore:
this
can
be
large
even
in
relatively
narrow
"low
frequency"
regions
of
the
spectrum
in
the
optical
region
from
400
to
700 nm
for
instance
the
above
equation
yields
around
220 erg/cm
we
showed
in
the
above
section
that
the
zero-point
energy
can
be
eliminated
from
the
hamiltonian
by
the
normal
ordering
prescription
however
this
elimination
does
not
mean
that
the
vacuum
field
has
been
rendered
unimportant
or
without
physical
consequences
to
illustrate
this
point
we
consider
a
linear
dipole
oscillator
in
the
vacuum
the
hamiltonian
for
the
oscillator
plus
the
field
with
which
it
interacts
is:
this
has
the
same
form
as
the
corresponding
classical
hamiltonian
and
the
heisenberg
equations
of
motion
for
the
oscillator
and
the
field
are
formally
the
same
as
their
classical
counterparts
for
instance
the
heisenberg
equations
for
the
coordinate
and
the
canonical
momentum
of
the
oscillator
are:
or:
since
the
rate
of
change
of
the
vector
potential
in
the
frame
of
the
moving
charge
is
given
by
the
convective
derivative
for
nonrelativistic
motion
we
may
neglect
the
magnetic
force
and
replace
the
expression
for
by:
above
we
have
made
the
electric
dipole
approximation
in
which
the
spatial
dependence
of
the
field
is
neglected
the
heisenberg
equation
for
is
found
similarly
from
the
hamiltonian
to
be:
in
the
electric
dipole
approximation
in
deriving
these
equations
for
and
we
have
used
the
fact
that
equal-time
particle
and
field
operators
commute
this
follows
from
the
assumption
that
particle
and
field
operators
commute
at
some
time
(say
)
when
the
matter-field
interpretation
is
presumed
to
begin
together
with
the
fact
that
a
heisenberg-picture
operator
evolves
in
time
as
where
is
the
time
evolution
operator
satisfying
alternatively
we
can
argue
that
these
operators
must
commute
if
we
are
to
obtain
the
correct
equations
of
motion
from
the
hamiltonian
just
as
the
corresponding
poisson
brackets
in
classical
theory
must
vanish
in
order
to
generate
the
correct
hamilton
equations
the
formal
solution
of
the
field
equation
is:
and
therefore
the
equation
for
may
be
written:
where:
and:
it
can
be
shown
that
in
the
radiation
reaction
field
if
the
mass
is
regarded
as
the
"observed"
mass
then
we
can
take:
the
total
field
acting
on
the
dipole
has
two
parts
and
is
the
free
or
zero-point
field
acting
on
the
dipole
it
is
the
homogeneous
solution
of
the
maxwell
equation
for
the
field
acting
on
the
dipole
ie
the
solution
at
the
position
of
the
dipole
of
the
wave
equation
satisfied
by
the
field
in
the
(source
free)
vacuum
for
this
reason
is
often
referred
to
as
the
"vacuum
field"
although
it
is
of
course
a
heisenberg-picture
operator
acting
on
whatever
state
of
the
field
happens
to
be
appropriate
at
is
the
source
field
the
field
generated
by
the
dipole
and
acting
on
the
dipole
using
the
above
equation
for
we
obtain
an
equation
for
the
heisenberg-picture
operator
formula_48
that
is
formally
the
same
as
the
classical
equation
for
a
linear
dipole
oscillator:
where
in
this
instance
we
have
considered
a
dipole
in
the
vacuum
without
any
"external"
field
acting
on
it
the
role
of
the
external
field
in
the
above
equation
is
played
by
the
vacuum
electric
field
acting
on
the
dipole
classically
a
dipole
in
the
vacuum
is
not
acted
upon
by
any
"external"
field:
if
there
are
no
sources
other
than
the
dipole
itself
then
the
only
field
acting
on
the
dipole
is
its
own
radiation
reaction
field
in
quantum
theory
however
there
is
always
an
"external"
field
namely
the
source-free
or
vacuum
field
according
to
our
earlier
equation
for
the
free
field
is
the
only
field
in
existence
at
as
the
time
at
which
the
interaction
between
the
dipole
and
the
field
is
"switched
on"
the
state
vector
of
the
dipole-field
system
at
is
therefore
of
the
form
where
is
the
vacuum
state
of
the
field
and
is
the
initial
state
of
the
dipole
oscillator
the
expectation
value
of
the
free
field
is
therefore
at
all
times
equal
to
zero:
since
however
the
energy
density
associated
with
the
free
field
is
infinite:
the
important
point
of
this
is
that
the
zero-point
field
energy
does
not
affect
the
heisenberg
equation
for
since
it
is
a
c-number
or
constant
(ie
an
ordinary
number
rather
than
an
operator)
and
commutes
with
we
can
therefore
drop
the
zero-point
field
energy
from
the
hamiltonian
as
is
usually
done
but
the
zero-point
field
re-emerges
as
the
homogeneous
solution
for
the
field
equation
a
charged
particle
in
the
vacuum
will
therefore
always
see
a
zero-point
field
of
infinite
density
this
is
the
origin
of
one
of
the
infinities
of
quantum
electrodynamics
and
it
cannot
be
eliminated
by
the
trivial
expedient
dropping
of
the
term
in
the
field
hamiltonian
the
free
field
is
in
fact
necessary
for
the
formal
consistency
of
the
theory
in
particular
it
is
necessary
for
the
preservation
of
the
commutation
relations
which
is
required
by
the
unitary
of
time
evolution
in
quantum
theory:
we
can
calculate
from
the
formal
solution
of
the
operator
equation
of
motion
using
the
fact
that
and
that
equal-time
particle
and
field
operators
commute
we
obtain:
for
the
dipole
oscillator
under
consideration
it
can
be
assumed
that
the
radiative
damping
rate
is
small
compared
with
the
natural
oscillation
frequency
ie
then
the
integrand
above
is
sharply
peaked
at
and:
the
necessity
of
the
vacuum
field
can
also
be
appreciated
by
making
the
small
damping
approximation
in
and
without
the
free
field
in
this
equation
the
operator
would
be
exponentially
dampened
and
commutators
like
would
approach
zero
for
with
the
vacuum
field
included
however
the
commutator
is
at
all
times
as
required
by
unitarity
and
as
we
have
just
shown
a
similar
result
is
easily
worked
out
for
the
case
of
a
free
particle
instead
of
a
dipole
oscillator
what
we
have
here
is
an
example
of
a
"fluctuation-dissipation
elation"
generally
speaking
if
a
system
is
coupled
to
a
bath
that
can
take
energy
from
the
system
in
an
effectively
irreversible
way
then
the
bath
must
also
cause
fluctuations
the
fluctuations
and
the
dissipation
go
hand
in
hand
we
cannot
have
one
without
the
other
in
the
current
example
the
coupling
of
a
dipole
oscillator
to
the
electromagnetic
field
has
a
dissipative
component
in
the
form
of
the
zero-point
(vacuum)
field;
given
the
existence
of
radiation
reaction
the
vacuum
field
must
also
exist
in
order
to
preserve
the
canonical
commutation
rule
and
all
it
entails
the
spectral
density
of
the
vacuum
field
is
fixed
by
the
form
of
the
radiation
reaction
field
or
vice
versa:
because
the
radiation
reaction
field
varies
with
the
third
derivative
of
the
spectral
energy
density
of
the
vacuum
field
must
be
proportional
to
the
third
power
of
in
order
for
to
hold
in
the
case
of
a
dissipative
force
proportional
to
by
contrast
the
fluctuation
force
must
be
proportional
to
formula_60
in
order
to
maintain
the
canonical
commutation
relation
this
relation
between
the
form
of
the
dissipation
and
the
spectral
density
of
the
fluctuation
is
the
essence
of
the
fluctuation-dissipation
theorem
the
fact
that
the
canonical
commutation
relation
for
a
harmonic
oscillator
coupled
to
the
vacuum
field
is
preserved
implies
that
the
zero-point
energy
of
the
oscillator
is
preserved
it
is
easy
to
show
that
after
a
few
damping
times
the
zero-point
motion
of
the
oscillator
is
in
fact
sustained
by
the
driving
zero-point
field
the
qcd
vacuum
is
the
vacuum
state
of
quantum
chromodynamics
(qcd)
it
is
an
example
of
a
"non-perturbative"
vacuum
state
characterized
by
a
non-vanishing
condensates
such
as
the
gluon
condensate
and
the
quark
condensate
in
the
complete
theory
which
includes
quarks
the
presence
of
these
condensates
characterizes
the
confined
phase
of
quark
matter
in
technical
terms
gluons
are
vector
gauge
bosons
that
mediate
strong
interactions
of
quarks
in
quantum
chromodynamics
(qcd)
gluons
themselves
carry
the
color
charge
of
the
strong
interaction
this
is
unlike
the
photon
which
mediates
the
electromagnetic
interaction
but
lacks
an
electric
charge
gluons
therefore
participate
in
the
strong
interaction
in
addition
to
mediating
it
making
qcd
significantly
harder
to
analyze
than
qed
(quantum
electrodynamics)
as
it
deals
with
nonlinear
equations
to
characterize
such
interactions
the
standard
model
hypothesises
a
field
called
the
higgs
field
(symbol:
)
which
has
the
unusual
property
of
a
non-zero
amplitude
in
its
ground
state
(zero-point)
energy
after
renormalization;
ie
a
non-zero
vacuum
expectation
value
it
can
have
this
effect
because
of
its
unusual
"mexican
hat"
shaped
potential
whose
lowest
"point"
is
not
at
its
"centre"
below
a
certain
extremely
high
energy
level
the
existence
of
this
non-zero
vacuum
expectation
spontaneously
breaks
electroweak
gauge
symmetry
which
in
turn
gives
rise
to
the
higgs
mechanism
and
triggers
the
acquisition
of
mass
by
those
particles
interacting
with
the
field
the
higgs
mechanism
occurs
whenever
a
charged
field
has
a
vacuum
expectation
value
this
effect
occurs
because
scalar
field
components
of
the
higgs
field
are
"absorbed"
by
the
massive
bosons
as
degrees
of
freedom
and
couple
to
the
fermions
via
yukawa
coupling
thereby
producing
the
expected
mass
terms
the
expectation
value
of
in
the
ground
state
(the
vacuum
expectation
value
or
vev)
is
then
where
the
measured
value
of
this
parameter
is
approximately
it
has
units
of
mass
and
is
the
only
free
parameter
of
the
standard
model
that
is
not
a
dimensionless
number
the
higgs
mechanism
is
a
type
of
superconductivity
which
occurs
in
the
vacuum
it
occurs
when
all
of
space
is
filled
with
a
sea
of
particles
which
are
charged
and
thus
the
field
has
a
nonzero
vacuum
expectation
value
interaction
with
the
vacuum
energy
filling
the
space
prevents
certain
forces
from
propagating
over
long
distances
(as
it
does
in
a
superconducting
medium;
eg
in
the
ginzburg–landau
theory)
zero-point
energy
has
many
observed
physical
consequences
it
is
important
to
note
that
zero-point
energy
is
not
merely
an
artefact
of
mathematical
formalism
that
can
for
instance
be
dropped
from
a
hamiltonian
by
redefining
the
zero
of
energy
or
by
arguing
that
it
is
a
constant
and
therefore
has
no
effect
on
heisenberg
equations
of
motion
without
latter
consequence
indeed
such
treatment
could
create
a
problem
at
a
deeper
as
of
yet
undiscovered
theory
for
instance
in
general
relativity
the
zero
of
energy
(ie
the
energy
density
of
the
vacuum)
contributes
to
a
cosmological
constant
of
the
type
introduced
by
einstein
in
order
to
obtain
static
solutions
to
his
field
equations
the
zero-point
energy
density
of
the
vacuum
due
to
all
quantum
fields
is
extremely
large
even
when
we
cut
off
the
largest
allowable
frequencies
based
on
plausible
physical
arguments
it
implies
a
cosmological
constant
larger
than
the
limits
imposed
by
observation
by
about
120
orders
of
magnitude
this
"cosmological
constant
problem"
remains
one
of
the
greatest
unsolved
mysteries
of
physics
a
phenomenon
that
is
commonly
presented
as
evidence
for
the
existence
of
zero-point
energy
in
vacuum
is
the
casimir
effect
proposed
in
1948
by
dutch
physicist
hendrik
casimir
who
considered
the
quantized
electromagnetic
field
between
a
pair
of
grounded
neutral
metal
plates
the
vacuum
energy
contains
contributions
from
all
wavelengths
except
those
excluded
by
the
spacing
between
plates
as
the
plates
draw
together
more
wavelengths
are
excluded
and
the
vacuum
energy
decreases
the
decrease
in
energy
means
there
must
be
a
force
doing
work
on
the
plates
as
they
move
early
experimental
tests
from
the
1950s
onwards
gave
positive
results
showing
the
force
was
real
but
other
external
factors
could
not
be
ruled
out
as
the
primary
cause
with
the
range
of
experimental
error
sometimes
being
nearly
100%
that
changed
in
1997
with
lamoreaux
conclusively
showing
that
the
casimir
force
was
real
results
have
been
repeatedly
replicated
since
then
in
2009
munday
et
al
published
experimental
proof
that
(as
predicted
in
1961)
the
casimir
force
could
also
be
repulsive
as
well
as
being
attractive
repulsive
casimir
forces
could
allow
quantum
levitation
of
objects
in
a
fluid
and
lead
to
a
new
class
of
switchable
nanoscale
devices
with
ultra-low
static
friction
an
interesting
hypothetical
side
effect
of
the
casimir
effect
is
the
scharnhorst
effect
a
hypothetical
phenomenon
in
which
light
signals
travel
slightly
faster
than
between
two
closely
spaced
conducting
plates
the
quantum
fluctuations
of
the
electromagnetic
field
have
important
physical
consequences
in
addition
to
the
casimir
effect
they
also
lead
to
a
splitting
between
the
two
energy
levels
and
(in
term
symbol
notation)
of
the
hydrogen
atom
which
was
not
predicted
by
the
dirac
equation
according
to
which
these
states
should
have
the
same
energy
charged
particles
can
interact
with
the
fluctuations
of
the
quantized
vacuum
field
leading
to
slight
shifts
in
energy
this
effect
is
called
the
lamb
shift
the
shift
of
about
is
roughly
of
the
difference
between
the
energies
of
the
1s
and
2s
levels
and
amounts
to
1058 mhz
in
frequency
units
a
small
part
of
this
shift
(27 mhz
≈
3%)
arises
not
from
fluctuations
of
the
electromagnetic
field
but
from
fluctuations
of
the
electron–positron
field
the
creation
of
(virtual)
electron–positron
pairs
has
the
effect
of
screening
the
coulomb
field
and
acts
as
a
vacuum
dielectric
constant
this
effect
is
much
more
important
in
muonic
atoms
taking
(planck's
constant
divided
by
)
(the
speed
of
light)
and
(the
electromagnetic
coupling
constant
ie
a
measure
of
the
strength
of
the
electromagnetic
force
(where
is
the
absolute
value
of
the
electronic
charge
and
formula_61
is
the
vacuum
permittivity))
we
can
form
a
dimensionless
quantity
called
the
fine-structure
constant:
the
fine-structure
constant
is
the
coupling
constant
of
quantum
electrodynamics
(qed)
determining
the
strength
of
the
interaction
between
electrons
and
photons
it
turns
out
that
the
fine
structure
constant
is
not
really
a
constant
at
all
owing
to
the
zero-point
energy
fluctuations
of
the
electron-positron
field
the
quantum
fluctuations
caused
by
zero-point
energy
have
the
effect
of
screening
electric
charges:
owing
to
(virtual)
electron-positron
pair
production
the
charge
of
the
particle
measured
far
from
the
particle
is
far
smaller
than
the
charge
measured
when
close
to
it
the
heisenberg
inequality
where
and
are
the
standard
deviations
of
position
and
momentum
states
that:
it
means
that
a
short
distance
implies
large
momentum
and
therefore
high
energy
ie
particles
of
high
energy
must
be
used
to
explore
short
distances
qed
concludes
that
the
fine
structure
constant
is
an
increasing
function
of
energy
it
has
been
shown
that
at
energies
of
the
order
of
the
z
boson
rest
energy
90 gev
that:
rather
than
the
low-energy
the
renormalization
procedure
of
eliminating
zero-point
energy
infinities
allows
the
choice
of
an
arbitrary
energy
(or
distance)
scale
for
defining
all
in
all
depends
on
the
energy
scale
characteristic
of
the
process
under
study
and
also
on
details
of
the
renormalization
procedure
the
energy
dependence
of
has
been
observed
for
several
years
now
in
precision
experiment
in
high-energy
physics
in
the
presence
of
strong
electrostatic
fields
it
is
predicted
that
virtual
particles
become
separated
from
the
vacuum
state
and
form
real
matter
the
fact
that
electromagnetic
radiation
can
be
transformed
into
matter
and
vice
versa
leads
to
fundamentally
new
features
in
quantum
electrodynamics
one
of
the
most
important
consequences
is
that
even
in
the
vacuum
the
maxwell
equations
have
to
be
exchanged
by
more
complicated
formulas
in
general
it
will
be
not
possible
to
separate
processes
in
the
vacuum
from
the
processes
involving
matter
since
electromagnetic
fields
can
create
matter
if
the
field
fluctuations
are
strong
enough
this
leads
to
highly
complex
nonlinear
interaction
-
gravity
will
have
an
effect
on
the
light
at
the
same
time
the
light
has
an
effect
on
gravity
these
effects
were
first
predicted
by
werner
heisenberg
and
hans
heinrich
euler
in
1936
and
independently
the
same
year
by
victor
weisskopf
who
stated:
"the
physical
properties
of
the
vacuum
originate
in
the
“zero-point
energy”
of
matter
which
also
depends
on
absent
particles
through
the
external
field
strengths
and
therefore
contributes
an
additional
term
to
the
purely
maxwellian
field
energy"
thus
strong
magnetic
fields
vary
the
energy
contained
in
the
vacuum
the
scale
above
which
the
electromagnetic
field
is
expected
to
become
nonlinear
is
known
as
the
schwinger
limit
at
this
point
the
vacuum
has
all
the
properties
of
a
birefringent
medium
thus
in
principle
a
rotation
of
the
polarization
frame
(the
faraday
effect)
can
be
observed
in
empty
space
both
einstein's
theory
of
special
and
general
relativity
state
that
light
should
pass
freely
through
a
vacuum
without
being
altered
a
principle
known
as
lorentz
invariance
yet
in
theory
large
nonlinear
self-interaction
of
light
due
to
quantum
fluctuations
should
lead
to
this
principle
being
measurably
violated
if
the
interactions
are
strong
enough
nearly
all
theories
of
quantum
gravity
predict
that
that
lorentz
invariance
is
not
an
exact
symmetry
of
nature
it
is
predicted
the
speed
at
which
light
travels
through
the
vacuum
depends
on
its
direction
polarization
and
the
local
strength
of
the
magnetic
field
there
have
been
a
number
of
inconclusive
results
which
claim
to
show
evidence
of
a
lorentz
violation
by
finding
a
rotation
of
the
polarization
plane
of
light
coming
from
distant
galaxies
the
first
concrete
evidence
for
vacuum
birefringence
was
published
in
2017
when
a
team
of
astronomers
looked
at
the
light
coming
from
the
star
rx
j18565-3754
the
closest
discovered
neutron
star
to
earth
roberto
mignani
at
the
national
institute
for
astrophysics
in
milan
who
led
the
team
of
astronomers
has
commented
that
"“when
einstein
came
up
with
the
theory
of
general
relativity
100
years
ago
he
had
no
idea
that
it
would
be
used
for
navigational
systems
the
consequences
of
this
discovery
probably
will
also
have
to
be
realised
on
a
longer
timescale”
the
team
found
that
visible
light
from
the
star
had
undergone
linear
polarisation
of
around
16%
if
the
birefringence
had
been
caused
by
light
passing
through
interstellar
gas
or
plasma
the
effect
should
have
been
no
more
than
1%
definitive
proof
would
require
repeating
the
observation
at
other
wavelengths
and
on
other
neutron
stars
at
x-ray
wavelengths
the
polarization
from
the
quantum
fluctuations
should
be
near
100%
although
no
telescope
currently
exists
that
can
make
such
measurements
there
are
several
proposed
x-ray
telescopes
that
may
soon
be
able
to
verify
the
result
conclusively
such
as
china's
hard
x-ray
modulation
telescope
(hxmt)
and
nasa's
imaging
x-ray
polarimetry
explorer
(ixpe)
in
the
late
1990s
it
was
discovered
that
very
distant
supernova
were
dimmer
than
expected
suggesting
that
the
universe's
expansion
was
accelerating
rather
than
slowing
down
this
revived
discussion
that
einstein's
cosmological
constant
long
disregarded
by
physicists
as
being
equal
to
zero
was
in
fact
some
small
positive
value
this
would
indicate
empty
space
exerted
some
form
of
negative
pressure
or
energy
there
is
no
natural
candidate
for
what
might
cause
what
has
been
called
dark
energy
but
the
current
best
guess
is
that
it
is
the
zero-point
energy
of
the
vacuum
one
difficulty
with
this
assumption
is
that
the
zero-point
energy
of
the
vacuum
is
absurdly
large
compared
to
the
observed
cosmological
constant
in
general
relativity
mass
and
energy
are
equivalent;
both
produce
a
gravitational
field
and
therefore
the
theorized
vacuum
energy
of
quantum
field
theory
should
have
led
the
universe
ripping
itself
to
pieces
this
obviously
has
not
happened
and
this
issue
called
the
cosmological
constant
problem
is
one
of
the
greatest
unsolved
mysteries
in
physics
the
european
space
agency
is
building
the
euclid
telescope
due
to
launch
in
2020
it
will
map
galaxies
up
to
10
billion
light
years
away
by
seeing
how
dark
energy
influences
their
arrangement
and
shape
the
mission
will
allow
scientists
to
see
if
the
strength
of
dark
energy
has
changed
if
dark
energy
is
found
to
vary
throughout
time
it
would
indicate
it
is
due
to
quintessence
where
observed
acceleration
is
due
to
the
energy
of
a
scalar
field
rather
than
the
cosmological
constant
no
evidence
of
quintessence
is
yet
available
but
it
has
not
been
ruled
out
either
it
generally
predicts
a
slightly
slower
acceleration
of
the
expansion
of
the
universe
than
the
cosmological
constant
some
scientists
think
that
the
best
evidence
for
quintessence
would
come
from
violations
of
einstein's
equivalence
principle
and
variation
of
the
fundamental
constants
in
space
or
time
scalar
fields
are
predicted
by
the
"standard
model
of
particle
physics"
and
string
theory
but
an
analogous
problem
to
the
cosmological
constant
problem
(or
the
problem
of
constructing
models
of
cosmological
inflation)
occurs:
renormalization
theory
predicts
that
scalar
fields
should
acquire
large
masses
again
due
to
zero-point
energy
cosmic
inflation
is
a
faster-than-light
expansion
of
space
just
after
the
big
bang
it
explains
the
origin
of
the
large-scale
structure
of
the
cosmos
it
is
believed
quantum
vacuum
fluctuations
caused
by
zero-point
energy
arising
in
the
microscopic
inflationary
period
later
became
magnified
to
a
cosmic
size
becoming
the
gravitational
seeds
for
galaxies
and
structure
in
the
universe
(see
galaxy
formation
and
evolution
and
structure
formation)
many
physicists
also
believe
that
inflation
explains
why
the
universe
appears
to
be
the
same
in
all
directions
(isotropic)
why
the
cosmic
microwave
background
radiation
is
distributed
evenly
why
the
universe
is
flat
and
why
no
magnetic
monopoles
have
been
observed
the
mechanism
for
inflation
is
unclear
it
is
similar
in
effect
to
dark
energy
but
is
a
far
more
energetic
and
short
lived
process
as
with
dark
energy
the
best
explanation
is
some
form
of
vacuum
energy
arising
from
quantum
fluctuations
it
may
be
that
inflation
caused
baryogenesis
the
hypothetical
physical
processes
that
produced
an
asymmetry
(imbalance)
between
baryons
and
antibaryons
produced
in
the
very
early
universe
but
this
is
far
from
certain
there
has
been
a
long
debate
over
the
question
of
whether
zero-point
fluctuations
of
quantized
vacuum
fields
are
“real”
ie
do
they
have
physical
effects
that
cannot
be
interpreted
by
an
equally
valid
alternative
theory?
schwinger
in
particular
attempted
to
formulate
qed
without
reference
to
zero-point
fluctuations
via
his
"source
theory"
from
such
an
approach
it
is
possible
to
derive
the
casimir
effect
without
reference
to
a
fluctuating
field
such
a
derivation
was
first
given
by
schwinger
(1975)
for
a
scalar
field
and
then
generalized
to
the
electromagnetic
case
by
schwinger
deraad
and
milton
(1978)
in
which
they
state
"the
vacuum
is
regarded
as
truly
a
state
with
all
physical
properties
equal
to
zero"
more
recently
jaffe
(2005)
has
highlighted
a
similar
approach
in
deriving
the
casimir
effect
stating
"the
concept
of
zero-point
fluctuations
is
a
heuristic
and
calculational
aid
in
the
description
of
the
casimir
effect
but
not
a
necessity
in
qed"
nevertheless
as
jaffe
himself
notes
in
his
paper
"no
one
has
shown
that
source
theory
or
another
s-matrix
based
approach
can
provide
a
complete
description
of
qed
to
all
orders"
furthermore
milonni
has
shown
the
necessity
of
the
vacuum
field
for
the
formal
consistency
of
qed
in
qcd
color
confinement
has
led
physicists
to
abandon
the
source
theory
or
s-matrix
based
approach
for
the
strong
interactions
the
higgs
mechanism
hawking
radiation
and
the
unruh
effect
are
also
theorized
to
be
dependent
on
zero-point
vacuum
fluctuations
the
field
contribution
being
an
inseparable
parts
of
these
theories
jaffe
continues
"even
if
one
could
argue
away
zero-point
contributions
to
the
quantum
vacuum
energy
the
problem
of
spontaneous
symmetry
breaking
remains:
condensates
[ground
state
vacua]
that
carry
energy
appear
at
many
energy
scales
in
the
standard
model
so
there
is
good
reason
to
be
skeptical
of
attempts
to
avoid
the
standard
formulation
of
quantum
field
theory
and
the
zero-point
energies
it
brings
with
it"
it
is
difficult
to
judge
the
physical
reality
of
infinite
zero-point
energies
that
are
inherent
in
field
theories
but
modern
physics
does
not
know
any
better
way
to
construct
gauge-invariant
renormalizable
theories
than
with
zero-point
energy
and
they
would
seem
to
be
a
necessity
for
any
attempt
at
a
unified
theory
the
mathematical
models
used
in
classical
electromagnetism
quantum
electrodynamics
(qed)
and
the
standard
model
all
view
the
electromagnetic
vacuum
as
a
linear
system
with
no
overall
observable
consequence
(eg
in
the
case
of
the
casimir
effect
lamb
shift
and
so
on)
these
phenomena
can
be
explained
by
alternative
mechanisms
other
than
action
of
the
vacuum
by
arbitrary
changes
to
the
normal
ordering
of
field
operators
see
alternative
theories
section)
this
is
a
consequence
of
viewing
electromagnetism
as
a
u(1)
gauge
theory
which
topologically
does
not
allow
the
complex
interaction
of
a
field
with
and
on
itself
in
higher
symmetry
groups
and
in
reality
the
vacuum
is
not
a
calm
randomly
fluctuating
largely
immaterial
and
passive
substance
but
at
times
can
be
viewed
as
a
turbulent
virtual
plasma
that
can
have
complex
vortices
(ie
solitons
vis-à-vis
particles)
entangled
states
and
a
rich
nonlinear
structure
there
are
many
observed
nonlinear
physical
electromagnetic
phenomena
such
as
aharonov–bohm
(ab)
and
altshuler–aronov–spivak
(aas)
effects
berry
aharonov–anandan
pancharatnam
and
chiao–wu
phase
rotation
effects
josephson
effect
what
are
called
maxwell's
equations
today
are
in
fact
a
simplified
version
of
the
original
equations
reformulated
by
heaviside
fitzgerald
lodge
and
hertz
the
original
equations
used
hamilton's
more
expressive
quaternion
notation
a
kind
of
clifford
algebra
which
fully
subsumes
the
standard
maxwell
vectorial
equations
largely
used
today
in
the
late
1880s
there
was
a
debate
over
the
relative
merits
of
vector
analysis
and
quaternions
according
to
heaviside
the
electromagnetic
potential
field
was
purely
metaphysical
an
arbitrary
mathematical
fiction
that
needed
to
be
"murdered"
it
was
concluded
that
there
was
no
need
for
the
greater
physical
insights
provided
by
the
quaternions
if
the
theory
was
purely
local
in
nature
local
vector
analysis
has
become
the
dominant
way
of
using
maxwell's
equations
ever
since
however
this
strictly
vectorial
approach
has
led
to
a
restrictive
topological
understanding
in
some
areas
of
electromagnetism
for
example
a
full
understanding
of
the
energy
transfer
dynamics
in
tesla's
oscillator-shuttle-circuit
can
only
be
achieved
in
quaternionic
algebra
or
higher
su(2)
symmetries
it
has
often
been
argued
that
quaternions
are
not
compatible
with
special
relativity
but
multiple
papers
have
shown
ways
of
incorporating
relativity
a
good
example
of
nonlinear
electromagnetics
is
in
high
energy
dense
plasmas
where
vortical
phenomena
occur
which
seemingly
violate
the
second
law
of
thermodynamics
by
increasing
the
energy
gradient
within
the
electromagnetic
field
and
violate
maxwell's
laws
by
creating
ion
currents
which
capture
and
concentrate
their
own
and
surrounding
magnetic
fields
in
particular
lorentz
force
law
which
elaborates
maxwell's
equations
is
violated
by
these
force
free
vortices
these
apparent
violations
are
due
to
the
fact
that
the
traditional
conservation
laws
in
classical
and
quantum
electrodynamics
(qed)
only
display
linear
u(1)
symmetry
(in
particular
by
the
extended
noether
theorem
conservation
laws
such
as
the
laws
of
thermodynamics
need
not
always
apply
to
dissipative
systems
which
are
expressed
in
gauges
of
higher
symmetry)
the
second
law
of
thermodynamics
states
that
in
a
closed
linear
system
entropy
flow
can
only
be
positive
(or
exactly
zero
at
the
end
of
a
cycle)
however
negative
entropy
(ie
increased
order
structure
or
self-organisation)
can
spontaneously
appear
in
an
open
nonlinear
thermodynamic
system
that
is
far
from
equilibrium
so
long
as
this
emergent
order
accelerates
the
overall
flow
of
entropy
in
the
total
system
the
1977
nobel
prize
in
chemistry
was
awarded
to
thermodynamicist
ilya
prigogine
for
his
theory
of
dissipative
systems
that
described
this
notion
prigogine
described
the
principle
as
"order
through
fluctuations"
or
"order
out
of
chaos"
it
has
been
argued
by
some
that
all
emergent
order
in
the
universe
from
galaxies
solar
systems
planets
weather
complex
chemistry
evolutionary
biology
to
even
consciousness
technology
and
civilizations
are
themselves
examples
of
thermodynamic
dissipative
systems;
nature
having
naturally
selected
these
structures
to
accelerate
entropy
flow
within
the
universe
to
an
ever-increasing
degree
for
example
it
has
been
estimated
that
human
body
is
10000
times
more
effective
at
dissipating
energy
per
unit
of
mass
than
the
sun
one
may
query
what
this
has
to
do
with
zero-point
energy
given
the
complex
and
adaptive
behaviour
that
arises
from
nonlinear
systems
considerable
attention
in
recent
years
has
gone
into
studying
a
new
class
of
phase
transitions
which
occur
at
absolute
zero
temperature
these
are
quantum
phase
transitions
which
are
driven
by
em
field
fluctuations
as
a
consequence
of
zero-point
energy
a
good
example
of
a
spontaneous
phase
transition
that
are
attributed
to
zero-point
fluctuations
can
be
found
in
superconductors
superconductivity
is
one
of
the
best
known
empirically
quantified
macroscopic
electromagnetic
phenomena
whose
basis
is
recognised
to
be
quantum
mechanical
in
origin
the
behaviour
of
the
electric
and
magnetic
fields
under
superconductivity
is
governed
by
the
london
equations
however
it
has
been
questioned
in
a
series
of
journal
articles
whether
the
quantum
mechanically
canonised
london
equations
can
be
given
a
purely
classical
derivation
bostick
for
instance
has
claimed
to
show
that
the
london
equations
do
indeed
have
a
classical
origin
that
applies
to
superconductors
and
to
some
collisionless
plasmas
as
well
in
particular
it
has
been
asserted
that
the
beltrami
vortices
in
the
plasma
focus
display
the
same
paired
flux-tube
morphology
as
type
ii
superconductors
others
have
also
pointed
out
this
connection
fröhlich
has
shown
that
the
hydrodynamic
equations
of
compressible
fluids
together
with
the
london
equations
lead
to
a
macroscopic
parameter
(formula_65
=
electric
charge
density
/
mass
density)
without
involving
either
quantum
phase
factors
or
planck's
constant
in
essence
it
has
been
asserted
that
beltrami
plasma
vortex
structures
are
able
to
at
least
simulate
the
morphology
of
type
i
and
type
ii
superconductors
this
occurs
because
the
"organised"
dissipative
energy
of
the
vortex
configuration
comprising
the
ions
and
electrons
far
exceeds
the
"disorganised"
dissipative
random
thermal
energy
the
transition
from
disorganised
fluctuations
to
organised
helical
structures
is
a
phase
transition
involving
a
change
in
the
condensate's
energy
(ie
the
ground
state
or
zero-point
energy)
but
"without
any
associated
rise
in
temperature"
this
is
an
example
of
zero-point
energy
having
multiple
stable
states
(see
quantum
phase
transition
quantum
critical
point
topological
degeneracy
topological
order)
and
where
the
overall
system
structure
is
independent
of
a
reductionist
or
deterministic
view
that
"classical"
macroscopic
order
can
also
causally
affect
quantum
phenomena
furthermore
the
pair
production
of
beltrami
vortices
has
been
compared
to
the
morphology
of
pair
production
of
virtual
particles
in
the
vacuum
the
idea
that
the
vacuum
energy
can
have
multiple
stable
energy
states
is
a
leading
hypothesis
for
the
cause
of
cosmic
inflation
in
fact
it
has
been
argued
that
these
early
vacuum
fluctuations
led
to
the
expansion
of
the
universe
and
in
turn
have
guaranteed
the
non-equilibrium
conditions
necessary
to
drive
order
from
chaos
as
without
such
expansion
the
universe
would
have
reached
thermal
equilibrium
and
no
complexity
could
have
existed
with
the
continued
accelerated
expansion
of
the
universe
the
cosmos
generates
an
energy
gradient
that
increases
the
"free
energy"
(ie
the
available
usable
or
potential
energy
for
useful
work)
which
the
universe
is
able
to
utilize
to
create
ever
more
complex
forms
of
order
the
only
reason
earth's
environment
does
not
decay
into
an
equilibrium
state
is
that
it
receives
a
daily
dose
of
sunshine
and
that
in
turn
is
due
to
the
sun
"polluting"
interstellar
space
with
decreasing
entropy
the
sun's
fusion
power
is
only
possible
due
to
the
gravitational
disequilibrium
of
matter
that
arose
from
cosmic
expansion
in
this
essence
the
vacuum
energy
can
be
viewed
as
the
key
cause
of
the
negative
entropy
(ie
structure)
throughout
the
universe
that
humanity
might
alter
the
morphology
of
the
vacuum
energy
to
create
an
energy
gradient
for
useful
work
is
the
subject
of
much
controversy
physicists
overwhelmingly
reject
any
possibility
that
the
zero-point
energy
field
can
be
exploited
to
obtain
useful
energy
(work)
or
uncompensated
momentum;
such
efforts
are
seen
as
tantamount
to
perpetual
motion
machines
nevertheless
the
allure
of
free
energy
has
motivated
such
research
usually
falling
in
the
category
of
fringe
science
as
long
ago
as
1889
(before
quantum
theory
or
discovery
of
the
zero
point
energy)
nikola
tesla
proposed
that
useful
energy
could
be
obtained
from
free
space
or
what
was
assumed
at
that
time
to
be
an
all-pervasive
aether
others
have
since
claimed
to
exploit
zero-point
or
vacuum
energy
with
a
large
amount
of
pseudoscientific
literature
causing
ridicule
around
the
subject
despite
rejection
by
the
scientific
community
harnessing
zero-point
energy
remains
an
interest
of
research
by
non-scientific
entities
particularly
in
the
us
where
it
has
attracted
the
attention
of
major
aerospace/defence
contractors
and
the
us
department
of
defence
as
well
as
in
china
germany
russia
and
brazil
a
common
assumption
is
that
the
casimir
force
is
of
little
practical
use;
the
argument
is
made
that
the
only
way
to
actually
gain
energy
from
the
two
plates
is
to
allow
them
to
come
together
(getting
them
apart
again
would
then
require
more
energy)
and
therefore
it
is
a
one-use-only
tiny
force
in
nature
in
1984
robert
forward
published
work
showing
how
a
"vacuum-fluctuation
battery"
could
be
constructed
the
battery
can
be
recharged
by
making
the
electrical
forces
slightly
stronger
than
the
casimir
force
to
reexpand
the
plates
in
1995
and
1998
maclay
et
al
published
the
first
models
of
a
microelectromechanical
system
(mems)
with
casimir
forces
while
not
exploiting
the
casimir
force
for
useful
work
the
papers
drew
attention
from
the
mems
community
due
to
the
revelation
that
casimir
effect
needs
to
be
considered
as
a
vital
factor
in
the
future
design
of
mems
in
particular
casimir
effect
might
be
the
critical
factor
in
the
stiction
failure
of
mems
in
1999
pinto
a
former
scientist
at
nasa's
jet
propulsion
laboratory
at
caltech
in
pasadena
published
in
"physical
review"
his
thought
experiment
(gedankenexperiment)
for
a
"casimir
engine"
the
paper
showed
that
continuous
positive
net
exchange
of
energy
from
the
casimir
effect
was
possible
even
stating
in
the
abstract
"in
the
event
of
no
other
alternative
explanations
one
should
conclude
that
major
technological
advances
in
the
area
of
endless
by-product
free-energy
production
could
be
achieved"
in
2001
capasso
et
al
showed
how
the
force
can
be
used
to
control
the
mechanical
motion
of
a
mems
device
the
researchers
suspended
a
polysilicon
plate
from
a
torsional
rod
–
a
twisting
horizontal
bar
just
a
few
microns
in
diameter
when
they
brought
a
metallized
sphere
close
up
to
the
plate
the
attractive
casimir
force
between
the
two
objects
made
the
plate
rotate
they
also
studied
the
dynamical
behaviour
of
the
mems
device
by
making
the
plate
oscillate
the
casimir
force
reduced
the
rate
of
oscillation
and
led
to
nonlinear
phenomena
such
as
hysteresis
and
bistability
in
the
frequency
response
of
the
oscillator
according
to
the
team
the
system’s
behaviour
agreed
well
with
theoretical
calculations
despite
this
and
several
similar
peer
reviewed
papers
there
is
not
a
consensus
as
to
whether
such
devices
can
produce
a
continuous
output
of
work
garret
moddel
at
university
of
colorado
has
highlighted
that
he
believes
such
devices
hinge
on
the
assumption
that
the
casimir
force
is
a
nonconservative
force
he
argues
that
there
is
sufficient
evidence
(eg
analysis
by
scandurra
(2001))
to
say
that
the
casimir
effect
is
a
conservative
force
and
therefore
even
though
such
an
engine
can
exploit
the
casimir
force
for
useful
work
it
cannot
produce
more
output
energy
than
has
been
input
into
the
system
in
2008
darpa
solicited
research
proposals
in
the
area
of
casimir
effect
enhancement
(cee)
the
goal
of
the
program
is
to
develop
new
methods
to
control
and
manipulate
attractive
and
repulsive
forces
at
surfaces
based
on
engineering
of
the
casimir
force
a
2008
patent
by
haisch
and
moddel
details
a
device
that
is
able
to
extract
power
from
zero-point
fluctuations
using
a
gas
that
circulates
through
a
casimir
cavity
as
gas
atoms
circulate
around
the
system
they
enter
the
cavity
upon
entering
the
electrons
spin
down
to
release
energy
via
electromagnetic
radiation
this
radiation
is
then
extracted
by
an
absorber
on
exiting
the
cavity
the
ambient
vacuum
fluctuations
(ie
the
zero-point
field)
impart
energy
on
the
electrons
to
return
the
orbitals
to
previous
energy
levels
as
predicted
by
senitzky
(1960)
the
gas
then
goes
through
a
pump
and
flows
through
the
system
again
a
published
test
of
this
concept
by
moddel
was
performed
in
2012
and
seemed
to
give
excess
energy
that
could
not
be
attributed
to
another
source
however
it
has
not
been
conclusively
shown
to
be
from
zero-point
energy
and
the
theory
requires
further
investigation
in
1951
callen
and
welton
proved
the
quantum
fluctuation-dissipation
theorem
(fdt)
which
was
originally
formulated
in
classical
form
by
nyquist
(1928)
as
an
explanation
for
observed
johnson
noise
in
electric
circuits
fluctuation-dissipation
theorem
showed
that
when
something
dissipates
energy
in
an
effectively
irreversible
way
a
connected
heat
bath
must
also
fluctuate
the
fluctuations
and
the
dissipation
go
hand
in
hand;
it
is
impossible
to
have
one
without
the
other
the
implication
of
fdt
being
that
the
vacuum
could
be
treated
as
a
heat
bath
coupled
to
a
dissipative
force
and
as
such
energy
could
in
part
be
extracted
from
the
vacuum
for
potentially
useful
work
such
a
theory
has
met
with
resistance:
macdonald
(1962)
and
harris
(1971)
claimed
that
extracting
power
from
the
zero-point
energy
to
be
impossible
so
fdt
could
not
be
true
grau
and
kleen
(1982)
and
kleen
(1986)
argued
that
the
johnson
noise
of
a
resistor
connected
to
an
antenna
must
satisfy
planck's
thermal
radiation
formula
thus
the
noise
must
be
zero
at
zero
temperature
and
fdt
must
be
invalid
kiss
(1988)
pointed
out
that
the
existence
of
the
zero-point
term
may
indicate
that
there
is
a
renormalization
problem—ie
a
mathematical
artifact—producing
an
unphysical
term
that
is
not
actually
present
in
measurements
(in
analogy
with
renormalization
problems
of
ground
states
in
quantum
electrodynamics)
later
abbott
et
al
(1996)
arrived
at
a
different
but
unclear
conclusion
that
"zero-point
energy
is
infinite
thus
it
should
be
renormalized
but
not
the
‘zero-point
fluctuations’"
despite
such
criticism
fdt
has
been
shown
to
be
true
experimentally
under
certain
quantum
non-classical
conditions
zero-point
fluctuations
can
and
do
contribute
towards
systems
which
dissipate
energy
a
paper
by
armen
allahverdyan
and
theo
nieuwenhuizen
in
2000
showed
the
feasibility
of
extracting
zero-point
energy
for
useful
work
from
a
single
bath
without
contradicting
the
laws
of
thermodynamics
by
exploiting
certain
quantum
mechanical
properties
there
have
been
a
growing
number
of
papers
showing
that
in
some
instances
the
classical
laws
of
thermodynamics
such
as
limits
on
the
carnot
efficiency
can
be
violated
by
exploiting
negative
entropy
of
quantum
fluctuations
despite
efforts
to
reconcile
quantum
mechanics
and
thermodynamics
over
the
years
their
compatibility
is
still
an
open
fundamental
problem
the
full
extent
that
quantum
properties
can
alter
classical
thermodynamic
bounds
is
unknown
the
use
of
zero-point
energy
for
space
travel
is
highly
speculative
a
complete
quantum
theory
of
gravitation
(that
would
deal
with
the
role
of
quantum
phenomena
like
zero-point
energy)
does
not
yet
exist
speculative
papers
explaining
a
relationship
between
zero-point
energy
and
gravitational
shielding
effects
have
been
proposed
but
the
interaction
(if
any)
is
not
yet
fully
understood
most
serious
scientific
research
in
this
area
depends
on
the
theorized
anti-gravitational
properties
of
antimatter
(currently
being
tested
at
the
alpha
experiment
at
cern)
and/or
the
effects
of
non-newtonian
forces
such
as
the
gravitomagnetic
field
under
specific
quantum
conditions
according
to
the
general
theory
of
relativity
rotating
matter
can
generate
a
new
force
of
nature
known
as
the
gravitomagnetic
interaction
whose
intensity
is
proportional
to
the
rate
of
spin
in
certain
conditions
the
gravitomagnetic
field
can
be
repulsive
in
neutrons
stars
for
example
it
can
produce
a
gravitational
analogue
of
the
meissner
effect
but
the
force
produced
in
such
an
example
is
theorized
to
be
exceedingly
weak
in
1963
robert
forward
a
physicist
and
aerospace
engineer
at
hughes
research
laboratories
published
a
paper
showing
how
within
the
framework
of
general
relativity
"anti-gravitational"
effects
might
be
achieved
since
all
atoms
have
spin
gravitational
permeability
may
be
able
to
differ
from
material
to
material
a
strong
toroidal
gravitational
field
that
acts
against
the
force
of
gravity
could
be
generated
by
materials
that
have
nonlinear
properties
that
enhance
time-varying
gravitational
fields
such
an
effect
would
be
analogous
to
the
nonlinear
electromagnetic
permeability
of
iron
making
it
an
effective
core
(ie
the
doughnut
of
iron)
in
a
transformer
whose
properties
are
dependent
on
magnetic
permeability
in
1966
dewitt
was
first
to
identify
the
significance
of
gravitational
effects
in
superconductors
dewitt
demonstrated
that
a
magnetic-type
gravitational
field
must
result
in
the
presence
of
fluxoid
quantization
in
1983
dewitt's
work
was
substantially
expanded
by
ross
from
1971
to
1974
henry
william
wallace
a
scientist
at
ge
aerospace
was
issued
with
three
patents
wallace
used
dewitt's
theory
to
develop
an
experimental
apparatus
for
generating
and
detecting
a
secondary
gravitational
field
which
he
named
the
kinemassic
field
(now
better
known
as
the
gravitomagnetic
field)
in
his
three
patents
wallace
describes
three
different
methods
used
for
detection
of
the
gravitomagnetic
field
–
change
in
the
motion
of
a
body
on
a
pivot
detection
of
a
transverse
voltage
in
a
semiconductor
crystal
and
a
change
in
the
specific
heat
of
a
crystal
material
having
spin-aligned
nuclei
there
are
no
publicly
available
independent
tests
verifying
wallace's
devices
such
an
effect
if
any
would
be
small
referring
to
wallace's
patents
a
new
scientist
article
in
1980
stated
"although
the
wallace
patents
were
initially
ignored
as
cranky
observers
believe
that
his
invention
is
now
under
serious
but
secret
investigation
by
the
military
authorities
in
the
usa
the
military
may
now
regret
that
the
patents
have
already
been
granted
and
so
are
available
for
anyone
to
read"
a
further
reference
to
wallace's
patents
occur
in
an
electric
propulsion
study
prepared
for
the
astronautics
laboratory
at
edwards
air
force
base
which
states:
"the
patents
are
written
in
a
very
believable
style
which
include
part
numbers
sources
for
some
components
and
diagrams
of
data
attempts
were
made
to
contact
wallace
using
patent
addresses
and
other
sources
but
he
was
not
located
nor
is
there
a
trace
of
what
became
of
his
work
the
concept
can
be
somewhat
justified
on
general
relativistic
grounds
since
rotating
frames
of
time
varying
fields
are
expected
to
emit
gravitational
waves"
in
1986
the
us
air
force's
then
rocket
propulsion
laboratory
(rpl)
at
edwards
air
force
base
solicited
"non
conventional
propulsion
concepts"
under
a
small
business
research
and
innovation
program
one
of
the
six
areas
of
interest
was
"esoteric
energy
sources
for
propulsion
including
the
quantum
dynamic
energy
of
vacuum
space"
in
the
same
year
bae
systems
launched
"project
greenglow"
to
provide
a
"focus
for
research
into
novel
propulsion
systems
and
the
means
to
power
them"
in
1988
kip
thorne
et
al
published
work
showing
how
traversable
wormholes
can
exist
in
spacetime
only
if
they
are
threaded
by
quantum
fields
generated
by
some
form
of
exotic
matter
that
has
negative
energy
in
1993
scharnhorst
and
barton
showed
that
the
speed
of
a
photon
will
be
increased
if
it
travels
between
two
casimir
plates
an
example
of
negative
energy
in
the
most
general
sense
the
exotic
matter
needed
to
create
wormholes
would
share
the
repulsive
properties
of
the
inflationary
energy
dark
energy
or
zero-point
radiation
of
the
vacuum
building
on
the
work
of
thorne
in
1994
miguel
alcubierre
proposed
a
method
for
changing
the
geometry
of
space
by
creating
a
wave
that
would
cause
the
fabric
of
space
ahead
of
a
spacecraft
to
contract
and
the
space
behind
it
to
expand
(see
alcubierre
drive)
the
ship
would
then
ride
this
wave
inside
a
region
of
flat
space
known
as
a
"warp
bubble"
and
would
not
move
within
this
bubble
but
instead
be
carried
along
as
the
region
itself
moves
due
to
the
actions
of
the
drive
in
1992
evgeny
podkletnov
published
a
heavily
debated
journal
article
claiming
a
specific
type
of
rotating
superconductor
could
shield
gravitational
force
independently
of
this
from
1991
to
1993
ning
li
and
douglas
torr
published
a
number
of
articles
about
gravitational
effects
in
superconductors
one
finding
they
derived
is
the
source
of
gravitomagnetic
flux
in
a
type
ii
superconductor
material
is
due
to
spin
alignment
of
the
lattice
ions
quoting
from
their
third
paper:
"it
is
shown
that
the
coherent
alignment
of
lattice
ion
spins
will
generate
a
detectable
gravitomagnetic
field
and
in
the
presence
of
a
time-dependent
applied
magnetic
vector
potential
field
a
detectable
gravitoelectric
field"
the
claimed
size
of
the
generated
force
has
been
disputed
by
some
but
defended
by
others
in
1997
li
published
a
paper
attempting
to
replicate
podkletnov's
results
and
showed
the
effect
was
very
small
if
it
existed
at
all
li
is
reported
to
have
left
the
university
of
alabama
in
1999
to
found
the
company
"ac
gravity
llc"
ac
gravity
was
awarded
a
us
dod
grant
for
$448970
in
2001
to
continue
anti-gravity
research
the
grant
period
ended
in
2002
but
no
results
from
this
research
were
ever
made
public
in
2002
phantom
works
boeing's
advanced
research
and
development
facility
in
seattle
approached
evgeny
podkletnov
directly
phantom
works
was
blocked
by
russian
technology
transfer
controls
at
this
time
lieutenant
general
george
muellner
the
outgoing
head
of
the
boeing
phantom
works
confirmed
that
attempts
by
boeing
to
work
with
podkletnov
had
been
blocked
by
moscow
also
commenting
that
"the
physical
principles
–
and
podkletnov's
device
is
not
the
only
one
–
appear
to
be
valid
there
is
basic
science
there
they're
not
breaking
the
laws
of
physics
the
issue
is
whether
the
science
can
be
engineered
into
something
workable"
froning
and
roach
(2002)
put
forward
a
paper
that
builds
on
the
work
of
puthoff
haisch
and
alcubierre
they
used
fluid
dynamic
simulations
to
model
the
interaction
of
a
vehicle
(like
that
proposed
by
alcubierre)
with
the
zero-point
field
vacuum
field
perturbations
are
simulated
by
fluid
field
perturbations
and
the
aerodynamic
resistance
of
viscous
drag
exerted
on
the
interior
of
the
vehicle
is
compared
to
the
lorentz
force
exerted
by
the
zero-point
field
(a
casimir-like
force
is
exerted
on
the
exterior
by
unbalanced
zero-point
radiation
pressures)
they
find
that
the
optimized
negative
energy
required
for
an
alcubierre
drive
is
where
it
is
a
saucer-shaped
vehicle
with
toroidal
electromagnetic
fields
the
em
fields
distort
the
vacuum
field
perturbations
surrounding
the
craft
sufficiently
to
affect
the
permeability
and
permittivity
of
space
in
2014
nasa's
eagleworks
laboratories
announced
that
they
had
successfully
validated
the
use
of
a
quantum
vacuum
plasma
thruster
which
makes
use
of
the
casimir
effect
for
propulsion
in
2016
a
scientific
paper
by
the
team
of
nasa
scientists
passed
peer
review
for
the
first
time
the
paper
suggests
that
the
zero-point
field
acts
as
pilot-wave
and
that
the
thrust
may
be
due
to
particles
pushing
off
the
quantum
vacuum
while
peer
review
doesn’t
guarantee
that
a
finding
or
observation
is
valid
it
does
indicate
that
independent
scientists
looked
over
the
experimental
setup
results
and
interpretation
and
that
they
could
not
find
any
obvious
errors
in
the
methodology
and
that
they
found
the
results
reasonable
in
the
paper
the
authors
identify
and
discuss
nine
potential
sources
of
experimental
errors
including
rogue
air
currents
leaky
electromagnetic
radiation
and
magnetic
interactions
not
all
of
them
could
be
completely
ruled
out
and
further
peer
reviewed
experimentation
is
needed
in
order
to
rule
these
potential
errors
out
williams
spray
equation
in
combustion
the
williams
spray
equation
also
known
as
the
williams–boltzmann
equation
describes
the
statistical
evolution
of
sprays
contained
in
another
fluid
analogous
to
the
boltzmann
equation
for
the
molecules
named
after
forman
a
williams
who
derived
the
equation
in
1958
the
sprays
are
assumed
to
be
spherical
with
radius
formula_1
even
though
the
assumption
is
valid
for
solid
particles(liquid
droplets)
when
their
shape
has
no
consequence
on
the
combustion
for
liquid
droplets
to
be
nearly
spherical
the
spray
has
to
be
dilute(total
volume
occupied
by
the
sprays
is
much
less
than
the
volume
of
the
gas)
and
the
weber
number
formula_2
where
formula_3
is
the
gas
density
formula_4
is
the
spray
droplet
velocity
formula_5
is
the
gas
velocity
and
formula_6
is
the
surface
tension
of
the
liquid
spray
should
be
formula_7
the
equation
is
described
by
a
number
density
function
formula_8
which
represents
the
probable
number
of
spray
particles
(droplets)
of
chemical
species
formula_9
(of
formula_10
total
species)
that
one
can
find
with
radii
between
formula_1
and
formula_12
located
in
the
spatial
range
between
formula_13
and
formula_14
traveling
with
a
velocity
in
between
formula_4
and
formula_16
having
the
temperature
in
between
formula_17
and
formula_18
at
time
formula_19
then
the
spray
equation
for
the
evolution
of
this
density
function
is
given
by
where
this
model
for
the
rocket
motor
was
developed
by
probert
williams
and
tanasawa
it
is
reasonable
to
neglect
formula_31
for
distances
not
very
close
to
the
spray
atomizer
where
major
portion
of
combustion
occurs
consider
a
one-dimensional
liquid-propellent
rocket
motor
situated
at
formula_32
where
fuel
is
sprayed
neglecting
formula_33(density
function
is
defined
without
the
temperature
so
accordingly
dimensions
of
formula_34
changes)
and
due
to
the
fact
that
the
mean
flow
is
parallel
to
formula_35
axis
the
steady
spray
equation
reduces
to
where
formula_37
is
the
velocity
in
formula_35
direction
integrating
with
respect
to
the
velocity
results
the
contribution
from
the
last
term
(spray
acceleration
term)
becomes
zero
(using
divergence
theorem)
since
formula_40
when
formula_41
is
very
large
which
is
typically
the
case
in
rocket
motors
the
drop
size
rate
formula_42
is
well
modeled
using
vaporization
mechanisms
as
where
formula_44
is
independent
of
formula_1
but
can
depend
on
the
surrounding
gas
defining
the
number
of
droplets
per
unit
volume
per
unit
radius
and
average
quantities
averaged
over
velocities
the
equation
becomes
if
further
assumed
that
formula_48
is
independent
of
formula_1
and
with
a
transformed
coordinate
formula_50
if
the
combustion
chamber
has
varying
cross-section
area
formula_51
a
known
function
for
formula_52
and
with
area
formula_53
at
the
spraying
location
then
the
solution
is
given
by
where
formula_55
are
the
number
distribution
and
mean
velocity
at
formula_32
respectively
resistive
plate
chamber
a
resistive
plate
chamber
(rpc)
is
a
particle
detector
widely
used
in
high
energy
physics
they
are
used
for
detecting
muons
in
most
of
the
modern
experiments
including
atlas
cms
and
bes
iii
physical
law
a
physical
law
or
a
law
of
physics
is
a
statement
"inferred
from
particular
facts
applicable
to
a
defined
group
or
class
of
phenomena
and
expressible
by
the
statement
that
a
particular
phenomenon
always
occurs
if
certain
conditions
be
present"
physical
laws
are
typically
conclusions
based
on
repeated
scientific
experiments
and
observations
over
many
years
and
which
have
become
accepted
universally
within
the
scientific
community
the
production
of
a
summary
description
of
our
environment
in
the
form
of
such
laws
is
a
fundamental
aim
of
science
these
terms
are
not
used
the
same
way
by
all
authors
the
distinction
between
natural
law
in
the
political-legal
sense
and
law
of
nature
or
physical
law
in
the
scientific
sense
is
a
modern
one
both
concepts
being
equally
derived
from
"physis"
the
greek
word
(translated
into
latin
as
"natura")
for
"nature"
several
general
properties
of
physical
laws
have
been
identified
physical
laws
are:
some
of
the
more
famous
laws
of
nature
are
found
in
isaac
newton's
theories
of
(now)
classical
mechanics
presented
in
his
"philosophiae
naturalis
principia
mathematica"
and
in
albert
einstein's
theory
of
relativity
other
examples
of
laws
of
nature
include
boyle's
law
of
gases
conservation
laws
the
four
laws
of
thermodynamics
etc
many
scientific
laws
are
couched
in
mathematical
terms
(eg
newton's
second
law
"f"
=
or
the
uncertainty
principle
or
the
principle
of
least
action
or
causality)
while
these
scientific
laws
explain
what
our
senses
perceive
they
are
still
empirical
and
so
are
not
"mathematical"
laws
(mathematical
laws
can
be
proved
purely
by
mathematics
and
not
by
scientific
experiment)
other
laws
reflect
mathematical
symmetries
found
in
nature
(say
pauli
exclusion
principle
reflects
identity
of
electrons
conservation
laws
reflect
homogeneity
of
space
time
lorentz
transformations
reflect
rotational
symmetry
of
spacetime)
laws
are
constantly
being
checked
experimentally
to
higher
and
higher
degrees
of
precision
this
is
one
of
the
main
goals
of
science
just
because
laws
have
never
been
observed
to
be
violated
does
not
preclude
testing
them
at
increased
accuracy
or
in
new
kinds
of
conditions
to
confirm
whether
they
continue
to
hold
or
whether
they
break
and
what
can
be
discovered
in
the
process
it
is
always
possible
for
laws
to
be
invalidated
or
proven
to
have
limitations
by
repeatable
experimental
evidence
should
any
be
observed
well-established
laws
have
indeed
been
invalidated
in
some
special
cases
but
the
new
formulations
created
to
explain
the
discrepancies
can
be
said
to
generalize
upon
rather
than
overthrow
the
originals
that
is
the
invalidated
laws
have
been
found
to
be
only
close
approximations
(see
below)
to
which
other
terms
or
factors
must
be
added
to
cover
previously
unaccounted-for
conditions
eg
very
large
or
very
small
scales
of
time
or
space
enormous
speeds
or
masses
etc
thus
rather
than
unchanging
knowledge
physical
laws
are
better
viewed
as
a
series
of
improving
and
more
precise
generalizations
many
fundamental
physical
laws
are
mathematical
consequences
of
various
symmetries
of
space
time
or
other
aspects
of
nature
specifically
noether's
theorem
connects
some
conservation
laws
to
certain
symmetries
for
example
conservation
of
energy
is
a
consequence
of
the
shift
symmetry
of
time
(no
moment
of
time
is
different
from
any
other)
while
conservation
of
momentum
is
a
consequence
of
the
symmetry
(homogeneity)
of
space
(no
place
in
space
is
special
or
different
than
any
other)
the
indistinguishability
of
all
particles
of
each
fundamental
type
(say
electrons
or
photons)
results
in
the
dirac
and
bose
quantum
statistics
which
in
turn
result
in
the
pauli
exclusion
principle
for
fermions
and
in
bose–einstein
condensation
for
bosons
the
rotational
symmetry
between
time
and
space
coordinate
axes
(when
one
is
taken
as
imaginary
another
as
real)
results
in
lorentz
transformations
which
in
turn
result
in
special
relativity
theory
symmetry
between
inertial
and
gravitational
mass
results
in
general
relativity
the
inverse
square
law
of
interactions
mediated
by
massless
bosons
is
the
mathematical
consequence
of
the
3-dimensionality
of
space
one
strategy
in
the
search
for
the
most
fundamental
laws
of
nature
is
to
search
for
the
most
general
mathematical
symmetry
group
that
can
be
applied
to
the
fundamental
interactions
some
laws
are
only
approximations
of
other
more
general
laws
and
are
good
approximations
with
a
restricted
domain
of
applicability
for
example
newtonian
dynamics
(which
is
based
on
galilean
transformations)
is
the
low-speed
limit
of
special
relativity
(since
the
galilean
transformation
is
the
low-speed
approximation
to
the
lorentz
transformation)
similarly
the
newtonian
gravitation
law
is
a
low-mass
approximation
of
general
relativity
and
coulomb's
law
is
an
approximation
to
quantum
electrodynamics
at
large
distances
(compared
to
the
range
of
weak
interactions)
in
such
cases
it
is
common
to
use
the
simpler
approximate
versions
of
the
laws
instead
of
the
more
accurate
general
laws
according
to
a
positivist
view
when
compared
to
pre-modern
accounts
of
causality
laws
of
nature
fill
the
role
played
by
divine
causality
on
the
one
hand
and
accounts
such
as
plato's
theory
of
forms
on
the
other
the
observation
that
there
are
underlying
regularities
in
nature
dates
from
prehistoric
times
since
the
recognition
of
cause-and-effect
relationships
is
an
implicit
recognition
that
there
are
laws
of
nature
the
recognition
of
such
regularities
as
independent
scientific
laws
"per
se"
though
was
limited
by
their
entanglement
in
animism
and
by
the
attribution
of
many
effects
that
do
not
have
readily
obvious
causes—such
as
meteorological
astronomical
and
biological
phenomena—to
the
actions
of
various
gods
spirits
supernatural
beings
etc
observation
and
speculation
about
nature
were
intimately
bound
up
with
metaphysics
and
morality
in
europe
systematic
theorizing
about
nature
("physis")
began
with
the
early
greek
philosophers
and
scientists
and
continued
into
the
hellenistic
and
roman
imperial
periods
during
which
times
the
intellectual
influence
of
roman
law
increasingly
became
paramountthe
formula
"law
of
nature"
first
appears
as
"a
live
metaphor"
favored
by
latin
poets
lucretius
virgil
ovid
manilius
in
time
gaining
a
firm
theoretical
presence
in
the
prose
treatises
of
seneca
and
pliny
why
this
roman
origin?
according
to
[historian
and
classicist
daryn]
lehoux's
persuasive
narrative
the
idea
was
made
possible
by
the
pivotal
role
of
codified
law
and
forensic
argument
in
roman
life
and
culture
for
the
romans
the
place
par
excellence
where
ethics
law
nature
religion
and
politics
overlap
is
the
law
court
when
we
read
seneca's
"natural
questions"
and
watch
again
and
again
just
how
he
applies
standards
of
evidence
witness
evaluation
argument
and
proof
we
can
recognize
that
we
are
reading
one
of
the
great
roman
rhetoricians
of
the
age
thoroughly
immersed
in
forensic
method
and
not
seneca
alone
legal
models
of
scientific
judgment
turn
up
all
over
the
place
and
for
example
prove
equally
integral
to
ptolemy's
approach
to
verification
where
the
mind
is
assigned
the
role
of
magistrate
the
senses
that
of
disclosure
of
evidence
and
dialectical
reason
that
of
the
law
itself
the
precise
formulation
of
what
are
now
recognized
as
modern
and
valid
statements
of
the
laws
of
nature
dates
from
the
17th
century
in
europe
with
the
beginning
of
accurate
experimentation
and
development
of
advanced
forms
of
mathematics
during
this
period
natural
philosophers
such
as
isaac
newton
were
influenced
by
a
religious
view
which
held
that
god
had
instituted
absolute
universal
and
immutable
physical
laws
in
chapter
7
of
"the
world"
rené
descartes
described
"nature"
as
matter
itself
unchanging
as
created
by
god
thus
changes
in
parts
"are
to
be
attributed
to
nature
the
rules
according
to
which
these
changes
take
place
i
call
the
'laws
of
nature'"
the
modern
scientific
method
which
took
shape
at
this
time
(with
francis
bacon
and
galileo)
aimed
at
total
separation
of
science
from
theology
with
minimal
speculation
about
metaphysics
and
ethics
natural
law
in
the
political
sense
conceived
as
universal
(ie
divorced
from
sectarian
religion
and
accidents
of
place)
was
also
elaborated
in
this
period
(by
grotius
spinoza
and
hobbes
to
name
a
few)
some
mathematical
theorems
and
axioms
are
referred
to
as
laws
because
they
provide
logical
foundation
to
empirical
laws
examples
of
other
observed
phenomena
sometimes
described
as
laws
include
the
titius–bode
law
of
planetary
positions
zipf's
law
of
linguistics
moore's
law
of
technological
growth
many
of
these
laws
fall
within
the
scope
of
uncomfortable
science
other
laws
are
pragmatic
and
observational
such
as
the
law
of
unintended
consequences
by
analogy
principles
in
other
fields
of
study
are
sometimes
loosely
referred
to
as
"laws"
these
include
occam's
razor
as
a
principle
of
philosophy
and
the
pareto
principle
of
economics
many-body
theory
many-body
theory
(or
many-body
physics)
is
an
area
of
physics
which
provides
the
framework
for
understanding
the
collective
behavior
of
large
numbers
of
interacting
particles
often
on
the
order
of
avogadro's
number
in
general
terms
many-body
theory
deals
with
effects
that
manifest
themselves
only
in
systems
containing
large
numbers
of
constituents
while
the
underlying
physical
laws
that
govern
the
motion
of
each
individual
particle
may
(or
may
not)
be
simple
the
study
of
the
collection
of
particles
can
be
extremely
complex
in
some
cases
emergent
phenomena
may
arise
which
bear
little
resemblance
to
the
underlying
elementary
laws
many-body
theory
plays
a
central
role
in
condensed
matter
physics
parallel
force
system
in
mechanical
engineering
a
parallel
force
system
is
a
situation
in
which
two
forces
of
equal
magnitude
act
in
the
same
direction
within
the
same
plane
with
the
counter
force
in
the
middle
an
example
of
this
is
a
see
saw
the
children
are
applying
the
two
forces
at
the
ends
and
the
fulcrum
in
the
middle
gives
the
counter
force
to
maintain
the
see
saw
in
neutral
position
another
example
are
the
major
vertical
forces
on
an
airplane
in
flight
(see
image
at
right)
infinite
derivative
gravity
infinite
derivative
gravity
is
a
theory
of
gravity
which
attempts
to
remove
cosmological
and
black
hole
singularities
by
adding
extra
terms
to
the
einstein–hilbert
action
which
weaken
gravity
at
short
distances
in
1987
krasnikov
considered
an
infinite
set
of
higher
derivative
terms
acting
on
the
curvature
terms
and
showed
that
by
choosing
the
coefficients
wisely
the
propagator
would
be
ghost-free
and
exponentially
suppressed
in
the
ultraviolet
regime
tomboulis
(1997)
later
extended
this
work
by
looking
at
an
equivalent
scalar-tensor
theory
biswas
mazumdar
and
siegel
(2005)
looked
at
bouncing
frw
solutions
in
2011
biswas
gerwick
koivisto
and
mazumdar
demonstrated
that
the
most
general
infinite
derivative
action
in
4
dimensions
around
constant
curvature
backgrounds
parity
invariant
and
torsion
free
can
be
expressed
by:
where
the
formula_2
are
functions
of
the
d'alembert
operator
formula_3
and
a
mass
scale
formula_4
formula_5
is
the
ricci
scalar
formula_6
is
the
ricci
tensor
and
formula_7
is
the
weyl
tensor
in
order
to
avoid
ghosts
the
propagator
(which
is
a
combination
of
the
formula_8s)
must
be
the
exponential
of
an
entire
function
a
lower
bound
was
obtained
on
the
mass
scale
of
idg
using
experimental
data
on
the
strength
of
gravity
at
short
distances
as
well
as
by
using
data
on
inflation
and
on
the
bending
of
light
around
the
sun
the
ghy
boundary
terms
were
found
using
the
adm
3+1
spacetime
decomposition
one
can
show
that
the
entropy
for
this
theory
is
finite
in
various
contexts
the
effect
of
idg
on
black
holes
and
the
propagator
was
examined
by
modesto
modesto
further
looked
at
the
renormalisability
of
the
theory
as
well
as
showing
that
it
could
generate
"super-accelerated"
bouncing
solutions
instead
of
a
big
bang
singularity
calcagni
and
nardelli
investigated
the
effect
of
idg
on
the
diffusion
equation
idg
modifies
the
way
gravitational
waves
are
produced
and
how
they
propagate
through
space
the
amount
of
power
radiated
away
through
gravitational
waves
by
binary
systems
is
reduced
although
this
effect
is
far
smaller
than
the
current
observational
precision
this
action
can
produce
a
bouncing
cosmology
by
taking
a
flat
frw
metric
with
a
scale
factor
formula_9
or
formula_10
thus
avoiding
the
cosmological
singularity
problem
the
propagator
around
a
flat
space
background
was
obtained
in
2013
this
action
avoids
a
curvature
singularity
for
a
small
perturbation
to
a
flat
background
near
the
origin
while
recovering
the
formula_11
fall
of
the
gr
potential
at
large
distances
this
is
done
using
the
linearised
equations
of
motion
which
is
a
valid
approximation
because
if
the
perturbation
is
small
enough
and
the
mass
scale
formula_4
is
large
enough
then
the
perturbation
will
always
be
small
enough
that
quadratic
terms
can
be
neglected
it
also
avoids
the
hawking-penrose
singularity
in
this
context
it
was
shown
that
in
non-local
gravity
schwarzschild
singularities
are
stable
to
small
perturbations
further
stability
analysis
of
black
holes
was
carried
out
by
myung
and
park
the
equations
of
motion
for
this
action
are
formula_13
where
camelback
potential
a
camelback
potential
is
potential
energy
curve
that
looks
like
a
normal
distribution
with
a
distinct
dip
where
the
peak
would
be
so
named
because
it
resembles
the
humps
on
a
camel's
back
the
term
was
applied
to
a
configuration
of
a
superconducting
quantum
interference
device
in
2009
and
to
an
arrangement
of
magnets
in
2014
the
latter
system
consists
of
two
parallel
diametric
cylindrical
magnets
that
is
magnets
that
are
magnetized
perpendicular
to
their
axis
with
the
north
and
south
poles
located
on
the
curved
surface
as
opposed
to
either
end
when
a
diamagnetic
rod
(usually
graphite)
is
placed
between
the
magnets
it
will
remain
in
place
and
move
back
and
forth
in
harmonic
motion
when
disturbed
this
arrangement
also
known
as
a
"pdl
trap"
for
"parallel
dipole
line"
was
the
subject
of
the
2017
international
physics
olympiad
in
the
magnetic
system
the
camelback
potential
effect
only
occurs
when
the
length
of
the
diamagnetic
rod
is
between
two
critical
lengths
below
the
minimum
length
the
magnet
is
hypothesized
to
align
with
magnetic
field
lines
hence
not
maintaining
its
orientation
and
touching
the
magnet
the
maximum
length
is
limited
by
the
distance
between
the
peaks
of
the
camelback
humps;
thus
a
rod
longer
than
that
will
be
unstable
and
fall
out
of
the
trap
both
the
radius
and
the
length
of
the
rod
determine
the
damping
of
the
system
the
damping
is
primarily
due
to
stokes
drag
as
damping
is
non-observable
under
vacuum
possible
practical
uses
of
the
concept
include
being
a
platform
for
custom-designed
1d
potentials
a
highly
sensitive
force-distance
transducer
or
a
trap
for
semiconductor
nanowires
elitzur–vaidman
bomb
tester
the
elitzur–vaidman
bomb-tester
is
a
quantum
mechanics
thought
experiment
that
uses
interaction-free
measurements
to
verify
that
a
bomb
is
functional
without
having
to
detonate
it
it
was
conceived
in
1993
by
avshalom
elitzur
and
lev
vaidman
since
their
publication
real-world
experiments
have
confirmed
that
their
theoretical
method
works
as
predicted
the
bomb
tester
takes
advantage
of
two
characteristics
of
elementary
particles
such
as
photons
or
electrons:
nonlocality
and
wave-particle
duality
by
placing
the
particle
in
a
quantum
superposition
the
experiment
can
verify
that
the
bomb
works
without
ever
triggering
its
detonation
although
there
is
a
50%
chance
that
the
bomb
will
explode
in
the
effort
the
bomb
test
is
an
interaction-free
measurement
the
idea
of
getting
information
about
an
object
without
interacting
with
it
is
not
a
new
one
for
example
there
are
two
boxes
one
of
which
contains
something
the
other
of
which
contains
nothing
if
you
open
one
box
and
see
nothing
you
know
that
the
other
contains
something
without
ever
opening
it
this
experiment
has
its
roots
in
the
double-slit
experiment
and
other
more
complex
concepts
it
inspired
including
schrodinger's
cat
and
wheeler's
delayed
choice
experiment
the
behavior
of
elementary
particles
is
very
different
from
what
we
experience
in
our
macroscopic
world
they
can
behave
like
a
wave
or
like
a
particle
(see
wave–particle
duality)
when
they
are
in
a
wave
state
they
are
in
what
is
called
a
"superposition"
in
this
state
some
properties
of
the
particle
for
example
its
location
are
not
definite
while
in
a
superposition
any
and
all
possibilities
are
equally
real
so
if
it
can
exist
in
more
than
one
location
it
"does"
exist
in
them
all
the
particle's
wave
can
later
be
"collapsed"
by
observing
it
at
which
time
its
location
once
again
becomes
definite
information
can
then
be
gleaned
not
only
about
the
actual
state
of
the
particle
but
also
other
states
or
locations
in
which
it
existed
before
the
collapse
this
is
possible
even
though
the
particle
was
never
factually
in
those
states
or
locations
consider
a
collection
of
light-sensitive
bombs
of
which
some
are
duds
when
their
triggers
detect
any
light
even
a
single
photon
the
light
is
absorbed
and
the
bomb
explodes
the
triggers
on
the
dud
bombs
have
no
sensor
so
the
photon
can't
be
absorbed
thus
the
dud
bomb
will
not
detect
the
photon
and
will
not
detonate
is
it
possible
to
determine
which
bombs
are
functional
and
which
are
duds
without
detonating
all
of
the
live
ones?
a
superposition
in
the
bomb
tester
is
created
with
an
angled
half-silvered
mirror
which
allows
a
photon
to
either
pass
through
it
or
be
reflected
off
it
at
a
90-degree
angle
(see
figure
3)
there
is
equal
probability
it
will
do
either
the
photon
enters
a
superposition
in
which
it
does
both
the
single
particle
both
passes
through
and
is
reflected
off
the
half-silvered
mirror
from
that
moment
on
the
single
photon
exists
in
two
different
locations
along
both
the
upper
and
lower
path
the
particle
will
encounter
an
ordinary
mirror
positioned
to
redirect
the
two
routes
toward
one
another
they
then
intersect
at
a
second
half-silvered
mirror
on
the
other
side
a
pair
of
detectors
are
placed
such
that
the
photon
can
be
detected
by
either
detector
but
never
by
both
it
is
also
possible
that
it
will
not
be
detected
by
either
based
on
this
outcome
with
a
live
bomb
there
is
a
50%
chance
it
will
explode
a
25%
chance
it
will
be
identified
as
good
without
exploding
and
a
25%
chance
there
will
be
no
result
a
light-sensitive
bomb
is
placed
along
the
lower
path
if
the
bomb
is
good
when
a
photon
arrives
it
will
explode
and
both
will
be
destroyed
if
it's
a
dud
the
photon
will
pass
by
unaffected
(see
figure
4)
to
understand
how
this
experiment
works
it
is
important
to
know
that
the
bomb
is
a
kind
of
observer
and
that
this
encounter
is
a
kind
of
observation
it
can
therefore
collapse
the
photon's
superposition
in
which
the
photon
is
travelling
along
both
the
upper
and
lower
paths
when
it
reaches
the
live
bomb
or
the
detectors
however
it
can
only
have
been
on
one
or
the
other
but
like
the
radioactive
material
in
the
box
with
schrödinger's
famous
cat
upon
its
encounter
with
the
half-silvered
mirror
at
the
beginning
of
the
experiment
the
photon
paradoxically
does
and
does
not
interact
with
the
bomb
according
to
the
authors
the
bomb
both
explodes
and
doesn't
explode
this
is
only
in
the
case
of
a
live
bomb
however
in
any
event
once
observed
by
the
detectors
it
will
have
only
traveled
one
of
the
paths
when
two
waves
collide
the
process
by
which
they
affect
each
other
is
called
interference
they
can
either
strengthen
each
other
by
"constructive
interference"
or
weaken
each
other
by
"destructive
interference"
this
is
true
whether
the
wave
is
in
water
or
a
single
photon
in
a
superposition
so
even
though
there
is
only
one
photon
in
the
experiment
because
of
its
encounter
with
the
half-silvered
mirror
it
acts
like
two
when
"it"
or
"they"
are
reflected
off
the
ordinary
mirrors
it
will
interfere
with
itself
as
if
it
were
two
different
photons
"but
that's
only
true
if
the
bomb
is
a
dud"
a
live
bomb
will
absorb
the
photon
when
it
explodes
and
there
will
be
no
opportunity
for
the
photon
to
interfere
with
itself
when
it
reaches
the
second
half-silvered
mirror
if
the
photon
in
the
experiment
is
behaving
like
a
particle
(in
other
words
if
it
is
not
in
a
superposition)
then
it
has
a
fifty-fifty
chance
it
will
pass
through
or
be
reflected
and
be
detected
by
one
or
the
other
detector
"but
that's
only
possible
if
the
bomb
is
live"
if
the
bomb
"observed"
the
photon
it
detonated
and
destroyed
the
photon
on
the
lower
path
therefore
only
the
photon
that
takes
the
upper
path
will
be
detected
either
at
detector
c
or
detector
d
detector
d
is
the
key
to
confirming
that
the
bomb
is
live
the
two
detectors
and
the
second
half-silvered
mirror
are
precisely
aligned
with
one
another
detector
c
is
positioned
to
detect
the
particle
if
the
bomb
is
a
dud
and
the
particle
traveled
both
paths
in
its
superposition
and
then
constructively
interfered
with
itself
detector
d
is
positioned
to
detect
the
photon
only
in
the
event
of
destructive
interference—an
impossibility
(see
figure
6)
in
other
words
if
the
photon
is
in
a
superposition
at
the
time
it
arrives
at
the
second
half-silvered
mirror
it
will
always
arrive
at
detector
c
and
never
at
detector
d
if
the
bomb
is
live
there
is
a
50/50
chance
that
the
photon
took
upper
path
if
it
"factually"
did
so
then
it
"counter-factually"
took
the
lower
path
(see
figure
7)
that
counter-factual
event
destroyed
that
photon
and
left
only
the
photon
on
the
upper
to
arrive
at
the
second
half-silvered
mirror
at
which
point
it
will
again
have
a
50/50
chance
of
passing
through
it
or
being
reflected
off
it
and
subsequently
it
will
be
detected
at
either
of
the
two
detectors
with
the
same
probability
this
is
what
makes
it
possible
for
the
experiment
to
verify
the
bomb
is
live
without
actually
blowing
it
up
with
a
dud
the
photon
will
always
arrive
at
detector
c
with
a
live
bomb
there
can
be
three
possible
outcomes:
these
correspond
with
the
following
conditions
of
the
bomb
being
tested:
1
no
photon
was
detected:
the
bomb
exploded
and
destroyed
the
photon
before
it
could
be
detected
this
is
because
the
photon
in
fact
took
the
lower
path
and
triggered
the
bomb
destroying
itself
in
the
process
there
is
a
50%
chance
that
this
will
be
the
outcome
if
the
bomb
is
live
2
the
photon
was
detected
at
c:
this
will
always
be
the
outcome
if
a
bomb
is
a
dud
however
there
is
a
25%
chance
that
this
will
be
the
outcome
if
the
bomb
is
live
if
the
bomb
is
a
dud
this
is
because
the
photon
remained
in
its
superposition
until
it
reached
the
second
half-silvered
mirror
and
constructively
interfered
with
itself
if
the
bomb
is
live
this
is
because
the
photon
in
fact
took
the
upper
path
and
reflected
off
the
second
half-silvered
mirror
3
the
photon
was
detected
at
d:
the
bomb
is
live
but
unexploded
that's
because
the
photon
in
fact
took
the
upper
path
and
passed
through
the
second
half-silvered
mirror
something
possible
only
because
there
was
no
photon
from
the
lower
path
with
which
it
could
interfere
"this
is
the
only
way
that
a
photon
can
ever
be
detected
at
d"
if
this
is
the
outcome
the
experiment
has
successfully
verified
that
the
bomb
is
live
despite
the
fact
that
the
photon
never
"factually"
encountered
the
bomb
itself
there
is
a
25%
chance
that
this
will
be
the
outcome
if
the
bomb
is
live
if
the
result
is
2
the
experiment
is
repeated
if
the
photon
continues
to
be
observed
at
c
and
the
bomb
doesn't
explode
it
can
eventually
be
concluded
that
the
bomb
is
a
dud
with
this
process
25%
of
live
bombs
can
be
identified
without
being
detonated
50%
will
be
detonated
and
25%
remain
uncertain
by
repeating
the
process
with
the
uncertain
ones
the
ratio
of
identified
non-detonated
live
bombs
approaches
33%
of
the
initial
population
of
bombs
see
the
"experiments"
section
below
for
a
modified
experiment
that
can
identify
the
live
bombs
with
a
yield
rate
approaching
100%
the
authors
point
out
that
the
ability
to
obtain
information
about
the
bomb's
functionality
without
ever
"touching"
it
appears
to
be
a
paradox
that
they
claim
is
based
on
the
assumption
that
there
is
only
a
single
"real"
result
but
according
to
the
many-worlds
interpretation
each
possible
state
of
a
particle's
superposition
is
real
therefore
the
particle
does
actually
interact
with
the
bomb
and
it
does
explode
just
not
in
our
"world"
in
1994
anton
zeilinger
paul
kwiat
harald
weinfurter
and
thomas
herzog
actually
performed
an
equivalent
of
the
above
experiment
proving
interaction-free
measurements
are
indeed
possible
in
1996
kwiat
"et
al"
devised
a
method
using
a
sequence
of
polarising
devices
that
efficiently
increases
the
yield
rate
to
a
level
arbitrarily
close
to
one
the
key
idea
is
to
split
a
fraction
of
the
photon
beam
into
a
large
number
of
beams
of
very
small
amplitude
and
reflect
all
of
them
off
the
mirror
recombining
them
with
the
original
beam
afterwards
it
can
also
be
argued
that
this
revised
construction
is
simply
equivalent
to
a
resonant
cavity
and
the
result
looks
much
less
shocking
in
this
language
see
watanabe
and
inoue
(2000)
in
2016
carsten
robens
wolfgang
alt
clive
emary
dieter
meschede
and
andrea
alberti
demonstrated
that
the
elitzur–vaidman
bomb
testing
experiment
can
be
recast
in
a
rigorous
test
of
the
macro-realistic
worldview
based
on
the
violation
of
the
leggett–garg
inequality
using
ideal
negative
measurements
in
their
experiment
they
perform
the
“bomb
test”
with
a
single
atom
trapped
in
a
polarization-synthesized
optical
lattice
this
optical
lattice
enables
interaction-free
measurements
by
entangling
the
spin
and
position
of
atoms
diffeomorphometry
diffeomorphometry
is
the
metric
study
of
imagery
shape
and
form
in
the
discipline
of
computational
anatomy
(ca)
in
medical
imaging
the
study
of
images
in
computational
anatomy
rely
on
high-dimensional
diffeomorphism
groups
formula_1
which
generate
orbits
of
the
form
formula_2
in
which
images
formula_3
can
be
dense
scalar
magnetic
resonance
or
computed
axial
tomography
images
for
deformable
shapes
these
are
the
collection
of
manifolds
formula_4
points
curves
and
surfaces
the
diffeomorphisms
move
the
images
and
shapes
through
the
orbit
according
to
formula_5
which
are
defined
as
the
group
actions
of
computational
anatomy
the
orbit
of
shapes
and
forms
is
made
into
a
metric
space
by
inducing
a
metric
on
the
group
of
diffeomorphisms
the
study
of
metrics
on
groups
of
diffeomorphisms
and
the
study
of
metrics
between
manifolds
and
surfaces
has
been
an
area
of
significant
investigation
in
computational
anatomy
the
diffeomorphometry
metric
measures
how
close
and
far
two
shapes
or
images
are
from
each
other
informally
the
metric
is
constructed
by
defining
a
flow
of
diffemorphisms
formula_6
which
connect
the
group
elements
from
one
to
another
so
for
formula_7
then
formula_8
the
metric
between
two
coordinate
systems
or
diffeomorphisms
is
then
the
shortest
length
or
geodesic
flow
connecting
them
the
metric
on
the
space
associated
to
the
geodesics
is
given
byformula_9
the
metrics
on
the
orbits
formula_10
are
inherited
from
the
metric
induced
on
the
diffeomorphism
group
the
group
formula_1
is
thusly
made
into
a
smooth
riemannian
manifold
with
riemannian
metric
formula_12
associated
to
the
tangent
spaces
at
all
formula_13
the
riemannian
metric
satisfies
at
every
point
of
the
manifold
formula_14
there
is
an
inner
product
inducing
the
norm
on
the
tangent
space
formula_15
that
varies
smoothly
across
formula_16
oftentimes
the
familiar
euclidean
metric
is
not
directly
applicable
because
the
patterns
of
shapes
and
images
don't
form
a
vector
space
in
the
riemannian
orbit
model
of
computational
anatomy
diffeomorphisms
acting
on
the
forms
formula_17
don't
act
linearly
there
are
many
ways
to
define
metrics
and
for
the
sets
associated
to
shapes
the
hausdorff
metric
is
another
the
method
used
to
induce
the
riemannian
metric
is
to
induce
the
metric
on
the
orbit
of
shapes
by
defining
it
in
terms
of
the
metric
length
between
diffeomorphic
coordinate
system
transformations
of
the
flows
measuring
the
lengths
of
the
geodesic
flow
between
coordinates
systems
in
the
orbit
of
shapes
is
called
diffeomorphometry
the
diffeomorphisms
in
computational
anatomy
are
generated
to
satisfy
the
lagrangian
and
eulerian
specification
of
the
flow
fields
formula_18
generated
via
the
ordinary
differential
equation
with
the
eulerian
vector
fields
formula_19
in
formula_20
for
formula_21
the
inverse
for
the
flow
is
given
by
formula_22
and
the
formula_23
jacobian
matrix
for
flows
in
formula_24
given
as
formula_25
to
ensure
smooth
flows
of
diffeomorphisms
with
inverse
the
vector
fields
formula_20
must
be
at
least
1-time
continuously
differentiable
in
space
which
are
modelled
as
elements
of
the
hilbert
space
formula_27
using
the
sobolev
embedding
theorems
so
that
each
element
formula_28
has
3-square-integrable
derivatives
thusly
implies
formula_27
embeds
smoothly
in
1-time
continuously
differentiable
functions
the
diffeomorphism
group
are
flows
with
vector
fields
absolutely
integrable
in
sobolev
norm:
shapes
in
computational
anatomy
(ca)
are
studied
via
the
use
of
diffeomorphic
mapping
for
establishing
correspondences
between
anatomical
coordinate
systems
in
this
setting
3-dimensional
medical
images
are
modelled
as
diffemorphic
transformations
of
some
exemplar
termed
the
template
formula_30
resulting
in
the
observed
images
to
be
elements
of
the
random
orbit
model
of
ca
for
images
these
are
defined
as
formula_31
with
for
charts
representing
sub-manifolds
denoted
as
formula_32
the
orbit
of
shapes
and
forms
in
computational
anatomy
are
generated
by
the
group
action
formula_33
formula_34
these
are
made
into
a
riemannian
orbits
by
introducing
a
metric
associated
to
each
point
and
associated
tangent
space
for
this
a
metric
is
defined
on
the
group
which
induces
the
metric
on
the
orbit
take
as
the
metric
for
computational
anatomy
at
each
element
of
the
tangent
space
formula_35
in
the
group
of
diffeomorphisms
with
the
vector
fields
modelled
to
be
in
a
hilbert
space
with
the
norm
in
the
hilbert
space
formula_27
we
model
formula_38
as
a
reproducing
kernel
hilbert
space
(rkhs)
defined
by
a
1-1
differential
operator
formula_39
where
formula_40
is
the
dual-space
in
general
formula_41
is
a
generalized
function
or
distribution
the
linear
form
associated
to
the
inner-product
and
norm
for
generalized
functions
are
interpreted
by
integration
by
parts
according
to
for
formula_42
when
formula_44
a
vector
density
formula_45
the
differential
operator
is
selected
so
that
the
green's
kernel
associated
to
the
inverse
is
sufficiently
smooth
so
that
the
vector
fields
support
1-continuous
derivative
the
sobolev
embedding
theorem
arguments
were
made
in
demonstrating
that
1-continuous
derivative
is
required
for
smooth
flows
the
green's
operator
generated
from
the
green's
function(scalar
case)
associated
to
the
differential
operator
smooths
for
proper
choice
of
formula_46
then
formula_47
is
an
rkhs
with
the
operator
formula_48
the
green's
kernels
associated
to
the
differential
operator
smooths
since
for
controlling
enough
derivatives
in
the
square-integral
sense
the
kernel
formula_49
is
continuously
differentiable
in
both
variables
implying
(ij)=\inf_{\phi
\in
\operatorname{diff}_v:
\phi
\cdot
i
=
j
}
d_{\operatorname{diff}_v}(id\phi)
\
;
the
distance
on
shapes
and
forms
formula_51
for
calculating
the
metric
the
geodesics
are
a
dynamical
system
the
flow
of
coordinates
formula_52
and
the
control
the
vector
field
formula_53
related
via
formula_54
the
hamiltonian
view
the
pontryagin
maximum
principle
gives
the
hamiltonian
formula_59
the
optimizing
vector
field
formula_60
with
dynamics
formula_61
along
the
geodesic
the
hamiltonian
is
constant:
formula_62
the
metric
distance
between
coordinate
systems
connected
via
the
geodesic
determined
by
the
induced
distance
between
identity
and
group
element:
for
landmarks
formula_64
the
hamiltonian
momentum
with
hamiltonian
dynamics
taking
the
form
with
the
metric
between
landmarks
formula_68
the
dynamics
associated
to
these
geodesics
is
shown
in
the
accompanying
figure
for
surfaces
the
hamiltonian
momentum
is
defined
across
the
surface
has
hamiltonian
and
dynamics
for
volumes
the
hamiltonian
droplet
vaporization
the
vaporizing
droplet
(droplet
vaporization)
problem
is
a
challenging
issue
in
fluid
dynamics
it
is
part
of
many
engineering
situations
involving
the
transport
and
computation
of
sprays:
fuel
injection
spray
painting
aerosol
spray
flashing
releases…
in
most
of
these
engineering
situations
there
is
a
relative
motion
between
the
droplet
and
the
surrounding
gas
the
gas
flow
over
the
droplet
has
many
features
of
the
gas
flow
over
a
rigid
sphere:
pressure
gradient
viscous
boundary
layer
wake
in
addition
to
these
common
flow
features
one
can
also
mention
the
internal
liquid
circulation
phenomenon
driven
by
surface-shear
forces
and
the
boundary
layer
blowing
effect
one
of
the
key
parameter
which
characterizes
the
gas
flow
over
the
droplet
is
the
droplet
reynolds
number
based
on
the
relative
velocity
droplet
diameter
and
gas
phase
properties
the
features
of
the
gas
flow
have
a
critical
impact
on
the
exchanges
of
mass
momentum
and
energy
between
the
gas
and
the
liquid
phases
and
thus
they
have
to
be
properly
accounted
for
in
any
vaporizing
droplet
model
as
a
first
step
it
is
worth
investigating
the
simple
case
where
there
is
no
relative
motion
between
the
droplet
and
the
surrounding
gas
it
will
provide
some
useful
insights
on
the
physics
involved
in
the
vaporizing
droplet
problem
in
a
second
step
models
used
in
engineering
situations
where
a
relative
motion
between
the
droplet
and
the
surrounding
exists
are
presented
in
this
section
we
assume
that
there
is
no
relative
motion
between
the
droplet
and
the
gas
formula_1
and
that
the
temperature
inside
the
droplet
is
uniform
(models
that
account
for
the
non-uniformity
of
the
droplet
temperature
are
presented
in
the
next
section)
the
time
evolution
of
the
droplet
radius
formula_2
and
droplet
temperature
formula_3
can
be
computed
by
solving
the
following
set
of
ordinary
differential
equations::
where:
the
heat
flux
entering
the
droplet
can
be
expressed
as:
where:
analytical
expressions
for
the
droplet
vaporization
rate
formula_7
and
for
the
heat
flux
formula_11
are
now
derived
a
single
pure
component
droplet
is
considered
and
the
gas
phase
is
assumed
to
behave
as
an
ideal
gas
a
spherically
symmetric
field
exists
for
the
gas
field
surrounding
the
droplet
analytical
expressions
for
formula_7
and
formula_11
are
found
by
considering
heat
and
mass
transfer
processes
in
the
gas
film
surrounding
the
droplet
the
droplet
vaporizes
and
creates
a
radial
flow
field
in
the
gas
film
the
vapor
from
the
droplet
convects
and
diffuses
away
from
the
droplet
surface
heat
conducts
radially
against
the
convection
toward
the
droplet
interface
this
process
is
called
stefan
convection
or
stefan
flow
the
gas
phase
conservation
equations
for
mass
fuel-vapor
mass
fraction
and
energy
are
written
in
a
spherical
coordinate
system:
where:
it
is
assumed
that
the
gas
phase
heat
and
mass
transfer
processes
are
quasi-steady
and
that
the
thermo-physical
properties
might
be
considered
as
constant
the
assumption
of
quasi-steadiness
of
the
gas
phase
finds
its
limitation
in
situations
in
which
the
gas
film
surrounding
the
droplet
is
in
a
near-critical
state
or
in
a
situation
in
which
the
gas
field
is
submitted
to
an
acoustic
field
the
assumption
of
constant
thermo-physical
properties
is
found
to
be
satisfying
provided
that
the
properties
are
evaluated
at
some
reference
conditions
where:
the
"1/3"
averaging
rule
formula_37
is
often
recommended
in
the
literature
the
conservation
equation
of
mass
simplifies
to:
combining
the
conservation
equations
for
mass
and
fuel
vapor
mass
fraction
the
following
differential
equation
for
the
fuel
vapor
mass
fraction
formula_39
is
obtained:
integrating
this
equation
between
formula_21
and
the
ambient
gas
phase
region
formula_42
and
applying
the
boundary
condition
at
formula_43
gives
the
expression
for
the
droplet
vaporization
rate:
and
where:
phase
equilibrium
is
assumed
at
the
droplet
surface
and
the
mole
fraction
of
fuel
vapor
at
the
droplet
surface
is
obtained
via
the
use
of
the
clapeyron's
equation
an
analytical
expression
for
the
heat
flux
formula_11
is
now
derived
after
some
manipulations
the
conservation
equation
of
energy
writes:
where:
applying
the
boundary
condition
at
the
droplet
surface
and
using
the
relation
formula_50
we
have:
where:
integrating
this
equation
from
formula_21
to
the
ambient
gas
phase
conditions
(formula_54)
gives
the
variation
of
the
gas
film
temperature
(formula_26)
as
a
function
of
the
radial
distance:
the
above
equation
provides
a
second
expression
for
the
droplet
vaporization
rate:
and
where:
finally
combining
the
new
expression
for
the
droplet
vaporization
rate
and
the
expression
for
the
variation
of
the
gas
film
temperature
the
following
equation
is
obtained
for
formula_11:
two
different
expressions
for
the
droplet
vaporization
rate
formula_7
have
been
derived
hence
a
relation
exists
between
the
spalding
mass
transfer
number
and
the
spalding
heat
transfer
number
and
writes:
where:
the
droplet
vaporization
rate
can
be
expressed
as
a
function
of
the
sherwood
number
the
sherwood
number
describes
the
non-dimensional
mass
transfer
rate
to
the
droplet
and
is
defined
as:
thus
the
expression
for
the
droplet
vaporization
rate
can
be
re-written
as:
similarly
the
conductive
heat
transfer
from
the
gas
to
the
droplet
can
be
expressed
as
a
function
of
the
nusselt
number
the
nusselt
number
describes
a
non-dimensional
heat
transfer
rate
to
the
droplet
and
is
defined
as:
and
then:
in
the
limit
where
formula_70
we
have
formula_71
which
corresponds
to
the
classical
heated
sphere
result
the
relative
motion
between
a
droplet
and
the
gas
results
in
an
increase
of
the
heat
and
mass
transfer
rates
in
the
gas
film
surrounding
the
droplet
a
convective
boundary
layer
and
a
wake
can
surround
the
droplet
furthermore
the
shear
force
on
the
liquid
surface
causes
an
internal
circulation
that
enhances
the
heating
of
the
liquid
as
a
consequence
the
vaporization
rate
increases
with
the
droplet
reynolds
number
many
different
models
exist
for
the
single
convective
droplet
vaporization
case
vaporizing
droplet
models
can
be
seen
to
belong
to
six
different
classes:
the
main
difference
between
all
these
models
is
the
treatment
of
the
heating
of
the
liquid
phase
which
is
usually
the
rate
controlling
phenomenon
in
droplet
vaporization
the
first
three
models
do
not
consider
internal
liquid
circulation
the
effective
conductivity
model
(4)
and
the
vortex
model
of
droplet
heating
(5)
account
for
internal
circulation
and
internal
convective
heating
the
direct
resolution
of
the
navier-stokes
equations
provide
in
principle
exact
solutions
both
for
the
gas
phase
and
the
liquid
phase
model
(1)
is
a
simplification
of
model
(2)
which
is
in
turn
a
simplification
of
model
(3)
the
spherically
symmetric
transient
droplet
heating
model
(3)
solves
the
equation
for
heat
diffusion
through
the
liquid
phase
a
droplet
heating
time
τ
can
be
defined
as
the
time
required
for
a
thermal
diffusion
wave
to
penetrate
from
the
droplet
surface
to
its
center
the
droplet
heating
time
is
compared
to
the
droplet
lifetime
τ
if
the
droplet
heating
time
is
short
compared
to
the
droplet
lifetime
we
can
assume
that
the
temperature
field
inside
the
droplet
is
uniform
and
model
(2)
is
obtained
in
the
infinite
liquid
conductivity
model
(2)
the
temperature
of
the
droplet
is
uniform
but
varies
with
time
it
is
possible
to
go
one
step
further
and
find
the
conditions
for
which
we
can
neglect
the
temporal
variation
of
the
droplet
temperature
the
liquid
temperature
varies
in
time
until
the
wet-bulb
temperature
is
reached
if
the
wet-bulb
temperature
is
reached
in
a
time
of
the
same
order
of
magnitude
than
the
droplet
heating
time
then
the
liquid
temperature
can
be
considered
to
be
constant
with
regards
to
time
model
(1)
the
d2-law
is
obtained
the
infinite
liquid
conductivity
model
is
widely
used
in
industrial
spray
calculations:
for
its
balance
between
computational
costs
and
accuracy
to
account
for
the
convective
effects
which
enhanced
the
heat
and
mass
transfer
rates
around
the
droplet
a
correction
is
applied
to
the
spherically
symmetric
expressions
of
the
sherwood
and
nusselt
numbers
abramzon
and
sirignano
suggest
the
following
formulation
for
the
modified
sherwood
and
nusselt
numbers:
where:
and
the
well
known
frossling
correlations
(or
ranz-marshall
correlations
)
can
be
used
to
express
"nu"
and
"sh":
where
the
expressions
above
show
that
the
heat
and
mass
transfer
rates
increase
with
increasing
reynolds
number
self-propulsion
self-propulsion
is
the
autonomous
displacement
of
nano-
micro-
and
macroscopic
natural
and
artificial
objects
containing
their
own
means
of
motion
self-propulsion
is
driven
mainly
by
interfacial
phenomena
various
mechanisms
of
self-propelling
have
been
introduced
and
investigated
which
exploited
phoretic
effects
gradient
surfaces
breaking
the
wetting
symmetry
of
a
droplet
on
a
surface
the
leidenfrost
effect
the
self-generated
hydrodynamic
and
chemical
fields
originating
from
the
geometrical
confinements
and
soluto-
and
thermo-capillary
marangoni
flows
self-propelled
system
demonstrate
a
potential
as
micro-fluidics
devices
and
micro-mixers
self-propelled
liquid
marbles
have
been
demonstrated
microfluidic
cell
culture
microfluidic
cell
culture
integrates
knowledge
from
biology
biochemistry
engineering
and
physics
to
develop
devices
and
techniques
for
culturing
maintaining
analyzing
and
experimenting
with
cells
at
the
microscale
it
merges
microfluidics
a
set
of
technologies
used
for
the
manipulation
of
small
fluid
volumes
(μl
nl
pl)
within
artificially
fabricated
microsystems
and
cell
culture
which
involves
the
maintenance
and
growth
of
cells
in
a
controlled
laboratory
environment
microfluidics
has
been
used
for
cell
biology
studies
as
the
dimensions
of
the
microfluidic
channels
are
well
suited
for
the
physical
scale
of
cells
for
example
eukaryotic
cells
have
linear
dimensions
between
10-100
μm
which
falls
within
the
range
of
microfluidic
dimensions
a
key
component
of
microfluidic
cell
culture
is
being
able
to
mimic
the
cell
microenvironment
which
includes
soluble
factors
that
regulate
cell
structure
function
behavior
and
growth
another
important
component
for
the
devices
is
the
ability
to
produce
stable
gradients
that
are
present
"in
vivo"
as
these
gradients
play
a
significant
role
in
understanding
chemotactic
durotactic
and
haptotactic
effects
on
cells
some
considerations
for
microfluidic
devices
relating
to
cell
culture
include:
fabrication
material
is
crucial
as
not
all
polymers
are
biocompatible
with
some
materials
such
as
pdms
causing
undesirable
adsorption
or
absorption
of
small
molecules
additionally
uncured
pdms
oligomers
can
leach
into
the
cell
culture
media
which
can
harm
the
microenvironment
as
an
alternative
to
commonly
used
pdms
there
have
been
advances
in
the
use
of
thermoplastics
(eg
polystyrene)
as
a
replacement
material
spatial
organization
of
cells
in
microscale
devices
largely
depends
on
the
culture
region
geometry
for
cells
to
perform
functions
"in
vivo"
for
example
long
narrow
channels
may
be
desired
to
culture
neurons
the
perfusion
system
chosen
might
also
affect
the
geometry
chosen
for
example
in
a
system
that
incorporates
syringe
pumps
channels
for
perfusion
inlet
perfusion
outlet
waste
and
cell
loading
would
need
to
be
added
for
the
cell
culture
maintenance
perfusion
in
microfluidic
cell
culture
is
important
to
enable
long
culture
periods
on-chip
and
cell
differentiation
other
critical
aspects
for
controlling
the
microenvironment
include:
cell
seeding
density
reduction
of
air
bubbles
as
they
can
rupture
cell
membranes
evaporation
of
media
due
to
an
insufficiently
humid
environment
and
cell
culture
maintenance
(ie
regular
timely
media
changes)
some
of
the
major
advantages
of
microfluidic
cell
culture
include
reduced
sample
volumes
(especially
important
when
using
primary
cells
which
are
often
limited)
and
the
flexibility
to
customize
and
study
multiple
microenvironments
within
the
same
device
a
reduced
cell
population
can
also
be
used
in
a
microscale
system
(eg
a
few
hundred
cells)
in
comparison
to
macroscale
culture
systems
(which
often
require
10
–
10
cells);
this
can
make
studying
certain
cell-cell
interactions
more
accessible
these
reduced
cell
numbers
make
studying
non-dividing
or
slow
dividing
cells
(eg
stem
cells)
easier
than
traditional
culture
methods
(eg
flasks
petri
dishes
or
well
plates)
due
to
the
smaller
sample
volumes
given
the
small
dimensions
in
microfluidics
laminar
flow
can
be
achieved
allowing
manipulations
with
the
culture
system
to
be
done
easily
without
affecting
other
culture
chambers
laminar
flow
is
also
useful
as
is
it
mimics
"in
vivo"
fluid
dynamics
more
accurately
often
making
microscale
culture
more
relevant
than
traditional
culture
methods
two-dimensional
(2d)
cell
culture
is
cell
culture
that
takes
place
on
a
flat
surface
eg
the
bottom
of
a
well-plate
and
is
known
as
the
conventional
method
while
these
platforms
are
useful
for
growing
and
passaging
cells
to
be
used
in
subsequent
experiments
they
are
not
ideal
environments
to
monitor
cell
responses
to
stimuli
as
cells
cannot
freely
move
or
perform
functions
as
observed
"in
vivo"
that
are
dependent
on
cell-extracellular
matrix
material
interactions
three-dimensional
(3d)
cell
culture
is
cell
culture
that
takes
place
in
a
biologically
relevant
matrix
usually
this
involves
cells
being
embedded
in
a
hydrogel
containing
extracellular
molecules
(eg
collagen)
by
adding
an
additional
dimension
more
advanced
cell
architectures
can
be
achieved
and
cell
behavior
is
more
representative
of
"in
vivo"
dynamics;
cells
can
engage
in
enhanced
communication
with
neighboring
cells
and
cell-extracellular
matrix
interactions
can
be
modeled
these
simplified
3d
cell
culture
models
can
be
combined
in
a
manner
that
recapitulates
tissue-
and
organ-level
functions
in
devices
known
as
organ-on-a-chip
in
these
devices
chambers
or
collagen
layers
containing
different
cell
types
can
interact
with
one
another
for
multiple
days
while
various
channels
deliver
nutrients
to
the
cells
an
advantage
of
these
devices
is
that
tissue
function
can
be
characterized
and
observed
under
controlled
conditions
(eg
effect
of
shear
stress
on
cells
effect
of
cyclic
strain
or
other
forces)
to
better
understand
the
overall
function
of
the
organ
while
these
3d
models
ofter
better
model
organ
function
on
a
cellular
level
compared
with
2d
models
there
are
still
challenges
some
of
the
challenges
include:
imaging
of
the
cells
control
of
gradients
in
static
models
(ie
without
a
perfusion
system)
and
difficulty
recreating
vasculature
despite
these
challenges
3d
models
are
still
used
as
tools
for
studying
and
testing
drug
responses
in
pharmacological
studies
in
recent
years
there
are
microfluidic
devices
reproducing
the
complex
"in
vivo"
microvascular
network
the
device
is
able
to
create
a
physiologically
realistic
3d
environment
which
is
desirable
as
a
tool
for
drug
screening
drug
delivery
cell-cell
interactions
tumor
metastasis
etc
outline
of
physics
the
following
outline
is
provided
as
an
overview
of
and
topical
guide
to
physics:
physics
–
natural
science
that
involves
the
study
of
matter
and
its
motion
through
spacetime
along
with
related
concepts
such
as
energy
and
force
more
broadly
it
is
the
general
analysis
of
nature
conducted
in
order
to
understand
how
the
universe
behaves
physics
can
be
described
as
all
of
the
following:
history
of
physics
–
history
of
the
physical
science
that
studies
matter
and
its
motion
through
space-time
and
related
concepts
such
as
energy
and
force
physics
–
branch
of
science
that
studies
matter
and
its
motion
through
space
and
time
along
with
related
concepts
such
as
energy
and
force
physics
is
one
of
the
"fundamental
sciences"
because
the
other
natural
sciences
(like
biology
geology
etc)
deal
with
systems
that
seem
to
obey
the
laws
of
physics
according
to
physics
the
physical
laws
of
matter
energy
and
the
fundamental
forces
of
nature
govern
the
interactions
between
particles
and
physical
entities
(such
as
planets
molecules
atoms
or
the
subatomic
particles)
some
of
the
basic
pursuits
of
physics
which
include
some
of
the
most
prominent
developments
in
modern
science
in
the
last
millennium
include:
gravity
light
physical
system
physical
observation
physical
quantity
physical
state
physical
unit
physical
theory
physical
experiment
theoretical
concepts
mass–energy
equivalence
particle
physical
field
physical
interaction
physical
law
fundamental
force
physical
constant
wave
physics
this
is
a
list
of
the
primary
theories
in
physics
major
subtopics
and
concepts
index
of
physics
articles
nonlinear
frictiophoresis
nonlinear
frictiophoresis
is
the
unidirectional
drift
of
a
particle
in
a
medium
caused
by
periodic
driving
force
with
zero
mean
the
effect
is
possible
due
to
nonlinear
dependence
of
the
friction-drag
force
on
the
particle's
velocity
it
was
discovered
theoretically
and
is
mainly
known
as
nonlinear
electrofrictiophoresis
at
first
glance
a
periodic
driving
force
with
zero
mean
is
able
to
entrain
a
particle
into
an
oscillating
movement
without
unidirectional
drift
because
integral
momentum
provided
to
the
particle
by
the
force
is
zero
the
possibility
of
unidirectional
drift
can
be
recognized
if
one
takes
into
account
that
the
particle
itself
loses
momentum
through
transferring
it
further
to
the
medium
it
moves
in/at
if
the
friction
is
nonlinear
then
it
may
so
happen
that
the
momentum
loss
during
movement
in
one
direction
does
not
equal
to
that
in
the
opposite
direction
and
this
causes
unidirectional
drift
for
this
to
happen
the
driving
force
time-dependence
must
be
more
complicated
than
it
is
in
a
single
sinusoidal
harmonic
the
simplest
case
of
friction-velocity
dependence
law
is
the
stokes's
one:
where
formula_2
is
the
friction/drag
force
applied
to
a
particle
moving
with
velocity
formula_3
in
a
medium
the
friction-velocity
law
(1)
is
observed
for
a
slowly
moving
spherical
particle
in
a
newtonian
fluid
it
is
linear
see
fig
1
and
is
not
suitable
for
nonlinear
frictiophoresis
to
take
place
the
characteristic
property
of
the
law
(1)
is
that
any
even
a
very
small
driving
force
is
able
to
get
particle
moving
this
is
not
the
case
for
such
media
as
bingham
plastic
for
those
media
it
is
necessary
to
apply
some
threshold
force
formula_4
to
get
the
particle
moving
this
kind
of
friction-velocity
(dry
friction)
law
has
a
jump
discontinuity
at
formula_5:
it
is
nonlinear
see
fig
2
and
is
used
in
this
example
let
formula_7
denote
the
period
of
driving
force
chose
a
time
value
formula_8
such
that
formula_9
and
two
force
values
formula_10
formula_11
such
that
the
following
relations
are
satisfied:
the
periodic
driving
force
formula_14
used
in
this
example
is
as
follows:
it
is
clear
that
due
to
(3)
formula_14
has
zero
mean:
see
also
fig
3
for
the
sake
of
simplicity
we
consider
here
the
physical
situation
when
inertia
may
be
neglected
the
latter
can
be
achieved
if
particle's
mass
is
small
velocity
is
low
and
friction
is
high
this
conditions
have
to
ensure
that
formula_18
where
formula_19
is
the
relaxation
time
in
this
situation
the
particle
driven
with
force
(4)
immediately
starts
moving
with
constant
velocity
formula_20
during
interval
formula_21
and
will
immediately
stop
moving
during
interval
formula_22
see
fig
4
this
results
in
the
positive
mean
velocity
of
unidirectional
drift:
analysis
of
possibility
to
get
a
nonzero
drift
by
periodic
force
with
zero
integral
has
been
made
in
the
dimensionless
equation
of
motion
for
a
particle
driven
by
periodic
force
formula_14
formula_25
formula_26
is
as
follows:
where
the
friction/drag
force
formula_28
satisfies
the
following:
it
is
proven
in
that
any
solution
to
(5)
settles
down
onto
periodic
regime
formula_30
formula_31
which
has
nonzero
mean:
almost
certainly
provided
formula_14
is
not
antiperiodic
for
formula_34
two
cases
of
formula_14
have
been
considered
explicitly:
1
saw-shaped
driving
force
see
fig
5:
in
this
case
found
in
first
order
in
formula_37
approximation
to
formula_30
formula_39
has
the
following
mean
value:
this
estimate
is
made
expecting
formula_41
2
two
harmonics
driving
force
in
this
case
the
first
order
in
formula_37
approximation
has
the
following
mean
value:
this
value
is
maximized
in
formula_45
formula_46
keeping
formula_47
constant
interesting
that
the
drift
value
depends
on
formula_45
and
changes
its
direction
twice
as
formula_45
spans
over
the
interval
formula_50
another
type
of
analysis
based
on
symmetry
breaking
suggests
as
well
that
a
zero
mean
driving
force
is
able
to
generate
a
directed
drift
in
applications
the
nature
of
force
formula_14
in
(5)
is
usually
electric
similar
to
forces
acting
during
standard
electrophoresis
the
only
differences
are
that
the
force
is
periodic
and
without
constant
component
for
the
effect
to
show
up
the
dependence
of
friction/drag
force
on
velocity
must
be
nonlinear
this
is
the
case
for
numerous
substances
known
as
non-newtonian
fluids
among
these
are
gels
and
dilatant
fluids
pseudoplastic
fluids
liquid
crystals
dedicated
experiments
have
determined
formula_52
for
a
standard
dna
ladder
up
to
1500
bp
long
in
15%
agarose
gel
the
dependence
found
see
fig
6
supports
the
possibility
of
nonlinear
frictiophoresis
in
such
a
system
based
on
data
in
fig
6
an
optimal
time
course
for
driving
electric
field
with
zero
mean
formula_53
has
been
found
in
which
ensures
maximal
drift
for
1500
bp
long
fragment
see
fig
7
the
effect
of
unidirectional
drift
caused
by
periodic
force
with
zero
integral
value
has
a
peculiar
dependence
on
the
time
course
of
the
force
applied
see
the
previous
section
for
examples
this
offers
a
new
dimension
to
a
set
of
separation
problems
in
the
dna
fragments
separation
zero
mean
periodic
electric
field
is
used
in
zero-integrated-field
electrophoresis
(zife)
where
the
field
time
dependence
similar
to
that
shown
in
fig
3
is
used
this
allows
to
separate
long
fragments
in
agarose
gel
nonseparable
by
standard
constant
field
electrophoresis
the
long
dna
geometry
and
its
manner
of
movement
in
a
gel
known
as
reptation
do
not
allow
to
apply
directly
the
consideration
based
on
eq
(5)
above
it
was
observed
that
under
certain
physical
conditions
the
mechanism
described
in
mathematical
analysis
section
above
can
be
used
for
separation
with
respect
to
specific
mass
like
particles
made
of
isotopes
of
the
same
material
the
idea
of
organizing
directed
drift
with
zero
mean
periodic
drive
have
obtained
further
development
for
other
configurations
and
other
physical
mechanism
of
nonlinearity
an
electric
dipole
rotating
freely
around
formula_54-axis
in
a
medium
with
nonlinear
friction
can
be
manipulated
by
applying
electromagnetic
wave
polarized
circularly
along
formula_55
and
composed
of
two
harmonics
the
equation
of
motion
for
this
system
is
as
follows:
where
formula_57
is
the
torque
acting
on
the
dipole
due
to
circular
wave:
where
formula_59
is
the
dipole
moment
component
orthogonal
to
formula_54-axis
and
formula_61
defines
the
dipole
direction
in
the
formula_62
plane
by
choosing
proper
phase
shift
formula_45
in
(6)
it
is
possible
to
orient
the
dipole
in
any
desired
direction
formula_64
the
direction
formula_64
is
attained
due
to
angular
directed
drift
which
becomes
zero
when
formula_66
a
small
detuning
between
the
first
and
second
harmonic
in
(6)
results
in
continuous
rotational
drift
if
a
particle
undergoes
a
directed
drift
while
moving
freely
in
accordance
with
eq
(5)
then
it
drifts
similarly
if
a
shallow
enough
potential
field
formula_67
is
imposed
equation
of
motion
in
that
case
is:
where
formula_69
is
the
force
due
to
potential
field
the
drift
continues
until
a
steep
enough
region
in
the
course
of
formula_67
is
met
which
is
able
to
stop
the
drift
this
kind
of
behavior
as
rigorous
mathematical
analysis
shows
results
in
modification
of
formula_67
by
adding
a
linear
in
formula_72
term
this
may
change
the
formula_67
qualitatively
by
eg
changing
the
number
of
equilibrium
points
see
fig
8
the
effect
may
be
essential
during
high
frequency
electric
field
acting
on
biopolymers
for
electrophoresis
of
colloid
particles
under
a
small
strength
electric
field
the
force
formula_14
in
the
right-hand
side
of
eq
(5)
is
linearly
proportional
to
the
strength
formula_53
of
the
electric
field
applied
for
a
high
strength
the
linearity
is
broken
due
to
nonlinear
polarization
as
a
result
the
force
may
depend
nonlinearly
on
the
applied
field:
in
the
last
expression
even
if
the
applied
field
formula_53
has
zero
mean
the
applied
force
formula_14
may
happen
to
have
a
constant
component
that
can
cause
a
directed
drift
as
above
for
this
to
happen
formula_53
must
have
more
than
a
single
sinusoidal
harmonic
this
same
effect
for
a
liquid
in
a
tube
may
serve
in
electroosmotic
pump
driven
with
zero
mean
electric
field
landolt–börnstein
landolt–börnstein
is
the
largest
collection
of
critically
evaluated
property
data
in
materials
science
and
the
closely
related
fields
of
chemistry
physics
and
engineering
published
by
springer
nature
on
july
28
1882
dr
hans
heinrich
landolt
and
dr
richard
börnstein
both
professors
at
the
"landwirtschaftliche
hochschule"
(agricultural
college)
at
berlin
signed
a
contract
with
the
publisher
ferdinand
springer
on
the
publication
of
a
collection
of
tables
with
physical-chemical
data
the
title
of
this
book
"physikalisch-chemische
tabellen"
(physical-chemical
tables)
published
in
1883
was
soon
forgotten
owing
to
its
success
the
data
collection
has
been
known
for
more
than
a
hundred
years
by
each
scientist
only
as
"the
landolt-börnstein"
1250
copies
of
the
1st
edition
were
printed
and
sold
in
1894
the
2nd
edition
was
published
in
1905
the
3rd
edition
in
1912
the
4th
edition
and
finally
in
1923
the
5th
edition
supplementary
volumes
of
the
latter
were
printed
until
as
late
as
1936
new
editions
saw
changes
in
large
expansion
of
volumes
number
of
authors
updated
structure
additional
tables
and
coverage
of
new
areas
of
physics
and
chemistry
the
5th
edition
was
eventually
published
in
1923
consisting
of
two
volumes
and
comprising
a
total
of
1695
pages
sixty
three
authors
had
contributed
to
it
the
growth
that
had
already
been
noticed
in
previous
editions
continued
it
was
clear
that
"another
edition
in
approximately
10
years"
was
no
solution
a
complete
conceptual
change
of
the
landolt-börnstein
had
thus
become
necessary
for
the
meantime
supplementary
volumes
in
two-year
intervals
should
be
provided
to
fill
in
the
blanks
and
add
the
latest
data
the
first
supplementary
volume
of
the
5th
edition
was
published
in
1927
the
second
in
1931
and
the
third
in
1935/36
the
latter
consisted
of
three
sub-volumes
with
a
total
of
3039
pages
and
contributions
from
82
authors
the
6th
edition
(1950)
was
published
in
line
with
the
revised
general
frame
the
basic
idea
was
to
have
four
volumes
instead
of
one
each
of
which
was
to
cover
different
fields
of
the
landolt-börnstein
under
different
editors
each
volume
was
given
a
detailed
table
of
contents
two
major
restrictions
were
also
imposed
the
author
of
a
contribution
was
asked
to
choose
a
"bestwert"
(optimum
value)
from
the
mass
of
statements
of
an
experimental
value
in
the
publications
of
different
authors
or
derive
a
"wahrscheinlichster
wert”
(most
possible
value)
the
other
change
of
importance
was
that
not
only
diagrams
became
as
important
as
tables
but
that
text
also
became
necessary
to
explain
the
presented
data
the
new
concept
of
the
6th
edition
–
splitting
of
the
edition
in
four
volumes
with
a
yet
unknown
number
of
sub-volumes
–
was
a
slight
progress
but
eventually
it
has
not
proven
to
be
of
much
use
instead
of
publishing
new
editions
of
the
data
collection
one
after
the
other
the
"new
series"
concept
was
intended
to
publish
independent
volumes
that
were
each
to
cover
a
certain
closed
theme
it
was
flexible
enough
to
follow
the
changes
in
the
scientific
interest
and
abandon
the
previous
structuring
of
physics
and
chemistry
in
favor
of
new
themes
thus
new
subjects
such
as
solid-state
physics
could
easily
be
comprised
in
groups
of
volumes
this
concept
has
been
adhered
to
up
to
now
the
new
series
represents
over
520
books
published
between
1961
and
2018
and
includes
more
than
220000
pages
covering
mechanical
optical
acoustical
thermal
spectroscopic
electrical
and
magnetic
properties
among
others
the
new
series
offers
critically
evaluated
data
by
over
1000
expert
authors
and
editors
in
materials
science
landolt–börnstein
books
have
gone
through
various
digitization
initiatives
from
cd-rom
to
ftp
and
pdf
formats
landolt–börnstein
books
content
is
now
available
on
springermaterials
a
comprehensive
database
for
identifying
material
properties
which
covers
data
from
materials
science
physics
physical
and
inorganic
chemistry
engineering
and
other
related
fields
springermaterials
offers
advanced
materials
science-specific
search
functionality
that
allows
easy
retrieval
of
accurate
results
recently
introduced
springermaterials
interactive
is
a
set
of
advanced
functionalities
for
visualizing
and
analyzing
materials
property
data
examples
of
these
functionalities
include
interactive
graphs
dynamic
data
tables
and
side-by-
side
comparisons
of
materials/properties
gravitationally-interacting
massive
particles
gravitationally-interacting
massive
particles
(gimps)
are
a
set
of
particles
theorised
to
explain
the
dark
matter
in
our
universe
as
opposed
to
an
alternative
theory
based
on
weakly-interacting
massive
particles
(wimps)
dark
matter
was
postulated
by
f
zwicky
in
1933
who
noticed
the
failure
of
the
velocity
curves
of
stars
to
decrease
when
plotted
as
functions
of
their
distance
from
the
center
of
galaxies
since
einstein's
work
our
universe
is
described
by
four-dimensional
spacetime
whose
metric
is
calculable
by
the
einstein
field
equations:
here
is
the
ricci
curvature
tensor
the
scalar
curvature
the
metric
tensor
newton's
gravitational
constant
the
speed
of
light
in
vacuum
and
the
stress–energy
tensor
the
constant
is
the
so-called
cosmological
constant
while
wimps
would
be
elementary
particles
described
by
the
standard
model
that
can
in
principle
be
studied
by
experimentalists
in
laboratories
such
as
cern
the
proposed
particles
called
gimps
would
follow
the
vacuum
solutions
of
einstein's
equation
they
are
just
singular
structures
of
spacetime
in
a
geometry
whose
average
forms
the
dark
energy
that
einstein
expressed
in
his
cosmological
constant
the
identification
of
"dark
matter"
with
gimps
proposed
makes
dark
matter
a
form
of
dark
energy
filled
with
singularities
ie
an
entangled
dark
energy
this
would
roughly
confirm
einstein's
hope
in
1919
that
all
particles
in
the
universe
would
follow
the
traceless
version
of
his
equation
if
we
identify
all
matter
as
the
sum
of
dark
energy
plus
dark
matter
in
the
form
of
gimps
his
expectation
would
turn
out
to
have
been
"almost"
right
matter
would
play
a
similar
role
as
the
point
charges
in
the
homogeneous
maxwell
equation
formula_1
in
which
delta
functions
are
ignored
the
sum
of
dark
matter
plus
dark
energy
makes
up
76%
of
all
matter
which
is
sufficient
to
allow
computer
simulations
to
produce
a
good
impression
of
the
behavior
of
all
matter
raon
raon
is
a
south
korean
particle
physics
laboratory
within
the
rare
isotope
science
project
(risp)
that
is
being
constructed
in
the
outskirts
of
daejeon
neighboring
sejong
south
korea
by
the
institute
for
basic
science
(ibs)
it
is
expected
to
be
finished
by
2021
the
name
rare
isotope
accelerator
complex
for
on-line
experiment
or
raon
was
selected
through
a
contest
open
to
the
public
in
2012
raon
comes
from
the
korean
word
meaning
"happy"
or
"joyful"
among
639
entries
the
winning
name
was
actually
raonhaje
(라온하제)
meaning
"happy
tomorrow"
but
was
shortened
for
easier
pronunciation
raon
is
also
the
name
of
their
chemical
element
mascot
with
atomic
number
41
and
niobium
written
on
the
stomach
raon
is
a
heavy
ion
particle
accelerator
that
will
include
both
isol
(isotope
separation
on-line)
and
if
(in-flight
fragmentation)
methods
and
aims
to
be
the
first
to
use
both
the
superconducting
linear
accelerator
will
have
a
maximum
beam
power
of
400 kw
and
projectile
fragmentation
will
be
powered
by
a
200
mev/u
uranium
beam
in
the
if
system
the
isol
system
will
have
a
h-
cyclotron
of
70 kw
due
to
the
complexity
of
the
project
raon's
researchers
are
working
in
collaboration
with
a
number
of
other
accelerator
research
groups
including
cern
fermilab
triumf
and
riken
the
cost
is
estimated
at
14523
trillion
krw
(roughly
14
billion
usd)
in
which
4602
billion
krw
is
for
device
construction
635
billion
krw
for
facility
construction
and
3571
billion
krw
for
land
purchase
the
size
of
the
site
is
652066
m
with
a
total
floor
area
of
130144
m
in
additional
to
the
primary
accelerator
site
under
construction
in
shindong
(신동)
risp
has
the
isol
off-line
test
facility
in
yuseong-gu
superconducting
radio
frequency
test
facility
in
kaist's
munji
campus
and
the
accelerator
and
ict
building
in
korea
university
sejong
campus
research
areas
in
the
field
of
nuclear
science
include
the
study
of
the
origin
of
elements
and
evolution
of
stars
nuclear
force
and
structure
nuclear
reactions
and
nuclear
science
theory
the
group
aims
to
develop
an
ultra-sensitive
device
for
measuring
the
physical
properties
of
muons
and
study
the
properties
of
new
materials
including
semiconductors
nano-magnetic
materials
high-temperature
superconductors
and
topological
insulators
in
these
fields
they
aim
to
precisely
measure
rare
isotope
mass
and
develop
atomic
manipulation
technology
develop
micro-measurement
technology
for
atomic
structures
and
find
the
precise
measurements
of
basic
physical
constants
research
the
application
of
rare
isotopes
in
cancer
treatment
stochastic
thermodynamics
stochastic
thermodynamics
is
an
emergent
field
of
research
in
statistical
mechanics
that
uses
stochastic
variables
to
better
understand
the
non-equilibrium
dynamics
present
in
microscopic
systems
such
as
colloidal
particles
biopolymers
(eg
dna
rna
and
proteins)
enzymes
molecular
motors
and
many
other
types
of
systems
when
a
microscopic
machine
(eg
a
mem)
performs
useful
work
it
generates
heat
and
entropy
as
a
byproduct
of
the
process
however
it
is
also
predicted
that
this
machine
will
operate
in
"reverse"
or
"backwards"
over
appreciable
short
periods
that
is
heat
energy
from
the
surroundings
will
be
converted
into
useful
work
for
larger
engines
this
would
be
described
as
a
violation
of
the
second
law
of
thermodynamics
as
entropy
is
consumed
rather
than
generated
loschmidt's
paradox
states
that
in
a
time
reversible
system
for
every
trajectory
there
exists
a
time-reversed
anti-trajectory
as
the
entropy
production
of
a
trajectory
and
its
equal
anti-trajectory
are
of
identical
magnitude
but
opposite
sign
then
so
the
argument
goes
one
cannot
prove
that
entropy
production
is
positive
for
a
long
time
exact
results
in
thermodynamics
were
only
possible
in
linear
systems
capable
of
reaching
equilibrium
leaving
other
questions
like
the
loschmidt
paradox
unsolved
during
the
last
few
decades
fresh
approaches
have
revealed
general
laws
applicable
to
non-equilibrium
system
which
are
described
by
nonlinear
equations
pushing
the
range
of
exact
thermodynamic
statements
beyond
the
realm
of
traditional
linear
solutions
these
exact
results
are
particularly
relevant
for
small
systems
where
appreciable
(typically
non-gaussian)
fluctuations
occur
thanks
to
stochastic
thermodynamics
it
is
now
possible
to
accurately
predict
distribution
functions
of
thermodynamic
quantities
relating
to
exchanged
heat
applied
work
or
entropy
production
for
these
systems
the
mathematical
resolution
to
loschmidt's
paradox
is
called
the
(steady
state)
fluctuation
theorem
(ft)
which
is
a
generalisation
of
the
second
law
of
thermodynamics
the
ft
shows
that
as
a
system
gets
larger
or
the
trajectory
duration
becomes
longer
entropy-consuming
trajectories
become
more
unlikely
and
the
expected
second
law
behaviour
is
recovered
the
ft
was
first
put
forward
by
and
much
of
the
work
done
in
developing
and
extending
the
theorem
was
accomplished
by
theoreticians
and
mathematicians
interested
in
nonequilibrium
statistical
mechanics
the
first
observation
and
experimental
proof
of
evan's
fluctuation
theorem
(ft)
was
performed
by
a
recent
review
states
that
"proved
a
remarkable
relation
which
allows
to
express
the
free
energy
difference
between
two
equilibrium
systems
by
a
nonlinear
average
over
the
work
required
to
drive
the
system
in
a
non-equilibrium
process
from
one
state
to
the
other
by
comparing
probability
distributions
for
the
work
spent
in
the
original
process
with
the
time-reversed
one
crooks
found
a
“refinement”
of
the
jarzynski
relation
(jr)
now
called
the
crooks
fluctuation
theorem
both
this
relation
and
another
refinement
of
the
jr
the
hummer-szabo
relation
became
particularly
useful
for
determining
free
energy
differences
and
landscapes
of
biomolecules
these
relations
are
the
most
prominent
ones
within
a
class
of
exact
results
(some
of
which
found
even
earlier
and
then
rediscovered)
valid
for
non-equilibrium
systems
driven
by
time-dependent
forces
a
close
analogy
to
the
jr
which
relates
different
equilibrium
states
is
the
hatano-sasa
relation
that
applies
to
transitions
between
two
different
non-equilibrium
steady
states"
this
is
shown
to
be
a
special
case
of
a
more
general
relation
classical
thermodynamics
at
its
heart
deals
with
general
laws
governing
the
transformations
of
a
system
in
particular
those
involving
the
exchange
of
heat
work
and
matter
with
an
environment
as
a
central
result
total
entropy
production
is
identified
that
in
any
such
process
can
never
decrease
leading
inter
alia
to
fundamental
limits
on
the
efficiency
of
heat
engines
and
refrigerators
the
thermodynamic
characterisation
of
systems
in
equilibrium
got
its
microscopic
justification
from
equilibrium
statistical
mechanics
which
states
that
for
a
system
in
contact
with
a
heat
bath
the
probability
to
find
it
in
any
specific
microstate
is
given
by
the
boltzmann
factor
for
small
deviations
from
equilibrium
linear
response
theory
allows
to
express
transport
properties
caused
by
small
external
fields
through
equilibrium
correlation
functions
on
a
more
phenomenological
level
linear
irreversible
thermodynamics
provides
a
relation
between
such
transport
coefficients
and
entropy
production
in
terms
of
forces
and
fluxes
beyond
this
linear
response
regime
for
a
long
time
no
universal
exact
results
were
available
during
the
last
20
years
fresh
approaches
have
revealed
general
laws
applicable
to
non-equilibrium
system
thus
pushing
the
range
of
validity
of
exact
thermodynamic
statements
beyond
the
realm
of
linear
response
deep
into
the
genuine
non-equilibrium
region
these
exact
results
which
become
particularly
relevant
for
small
systems
with
appreciable
(typically
non-gaussian)
fluctuations
generically
refer
to
distribution
functions
of
thermodynamic
quantities
like
exchanged
heat
applied
work
or
entropy
production
stochastic
thermodynamics
combines
the
stochastic
energetics
introduced
by
with
the
idea
that
entropy
can
consistently
be
assigned
to
a
single
fluctuating
trajectory
stochastic
thermodynamics
can
be
applied
to
driven
(ie
open)
quantum
systems
whenever
the
effects
of
quantum
coherence
can
be
ignored
the
dynamics
of
an
open
quantum
system
is
then
equivalent
to
a
classical
stochastic
one
however
this
is
sometimes
at
the
cost
of
requiring
unrealistic
measurements
at
the
beginning
and
end
of
a
process
understanding
non-equilibrium
quantum
thermodynamics
more
broadly
is
an
important
and
active
area
of
research
the
efficiency
of
some
computing
and
information
theory
tasks
can
be
greatly
enhanced
when
using
quantum
correlated
states;
quantum
correlations
can
be
used
not
as
a
valuable
resource
in
quantum
computation
but
also
in
the
realm
of
quantum
thermodynamics
new
types
of
quantum
devices
in
non-equilibrium
states
function
very
differently
to
their
classical
counterparts
for
example
it
has
been
theoretically
shown
that
non-equilibrium
quantum
ratchet
systems
function
far
more
efficiently
then
that
predicted
by
classical
thermodynamics
it
has
also
been
shown
that
quantum
coherence
can
be
used
to
enhance
the
efficiency
of
systems
beyond
the
classical
carnot
limit
this
is
because
it
could
be
possible
to
extract
work
in
the
form
of
photons
from
a
single
heat
bath
quantum
coherence
can
be
used
in
effect
to
play
the
role
of
maxwell's
demon
though
it
should
be
emphasized
that
the
broader
information
theory
based
interpretation
of
the
second
law
of
thermodynamics
is
not
violated
quantum
versions
of
stochastic
thermodynamics
have
been
studied
for
some
time
and
the
past
few
years
have
seen
a
surge
of
interest
in
this
topic
quantum
mechanics
involves
profound
issues
around
the
interpretation
of
reality
(eg
the
copenhagen
interpretation
many-worlds
de
broglie-bohm
theory
etc
are
all
competing
interpretations
that
try
to
explain
the
unintuitive
results
of
quantum
theory)
it
is
hoped
that
by
trying
to
specify
the
quantum-mechanical
definition
of
work
dealing
with
open
quantum
systems
analyzing
exactly
solvable
models
or
proposing
and
performing
experiments
to
test
non-equilibrium
predictions
important
insights
into
the
interpretation
of
quantum
mechanics
and
the
true
nature
of
reality
will
be
gained
applications
of
non-equilibrium
work
relations
like
the
jarzynski
equality
have
recently
been
proposed
for
the
purposes
of
detectiing
quantum
entanglement
and
to
improving
optimization
problems
(minimize
or
maximize
a
function
of
multivariables
called
the
cost
function)
via
quantum
annealing
until
recently
thermodynamics
has
only
considered
systems
coupled
to
a
thermal
bath
and
therefore
satisfying
boltzmann
statistics
however
systems
satisfying
these
conditions
do
not
include
many
systems
that
are
far
from
equilibrium
such
as
living
matter
for
which
fluctuations
are
expected
to
be
non-gaussian
active
particle
systems
are
able
to
take
energy
from
their
environment
and
drive
themselves
far
from
equilibrium
an
important
example
of
active
matter
is
constituted
by
objects
capable
of
self
propulsion
thanks
to
this
property
they
feature
a
series
of
novel
behaviours
that
are
not
attainable
by
matter
at
thermal
equilibrium
including
for
example
swarming
and
the
emergence
of
other
collective
properties
a
passive
particle
is
considered
in
an
active
bath
when
it
is
in
an
environment
where
a
wealth
of
active
particles
are
present
these
particles
will
exert
nonthermal
forces
on
the
passive
object
so
that
it
will
experience
non-thermal
fluctuations
and
will
behave
widely
different
from
a
passive
brownian
particle
in
a
thermal
bath
the
presence
of
an
active
bath
can
significantly
influence
the
microscopic
thermodynamics
of
a
particle
experiments
have
suggested
that
the
jarzynski
equality
does
not
hold
in
some
cases
due
to
the
presence
of
non-boltzmann
statistics
in
active
baths
this
observation
points
towards
a
new
direction
in
the
study
of
non-equilibrium
statistical
physics
and
stochastic
thermodynamics
where
also
the
environment
itself
is
far
from
equilibrium
active
baths
are
a
question
of
particular
importance
in
biochemistry
for
example
biomolecules
within
cells
are
coupled
with
an
active
bath
due
to
the
presence
of
molecular
motors
within
the
cytoplasm
which
leads
to
striking
and
largely
not
yet
understood
phenomena
such
as
the
emergence
of
anomalous
diffusion
(barkai
et
al
2012)
also
protein
folding
might
be
facilitated
by
the
presence
of
active
fluctuations
(harder
et
al
2014b)
and
active
matter
dynamics
could
play
a
central
role
in
several
biological
functions
(mallory
et
al
2015;
shin
et
al
2015;
suzuki
et
al
2015)
it
is
an
open
question
to
what
degree
stochastic
thermodynamics
can
be
applied
to
systems
coupled
to
active
baths
diffraction-limited
storage
ring
diffraction-limited
storage
rings
(dlsr)
or
ultra-low
emittance
storage
rings
are
synchrotron
light
sources
where
the
emittance
of
the
electron-beam
in
the
storage
ring
is
smaller
or
comparable
to
the
emittance
of
the
x-ray
photon
beam
they
produce
at
the
end
of
their
insertion
devices
these
facilities
operate
in
the
soft
to
hard
x-ray
range
(100ev—100kev)
with
extremely
high
brilliance
(in
the
order
of
10—10
photons/s/m/mrad/01%bw)
together
with
x-ray
free-electron
laser
they
constitute
the
fourth
generation
of
light
sources
characterized
by
a
relatively
high
coherent
flux
(in
the
order
of
10—10photons/s/01%bw
for
dlsr)
and
enable
extended
physical
and
chemical
characterizations
at
the
nano-scale
pseudo
jahn–teller
effect
the
pseudo
jahn–teller
effect
(pjte)
occasionally
also
known
as
second-order
jte
is
a
direct
extension
of
the
jahn–teller
effect
(jte)
where
spontaneous
symmetry
breaking
in
polyatomic
systems
(molecules
and
solids)
occurs
even
in
nondegenerate
electronic
states
under
the
influence
of
sufficiently
low-lying
excited
states
of
appropriate
symmetry
"the
pseudo
jahn–teller
effect
is
the
only
source
of
instability
and
distortions
of
high-symmetry
configurations
of
polyatomic
systems
in
nondegenerate
states
and
it
contributes
significantly
to
the
instability
in
degenerate
states"
in
the
first
publication
in
1957
on
the
(what
is
now
called)
pseudo
jahn–teller
effect
(pjte)
öpik
and
pryce
showed
that
a
small
splitting
of
the
degenerate
electronic
term
does
not
necessarily
remove
the
instability
and
distortion
of
the
polyatomic
system
induced
by
the
jahn–teller
effect
(jte)
provided
the
splitting
is
sufficiently
small
(the
two
split
states
remain
“pseudodegenerate”)
and
the
vibronic
coupling
between
them
is
strong
enough
from
another
perspective
the
idea
of
vibronic
admixture
of
different
electronic
terms
by
low-symmetry
vibrations
was
introduced
in
1933
by
herzberg
and
teller
to
explore
forbidden
electronic
transitions
and
extended
in
the
late
1950s
by
murrell
and
pople
and
by
liehr
the
role
of
excited
states
in
softening
the
ground
state
with
respect
to
distortions
in
benzene
was
demonstrated
qualitatively
by
longuet-higgins
and
salem
by
analyzing
the
π
electron
levels
in
the
hückel
approximation
while
a
general
second-order
perturbation
formula
for
such
vibronic
softening
was
derived
by
bader
in
1960
in
1961
fulton
and
gouterman
presented
a
symmetry
analysis
of
the
two-level
case
in
dimers
and
introduced
the
term
"pseudo
jahn–teller
effect"
the
first
application
of
the
pjte
to
solving
a
major
solid-state
structural
problem
with
regard
to
the
origin
of
ferroelectricity
was
published
in
1966
by
bersuker
and
the
first
book
on
the
jte
covering
the
pjte
was
published
in
1972
by
englman
the
second-order
perturbation
approach
was
employed
by
pearson
in
1975
to
predict
instabilities
and
distortions
in
molecular
systems;
he
called
it
"second-order
jte"
(sojte)
the
first
explanation
of
pjt
origin
of
puckering
distortion
as
due
to
the
vibronic
coupling
to
the
excited
state
was
given
for
the
nh
radical
by
borden
davidson
and
feller
in
1980
(they
called
it
"piramidalization")
methods
of
numerical
calculation
of
the
pjt
vibronic
coupling
effect
with
applications
to
spectroscopic
problems
were
developed
in
the
early
1980s
the
equilibrium
geometry
of
any
polyatomic
system
in
nondegenerate
states
is
defined
as
corresponding
to
the
point
of
the
minimum
of
the
adiabatic
potential
energy
surface
(apes)
where
its
first
derivatives
are
zero
and
the
second
derivatives
are
positive
denote
the
energy
of
the
system
as
a
function
of
normal
displacements
q
by
e(q)
at
the
point
of
minimum
(q=0)
of
the
apes
the
curvature
k
of
e(q)
in
the
q
direction
is
positive
k
>
0
very
often
the
geometry
of
the
system
at
this
point
of
equilibrium
on
the
apes
does
not
coincide
with
the
highest
possible
(or
even
with
any
high)
symmetry
expected
from
general
symmetry
considerations
for
instance
linear
molecules
are
bent
at
equilibrium
planar
molecules
are
puckered
octahedral
complexes
are
elongated
or
compressed
or
tilted
cubic
crystals
are
tetragonally
polarized
(or
have
several
structural
phases)
etc
the
pjte
is
the
general
driving
force
of
all
these
distortions
if
they
occur
in
the
nondegenerate
electronic
states
of
the
high-symmetry
(reference)
geometry
k=
(3)
k=-σ||/[e-e]
(4)
where
ψ
are
the
wavefunctions
of
the
excited
states
and
the
k
expression
obtained
as
a
second
order
perturbation
correction
is
always
negative
k0
the
k
contribution
is
the
only
source
of
instability
the
matrix
elements
in
eq
(4)
are
off-diagonal
vibronic
coupling
constants
they
measure
the
mixing
of
the
ground
and
excited
states
under
the
nuclear
displacements
q
and
therefore
k
is
termed
the
vibronic
contribution
together
with
the
k
value
and
the
energy
gap
2∆=e-e
between
the
mixing
states
f
are
the
main
parameters
of
the
pjte
(see
below)
in
a
series
of
papers
beginning
in
1980
(see
references
in
)
it
was
proved
that
for
any
polyatomic
system
in
the
high-symmetry
configuration
k>0
(6)
and
hence
the
vibronic
contribution
is
the
only
source
of
instability
of
any
polyatomic
system
in
nondegenerate
states
if
k
>0
for
the
high-symmetry
configuration
of
any
polyatomic
system
then
a
negative
curvature
k
=(k
+
k)
k
it
follows
that
any
distortion
of
the
high-symmetry
configuration
is
due
to
and
only
to
the
mixing
of
its
ground
state
with
excited
electronic
states
by
the
distortive
nuclear
displacements
realized
via
the
vibronic
coupling
in
eq
(5)
the
latter
softens
the
system
with
respect
to
certain
nuclear
displacements
(k<0)
and
if
this
softening
is
larger
than
the
original
(nonvibronic)
hardness
k
in
this
direction
the
system
becomes
unstable
with
respect
to
the
distortions
under
consideration
leading
to
its
equilibrium
geometry
of
lower
symmetry
or
to
dissociation
there
are
many
cases
when
neither
the
ground
state
is
degenerate
nor
is
there
a
significant
vibronic
coupling
to
the
lowest
excited
states
to
realize
pjte
instability
of
the
high-symmetry
configuration
of
the
system
and
still
there
is
a
ground
state
equilibrium
configuration
with
lower
symmetry
in
such
cases
the
symmetry
breaking
is
produced
by
a
hidden
pjte
(similar
to
a
hidden
jte);
it
takes
place
due
to
a
strong
pjte
mixing
of
two
excited
states
one
of
which
crosses
the
ground
state
to
create
a
new
(lower)
minimum
of
the
apes
with
a
distorted
configuration
the
use
of
the
second
order
perturbation
correction
eq
(4)
for
the
calculation
of
the
k
value
in
the
case
of
pjte
instability
is
incorrect
because
in
this
case
|k|>k
meaning
the
first
perturbation
correction
is
larger
than
the
main
term
and
hence
the
criterion
of
applicability
of
the
perturbation
theory
in
its
simplest
form
does
not
hold
in
this
case
we
should
consider
the
contribution
of
the
lowest
excited
states
(that
make
the
total
curvature
negative)
in
a
pseudodegenerate
problem
of
perturbation
theory
for
the
simplest
case
when
only
one
excited
state
creates
the
main
instability
of
the
ground
state
we
can
treat
the
problem
via
a
pseudodegenerate
two-level
problem
including
the
contribution
of
the
higher
weaker-influencing
states
as
a
second
order
correction
in
the
pjte
two-level
problem
we
have
two
electronic
states
of
the
high-symmetry
configuration
ground
β
and
excited
γ
separated
by
an
energy
interval
of
2δ
that
become
mixed
under
nuclear
displacements
of
certain
symmetry
q=q;
the
denotations
α
β
and
γ
indicate
respectively
the
irreducible
representations
to
which
the
symmetry
coordinate
and
the
two
states
belong
in
essence
this
is
the
original
formulation
of
the
pjte
assuming
that
the
excited
state
is
sufficiently
close
to
the
ground
one
the
vibronic
coupling
between
them
should
be
treated
as
a
perturbation
problem
for
two
near-degenerate
states
with
both
interacting
states
non-degenerate
the
vibronic
coupling
constant
f
in
eq(5)
(omitting
indices)is
non-zero
for
only
one
coordinate
q=q
with
α=β×γ
this
gives
us
directly
the
symmetry
of
the
direction
of
softening
and
possible
distortion
of
the
ground
state
assuming
that
the
primary
force
constants
k
in
the
two
states
are
the
same
(for
different
k
see
[1])
we
get
a
2×2
secular
equation
with
the
following
solution
for
the
energies
ε
of
the
two
states
interacting
under
the
linear
vibronic
coupling
(the
energy
is
read
off
the
middle
of
the
2δ
interval
between
the
initial
levels):
ε=
(1/2)q±[δ+fq]
(7)
it
is
seen
from
this
expressions
that
on
taking
into
account
the
vibronic
coupling
f≠0
the
two
apes
curves
change
in
different
ways:
in
the
upper
sheet
the
curvature
(the
coefficient
at
q
in
the
expansion
on
q)
increases
whereas
in
the
lower
one
it
decreases
but
until
(f/k)<δ
the
minima
of
both
states
correspond
to
the
point
q
=
0
as
in
the
absence
of
vibronic
mixing
however
if
the
curvature
of
the
lower
curve
of
the
apes
becomes
negative
and
the
system
is
unstable
with
respect
to
the
q
displacements
(fig
1)
the
minima
points
on
the
apes
in
this
case
are
given
by
±q=[f/k-δ/f]
(9)
from
these
expressions
and
fig
1
it
is
seen
that
while
the
ground
state
is
softened
(destabilized)
by
the
pjte
the
excited
state
is
hardened
(stabilized)
and
this
effect
is
the
larger
the
smaller
δ
and
the
larger
f
it
takes
place
in
any
polyatomic
system
and
influences
many
molecular
properties
including
the
existence
of
stable
excited
states
of
molecular
systems
that
are
unstable
in
the
ground
state
(eg
excited
states
of
intermediates
of
chemical
reactions);
in
general
even
in
the
absence
of
instability
the
pjte
softens
the
ground
state
and
increases
the
vibrational
frequencies
in
the
excited
state
the
two
branches
of
the
apes
for
the
case
of
strong
pjte
resulting
in
the
instability
of
the
ground
state
(when
the
condition
of
instability
(11)
holds)
are
illustrated
in
fig
1b
in
comparison
with
the
case
when
the
two
states
have
the
same
energy
(fig
1a)
i
e
when
they
are
degenerate
and
the
jahn–teller
effect
(jte)
takes
place
we
see
that
the
two
cases
degenerate
and
nondegenerate
but
close-in-energy
(pseudodegenerate)
are
similar
in
generating
two
minima
with
distorted
configurations
but
there
are
important
differences:
while
in
the
jte
there
is
a
crossing
of
the
two
terms
at
the
point
of
degeneracy
(leading
to
conical
intersections
in
more
complicated
cases)
in
the
nondegenerate
case
with
strong
vibronic
coupling
there
is
an
“avoided
crossing”
or
“pseudo
crossing”
even
a
more
important
difference
between
the
two
vibronic
coupling
effects
emerges
from
the
fact
that
the
two
interacting
states
in
the
jte
are
components
of
the
same
symmetry
type
whereas
in
the
pjte
each
of
the
two
states
may
have
any
symmetry
for
this
reason
the
possible
kinds
of
distortion
is
very
limited
in
the
jte
and
unlimited
in
the
pjte
it
is
also
noticeable
that
while
the
systems
with
jte
are
limited
by
the
condition
of
electron
degeneracy
the
applicability
of
the
pjte
has
no
a
priori
limitations
as
it
includes
also
the
cases
of
degeneracy
even
when
the
pjt
coupling
is
weak
and
the
inequality
(11)
does
not
hold
the
pjte
is
still
significant
in
softening
(lowering
the
corresponding
vibrational
frequency)
of
the
ground
state
and
increasing
it
in
the
excited
state
when
considering
the
pjte
in
an
excited
state
all
the
higher
in
energy
states
destabilize
it
while
the
lower
ones
stabilize
it
for
a
better
understanding
it
is
important
to
follow
up
on
how
the
pjte
is
related
to
intramolecular
interactions
in
other
words
what
is
the
physical
driving
force
of
the
pjte
distortions
(transformations)
in
terms
of
well-known
electronic
structure
and
bonding?
the
driving
force
of
the
pjte
is
added
(improved)
covalence:
the
pjte
distortion
takes
place
when
it
results
in
energy
gain
due
to
better
covalence
bonding
between
the
atoms
in
the
distorted
configuration
indeed
in
the
starting
high-symmetry
configuration
the
wavefunctions
of
the
electronic
states
ground
and
excited
are
orthogonal
by
definition
by
distortion
their
orthogonality
is
violated
and
a
nonzero
overlap
between
them
occurs
if
for
two
near-neighbor
atoms
the
ground
state
wavefunction
pertains
(mainly)
to
one
of
them
while
the
excited
state
wavefunction
belongs
(mainly)
to
the
other
one
the
overlap
by
distortion
adds
covalency
to
the
bonding
between
them
facilitating
the
distortion
(fig
2)
applications
of
the
pjte
to
solving
chemical
physical
biological
and
materials
science
problems
are
innumerable;
as
stated
above
the
pjte
is
the
only
source
of
instability
and
distortions
in
high-symmetry
configurations
of
molecular
systems
and
solids
with
nondegenerate
states
hence
any
problem
steaming
from
such
instability
can
be
treated
by
the
pjte
below
are
some
illustrative
examples
pjte
versus
renner–teller
effect
in
bending
distortions
linear
molecules
are
exceptions
from
the
jte
and
for
a
long
time
it
was
assumed
that
their
bending
distortions
in
degenerate
states
(observed
in
many
molecules)
is
produced
by
the
renner–teller
effect
(rte)
(the
splitting
of
the
generate
state
by
the
quadratic
terms
of
the
vibronic
coupling)
however
recently
it
was
proved
(see
in
the
review
)
that
the
rte
by
splitting
the
degenerate
electronic
state
just
softens
the
lower
branch
of
the
apes
but
this
lowering
of
the
energy
is
not
enough
to
overcome
the
rigidity
of
the
linear
configuration
and
to
produce
bending
distortions
it
follows
that
the
bending
distortion
of
linear
molecular
systems
is
due
to
and
only
to
the
pjte
that
mixes
the
electronic
state
under
consideration
with
higher
in
energy
(excited)
states
this
statement
is
enhanced
by
the
fact
that
many
linear
molecules
in
nondegenerate
states
(and
hence
with
no
rte)
are
too
bent
in
the
equilibrium
configuration
the
physical
reason
for
the
difference
between
the
pjte
and
the
rte
in
influencing
the
degenerate
term
is
that
while
in
the
former
case
the
vibronic
coupling
with
the
excited
state
produces
additional
covalent
bonding
that
makes
the
distorted
configuration
preferable
(see
above
section
23)
the
rte
has
no
such
influence;
the
splitting
of
the
degenerate
term
in
the
rte
takes
place
just
because
the
charge
distribution
in
the
two
states
becomes
nonequivalent
under
the
bending
distortion
peierls
distortions
in
linear
chains
in
linear
molecules
with
three
or
more
atoms
there
may
be
pjte
distortions
that
do
not
violate
the
linearity
but
change
the
interatomic
distances
for
instance
as
a
result
of
the
pjte
a
centrosymmetric
linear
system
may
become
non-centrosymmetric
in
the
equilibrium
configurations
as
for
example
in
the
bnb
molecule
(see
in
)
an
interesting
extension
of
such
distortions
in
sufficiently
long
(infinite)
linear
chains
was
first
considered
by
peierls
in
this
case
the
electronic
states
combinations
of
atomic
states
are
in
fact
band
states
and
it
was
shown
that
if
the
chain
is
composed
by
atoms
with
unpaired
electrons
the
valence
band
is
only
half
filled
and
the
pjte
interaction
between
the
occupied
and
unoccupied
band
states
leads
to
the
doubling
of
the
period
of
the
linear
chain
(see
also
in
the
books
)
broken
cylindrical
symmetry
it
was
shown
also
that
the
pjte
not
only
produces
the
bending
instability
of
linear
molecules
but
if
the
mixing
electronic
states
involve
a
δ
state
(a
state
with
a
nonzero
momentum
with
respect
to
the
axis
of
the
molecule
its
projection
quantum
number
being
λ=2)
the
apes
simultaneously
with
the
bending
becomes
warped
along
the
coordinate
of
rotations
around
the
molecular
axis
thus
violating
both
the
linear
and
cylindrical
symmetry
it
happens
because
the
pjte
by
mixing
the
wavefunctions
of
the
two
interacting
states
transfers
the
high
momentum
of
the
electrons
from
states
with
λ=2
to
states
with
lower
momentum
and
this
may
alter
significantly
their
expected
rovibronic
spectra
pjte
and
combined
pjte
plus
jte
effects
in
molecular
structures
there
is
a
practically
unlimited
number
of
molecular
systems
for
which
the
origin
of
their
structural
properties
was
revealed
and/or
rationalized
based
on
the
pjte
or
a
combination
of
the
pjte
and
jte
the
latter
stems
from
the
fact
that
in
any
system
with
a
jte
in
the
ground
state
the
presence
of
a
pjt
active
excited
state
is
not
excluded
and
vice
versa
the
active
excited
state
for
the
pjte
of
the
ground
one
may
be
degenerate
and
hence
jt
active
examples
are
shown
eg
in
refs
including
molecular
systems
like
na
ch
cx
(x=
h
f
cl
br)
co
sir
(with
r
as
large
ligands)
planar
cyclic
ch
all
kind
of
coordination
systems
of
transition
metals
mixed-valence
compounds
biological
systems
origin
of
conformations
geometry
of
ligands’
coordination
etc
etc
in
fact
it
is
difficult
to
find
a
molecular
system
for
which
the
pjte
implications
are
a
priori
excluded
which
is
understandable
in
view
of
the
mentioned
above
unique
role
of
the
pjte
in
such
instabilities
hidden
pjte
spin
crossover
and
magnetic-dielectric
bistability
as
mentioned
above
there
are
molecular
systems
in
which
the
ground
state
in
the
high-symmetry
configuration
is
neither
degenerate
to
trigger
the
jte
nor
does
it
interact
with
the
low-lying
excited
states
to
produce
the
pjte
(eg
because
of
their
different
spin
multiplicity)
in
these
situations
the
instability
is
produced
by
a
strong
pjte
in
the
excited
states;
this
is
termed
“hidden
pjte”
in
the
sense
that
its
origin
is
not
seen
explicitly
as
a
pjte
in
the
ground
state
an
interesting
typical
situation
of
hidden
pjte
emerges
in
molecular
and
solid
state
systems
with
valence
half-filed
closed
shells
electronic
configurations
e
and
t
for
instance
in
the
e
case
the
ground
state
in
the
high-symmetry
equilibrium
geometry
is
an
orbital
non-degenerate
triplet
a
while
the
nearby
low-lying
two
excited
electronic
states
are
close-in-energy
singlets
e
and
a;
due
to
the
strong
pjt
interaction
between
the
latter
the
lower
component
of
e
crosses
the
triplet
state
to
produce
a
global
minimum
with
lower
symmetry
fig
3
illustrates
the
hidden
pjte
in
the
cuf
molecule
showing
also
the
singlet-triplet
spin
crossover
and
the
resulting
two
coexisting
configurations
of
the
molecule:
high-symmetry
(undistorted)
spin-triplet
state
with
a
nonzero
magnetic
moment
and
a
lower
in
energy
dipolar-distorted
singlet
state
with
zero
magnetic
moment
such
magnetic-dielectric
bistability
is
inherent
to
a
whole
class
of
molecular
systems
and
solids
puckering
in
planar
molecules
and
graphene-like
2d
and
quasi
2d
systems
special
attention
has
been
paid
recently
to
two-dimensional
(2d)
systems
in
view
of
a
variety
of
their
planar-surface-specific
physical
and
chemical
properties
and
possible
graphene-like
applications
in
electronics
similar-to-graphene
properties
are
sought
for
in
silicene
phosphorene
boron
nitride
zinc
oxide
gallium
nitride
as
well
as
in
other
attractive
subjects
like
2d
transition
metal
dichalcogenids
and
oxides
and
there
is
a
number
of
other
organic
and
inorganic
2d
and
quasi-2d
compounds
with
expected
similar
properties
one
of
the
main
important
features
of
these
systems
is
their
planarity
or
quasi-planarity
but
many
of
the
quasi-2d
compounds
are
subject
to
out-of-plane
deviations
known
as
puckering
(buckling)
it
was
shown
as
expected
that
the
instability
and
distortions
of
the
planar
configuration
(as
in
any
other
systems
in
nondegenerate
state)
is
due
to
the
pjte
detailed
exploration
of
the
pjte
in
such
systems
allows
one
to
reveal
the
excited
states
that
are
responsible
for
the
puckering
and
suggest
possible
external
influence
that
restores
their
planarity
including
oxidation
reduction
substitutions
or
coordination
to
other
species
cooperative
pjte
in
batio-type
crystals
and
ferroelectricity
in
crystals
with
pjte
centers
the
interaction
between
the
local
distortions
may
lead
to
their
ordering
to
produce
a
phase
transition
to
a
regular
crystal
phase
with
lower
symmetry
such
cooperative
pjte
is
quite
similar
to
the
cooperative
jte;
it
was
shown
in
one
of
the
first
applications
of
the
pjte
to
solid
state
systems
that
in
case
of
abo
crystals
with
perovskite
structure
the
local
dipolar
pjte
distortions
at
the
transition
metal
b
center
and
their
cooperative
interactions
leads
to
ferroelectric
phase
transitions
provided
the
criterion
of
pjte
is
obeyed
each
[bo]
center
has
an
apes
with
eight
equivalent
minima
along
the
trigonal
axes
six
orthorhombic
and
(higher)
twelve
tetragonal
saddle-points
between
them
with
temperature
the
gradually
reached
transitions
between
the
minima
via
the
different
kind
of
saddle-points
explains
the
origin
of
all
the
four
phases
(three
ferroelectric
and
one
paraelectric)
in
perovskites
of
the
type
batio
and
their
properties
the
predicted
by
the
theory
trigonal
displacement
of
the
ti
ion
in
all
four
phases
the
fully
disordered
pjte
distortions
in
the
paraelectric
phase
and
their
partially
disordered
state
in
two
other
phases
was
confirmed
by
a
variety
of
experimental
investigations
(see
in
)
multiferroicity
and
magnetic-ferroelectric
crossover
the
pjte
theory
of
ferroelectricity
in
abo3
crystals
was
expanded
to
show
that
dependent
of
the
number
of
electrons
in
the
d
shell
of
the
transition
metal
ion
b
and
their
low
spin
or
high
spin
arrangement
(which
controls
the
symmetry
and
spin
multiplicity
of
the
ground
and
pjte
active
excited
states
of
the
[bo]
center)
their
ferroelectricity
may
coexist
with
a
magnetic
moment
(multiferroicity)
moreover
in
combination
with
the
temperature
dependent
spin
crossover
phenomenon
(which
changes
the
spin
multiplicity)
this
kind
of
multiferroicity
may
lead
to
a
novel
effect
of
magnetic-ferroelectric
crossover
solid
state
magnetic-dielectric
bistability
similar
to
the
mentioned
above
molecular
bistability
induced
by
the
hidden
pjte
a
magnetic-dielectric
bistability
due
to
two
coexisting
equilibrium
configurations
with
corresponding
properties
may
take
place
also
in
crystals
with
transition
metal
centers
subject
to
the
electronic
configuration
with
half-filled
e
or
t
shells
as
in
molecular
systems
the
latter
produce
a
hidden
pjte
and
local
bistability
which
distinguished
from
the
molecular
case
are
enhanced
by
the
cooperative
interactions
thus
acquiring
larger
lifetimes
this
crystal
bistability
was
proved
by
calculations
for
licuo
and
nacuo
crystals
in
which
the
cu
ion
has
the
electronic
e(d)
configuration
(similar
to
the
cuf
molecule)
giant
enhancement
of
observable
properties
in
interaction
with
external
perturbations
in
a
recent
development
of
the
pjte
applications
in
materials
science
it
was
shown
that
in
crystals
with
pjte
centers
in
which
the
local
distortion
are
not
ordered
(before
the
phase
transition
to
the
cooperative
phases)
the
effect
of
interaction
with
external
perturbations
acquires
an
orientational
contribution
which
enhances
the
observable
properties
by
several
orders
of
magnitude
this
was
demonstrated
on
the
properties
of
crystals
like
paraelectric
batio
in
interaction
with
electric
fields
(in
permittivity
and
electrostriction)
or
under
a
strain
gradient
(flexoelectricity)
these
giant
enhancement
effects
occur
due
to
the
dynamic
nature
of
the
pjte
local
dipolar
distortions
(their
tunneling
between
the
equivalent
minima);
the
independently
rotating
dipole
moments
on
each
center
become
oriented
(frozen)
along
the
external
perturbation
resulting
in
an
orientational
polarization
which
is
not
there
in
the
absence
of
the
pjte
action-angle
coordinates
in
classical
mechanics
action-angle
coordinates
are
a
set
of
canonical
coordinates
useful
in
solving
many
integrable
systems
the
method
of
action-angles
is
useful
for
obtaining
the
frequencies
of
oscillatory
or
rotational
motion
without
solving
the
equations
of
motion
action-angle
coordinates
are
chiefly
used
when
the
hamilton–jacobi
equations
are
completely
separable
(hence
the
hamiltonian
does
not
depend
explicitly
on
time
ie
the
energy
is
conserved)
action-angle
variables
define
an
invariant
torus
so
called
because
holding
the
action
constant
defines
the
surface
of
a
torus
while
the
angle
variables
parameterize
the
coordinates
on
the
torus
the
bohr–sommerfeld
quantization
conditions
used
to
develop
quantum
mechanics
before
the
advent
of
wave
mechanics
state
that
the
action
must
be
an
integral
multiple
of
planck's
constant;
similarly
einstein's
insight
into
ebk
quantization
and
the
difficulty
of
quantizing
non-integrable
systems
was
expressed
in
terms
of
the
invariant
tori
of
action-angle
coordinates
action-angle
coordinates
are
also
useful
in
perturbation
theory
of
hamiltonian
mechanics
especially
in
determining
adiabatic
invariants
one
of
the
earliest
results
from
chaos
theory
for
the
non-linear
perturbations
of
dynamical
systems
with
a
small
number
of
degrees
of
freedom
is
the
kam
theorem
which
states
that
the
invariant
tori
are
stable
under
small
perturbations
the
use
of
action-angle
variables
was
central
to
the
solution
of
the
toda
lattice
and
to
the
definition
of
lax
pairs
or
more
generally
the
idea
of
the
isospectral
evolution
of
a
system
action
angles
result
from
a
type-2
canonical
transformation
where
the
generating
function
is
hamilton's
characteristic
function
formula_1
("not"
hamilton's
principal
function
formula_2)
since
the
original
hamiltonian
does
not
depend
on
time
explicitly
the
new
hamiltonian
formula_3
is
merely
the
old
hamiltonian
formula_4
expressed
in
terms
of
the
new
canonical
coordinates
which
we
denote
as
formula_5
(the
action
angles
which
are
the
generalized
coordinates)
and
their
new
generalized
momenta
formula_6
we
will
not
need
to
solve
here
for
the
generating
function
formula_7
itself;
instead
we
will
use
it
merely
as
a
vehicle
for
relating
the
new
and
old
canonical
coordinates
rather
than
defining
the
action
angles
formula_5
directly
we
define
instead
their
generalized
momenta
which
resemble
the
classical
action
for
each
original
generalized
coordinate
where
the
integration
path
is
implicitly
given
by
the
constant
energy
function
formula_10
since
the
actual
motion
is
not
involved
in
this
integration
these
generalized
momenta
formula_11
are
constants
of
the
motion
implying
that
the
transformed
hamiltonian
formula_12
does
not
depend
on
the
conjugate
generalized
coordinates
formula_13
where
the
formula_13
are
given
by
the
typical
equation
for
a
type-2
canonical
transformation
hence
the
new
hamiltonian
formula_17
depends
only
on
the
new
generalized
momenta
formula_6
the
dynamics
of
the
action
angles
is
given
by
hamilton's
equations
the
right-hand
side
is
a
constant
of
the
motion
(since
all
the
formula_20's
are)
hence
the
solution
is
given
by
where
formula_22
is
a
constant
of
integration
in
particular
if
the
original
generalized
coordinate
undergoes
an
oscillation
or
rotation
of
period
formula_23
the
corresponding
action
angle
formula_13
changes
by
formula_25
these
formula_26
are
the
frequencies
of
oscillation/rotation
for
the
original
generalized
coordinates
formula_27
to
show
this
we
integrate
the
net
change
in
the
action
angle
formula_13
over
exactly
one
complete
variation
(ie
oscillation
or
rotation)
of
its
generalized
coordinates
formula_27
setting
the
two
expressions
for
formula_31
equal
we
obtain
the
desired
equation
the
action
angles
formula_5
are
an
independent
set
of
generalized
coordinates
thus
in
the
general
case
each
original
generalized
coordinate
formula_34
can
be
expressed
as
a
fourier
series
in
"all"
the
action
angles
where
formula_36
is
the
fourier
series
coefficient
in
most
practical
cases
however
an
original
generalized
coordinate
formula_27
will
be
expressible
as
a
fourier
series
in
only
its
own
action
angles
formula_13
the
general
procedure
has
three
steps:
in
some
cases
the
frequencies
of
two
different
generalized
coordinates
are
identical
ie
formula_42
for
formula_43
in
such
cases
the
motion
is
called
degenerate
degenerate
motion
signals
that
there
are
additional
general
conserved
quantities;
for
example
the
frequencies
of
the
kepler
problem
are
degenerate
corresponding
to
the
conservation
of
the
laplace–runge–lenz
vector
degenerate
motion
also
signals
that
the
hamilton–jacobi
equations
are
completely
separable
in
more
than
one
coordinate
system;
for
example
the
kepler
problem
is
completely
separable
in
both
spherical
coordinates
and
parabolic
coordinates
spectroscopy
of
multiply
ionized
atoms
this
branch
of
spectroscopy
deals
with
radiation
related
to
atoms
that
are
stripped
of
several
electrons
(multiply
ionized
atoms
(mia)
multiply
charged
ions
highly
charged
ions)
these
are
observed
in
very
hot
plasmas
(laboratory
or
astrophysical)
or
in
accelerator
experiments
(beam-foil
electron
beam
ion
trap
(ebit))
the
lowest
exited
electron
shells
of
such
ions
decay
into
stable
ground
states
producing
photons
in
vuv
euv
and
soft
x-ray
spectral
regions
(so-called
resonance
transitions)
after
newton's
discovery
of
spectral
structure
of
white
light
(17th
century)
and
subsequent
studies
of
the
nature
of
light
(hooke
huygens
young)
j
fraunhofer
observed
and
measured
dark
lines
in
the
sun's
spectrum
(they
bear
now
his
name
although
several
of
them
were
observed
earlier
by
wollaston)
it
may
be
the
first
example
of
fundamental
research
in
spectroscopy
later
bunsen
and
kirchhoff
found
that
fraunhofer
lines
correspond
to
emission
spectral
lines
observed
in
laboratory
light
sources
and
so
they
laid
way
for
spectrochemical
analysis
in
laboratory
and
astrophysics
in
the
19th
century
new
developments
such
as
the
discovery
of
photography
rowland's
invention
of
the
concave
diffraction
grating
and
schumann's
works
on
discovery
of
vacuum
ultraviolet
(fluorite
for
prisms
and
lenses
low-gelatin
photographic
plates
and
absorption
of
uv
in
air
below
185
nm)
made
advance
to
shorter
wavelengths
very
fast
at
the
same
time
dewar
observed
series
in
alkali
spectra
hartley
found
constant
wave-number
differences
balmer
discovered
a
relation
connecting
wavelengths
in
the
visible
hydrogen
spectrum
and
finally
rydberg
derived
a
formula
for
wave-numbers
of
spectral
series
the
first
decade
of
the
20th
century
brought
the
basics
of
quantum
theory
(plank
einstein)
and
interpretation
of
spectral
series
of
hydrogen
by
lyman
in
vuv
and
by
paschen
in
infrared
ritz
formulated
the
combination
principle
in
1913
bohr
formulated
his
quantum
mechanical
model
of
atom
this
stimulated
empirical
term
analysis
(see
references
in
page
83)
between
1920
and
1930
fundamental
concepts
of
quantum
mechanics
were
developed
by
pauli
heisenberg
schrödinger
and
dirac
understanding
of
the
spin
and
exclusion
principle
allowed
conceiving
how
electron
shells
of
atoms
are
filled
with
the
increasing
atomic
number
further
progress
in
studies
of
atomic
structure
was
in
tight
connection
with
the
advance
to
shorter
wavelength
in
euv
region
millikan
sawyer
bowen
used
electric
discharges
in
vacuum
to
observe
some
emission
spectral
lines
down
to
13 nm
they
prescribed
to
stripped
atoms
in
1927
osgood
and
hoag
reported
on
grazing
incidence
concave
grating
spectrographs
and
photographed
lines
down
to
44 nm
(k
of
carbon)
dauvillier
used
a
fatty
acid
crystal
of
large
crystal
grating
space
to
extend
soft
x-ray
spectra
up
to
121 nm
and
the
gap
was
closed
in
the
same
period
manne
siegbahn
constructed
a
very
sophisticated
grazing
incidence
spectrograph
that
enabled
ericson
and
edlén
to
obtain
spectra
of
vacuum
spark
with
high
quality
and
to
reliably
identify
lines
of
multiply
ionized
atoms
up
to
o
vi
with
five
stripped
electrons
grotrian
developed
his
graphic
presentation
of
energy
structure
of
the
atoms
russel
and
saunders
proposed
their
coupling
scheme
for
the
spin-orbit
interaction
and
their
generally
recognized
notation
for
spectral
terms
theoretical
quantum-mechanical
calculations
become
rather
accurate
to
describe
the
energy
structure
of
some
simple
electronic
configurations
the
results
of
theoretical
developments
were
summarized
by
condon
and
shortley
in
1935
edlén
thoroughly
analyzed
spectra
of
mia
for
many
chemical
elements
and
derived
regularities
in
energy
structures
of
mia
for
many
isoelectronic
sequences
(ions
with
the
same
number
of
electrons
but
different
nuclear
charges)
spectra
of
rather
high
ionization
stages
(eg
cu
xix)
were
observed
the
most
exciting
event
was
in
1942
when
edlén
proved
the
identification
of
some
solar
coronal
lines
on
the
basis
of
his
precise
analyses
of
spectra
of
mia
this
implied
that
the
solar
corona
has
a
temperature
of
a
million
degrees
and
strongly
advanced
understanding
of
solar
and
stellar
physics
after
the
ww
ii
experiments
on
balloons
and
rockets
were
started
to
observe
the
vuv
radiation
of
the
sun
(see
x-ray
astronomy)
more
intense
research
continued
since
1960
including
spectrometers
on
satellites
in
the
same
period
the
laboratory
spectroscopy
of
mia
becomes
relevant
as
a
diagnostic
tool
for
hot
plasmas
of
thermonuclear
devices
(see
nuclear
fusion)
which
begun
with
building
stellarator
in
1951
by
spitzer
and
continued
with
tokamaks
z-pinches
and
the
laser
produced
plasmas
progress
in
ion
accelerators
stimulated
beam-foil
spectroscopy
as
a
means
to
measure
lifetimes
of
exited
states
of
mia
many
various
data
on
highly
exited
energy
levels
autoionization
and
inner-core
ionization
states
were
obtained 
simultaneously
theoretical
and
computational
approaches
provided
data
necessary
for
identification
of
new
spectra
and
interpretation
of
observed
line
intensities
new
laboratory
and
theoretical
data
become
very
useful
for
spectral
observation
in
space
it
was
a
real
upheaval
of
works
on
mia
in
usa
england
france
italy
israel
sweden
russia
and
other
countries
a
new
page
in
the
spectroscopy
of
mia
may
be
dated
as
1986
with
development
of
ebit
(levine
and
marrs
llnl)
due
to
a
favorable
composition
of
modern
high
technologies
such
as
cryogenics
ultra-high
vacuum
superconducting
magnets
powerful
electron
beams
and
semiconductor
detectors
very
quickly
ebit
sources
were
created
in
many
countries
(see
nist
summary
for
many
details
as
well
as
reviews)
a
wide
field
of
spectroscopic
research
with
ebit
is
enabled
including
achievement
of
highest
grades
of
ionization
(u)
wavelength
measurement
hyperfine
structure
of
energy
levels
quantum
electrodynamic
studies
ionization
cross-sections
(cs)
measurements
electron-impact
excitation
cs
x-ray
polarization
relative
line
intensities
dielectronic
recombination
cs
magnetic
octupole
decay
lifetimes
of
forbidden
transitions
charge-exchange
recombination
etc
perpendicular
paramagnetic
bond
a
perpendicular
paramagnetic
bond
is
a
type
of
chemical
bond
(in
contrast
to
covalent
or
ionic
bonds)
that
does
not
exist
under
normal
atmospheric
conditions
such
a
phenomenon
was
first
hypothesized
through
simulation
to
exist
in
the
atmospheres
of
white
dwarf
stars
whose
magnetic
fields
on
the
order
of
10
teslas
allow
such
interactions
to
exist
normally
at
such
intense
temperatures
as
those
near
a
white
dwarf
more
common
molecular
bonds
cannot
form
and
existing
ones
decompose
vaiśeṣika
sūtra
vaiśeṣika
sūtra
(sanskrit:
वैशेषिक
सूत्र)
also
called
kanada
sutra
is
an
ancient
sanskrit
text
at
the
foundation
of
the
vaisheshika
school
of
hindu
philosophy
the
sutra
was
authored
by
the
hindu
sage
kanada
also
known
as
kashyapa
according
to
some
scholars
he
flourished
before
the
advent
of
buddhism
because
the
"vaiśeṣika
sūtra"
makes
no
mention
of
buddhism
or
buddhist
doctrines;
however
the
details
of
kanada's
life
are
uncertain
and
the
"vaiśeṣika
sūtra"
was
likely
compiled
sometime
between
6th
and
2nd
century
bce
and
finalized
in
the
currently
existing
version
before
the
start
of
the
common
era
a
number
of
scholars
have
commented
on
it
since
the
beginning
of
common
era;
the
earliest
commentary
known
is
the
"padartha
dharma
sangraha"
of
prashastapada
another
important
secondary
work
on
"vaiśeṣika
sūtra"
is
maticandra's
"dasha
padartha
sastra"
which
exists
both
in
sanskrit
and
its
chinese
translation
in
648 ce
by
yuanzhuang
the
"vaiśeṣika
sūtra"
is
written
in
aphoristic
sutras
style
and
presents
its
theories
on
the
creation
and
existence
of
the
universe
using
naturalistic
atomism
applying
logic
and
realism
and
is
one
of
the
earliest
known
systematic
realist
ontology
in
human
history
the
text
discusses
motions
of
different
kind
and
laws
that
govern
it
the
meaning
of
dharma
a
theory
of
epistemology
the
basis
of
atman
(self
soul)
and
the
nature
of
yoga
and
moksha
the
explicit
mention
of
motion
as
the
cause
of
all
phenomena
in
the
world
and
several
propositions
about
it
make
it
one
of
the
earliest
texts
on
physics
the
name
"vaiśeṣika
sūtra"
(sanskrit:
वैशेषिक
सूत्र)
is
derived
from
"viśeṣa"
विशेष
which
means
"particularity"
that
is
to
be
contrasted
from
"universality"
the
classes
particularity
and
universality
belong
to
different
categories
of
experience
till
the
1950s
only
one
manuscript
of
"vaiseshika
sutra"
was
known
and
this
manuscript
was
part
of
a
bhasya
by
the
15th
century
sankaramisra
scholars
had
doubted
its
authenticity
given
the
inconsistencies
in
this
manuscript
and
the
quotes
in
other
hindu
jaina
and
buddhist
literature
claiming
to
be
from
the
"vaisheshika
sutra"
in
the
1950s
and
early
1960s
new
manuscripts
of
"vaiśeṣika
sūtra"
were
discovered
in
distant
parts
of
india
which
were
later
identified
as
this
sutra
these
newer
manuscripts
are
quite
different
more
consistent
with
the
historical
literature
and
suggests
that
like
other
major
texts
and
scriptures
of
hinduism
"vaiśeṣika
sūtra"
too
suffered
interpolations
errors
in
transmission
and
distortion
over
time
a
critical
edition
of
the
"vaiśeṣika
sūtra"
is
now
available
the
"vaisheshika
sutras"
mention
the
doctrines
of
competing
schools
of
indian
philosophy
such
as
samkhya
and
mimamsa
but
make
no
mention
of
buddhism
which
has
led
scholars
in
more
recent
publications
to
posit
estimates
of
6th
to
2nd
century
bce
the
critical
edition
studies
of
"vaisheshika
sutras"
manuscripts
discovered
after
1950
suggest
that
the
text
attributed
to
kanada
existed
in
a
finalized
form
sometime
between
200
bce
and
the
start
of
the
common
era
with
the
possibility
that
its
key
doctrines
are
much
older
multiple
hindu
texts
dated
to
the
1st
and
2nd
century
ce
such
as
the
"mahavibhasa"
and
"jnanaprasthana"
from
the
kushan
empire
quote
and
comment
on
kanada's
doctrines
although
the
"vaisheshika
sutras"
makes
no
mention
of
the
doctrines
of
jainism
and
buddhism
their
ancient
texts
mention
"vaisheshika
sutras"
doctrines
and
use
its
terminology
particularly
buddhism's
sarvastivada
tradition
as
well
as
the
works
of
nagarjuna
physics
is
central
to
kaṇāda’s
assertion
that
all
that
is
knowable
is
based
on
motion
his
ascribing
centrality
to
physics
in
the
understanding
of
the
universe
also
follows
from
his
invariance
principles
for
example
he
says
that
the
atom
must
be
spherical
since
it
should
be
the
same
in
all
dimensions
he
asserts
that
all
substances
are
composed
of
atoms
two
of
which
have
mass
and
two
are
massless
the
philosophy
in
"vaiseshika
sutra"
is
atomistic
pluralism
states
jayatilleke
its
ideas
are
known
for
its
contributions
to
"inductive
inference"
and
often
coupled
with
the
"deductive
logic"
of
the
sister
school
of
hinduism
called
the
nyaya
james
thrower
and
others
call
vaiśeṣika
philosophy
to
be
naturalism
one
that
rejects
the
supernatural
the
text
states:
several
traits
of
substances
(dravya)
are
given
as
colour
taste
smell
touch
number
size
the
separate
coupling
and
uncoupling
priority
and
posterity
comprehension
pleasure
and
pain
attraction
and
revulsion
and
wishes
like
many
foundational
texts
of
classical
schools
of
hindu
philosophy
god
is
not
mentioned
in
the
sutra
and
the
text
is
non-theistic
the
critical
edition
of
the
"vaisheshika
sutras"
are
divided
into
ten
chapters
each
subdivided
into
two
sections
called
āhnikas:
gapped
hamiltonian
in
many-body
physics
most
commonly
within
condensed-matter
physics
a
gapped
hamiltonian
is
a
hamiltonian
for
an
infinitely
large
many-body
system
where
there
is
a
finite
energy
gap
separating
the
(possibly
degenerate)
ground
space
from
the
first
excited
states
a
hamiltonian
that
is
not
gapped
is
called
gapless
the
property
of
being
gapped
or
gapless
is
formally
defined
through
a
sequence
of
hamiltonians
on
finite
lattices
in
the
thermodynamic
limit
an
example
is
the
bcs
hamiltonian
in
the
theory
of
superconductivity
in
quantum
field
theory
a
continuum
limit
of
many-body
physics
a
gapped
hamiltonian
induces
a
mass
gap
mixlink
mixlink
(mixlink
ii)
is
a
computer
used
with
agfa
scales
it
was
developed
to
facilitate
calculation
of
color
mixes
mixlink
contains
the
intel
80386
processor
with
the
clock
rate
which
may
be
set
to
either
92
or
33 mhz
the
ram
size
is
640
kb
the
hdd
function
is
served
by
the
built-in
flash
memory
that
has
the
size
of
approximately
800
kb
mixlink
has
the
monochrome
display
mixlink
is
also
staffed
with
cd
drive
and
floppy
drive
mixlink
was
intended
to
be
used
with
the
supplied
floppy
and
cd
disk
which
provided
the
system
environment
("operating
system")
and
the
application
to
be
used
for
calculating
color
mixes
however
it
is
possible
to
install
and
run
ms-dos
on
mixlink
currently
mixlink
computers
are
not
used;
their
functions
may
solely
be
performed
by
personal
computers
(pcs)
outline
of
computers
the
following
outline
is
provided
as
an
overview
of
and
topical
guide
to
computers:
computers
–
programmable
machines
designed
to
automatically
carry
out
sequences
of
arithmetic
or
logical
operations
the
sequences
of
operations
can
be
changed
readily
allowing
computers
to
solve
more
than
one
kind
of
problem
computers
can
be
described
as
all
of
the
following:
computer
architecture
–
history
of
computing
hardware
hp
toshiba
dell
apple
acer
asus
software
development
–
computer
magazines
–
"see
list
of
computer
magazines"
online
–
electronic
media
and
sleep
the
use
of
computers
(including
devices
such
as
smartphones
tablet
computers
and
laptops)
by
children
and
adolescents
before
bed
has
been
associated
with
a
reduction
in
the
hours
of
sleep
experienced
by
frequent
users
along
with
a
decreased
quality
of
sleep
in
most
cases
the
results
of
computer
use
at
night
have
been
linked
with
tiredness
a
2010
review
concluded
that
"the
use
of
electronic
media
by
children
and
adolescents
does
have
a
negative
impact
on
their
sleep
although
the
precise
effects
and
mechanisms
remain
unclear"
with
the
most
consistent
results
associating
excessive
media
use
with
shorter
sleep
duration
and
delayed
bed
times
a
2016
meta-analysis
found
that
"bedtime
access
and
use
of
media
devices
was
significantly
associated
with
inadequate
sleep
quantity;
poor
sleep
quality;
and
excessive
daytime
sleepiness"
the
american
academy
of
pediatrics
recommends
screen
time
for
children
be
limited
for
multiple
reasons
among
them
that
"too
much
screen
time
can
also
harm
the
amount
and
quality
of
sleep"
many
apps
promise
to
improve
sleep
by
filtering
out
blue
light
produced
by
media
devices;
there
have
been
no
large
studies
to
assess
whether
such
apps
work
some
users
express
dissatisfaction
with
the
resultant
orange
tint
of
screens
some
people
use
blue-blocking
glasses
for
the
purpose
of
attempting
to
block
out
blue
light
both
from
electronic
media
and
from
other
artificial
light
sources
computer
a
computer
is
a
device
that
can
be
instructed
to
carry
out
sequences
of
arithmetic
or
logical
operations
automatically
via
computer
programming
modern
computers
have
the
ability
to
follow
generalized
sets
of
operations
called
"programs"
these
programs
enable
computers
to
perform
an
extremely
wide
range
of
tasks
a
"complete"
computer
including
the
hardware
the
operating
system
(main
software)
and
peripheral
equipment
required
and
used
for
"full"
operation
can
be
referred
to
as
a
computer
system
this
term
may
as
well
be
used
for
a
group
of
computers
that
are
connected
and
work
together
in
particular
a
computer
network
or
computer
cluster
computers
are
used
as
control
systems
for
a
wide
variety
of
industrial
and
consumer
devices
this
includes
simple
special
purpose
devices
like
microwave
ovens
and
remote
controls
factory
devices
such
as
industrial
robots
and
computer-aided
design
and
also
general
purpose
devices
like
personal
computers
and
mobile
devices
such
as
smartphones
the
internet
is
run
on
computers
and
it
connects
hundreds
of
millions
of
other
computers
and
their
users
early
computers
were
only
conceived
as
calculating
devices
since
ancient
times
simple
manual
devices
like
the
abacus
aided
people
in
doing
calculations
early
in
the
industrial
revolution
some
mechanical
devices
were
built
to
automate
long
tedious
tasks
such
as
guiding
patterns
for
looms
more
sophisticated
electrical
machines
did
specialized
analog
calculations
in
the
early
20th
century
the
first
digital
electronic
calculating
machines
were
developed
during
world
war
ii
the
speed
power
and
versatility
of
computers
have
been
increasing
dramatically
ever
since
then
conventionally
a
modern
computer
consists
of
at
least
one
processing
element
typically
a
central
processing
unit
(cpu)
and
some
form
of
memory
the
processing
element
carries
out
arithmetic
and
logical
operations
and
a
sequencing
and
control
unit
can
change
the
order
of
operations
in
response
to
stored
information
peripheral
devices
include
input
devices
(keyboards
mice
joystick
etc)
output
devices
(monitor
screens
printers
etc)
and
input/output
devices
that
perform
both
functions
(eg
the
2000s-era
touchscreen)
peripheral
devices
allow
information
to
be
retrieved
from
an
external
source
and
they
enable
the
result
of
operations
to
be
saved
and
retrieved
according
to
the
"oxford
english
dictionary"
the
first
known
use
of
the
word
"computer"
was
in
1613
in
a
book
called
"the
yong
mans
gleanings"
by
english
writer
richard
braithwait:
"i
haue
[sic]
read
the
truest
computer
of
times
and
the
best
arithmetician
that
euer
[sic]
breathed
and
he
reduceth
thy
dayes
into
a
short
number"
this
usage
of
the
term
referred
to
a
human
computer
a
person
who
carried
out
calculations
or
computations
the
word
continued
with
the
same
meaning
until
the
middle
of
the
20th
century
during
the
latter
part
of
this
period
women
were
often
hired
as
computers
because
they
could
be
paid
less
than
their
male
counterparts
by
1943
most
human
computers
were
women
from
the
end
of
the
19th
century
the
word
slowly
began
to
take
on
its
more
familiar
meaning
a
machine
that
carries
out
computations
the
"online
etymology
dictionary"
gives
the
first
attested
use
of
"computer"
in
the
"1640s
[meaning]
"one
who
calculates";
this
is
an
" agent
noun
from
compute
(v)"
the
"online
etymology
dictionary"
states
that
the
use
of
the
term
to
mean
"calculating
machine"
(of
any
type)
is
from
1897"
the
"online
etymology
dictionary"
indicates
that
the
"modern
use"
of
the
term
to
mean
"programmable
digital
electronic
computer"
dates
from
" 1945
under
this
name;
[in
a]
theoretical
[sense]
from
1937
as
turing
machine"
devices
have
been
used
to
aid
computation
for
thousands
of
years
mostly
using
one-to-one
correspondence
with
fingers
the
earliest
counting
device
was
probably
a
form
of
tally
stick
later
record
keeping
aids
throughout
the
fertile
crescent
included
calculi
(clay
spheres
cones
etc)
which
represented
counts
of
items
probably
livestock
or
grains
sealed
in
hollow
unbaked
clay
containers
the
use
of
counting
rods
is
one
example
the
abacus
was
initially
used
for
arithmetic
tasks
the
roman
abacus
was
developed
from
devices
used
in
babylonia
as
early
as
2400
bc
since
then
many
other
forms
of
reckoning
boards
or
tables
have
been
invented
in
a
medieval
european
counting
house
a
checkered
cloth
would
be
placed
on
a
table
and
markers
moved
around
on
it
according
to
certain
rules
as
an
aid
to
calculating
sums
of
money
the
antikythera
mechanism
is
believed
to
be
the
earliest
mechanical
analog
"computer"
according
to
derek
j
de
solla
price
it
was
designed
to
calculate
astronomical
positions
it
was
discovered
in
1901
in
the
antikythera
wreck
off
the
greek
island
of
antikythera
between
kythera
and
crete
and
has
been
dated
to
devices
of
a
level
of
complexity
comparable
to
that
of
the
antikythera
mechanism
would
not
reappear
until
a
thousand
years
later
many
mechanical
aids
to
calculation
and
measurement
were
constructed
for
astronomical
and
navigation
use
the
planisphere
was
a
star
chart
invented
by
abū
rayhān
al-bīrūnī
in
the
early
11th
century
the
astrolabe
was
invented
in
the
hellenistic
world
in
either
the
1st
or
2nd
centuries
bc
and
is
often
attributed
to
hipparchus
a
combination
of
the
planisphere
and
dioptra
the
astrolabe
was
effectively
an
analog
computer
capable
of
working
out
several
different
kinds
of
problems
in
spherical
astronomy
an
astrolabe
incorporating
a
mechanical
calendar
computer
and
gear-wheels
was
invented
by
abi
bakr
of
isfahan
persia
in
1235
abū
rayhān
al-bīrūnī
invented
the
first
mechanical
geared
lunisolar
calendar
astrolabe
an
early
fixed-wired
knowledge
processing
machine
with
a
gear
train
and
gear-wheels
the
sector
a
calculating
instrument
used
for
solving
problems
in
proportion
trigonometry
multiplication
and
division
and
for
various
functions
such
as
squares
and
cube
roots
was
developed
in
the
late
16th
century
and
found
application
in
gunnery
surveying
and
navigation
the
planimeter
was
a
manual
instrument
to
calculate
the
area
of
a
closed
figure
by
tracing
over
it
with
a
mechanical
linkage
the
slide
rule
was
invented
around
1620–1630
shortly
after
the
publication
of
the
concept
of
the
logarithm
it
is
a
hand-operated
analog
computer
for
doing
multiplication
and
division
as
slide
rule
development
progressed
added
scales
provided
reciprocals
squares
and
square
roots
cubes
and
cube
roots
as
well
as
transcendental
functions
such
as
logarithms
and
exponentials
circular
and
hyperbolic
trigonometry
and
other
functions
slide
rules
with
special
scales
are
still
used
for
quick
performance
of
routine
calculations
such
as
the
e6b
circular
slide
rule
used
for
time
and
distance
calculations
on
light
aircraft
in
the
1770s
pierre
jaquet-droz
a
swiss
watchmaker
built
a
mechanical
doll
(automaton)
that
could
write
holding
a
quill
pen
by
switching
the
number
and
order
of
its
internal
wheels
different
letters
and
hence
different
messages
could
be
produced
in
effect
it
could
be
mechanically
"programmed"
to
read
instructions
along
with
two
other
complex
machines
the
doll
is
at
the
musée
d'art
et
d'histoire
of
neuchâtel
switzerland
and
still
operates
the
tide-predicting
machine
invented
by
sir
william
thomson
in
1872
was
of
great
utility
to
navigation
in
shallow
waters
it
used
a
system
of
pulleys
and
wires
to
automatically
calculate
predicted
tide
levels
for
a
set
period
at
a
particular
location
the
differential
analyser
a
mechanical
analog
computer
designed
to
solve
differential
equations
by
integration
used
wheel-and-disc
mechanisms
to
perform
the
integration
in
1876
lord
kelvin
had
already
discussed
the
possible
construction
of
such
calculators
but
he
had
been
stymied
by
the
limited
output
torque
of
the
ball-and-disk
integrators
in
a
differential
analyzer
the
output
of
one
integrator
drove
the
input
of
the
next
integrator
or
a
graphing
output
the
torque
amplifier
was
the
advance
that
allowed
these
machines
to
work
starting
in
the
1920s
vannevar
bush
and
others
developed
mechanical
differential
analyzers
charles
babbage
an
english
mechanical
engineer
and
polymath
originated
the
concept
of
a
programmable
computer
considered
the
"father
of
the
computer"
he
conceptualized
and
invented
the
first
mechanical
computer
in
the
early
19th
century
after
working
on
his
revolutionary
difference
engine
designed
to
aid
in
navigational
calculations
in
1833
he
realized
that
a
much
more
general
design
an
analytical
engine
was
possible
the
input
of
programs
and
data
was
to
be
provided
to
the
machine
via
punched
cards
a
method
being
used
at
the
time
to
direct
mechanical
looms
such
as
the
jacquard
loom
for
output
the
machine
would
have
a
printer
a
curve
plotter
and
a
bell
the
machine
would
also
be
able
to
punch
numbers
onto
cards
to
be
read
in
later
the
engine
incorporated
an
arithmetic
logic
unit
control
flow
in
the
form
of
conditional
branching
and
loops
and
integrated
memory
making
it
the
first
design
for
a
general-purpose
computer
that
could
be
described
in
modern
terms
as
turing-complete
the
machine
was
about
a
century
ahead
of
its
time
all
the
parts
for
his
machine
had
to
be
made
by
hand –
this
was
a
major
problem
for
a
device
with
thousands
of
parts
eventually
the
project
was
dissolved
with
the
decision
of
the
british
government
to
cease
funding
babbage's
failure
to
complete
the
analytical
engine
can
be
chiefly
attributed
to
difficulties
not
only
of
politics
and
financing
but
also
to
his
desire
to
develop
an
increasingly
sophisticated
computer
and
to
move
ahead
faster
than
anyone
else
could
follow
nevertheless
his
son
henry
babbage
completed
a
simplified
version
of
the
analytical
engine's
computing
unit
(the
"mill")
in
1888
he
gave
a
successful
demonstration
of
its
use
in
computing
tables
in
1906
during
the
first
half
of
the
20th
century
many
scientific
computing
needs
were
met
by
increasingly
sophisticated
analog
computers
which
used
a
direct
mechanical
or
electrical
model
of
the
problem
as
a
basis
for
computation
however
these
were
not
programmable
and
generally
lacked
the
versatility
and
accuracy
of
modern
digital
computers
the
first
modern
analog
computer
was
a
tide-predicting
machine
invented
by
sir
william
thomson
in
1872
the
differential
analyser
a
mechanical
analog
computer
designed
to
solve
differential
equations
by
integration
using
wheel-and-disc
mechanisms
was
conceptualized
in
1876
by
james
thomson
the
brother
of
the
more
famous
lord
kelvin
the
art
of
mechanical
analog
computing
reached
its
zenith
with
the
differential
analyzer
built
by
h
l
hazen
and
vannevar
bush
at
mit
starting
in
1927
this
built
on
the
mechanical
integrators
of
james
thomson
and
the
torque
amplifiers
invented
by
h
w
nieman
a
dozen
of
these
devices
were
built
before
their
obsolescence
became
obvious
by
the
1950s
the
success
of
digital
electronic
computers
had
spelled
the
end
for
most
analog
computing
machines
but
analog
computers
remained
in
use
during
the
1950s
in
some
specialized
applications
such
as
education
(control
systems)
and
aircraft
(slide
rule)
by
1938
the
united
states
navy
had
developed
an
electromechanical
analog
computer
small
enough
to
use
aboard
a
submarine
this
was
the
torpedo
data
computer
which
used
trigonometry
to
solve
the
problem
of
firing
a
torpedo
at
a
moving
target
during
world
war
ii
similar
devices
were
developed
in
other
countries
as
well
early
digital
computers
were
electromechanical;
electric
switches
drove
mechanical
relays
to
perform
the
calculation
these
devices
had
a
low
operating
speed
and
were
eventually
superseded
by
much
faster
all-electric
computers
originally
using
vacuum
tubes
the
z2
created
by
german
engineer
konrad
zuse
in
1939
was
one
of
the
earliest
examples
of
an
electromechanical
relay
computer
in
1941
zuse
followed
his
earlier
machine
up
with
the
z3
the
world's
first
working
electromechanical
programmable
fully
automatic
digital
computer
the
z3
was
built
with
2000
relays
implementing
a
22 bit
word
length
that
operated
at
a
clock
frequency
of
about
5–10 hz
program
code
was
supplied
on
punched
film
while
data
could
be
stored
in
64
words
of
memory
or
supplied
from
the
keyboard
it
was
quite
similar
to
modern
machines
in
some
respects
pioneering
numerous
advances
such
as
floating
point
numbers
rather
than
the
harder-to-implement
decimal
system
(used
in
charles
babbage's
earlier
design)
using
a
binary
system
meant
that
zuse's
machines
were
easier
to
build
and
potentially
more
reliable
given
the
technologies
available
at
that
time
the
z3
was
turing
complete
purely
electronic
circuit
elements
soon
replaced
their
mechanical
and
electromechanical
equivalents
at
the
same
time
that
digital
calculation
replaced
analog
the
engineer
tommy
flowers
working
at
the
post
office
research
station
in
london
in
the
1930s
began
to
explore
the
possible
use
of
electronics
for
the
telephone
exchange
experimental
equipment
that
he
built
in
1934
went
into
operation
five
years
later
converting
a
portion
of
the
telephone
exchange
network
into
an
electronic
data
processing
system
using
thousands
of
vacuum
tubes
in
the
us
john
vincent
atanasoff
and
clifford
e
berry
of
iowa
state
university
developed
and
tested
the
atanasoff–berry
computer
(abc)
in
1942
the
first
"automatic
electronic
digital
computer"
this
design
was
also
all-electronic
and
used
about
300
vacuum
tubes
with
capacitors
fixed
in
a
mechanically
rotating
drum
for
memory
during
world
war
ii
the
british
at
bletchley
park
achieved
a
number
of
successes
at
breaking
encrypted
german
military
communications
the
german
encryption
machine
enigma
was
first
attacked
with
the
help
of
the
electro-mechanical
bombes
which
were
often
run
by
women
to
crack
the
more
sophisticated
german
lorenz
sz
40/42
machine
used
for
high-level
army
communications
max
newman
and
his
colleagues
commissioned
flowers
to
build
the
colossus
he
spent
eleven
months
from
early
february
1943
designing
and
building
the
first
colossus
after
a
functional
test
in
december
1943
colossus
was
shipped
to
bletchley
park
where
it
was
delivered
on
18
january
1944
and
attacked
its
first
message
on
5
february
colossus
was
the
world's
first
electronic
digital
programmable
computer
it
used
a
large
number
of
valves
(vacuum
tubes)
it
had
paper-tape
input
and
was
capable
of
being
configured
to
perform
a
variety
of
boolean
logical
operations
on
its
data
but
it
was
not
turing-complete
nine
mk
ii
colossi
were
built
(the
mk
i
was
converted
to
a
mk
ii
making
ten
machines
in
total)
colossus
mark
i
contained
1500
thermionic
valves
(tubes)
but
mark
ii
with
2400
valves
was
both
5
times
faster
and
simpler
to
operate
than
mark
i
greatly
speeding
the
decoding
process
the
us-built
eniac
(electronic
numerical
integrator
and
computer)
was
the
first
electronic
programmable
computer
built
in
the
us
although
the
eniac
was
similar
to
the
colossus
it
was
much
faster
more
flexible
and
it
was
turing-complete
like
the
colossus
a
"program"
on
the
eniac
was
defined
by
the
states
of
its
patch
cables
and
switches
a
far
cry
from
the
stored
program
electronic
machines
that
came
later
once
a
program
was
written
it
had
to
be
mechanically
set
into
the
machine
with
manual
resetting
of
plugs
and
switches
the
programmers
of
the
eniac
were
six
women
often
known
collectively
as
the
"eniac
girls"
it
combined
the
high
speed
of
electronics
with
the
ability
to
be
programmed
for
many
complex
problems
it
could
add
or
subtract
5000
times
a
second
a
thousand
times
faster
than
any
other
machine
it
also
had
modules
to
multiply
divide
and
square
root
high
speed
memory
was
limited
to
20
words
(about
80
bytes)
built
under
the
direction
of
john
mauchly
and
j
presper
eckert
at
the
university
of
pennsylvania
eniac's
development
and
construction
lasted
from
1943
to
full
operation
at
the
end
of
1945
the
machine
was
huge
weighing
30
tons
using
200
kilowatts
of
electric
power
and
contained
over
18000
vacuum
tubes
1500
relays
and
hundreds
of
thousands
of
resistors
capacitors
and
inductors
the
principle
of
the
modern
computer
was
proposed
by
alan
turing
in
his
seminal
1936
paper
"on
computable
numbers"
turing
proposed
a
simple
device
that
he
called
"universal
computing
machine"
and
that
is
now
known
as
a
universal
turing
machine
he
proved
that
such
a
machine
is
capable
of
computing
anything
that
is
computable
by
executing
instructions
(program)
stored
on
tape
allowing
the
machine
to
be
programmable
the
fundamental
concept
of
turing's
design
is
the
stored
program
where
all
the
instructions
for
computing
are
stored
in
memory
von
neumann
acknowledged
that
the
central
concept
of
the
modern
computer
was
due
to
this
paper
turing
machines
are
to
this
day
a
central
object
of
study
in
theory
of
computation
except
for
the
limitations
imposed
by
their
finite
memory
stores
modern
computers
are
said
to
be
turing-complete
which
is
to
say
they
have
algorithm
execution
capability
equivalent
to
a
universal
turing
machine
early
computing
machines
had
fixed
programs
changing
its
function
required
the
re-wiring
and
re-structuring
of
the
machine
with
the
proposal
of
the
stored-program
computer
this
changed
a
stored-program
computer
includes
by
design
an
instruction
set
and
can
store
in
memory
a
set
of
instructions
(a
program)
that
details
the
computation
the
theoretical
basis
for
the
stored-program
computer
was
laid
by
alan
turing
in
his
1936
paper
in
1945
turing
joined
the
national
physical
laboratory
and
began
work
on
developing
an
electronic
stored-program
digital
computer
his
1945
report
"proposed
electronic
calculator"
was
the
first
specification
for
such
a
device
john
von
neumann
at
the
university
of
pennsylvania
also
circulated
his
"first
draft
of
a
report
on
the
edvac"
in
1945
the
manchester
baby
was
the
world's
first
stored-program
computer
it
was
built
at
the
victoria
university
of
manchester
by
frederic
c
williams
tom
kilburn
and
geoff
tootill
and
ran
its
first
program
on
21
june
1948
it
was
designed
as
a
testbed
for
the
williams
tube
the
first
random-access
digital
storage
device
although
the
computer
was
considered
"small
and
primitive"
by
the
standards
of
its
time
it
was
the
first
working
machine
to
contain
all
of
the
elements
essential
to
a
modern
electronic
computer
as
soon
as
the
baby
had
demonstrated
the
feasibility
of
its
design
a
project
was
initiated
at
the
university
to
develop
it
into
a
more
usable
computer
the
manchester
mark
1
grace
hopper
was
the
first
person
to
develop
a
compiler
for
programming
language
the
mark
1
in
turn
quickly
became
the
prototype
for
the
ferranti
mark
1
the
world's
first
commercially
available
general-purpose
computer
built
by
ferranti
it
was
delivered
to
the
university
of
manchester
in
february
1951
at
least
seven
of
these
later
machines
were
delivered
between
1953
and
1957
one
of
them
to
shell
labs
in
amsterdam
in
october
1947
the
directors
of
british
catering
company
j
lyons
company
decided
to
take
an
active
role
in
promoting
the
commercial
development
of
computers
the
leo
i
computer
became
operational
in
april
1951
and
ran
the
world's
first
regular
routine
office
computer
job
the
bipolar
transistor
was
invented
in
1947
from
1955
onwards
transistors
replaced
vacuum
tubes
in
computer
designs
giving
rise
to
the
"second
generation"
of
computers
compared
to
vacuum
tubes
transistors
have
many
advantages:
they
are
smaller
and
require
less
power
than
vacuum
tubes
so
give
off
less
heat
silicon
junction
transistors
were
much
more
reliable
than
vacuum
tubes
and
had
longer
indefinite
service
life
transistorized
computers
could
contain
tens
of
thousands
of
binary
logic
circuits
in
a
relatively
compact
space
at
the
university
of
manchester
a
team
under
the
leadership
of
tom
kilburn
designed
and
built
a
machine
using
the
newly
developed
transistors
instead
of
valves
their
first
transistorised
computer
and
the
first
in
the
world
was
operational
by
1953
and
a
second
version
was
completed
there
in
april
1955
however
the
machine
did
make
use
of
valves
to
generate
its
125 khz
clock
waveforms
and
in
the
circuitry
to
read
and
write
on
its
magnetic
drum
memory
so
it
was
not
the
first
completely
transistorized
computer
that
distinction
goes
to
the
harwell
cadet
of
1955
built
by
the
electronics
division
of
the
atomic
energy
research
establishment
at
harwell
the
next
great
advance
in
computing
power
came
with
the
advent
of
the
integrated
circuit
the
idea
of
the
integrated
circuit
was
first
conceived
by
a
radar
scientist
working
for
the
royal
radar
establishment
of
the
ministry
of
defence
geoffrey
wa
dummer
dummer
presented
the
first
public
description
of
an
integrated
circuit
at
the
symposium
on
progress
in
quality
electronic
components
in
washington dc
on
7
may
1952
the
first
practical
ics
were
invented
by
jack
kilby
at
texas
instruments
and
robert
noyce
at
fairchild
semiconductor
kilby
recorded
his
initial
ideas
concerning
the
integrated
circuit
in
july
1958
successfully
demonstrating
the
first
working
integrated
example
on
12
september
1958
in
his
patent
application
of
6
february
1959
kilby
described
his
new
device
as
"a
body
of
semiconductor
material 
wherein
all
the
components
of
the
electronic
circuit
are
completely
integrated"
noyce
also
came
up
with
his
own
idea
of
an
integrated
circuit
half
a
year
later
than
kilby
his
chip
solved
many
practical
problems
that
kilby's
had
not
produced
at
fairchild
semiconductor
it
was
made
of
silicon
whereas
kilby's
chip
was
made
of
germanium
this
new
development
heralded
an
explosion
in
the
commercial
and
personal
use
of
computers
and
led
to
the
invention
of
the
microprocessor
while
the
subject
of
exactly
which
device
was
the
first
microprocessor
is
contentious
partly
due
to
lack
of
agreement
on
the
exact
definition
of
the
term
"microprocessor"
it
is
largely
undisputed
that
the
first
single-chip
microprocessor
was
the
intel
4004
designed
and
realized
by
ted
hoff
federico
faggin
and
stanley
mazor
at
intel
the
first
mobile
computers
were
heavy
and
ran
from
mains
power
the
50lb
ibm
5100
was
an
early
example
later
portables
such
as
the
osborne
1
and
compaq
portable
were
considerably
lighter
but
still
needed
to
be
plugged
in
the
first
laptops
such
as
the
grid
compass
removed
this
requirement
by
incorporating
batteries
–
and
with
the
continued
miniaturization
of
computing
resources
and
advancements
in
portable
battery
life
portable
computers
grew
in
popularity
in
the
2000s
the
same
developments
allowed
manufacturers
to
integrate
computing
resources
into
cellular
phones
these
smartphones
and
tablets
run
on
a
variety
of
operating
systems
and
soon
became
the
dominant
computing
device
on
the
market
with
manufacturers
reporting
having
shipped
an
estimated
237
million
devices
in
2q
2013
computers
are
typically
classified
based
on
their
uses:
the
term
"hardware"
covers
all
of
those
parts
of
a
computer
that
are
tangible
physical
objects
circuits
computer
chips
graphic
cards
sound
cards
memory
(ram)
motherboard
displays
power
supplies
cables
keyboards
printers
and
"mice"
input
devices
are
all
hardware
a
general
purpose
computer
has
four
main
components:
the
arithmetic
logic
unit
(alu)
the
control
unit
the
memory
and
the
input
and
output
devices
(collectively
termed
i/o)
these
parts
are
interconnected
by
buses
often
made
of
groups
of
wires
inside
each
of
these
parts
are
thousands
to
trillions
of
small
electrical
circuits
which
can
be
turned
off
or
on
by
means
of
an
electronic
switch
each
circuit
represents
a
bit
(binary
digit)
of
information
so
that
when
the
circuit
is
on
it
represents
a
"1"
and
when
off
it
represents
a
"0"
(in
positive
logic
representation)
the
circuits
are
arranged
in
logic
gates
so
that
one
or
more
of
the
circuits
may
control
the
state
of
one
or
more
of
the
other
circuits
when
unprocessed
data
is
sent
to
the
computer
with
the
help
of
input
devices
the
data
is
processed
and
sent
to
output
devices
the
input
devices
may
be
hand-operated
or
automated
the
act
of
processing
is
mainly
regulated
by
the
cpu
some
examples
of
input
devices
are:
the
means
through
which
computer
gives
output
are
known
as
output
devices
some
examples
of
output
devices
are:
the
control
unit
(often
called
a
control
system
or
central
controller)
manages
the
computer's
various
components;
it
reads
and
interprets
(decodes)
the
program
instructions
transforming
them
into
control
signals
that
activate
other
parts
of
the
computer
control
systems
in
advanced
computers
may
change
the
order
of
execution
of
some
instructions
to
improve
performance
a
key
component
common
to
all
cpus
is
the
program
counter
a
special
memory
cell
(a
register)
that
keeps
track
of
which
location
in
memory
the
next
instruction
is
to
be
read
from
the
control
system's
function
is
as
follows—note
that
this
is
a
simplified
description
and
some
of
these
steps
may
be
performed
concurrently
or
in
a
different
order
depending
on
the
type
of
cpu:
since
the
program
counter
is
(conceptually)
just
another
set
of
memory
cells
it
can
be
changed
by
calculations
done
in
the
alu
adding
100
to
the
program
counter
would
cause
the
next
instruction
to
be
read
from
a
place
100
locations
further
down
the
program
instructions
that
modify
the
program
counter
are
often
known
as
"jumps"
and
allow
for
loops
(instructions
that
are
repeated
by
the
computer)
and
often
conditional
instruction
execution
(both
examples
of
control
flow)
the
sequence
of
operations
that
the
control
unit
goes
through
to
process
an
instruction
is
in
itself
like
a
short
computer
program
and
indeed
in
some
more
complex
cpu
designs
there
is
another
yet
smaller
computer
called
a
microsequencer
which
runs
a
microcode
program
that
causes
all
of
these
events
to
happen
the
control
unit
alu
and
registers
are
collectively
known
as
a
central
processing
unit
(cpu)
early
cpus
were
composed
of
many
separate
components
but
since
the
mid-1970s
cpus
have
typically
been
constructed
on
a
single
integrated
circuit
called
a
"microprocessor"
the
alu
is
capable
of
performing
two
classes
of
operations:
arithmetic
and
logic
the
set
of
arithmetic
operations
that
a
particular
alu
supports
may
be
limited
to
addition
and
subtraction
or
might
include
multiplication
division
trigonometry
functions
such
as
sine
cosine
etc
and
square
roots
some
can
only
operate
on
whole
numbers
(integers)
while
others
use
floating
point
to
represent
real
numbers
albeit
with
limited
precision
however
any
computer
that
is
capable
of
performing
just
the
simplest
operations
can
be
programmed
to
break
down
the
more
complex
operations
into
simple
steps
that
it
can
perform
therefore
any
computer
can
be
programmed
to
perform
any
arithmetic
operation—although
it
will
take
more
time
to
do
so
if
its
alu
does
not
directly
support
the
operation
an
alu
may
also
compare
numbers
and
return
boolean
truth
values
(true
or
false)
depending
on
whether
one
is
equal
to
greater
than
or
less
than
the
other
("is
64
greater
than
65?")
logic
operations
involve
boolean
logic:
and
or
xor
and
not
these
can
be
useful
for
creating
complicated
conditional
statements
and
processing
boolean
logic
superscalar
computers
may
contain
multiple
alus
allowing
them
to
process
several
instructions
simultaneously
graphics
processors
and
computers
with
simd
and
mimd
features
often
contain
alus
that
can
perform
arithmetic
on
vectors
and
matrices
a
computer's
memory
can
be
viewed
as
a
list
of
cells
into
which
numbers
can
be
placed
or
read
each
cell
has
a
numbered
"address"
and
can
store
a
single
number
the
computer
can
be
instructed
to
"put
the
number
123
into
the
cell
numbered
1357"
or
to
"add
the
number
that
is
in
cell
1357
to
the
number
that
is
in
cell
2468
and
put
the
answer
into
cell
1595"
the
information
stored
in
memory
may
represent
practically
anything
letters
numbers
even
computer
instructions
can
be
placed
into
memory
with
equal
ease
since
the
cpu
does
not
differentiate
between
different
types
of
information
it
is
the
software's
responsibility
to
give
significance
to
what
the
memory
sees
as
nothing
but
a
series
of
numbers
in
almost
all
modern
computers
each
memory
cell
is
set
up
to
store
binary
numbers
in
groups
of
eight
bits
(called
a
byte)
each
byte
is
able
to
represent
256
different
numbers
(2
=
256);
either
from
0
to
255
or
−128
to
+127
to
store
larger
numbers
several
consecutive
bytes
may
be
used
(typically
two
four
or
eight)
when
negative
numbers
are
required
they
are
usually
stored
in
two's
complement
notation
other
arrangements
are
possible
but
are
usually
not
seen
outside
of
specialized
applications
or
historical
contexts
a
computer
can
store
any
kind
of
information
in
memory
if
it
can
be
represented
numerically
modern
computers
have
billions
or
even
trillions
of
bytes
of
memory
the
cpu
contains
a
special
set
of
memory
cells
called
registers
that
can
be
read
and
written
to
much
more
rapidly
than
the
main
memory
area
there
are
typically
between
two
and
one
hundred
registers
depending
on
the
type
of
cpu
registers
are
used
for
the
most
frequently
needed
data
items
to
avoid
having
to
access
main
memory
every
time
data
is
needed
as
data
is
constantly
being
worked
on
reducing
the
need
to
access
main
memory
(which
is
often
slow
compared
to
the
alu
and
control
units)
greatly
increases
the
computer's
speed
computer
main
memory
comes
in
two
principal
varieties:
ram
can
be
read
and
written
to
anytime
the
cpu
commands
it
but
rom
is
preloaded
with
data
and
software
that
never
changes
therefore
the
cpu
can
only
read
from
it
rom
is
typically
used
to
store
the
computer's
initial
start-up
instructions
in
general
the
contents
of
ram
are
erased
when
the
power
to
the
computer
is
turned
off
but
rom
retains
its
data
indefinitely
in
a
pc
the
rom
contains
a
specialized
program
called
the
bios
that
orchestrates
loading
the
computer's
operating
system
from
the
hard
disk
drive
into
ram
whenever
the
computer
is
turned
on
or
reset
in
embedded
computers
which
frequently
do
not
have
disk
drives
all
of
the
required
software
may
be
stored
in
rom
software
stored
in
rom
is
often
called
firmware
because
it
is
notionally
more
like
hardware
than
software
flash
memory
blurs
the
distinction
between
rom
and
ram
as
it
retains
its
data
when
turned
off
but
is
also
rewritable
it
is
typically
much
slower
than
conventional
rom
and
ram
however
so
its
use
is
restricted
to
applications
where
high
speed
is
unnecessary
in
more
sophisticated
computers
there
may
be
one
or
more
ram
cache
memories
which
are
slower
than
registers
but
faster
than
main
memory
generally
computers
with
this
sort
of
cache
are
designed
to
move
frequently
needed
data
into
the
cache
automatically
often
without
the
need
for
any
intervention
on
the
programmer's
part
i/o
is
the
means
by
which
a
computer
exchanges
information
with
the
outside
world
devices
that
provide
input
or
output
to
the
computer
are
called
peripherals
on
a
typical
personal
computer
peripherals
include
input
devices
like
the
keyboard
and
mouse
and
output
devices
such
as
the
display
and
printer
hard
disk
drives
floppy
disk
drives
and
optical
disc
drives
serve
as
both
input
and
output
devices
computer
networking
is
another
form
of
i/o
i/o
devices
are
often
complex
computers
in
their
own
right
with
their
own
cpu
and
memory
a
graphics
processing
unit
might
contain
fifty
or
more
tiny
computers
that
perform
the
calculations
necessary
to
display
3d
graphics
modern
desktop
computers
contain
many
smaller
computers
that
assist
the
main
cpu
in
performing
i/o
a
2016-era
flat
screen
display
contains
its
own
computer
circuitry
while
a
computer
may
be
viewed
as
running
one
gigantic
program
stored
in
its
main
memory
in
some
systems
it
is
necessary
to
give
the
appearance
of
running
several
programs
simultaneously
this
is
achieved
by
multitasking
ie
having
the
computer
switch
rapidly
between
running
each
program
in
turn
one
means
by
which
this
is
done
is
with
a
special
signal
called
an
interrupt
which
can
periodically
cause
the
computer
to
stop
executing
instructions
where
it
was
and
do
something
else
instead
by
remembering
where
it
was
executing
prior
to
the
interrupt
the
computer
can
return
to
that
task
later
if
several
programs
are
running
"at
the
same
time"
then
the
interrupt
generator
might
be
causing
several
hundred
interrupts
per
second
causing
a
program
switch
each
time
since
modern
computers
typically
execute
instructions
several
orders
of
magnitude
faster
than
human
perception
it
may
appear
that
many
programs
are
running
at
the
same
time
even
though
only
one
is
ever
executing
in
any
given
instant
this
method
of
multitasking
is
sometimes
termed
"time-sharing"
since
each
program
is
allocated
a
"slice"
of
time
in
turn
before
the
era
of
inexpensive
computers
the
principal
use
for
multitasking
was
to
allow
many
people
to
share
the
same
computer
seemingly
multitasking
would
cause
a
computer
that
is
switching
between
several
programs
to
run
more
slowly
in
direct
proportion
to
the
number
of
programs
it
is
running
but
most
programs
spend
much
of
their
time
waiting
for
slow
input/output
devices
to
complete
their
tasks
if
a
program
is
waiting
for
the
user
to
click
on
the
mouse
or
press
a
key
on
the
keyboard
then
it
will
not
take
a
"time
slice"
until
the
event
it
is
waiting
for
has
occurred
this
frees
up
time
for
other
programs
to
execute
so
that
many
programs
may
be
run
simultaneously
without
unacceptable
speed
loss
some
computers
are
designed
to
distribute
their
work
across
several
cpus
in
a
multiprocessing
configuration
a
technique
once
employed
only
in
large
and
powerful
machines
such
as
supercomputers
mainframe
computers
and
servers
multiprocessor
and
multi-core
(multiple
cpus
on
a
single
integrated
circuit)
personal
and
laptop
computers
are
now
widely
available
and
are
being
increasingly
used
in
lower-end
markets
as
a
result
supercomputers
in
particular
often
have
highly
unique
architectures
that
differ
significantly
from
the
basic
stored-program
architecture
and
from
general
purpose
computers
they
often
feature
thousands
of
cpus
customized
high-speed
interconnects
and
specialized
computing
hardware
such
designs
tend
to
be
useful
only
for
specialized
tasks
due
to
the
large
scale
of
program
organization
required
to
successfully
utilize
most
of
the
available
resources
at
once
supercomputers
usually
see
usage
in
large-scale
simulation
graphics
rendering
and
cryptography
applications
as
well
as
with
other
so-called
"embarrassingly
parallel"
tasks
"software"
refers
to
parts
of
the
computer
which
do
not
have
a
material
form
such
as
programs
data
protocols
etc
software
is
that
part
of
a
computer
system
that
consists
of
encoded
information
or
computer
instructions
in
contrast
to
the
physical
hardware
from
which
the
system
is
built
computer
software
includes
computer
programs
libraries
and
related
non-executable
data
such
as
online
documentation
or
digital
media
it
is
often
divided
into
system
software
and
application
software]]
computer
hardware
and
software
require
each
other
and
neither
can
be
realistically
used
on
its
own
when
software
is
stored
in
hardware
that
cannot
easily
be
modified
such
as
with
bios
rom
in
an
ibm
pc
compatible
computer
it
is
sometimes
called
"firmware"
there
are
thousands
of
different
programming
languages—some
intended
to
be
general
purpose
others
useful
only
for
highly
specialized
applications
the
defining
feature
of
modern
computers
which
distinguishes
them
from
all
other
machines
is
that
they
can
be
programmed
that
is
to
say
that
some
type
of
instructions
(the
program)
can
be
given
to
the
computer
and
it
will
process
them
modern
computers
based
on
the
von
neumann
architecture
often
have
machine
code
in
the
form
of
an
imperative
programming
language
in
practical
terms
a
computer
program
may
be
just
a
few
instructions
or
extend
to
many
millions
of
instructions
as
do
the
programs
for
word
processors
and
web
browsers
for
example
a
typical
modern
computer
can
execute
billions
of
instructions
per
second
(gigaflops)
and
rarely
makes
a
mistake
over
many
years
of
operation
large
computer
programs
consisting
of
several
million
instructions
may
take
teams
of
programmers
years
to
write
and
due
to
the
complexity
of
the
task
almost
certainly
contain
errors
this
section
applies
to
most
common
ram
machine–based
computers
in
most
cases
computer
instructions
are
simple:
add
one
number
to
another
move
some
data
from
one
location
to
another
send
a
message
to
some
external
device
etc
these
instructions
are
read
from
the
computer's
memory
and
are
generally
carried
out
(executed)
in
the
order
they
were
given
however
there
are
usually
specialized
instructions
to
tell
the
computer
to
jump
ahead
or
backwards
to
some
other
place
in
the
program
and
to
carry
on
executing
from
there
these
are
called
"jump"
instructions
(or
branches)
furthermore
jump
instructions
may
be
made
to
happen
conditionally
so
that
different
sequences
of
instructions
may
be
used
depending
on
the
result
of
some
previous
calculation
or
some
external
event
many
computers
directly
support
subroutines
by
providing
a
type
of
jump
that
"remembers"
the
location
it
jumped
from
and
another
instruction
to
return
to
the
instruction
following
that
jump
instruction
program
execution
might
be
likened
to
reading
a
book
while
a
person
will
normally
read
each
word
and
line
in
sequence
they
may
at
times
jump
back
to
an
earlier
place
in
the
text
or
skip
sections
that
are
not
of
interest
similarly
a
computer
may
sometimes
go
back
and
repeat
the
instructions
in
some
section
of
the
program
over
and
over
again
until
some
internal
condition
is
met
this
is
called
the
flow
of
control
within
the
program
and
it
is
what
allows
the
computer
to
perform
tasks
repeatedly
without
human
intervention
comparatively
a
person
using
a
pocket
calculator
can
perform
a
basic
arithmetic
operation
such
as
adding
two
numbers
with
just
a
few
button
presses
but
to
add
together
all
of
the
numbers
from
1
to
1000
would
take
thousands
of
button
presses
and
a
lot
of
time
with
a
near
certainty
of
making
a
mistake
on
the
other
hand
a
computer
may
be
programmed
to
do
this
with
just
a
few
simple
instructions
the
following
example
is
written
in
the
mips
assembly
language:
once
told
to
run
this
program
the
computer
will
perform
the
repetitive
addition
task
without
further
human
intervention
it
will
almost
never
make
a
mistake
and
a
modern
pc
can
complete
the
task
in
a
fraction
of
a
second
in
most
computers
individual
instructions
are
stored
as
machine
code
with
each
instruction
being
given
a
unique
number
(its
operation
code
or
opcode
for
short)
the
command
to
add
two
numbers
together
would
have
one
opcode;
the
command
to
multiply
them
would
have
a
different
opcode
and
so
on
the
simplest
computers
are
able
to
perform
any
of
a
handful
of
different
instructions;
the
more
complex
computers
have
several
hundred
to
choose
from
each
with
a
unique
numerical
code
since
the
computer's
memory
is
able
to
store
numbers
it
can
also
store
the
instruction
codes
this
leads
to
the
important
fact
that
entire
programs
(which
are
just
lists
of
these
instructions)
can
be
represented
as
lists
of
numbers
and
can
themselves
be
manipulated
inside
the
computer
in
the
same
way
as
numeric
data
the
fundamental
concept
of
storing
programs
in
the
computer's
memory
alongside
the
data
they
operate
on
is
the
crux
of
the
von
neumann
or
stored
program
architecture
in
some
cases
a
computer
might
store
some
or
all
of
its
program
in
memory
that
is
kept
separate
from
the
data
it
operates
on
this
is
called
the
harvard
architecture
after
the
harvard
mark
i
computer
modern
von
neumann
computers
display
some
traits
of
the
harvard
architecture
in
their
designs
such
as
in
cpu
caches
while
it
is
possible
to
write
computer
programs
as
long
lists
of
numbers
(machine
language)
and
while
this
technique
was
used
with
many
early
computers
it
is
extremely
tedious
and
potentially
error-prone
to
do
so
in
practice
especially
for
complicated
programs
instead
each
basic
instruction
can
be
given
a
short
name
that
is
indicative
of
its
function
and
easy
to
remember –
a
mnemonic
such
as
add
sub
mult
or
jump
these
mnemonics
are
collectively
known
as
a
computer's
assembly
language
converting
programs
written
in
assembly
language
into
something
the
computer
can
actually
understand
(machine
language)
is
usually
done
by
a
computer
program
called
an
assembler
programming
languages
provide
various
ways
of
specifying
programs
for
computers
to
run
unlike
natural
languages
programming
languages
are
designed
to
permit
no
ambiguity
and
to
be
concise
they
are
purely
written
languages
and
are
often
difficult
to
read
aloud
they
are
generally
either
translated
into
machine
code
by
a
compiler
or
an
assembler
before
being
run
or
translated
directly
at
run
time
by
an
interpreter
sometimes
programs
are
executed
by
a
hybrid
method
of
the
two
techniques
machine
languages
and
the
assembly
languages
that
represent
them
(collectively
termed
"low-level
programming
languages")
tend
to
be
unique
to
a
particular
type
of
computer
for
instance
an
arm
architecture
computer
(such
as
may
be
found
in
a
smartphone
or
a
hand-held
videogame)
cannot
understand
the
machine
language
of
an
x86
cpu
that
might
be
in
a
pc
although
considerably
easier
than
in
machine
language
writing
long
programs
in
assembly
language
is
often
difficult
and
is
also
error
prone
therefore
most
practical
programs
are
written
in
more
abstract
high-level
programming
languages
that
are
able
to
express
the
needs
of
the
programmer
more
conveniently
(and
thereby
help
reduce
programmer
error)
high
level
languages
are
usually
"compiled"
into
machine
language
(or
sometimes
into
assembly
language
and
then
into
machine
language)
using
another
computer
program
called
a
compiler
high
level
languages
are
less
related
to
the
workings
of
the
target
computer
than
assembly
language
and
more
related
to
the
language
and
structure
of
the
problem(s)
to
be
solved
by
the
final
program
it
is
therefore
often
possible
to
use
different
compilers
to
translate
the
same
high
level
language
program
into
the
machine
language
of
many
different
types
of
computer
this
is
part
of
the
means
by
which
software
like
video
games
may
be
made
available
for
different
computer
architectures
such
as
personal
computers
and
various
video
game
consoles
program
design
of
small
programs
is
relatively
simple
and
involves
the
analysis
of
the
problem
collection
of
inputs
using
the
programming
constructs
within
languages
devising
or
using
established
procedures
and
algorithms
providing
data
for
output
devices
and
solutions
to
the
problem
as
applicable
as
problems
become
larger
and
more
complex
features
such
as
subprograms
modules
formal
documentation
and
new
paradigms
such
as
object-oriented
programming
are
encountered
large
programs
involving
thousands
of
line
of
code
and
more
require
formal
software
methodologies
the
task
of
developing
large
software
systems
presents
a
significant
intellectual
challenge
producing
software
with
an
acceptably
high
reliability
within
a
predictable
schedule
and
budget
has
historically
been
difficult;
the
academic
and
professional
discipline
of
software
engineering
concentrates
specifically
on
this
challenge
errors
in
computer
programs
are
called
"bugs"
they
may
be
benign
and
not
affect
the
usefulness
of
the
program
or
have
only
subtle
effects
but
in
some
cases
they
may
cause
the
program
or
the
entire
system
to
"hang"
becoming
unresponsive
to
input
such
as
mouse
clicks
or
keystrokes
to
completely
fail
or
to
crash
otherwise
benign
bugs
may
sometimes
be
harnessed
for
malicious
intent
by
an
unscrupulous
user
writing
an
exploit
code
designed
to
take
advantage
of
a
bug
and
disrupt
a
computer's
proper
execution
bugs
are
usually
not
the
fault
of
the
computer
since
computers
merely
execute
the
instructions
they
are
given
bugs
are
nearly
always
the
result
of
programmer
error
or
an
oversight
made
in
the
program's
design
admiral
grace
hopper
an
american
computer
scientist
and
developer
of
the
first
compiler
is
credited
for
having
first
used
the
term
"bugs"
in
computing
after
a
dead
moth
was
found
shorting
a
relay
in
the
harvard
mark
ii
computer
in
september
1947
computers
have
been
used
to
coordinate
information
between
multiple
locations
since
the
1950s
the
us
military's
sage
system
was
the
first
large-scale
example
of
such
a
system
which
led
to
a
number
of
special-purpose
commercial
systems
such
as
sabre
in
the
1970s
computer
engineers
at
research
institutions
throughout
the
united
states
began
to
link
their
computers
together
using
telecommunications
technology
the
effort
was
funded
by
arpa
(now
darpa)
and
the
computer
network
that
resulted
was
called
the
arpanet
the
technologies
that
made
the
arpanet
possible
spread
and
evolved
in
time
the
network
spread
beyond
academic
and
military
institutions
and
became
known
as
the
internet
the
emergence
of
networking
involved
a
redefinition
of
the
nature
and
boundaries
of
the
computer
computer
operating
systems
and
applications
were
modified
to
include
the
ability
to
define
and
access
the
resources
of
other
computers
on
the
network
such
as
peripheral
devices
stored
information
and
the
like
as
extensions
of
the
resources
of
an
individual
computer
initially
these
facilities
were
available
primarily
to
people
working
in
high-tech
environments
but
in
the
1990s
the
spread
of
applications
like
e-mail
and
the
world
wide
web
combined
with
the
development
of
cheap
fast
networking
technologies
like
ethernet
and
adsl
saw
computer
networking
become
almost
ubiquitous
in
fact
the
number
of
computers
that
are
networked
is
growing
phenomenally
a
very
large
proportion
of
personal
computers
regularly
connect
to
the
internet
to
communicate
and
receive
information
"wireless"
networking
often
utilizing
mobile
phone
networks
has
meant
networking
is
becoming
increasingly
ubiquitous
even
in
mobile
computing
environments
a
computer
does
not
need
to
be
electronic
nor
even
have
a
processor
nor
ram
nor
even
a
hard
disk
while
popular
usage
of
the
word
"computer"
is
synonymous
with
a
personal
electronic
computer
the
modern
definition
of
a
computer
is
literally:
""a
device
that
computes"
especially
a
programmable
[usually]
electronic
machine
that
performs
high-speed
mathematical
or
logical
operations
or
that
assembles
stores
correlates
or
otherwise
processes
information"
any
device
which
"processes
information"
qualifies
as
a
computer
especially
if
the
processing
is
purposeful
there
is
active
research
to
make
computers
out
of
many
promising
new
types
of
technology
such
as
optical
computers
dna
computers
neural
computers
and
quantum
computers
most
computers
are
universal
and
are
able
to
calculate
any
computable
function
and
are
limited
only
by
their
memory
capacity
and
operating
speed
however
different
designs
of
computers
can
give
very
different
performance
for
particular
problems;
for
example
quantum
computers
can
potentially
break
some
modern
encryption
algorithms
(by
quantum
factoring)
very
quickly
there
are
many
types
of
computer
architectures:
of
all
these
abstract
machines
a
quantum
computer
holds
the
most
promise
for
revolutionizing
computing
logic
gates
are
a
common
abstraction
which
can
apply
to
most
of
the
above
digital
or
analog
paradigms
the
ability
to
store
and
execute
lists
of
instructions
called
programs
makes
computers
extremely
versatile
distinguishing
them
from
calculators
the
church–turing
thesis
is
a
mathematical
statement
of
this
versatility:
any
computer
with
a
minimum
capability
(being
turing-complete)
is
in
principle
capable
of
performing
the
same
tasks
that
any
other
computer
can
perform
therefore
any
type
of
computer
(netbook
supercomputer
cellular
automaton
etc)
is
able
to
perform
the
same
computational
tasks
given
enough
time
and
storage
capacity
a
computer
will
solve
problems
in
exactly
the
way
it
is
programmed
to
without
regard
to
efficiency
alternative
solutions
possible
shortcuts
or
possible
errors
in
the
code
computer
programs
that
learn
and
adapt
are
part
of
the
emerging
field
of
artificial
intelligence
and
machine
learning
artificial
intelligence
based
products
generally
fall
into
two
major
categories:
rule
based
systems
and
pattern
recognition
systems
rule
based
systems
attempt
to
represent
the
rules
used
by
human
experts
and
tend
to
be
expensive
to
develop
pattern
based
systems
use
data
about
a
problem
to
generate
conclusions
examples
of
pattern
based
systems
include
voice
recognition
font
recognition
translation
and
the
emerging
field
of
on-line
marketing
as
the
use
of
computers
has
spread
throughout
society
there
are
an
increasing
number
of
careers
involving
computers
the
need
for
computers
to
work
well
together
and
to
be
able
to
exchange
information
has
spawned
the
need
for
many
standards
organizations
clubs
and
societies
of
both
a
formal
and
informal
nature
cognitive
computer
a
cognitive
computer
combines
artificial
intelligence
and
machine-learning
algorithms
in
an
approach
which
attempts
to
reproduce
the
behaviour
of
the
human
brain
it
generally
adopts
a
neuromorphic
engineering
approach
an
example
of
neural
network
implementations
of
cognitive
convolution
and
deep
learning
is
provided
by
the
ibm
company's
watson
machine
a
subsequent
development
by
ibm
is
the
truenorth
microchip
architecture
which
is
designed
to
be
closer
in
structure
to
the
human
brain
than
the
von
neumann
architecture
used
in
conventional
computers
in
2017
intel
announced
its
own
version
of
a
cognitive
chip
in
"loihi"
which
will
be
available
to
university
and
research
labs
in
2018
intel's
self-learning
neuromorphic
chip
named
loihi
perhaps
named
after
the
hawaiian
seamount
loihi
offers
substantial
power
efficiency
designed
after
the
human
brain
intel
claims
loihi
is
about
1000
times
more
energy
efficient
than
the
general-purpose
computing
power
needed
to
train
the
neural
networks
that
rival
loihi's
performance
in
theory
this
would
support
both
machine
learning
training
and
inference
on
the
same
silicon
independently
of
a
cloud
connection
and
more
efficient
than
using
convolutional
neural
networks
(cnns)
or
deep
learning
neural
networks
intel
points
to
a
system
for
monitoring
a
person's
heartbeat
taking
readings
after
events
such
as
exercise
or
eating
and
uses
the
cognitive
computing
chip
to
normalize
the
data
and
work
out
the
‘normal’
heartbeat
it
can
then
spot
abnormalities
but
also
deal
with
any
new
events
or
conditions
the
first
iteration
of
the
loihi
chip
was
made
using
intel's
14 nm
fabrication
process
and
houses
128
clusters
of
1024
artificial
neurons
each
for
a
total
of
131072
simulated
neurons
this
offers
around
130
million
synapses
which
is
still
a
rather
long
way
from
the
human
brain's
800
trillion
synapses
and
behind
ibm's
truenorth
which
has
around
16
billion
by
using
64
by
4096
cores
loihi
is
now
available
for
research
purposes
among
more
than
40
academic
research
groups
as
a
usb
from
factor
the
ibm
cognitive
computers
implement
learning
using
hebbian
theory
instead
of
being
programmable
in
a
traditional
sense
within
machine
language
or
a
higher
level
programming
language
such
a
device
learns
by
inputting
instances
through
an
input
device
that
are
aggregated
within
a
computational
convolution
or
neural
network
architecture
consisting
of
weights
within
a
parallel
memory
system
an
early
instantiation
of
such
a
device
has
been
developed
in
2012
under
the
darpa
synapse
program
at
ibm
directed
by
dharmendra
modha
in
2017
this
ibm
64-chip
array
will
contain
the
processing
equivalent
of
64
million
neurons
and
16
billion
synapses
yet
each
processor
consumes
just
10
watts
of
electricity
like
other
neural
networks
this
system
will
be
put
to
use
in
pattern
recognition
and
sensory
processing
roles
the
air
force
wants
to
combine
the
truenorth
ability
to
convert
multiple
data
feeds
— whether
it's
audio
video
or
text —
into
machine
readable
symbols
with
a
conventional
supercomputer's
ability
to
crunch
data
this
isn't
the
first
time
that
ibm's
neural
chip
system
has
been
integrated
into
cutting-edge
technology
in
august
2017
samsung
installed
the
chips
in
its
dynamic
vision
sensors
enabling
cameras
to
capture
images
at
up
to
2000
fps
while
using
just
300
milliwatts
of
power
google
has
created
three
generations
of
a
similar
device
tensor
processing
unit
using
low
resolution
8
bit
computing
rather
than
a
spiking
neural
network
there
are
many
approaches
and
definitions
for
a
cognitive
computer
and
other
approaches
may
be
more
fruitful
than
the
others
specifically
there
are
critics
who
argue
that
a
room-sized
computer
-
like
the
case
of
watson
-
is
not
a
viable
alternative
to
a
three-pound
human
brain
some
also
cite
the
difficulty
for
a
single
system
to
bring
so
many
elements
together
such
as
the
disparate
sources
of
information
as
well
as
computing
resources
during
the
2018
world
economic
forum
there
are
experts
who
claim
that
cognitive
systems
could
adopt
the
biases
of
their
developers
and
this
was
demonstrated
in
the
case
of
the
google
image-recognition
or
computer
vision
algorithm
which
identified
african
americans
unfavorably
http://wwwfoxnewscom/tech/2018/01/09/ces-2018-intel-gives-glimpse-into-mind-blowing-future-computinghtml
microcomputer
a
microcomputer
is
a
small
relatively
inexpensive
computer
with
a
microprocessor
as
its
central
processing
unit
(cpu)
it
includes
a
microprocessor
memory
and
minimal
input/output
(i/o)
circuitry
mounted
on
a
single
printed
circuit
board
microcomputers
became
popular
in
the
1970s
and
1980s
with
the
advent
of
increasingly
powerful
microprocessors
the
predecessors
to
these
computers
mainframes
and
minicomputers
were
comparatively
much
larger
and
more
expensive
(though
indeed
present-day
mainframes
such
as
the
ibm
system
z
machines
use
one
or
more
custom
microprocessors
as
their
cpus)
many
microcomputers
(when
equipped
with
a
keyboard
and
screen
for
input
and
output)
are
also
personal
computers
(in
the
generic
sense)
the
abbreviation
"micro"
was
common
during
the
1970s
and
1980s
but
has
now
fallen
out
of
common
usage
the
term
"microcomputer"
came
into
popular
use
after
the
introduction
of
the
minicomputer
although
isaac
asimov
used
the
term
in
his
short
story
"the
dying
night"
as
early
as
1956
(published
in
"the
magazine
of
fantasy
and
science
fiction"
in
july
that
year)
most
notably
the
microcomputer
replaced
the
many
separate
components
that
made
up
the
minicomputer's
cpu
with
one
integrated
microprocessor
chip
the
french
developers
of
the
micral
n
(1973)
filed
their
patents
with
the
term
"micro-ordinateur"
a
literal
equivalent
of
"microcomputer"
to
designate
a
solid
state
machine
designed
with
a
microprocessor
in
the
usa
the
earliest
models
such
as
the
altair
8800
were
often
sold
as
kits
to
be
assembled
by
the
user
and
came
with
as
little
as
256
bytes
of
ram
and
no
input/output
devices
other
than
indicator
lights
and
switches
useful
as
a
proof
of
concept
to
demonstrate
what
such
a
simple
device
could
do
however
as
microprocessors
and
semiconductor
memory
became
less
expensive
microcomputers
in
turn
grew
cheaper
and
easier
to
use:
all
these
improvements
in
cost
and
usability
resulted
in
an
explosion
in
their
popularity
during
the
late
1970s
and
early
1980s
a
large
number
of
computer
makers
packaged
microcomputers
for
use
in
small
business
applications
by
1979
many
companies
such
as
cromemco
processor
technology
imsai
north
star
computers
southwest
technical
products
corporation
ohio
scientific
altos
computer
systems
morrow
designs
and
others
produced
systems
designed
either
for
a
resourceful
end
user
or
consulting
firm
to
deliver
business
systems
such
as
accounting
database
management
and
word
processing
to
small
businesses
this
allowed
businesses
unable
to
afford
leasing
of
a
minicomputer
or
time-sharing
service
the
opportunity
to
automate
business
functions
without
(usually)
hiring
a
full-time
staff
to
operate
the
computers
a
representative
system
of
this
era
would
have
used
an
s100
bus
an
8-bit
processor
such
as
an
intel
8080
or
zilog
z80
and
either
cp/m
or
mp/m
operating
system
the
increasing
availability
and
power
of
desktop
computers
for
personal
use
attracted
the
attention
of
more
software
developers
in
time
and
as
the
industry
matured
the
market
for
personal
computers
standardized
around
ibm
pc
compatibles
running
dos
and
later
windows
modern
desktop
computers
video
game
consoles
laptops
tablet
pcs
and
many
types
of
handheld
devices
including
mobile
phones
pocket
calculators
and
industrial
embedded
systems
may
all
be
considered
examples
of
microcomputers
according
to
the
definition
given
above
everyday
use
of
the
expression
"microcomputer"
(and
in
particular
the
"micro"
abbreviation)
has
declined
significantly
from
the
mid-1980s
and
has
declined
in
commonplace
usage
since
2000
the
term
is
most
commonly
associated
with
the
first
wave
of
all-in-one
8-bit
home
computers
and
small
business
microcomputers
(such
as
the
apple
ii
commodore
64
bbc
micro
and
trs
80)
although
or
perhaps
because
an
increasingly
diverse
range
of
modern
microprocessor-based
devices
fit
the
definition
of
"microcomputer"
they
are
no
longer
referred
to
as
such
in
everyday
speech
in
common
usage
"microcomputer"
has
been
largely
supplanted
by
the
term
"personal
computer"
or
"pc"
which
specifies
a
computer
that
has
been
designed
to
be
used
by
one
individual
at
a
time
a
term
first
coined
in
1959
ibm
first
promoted
the
term
"personal
computer"
to
differentiate
themselves
from
other
microcomputers
often
called
"home
computers"
and
also
ibm's
own
mainframes
and
minicomputers
however
following
its
release
the
ibm
pc
itself
was
widely
imitated
as
well
as
the
term
the
component
parts
were
commonly
available
to
producers
and
the
bios
was
reverse
engineered
through
cleanroom
design
techniques
ibm
pc
compatible
"clones"
became
commonplace
and
the
terms
"personal
computer"
and
especially
"pc"
stuck
with
the
general
public
often
specifically
for
a
dos
or
(nowadays)
windows-compatible
computer
since
the
advent
of
microcontrollers
(monolithic
integrated
circuits
containing
ram
rom
and
cpu
all
onboard)
the
term
"micro"
is
more
commonly
used
to
refer
to
that
meaning
monitors
keyboards
and
other
devices
for
input
and
output
may
be
integrated
or
separate
computer
memory
in
the
form
of
ram
and
at
least
one
other
less
volatile
memory
storage
device
are
usually
combined
with
the
cpu
on
a
system
bus
in
one
unit
other
devices
that
make
up
a
complete
microcomputer
system
include
batteries
a
power
supply
unit
a
keyboard
and
various
input/output
devices
used
to
convey
information
to
and
from
a
human
operator
(printers
monitors
human
interface
devices)
microcomputers
are
designed
to
serve
only
one
user
at
a
time
although
they
can
often
be
modified
with
software
or
hardware
to
concurrently
serve
more
than
one
user
microcomputers
fit
well
on
or
under
desks
or
tables
so
that
they
are
within
easy
access
of
users
bigger
computers
like
minicomputers
mainframes
and
supercomputers
take
up
large
cabinets
or
even
dedicated
rooms
a
microcomputer
comes
equipped
with
at
least
one
type
of
data
storage
usually
ram
although
some
microcomputers
(particularly
early
8-bit
home
micros)
perform
tasks
using
ram
alone
some
form
of
secondary
storage
is
normally
desirable
in
the
early
days
of
home
micros
this
was
often
a
data
cassette
deck
(in
many
cases
as
an
external
unit)
later
secondary
storage
(particularly
in
the
form
of
floppy
disk
and
hard
disk
drives)
were
built
into
the
microcomputer
case
although
they
did
not
contain
any
microprocessors
but
were
built
around
transistor-transistor
logic
(ttl)
hewlett-packard
calculators
as
far
back
as
1968
had
various
levels
of
programmability
comparable
to
microcomputers
the
hp
9100b
(1968)
had
rudimentary
conditional
(if)
statements
statement
line
numbers
jump
statements
(go
to)
registers
that
could
be
used
as
variables
and
primitive
subroutines
the
programming
language
resembled
assembly
language
in
many
ways
later
models
incrementally
added
more
features
including
the
basic
programming
language
(hp
9830a
in
1971)
some
models
had
tape
storage
and
small
printers
however
displays
were
limited
to
one
line
at
a
time
the
hp
9100a
was
referred
to
as
a
personal
computer
in
an
advertisement
in
a
1968
science
magazine
but
that
advertisement
was
quickly
dropped
hp
was
reluctant
to
sell
them
as
"computers"
because
the
perception
at
that
time
was
that
a
computer
had
to
be
big
in
size
to
be
powerful
and
thus
decided
to
market
them
as
calculators
additionally
at
that
time
people
were
more
likely
to
buy
calculators
than
computers
and
purchasing
agents
also
preferred
the
term
"calculator"
because
purchasing
a
"computer"
required
additional
layers
of
purchasing
authority
approvals
hp
virtual
museum
the
datapoint
2200
made
by
ctc
in
1970
was
also
comparable
to
microcomputers
while
it
contains
no
microprocessor
the
instruction
set
of
its
custom
ttl
processor
was
the
basis
of
the
instruction
set
for
the
intel
8008
and
for
practical
purposes
the
system
behaves
approximately
as
if
it
contains
an
8008
this
is
because
intel
was
the
contractor
in
charge
of
developing
the
datapoint's
cpu
but
ultimately
ctc
rejected
the
8008
design
because
it
needed
20
support
chips
another
early
system
the
kenbak-1
was
released
in
1971
like
the
datapoint
2200
it
used
discrete
transistor–transistor
logic
instead
of
a
microprocessor
but
it
functioned
like
a
microcomputer
in
some
ways
it
was
marketed
as
an
educational
and
hobbyist
tool
but
it
was
not
a
commercial
success;
production
ceased
shortly
after
introduction
in
late
1972
a
french
team
headed
by
françois
gernelle
within
a
small
company
réalisations
etudes
electroniqes
(r2e)
developed
and
patented
a
computer
based
on
a
microprocessor
–
the
intel
8008
8-bit
microprocessor
this
micral-n
was
marketed
in
early
1973
as
a
"micro-ordinateur"
or
"microcomputer"
mainly
for
scientific
and
process-control
applications
about
a
hundred
micral-n
were
installed
in
the
next
two
years
followed
by
a
new
version
based
on
the
intel
8080
meanwhile
another
french
team
developed
the
alvan
a
small
computer
for
office
automation
which
found
clients
in
banks
and
other
sectors
the
first
version
was
based
on
lsi
chips
with
an
intel
8008
as
peripheral
controller
(keyboard
monitor
and
printer)
before
adopting
the
zilog
z80
as
main
processor
in
late
1972
a
sacramento
state
university
team
led
by
bill
pentz
built
the
sac
state
8008
computer
able
to
handle
thousands
of
patients'
medical
records
the
sac
state
8008
was
designed
with
the
intel
8008
it
had
a
full
set
of
hardware
and
software
components:
a
disk
operating
system
included
in
a
series
of
programmable
read-only
memory
chips
(proms);
8
kilobytes
of
ram;
ibm's
basic
assembly
language
(bal);
a
hard
drive;
a
color
display;
a
printer
output;
a
150
bit/s
serial
interface
for
connecting
to
a
mainframe;
and
even
the
world's
first
microcomputer
front
panel
in
early
1973
sord
computer
corporation
(now
toshiba
personal
computer
system
corporation)
completed
the
smp80/08
which
used
the
intel
8008
microprocessor
the
smp80/08
however
did
not
have
a
commercial
release
after
the
first
general-purpose
microprocessor
the
intel
8080
was
announced
in
april
1974
sord
announced
the
smp80/x
the
first
microcomputer
to
use
the
8080
in
may
1974
virtually
all
early
microcomputers
were
essentially
boxes
with
lights
and
switches;
one
had
to
read
and
understand
binary
numbers
and
machine
language
to
program
and
use
them
(the
datapoint
2200
was
a
striking
exception
bearing
a
modern
design
based
on
a
monitor
keyboard
and
tape
and
disk
drives)
of
the
early
"box
of
switches"-type
microcomputers
the
mits
altair
8800
(1975)
was
arguably
the
most
famous
most
of
these
simple
early
microcomputers
were
sold
as
electronic
kits—bags
full
of
loose
components
which
the
buyer
had
to
solder
together
before
the
system
could
be
used
the
period
from
about
1971
to
1976
is
sometimes
called
the
of
microcomputers
many
companies
such
as
dec
national
semiconductor
texas
instruments
offered
their
microcomputers
for
use
in
terminal
control
peripheral
device
interface
control
and
industrial
machine
control
there
were
also
machines
for
engineering
development
and
hobbyist
personal
use
in
1975
the
processor
technology
sol-20
was
designed
which
consisted
of
one
board
which
included
all
the
parts
of
the
computer
system
the
sol-20
had
built-in
eprom
software
which
eliminated
the
need
for
rows
of
switches
and
lights
the
mits
altair
just
mentioned
played
an
instrumental
role
in
sparking
significant
hobbyist
interest
which
itself
eventually
led
to
the
founding
and
success
of
many
well-known
personal
computer
hardware
and
software
companies
such
as
microsoft
and
apple
computer
although
the
altair
itself
was
only
a
mild
commercial
success
it
helped
spark
a
huge
industry
by
1977
the
introduction
of
the
second
generation
known
as
home
computers
made
microcomputers
considerably
easier
to
use
than
their
predecessors
because
their
predecessors'
operation
often
demanded
thorough
familiarity
with
practical
electronics
the
ability
to
connect
to
a
monitor
(screen)
or
tv
set
allowed
visual
manipulation
of
text
and
numbers
the
basic
language
which
was
easier
to
learn
and
use
than
raw
machine
language
became
a
standard
feature
these
features
were
already
common
in
minicomputers
with
which
many
hobbyists
and
early
produces
were
familiar
in
1979
the
launch
of
the
visicalc
spreadsheet
(initially
for
the
apple
ii)
first
turned
the
microcomputer
from
a
hobby
for
computer
enthusiasts
into
a
business
tool
after
the
1981
release
by
ibm
of
its
ibm
pc
the
term
personal
computer
became
generally
used
for
microcomputers
compatible
with
the
ibm
pc
architecture
(pc
compatible)
data
(word)
the
word
data
has
generated
considerable
controversy
on
whether
it
is
an
uncountable
noun
used
with
verbs
conjugated
in
the
singular
or
should
be
treated
as
the
plural
of
the
now-rarely-used
"datum"
in
one
sense
"data"
is
the
plural
form
of
"datum"
"datum"
actually
can
also
be
a
count
noun
with
the
plural
"datums"
(see
usage
in
datum
article)
that
can
be
used
with
cardinal
numbers
(eg
"80
datums");
"data"
(originally
a
latin
plural)
is
not
used
like
a
normal
count
noun
with
cardinal
numbers
and
can
be
plural
with
such
plural
determiners
as
"these"
and
"many"
or
as
an
uncountable
noun
with
a
verb
in
the
singular
form
even
when
a
very
small
quantity
of
data
is
referenced
(one
number
for
example)
the
phrase
"piece
of
data"
is
often
used
as
opposed
to
"datum"
the
debate
over
appropriate
usage
continues
but
"data"
as
a
singular
form
is
far
more
common
in
english
the
word
"datum"
is
still
used
in
the
general
sense
of
"an
item
given"
in
cartography
geography
nuclear
magnetic
resonance
and
technical
drawing
it
is
often
used
to
refer
to
a
single
specific
reference
datum
from
which
distances
to
all
other
data
are
measured
any
measurement
or
result
is
a
"datum"
though
"data
point"
is
now
far
more
common
"data"
is
most
often
used
as
a
singular
mass
noun
in
everyday
usage
some
major
newspapers
such
as
"the
new
york
times"
use
it
either
in
the
singular
or
plural
in
the
"new
york
times"
the
phrases
"the
survey
data
are
still
being
analyzed"
and
"the
first
year
for
which
data
is
available"
have
appeared
within
one
day
the
"wall
street
journal"
explicitly
allows
this
usage
in
its
style
guide
the
associated
press
style
guide
classifies
"data"
as
a
collective
noun
that
takes
the
singular
when
treated
as
a
unit
but
the
plural
when
referring
to
individual
items
(eg
"the
data
is
sound"
and
"the
data
have
been
carefully
collected")
in
scientific
writing
"data"
is
often
treated
as
a
plural
as
in
"these
data
do
not
support
the
conclusions"
but
the
word
is
also
used
as
a
singular
mass
entity
like
"information"
for
instance
in
computing
and
related
disciplines
british
usage
now
widely
accepts
treating
"data"
as
singular
in
standard
english
including
everyday
newspaper
usage
at
least
in
non-scientific
use
uk
scientific
publishing
still
prefers
treating
it
as
a
plural
some
uk
university
style
guides
recommend
using
"data"
for
both
singular
and
plural
use
and
others
recommend
treating
it
only
as
a
singular
in
connection
with
computers
the
ieee
computer
society
allows
usage
of
"data"
as
either
a
mass
noun
or
plural
based
on
author
preference
while
ieee
in
the
editorial
style
manual
indicates
to
always
use
the
plural
form
some
professional
organizations
and
style
guides
require
that
authors
treat
"data"
as
a
plural
noun
for
example
the
air
force
flight
test
center
specifically
states
that
the
word
"data"
is
always
plural
never
singular
motion
history
images
the
motion
history
image
(mhi)
is
a
static
image
template
helps
in
understanding
the
motion
location
and
path
as
it
progresses
in
mhi
the
temporal
motion
information
is
collapsed
into
a
single
image
template
where
intensity
is
a
function
of
recency
of
motion
thus
the
mhi
pixel
intensity
is
a
function
of
the
motion
history
at
that
location
where
brighter
values
correspond
to
a
more
recent
motion
using
mhi
moving
parts
of
a
video
sequence
can
be
engraved
with
a
single
image
from
where
one
can
predict
the
motion
flow
as
well
as
the
moving
parts
of
the
video
action
some
important
features
of
the
mhi
representation
are:
for
each
time
"t"
list
of
countries
by
computer
exports
the
following
is
a
list
of
countries
by
computer
exports
data
is
for
2014
in
millions
of
united
states
dollars
as
reported
by
the
observatory
of
economic
complexity
currently
the
top
fifteen
countries
are
listed
atlasmediamitedu
-
observatory
of
economic
complexity
-
countries
that
export
computers
(2012)
computer
says
no
"computer
says
no"
or
the
"computer
says
no
attitude"
is
the
popular
name
given
to
an
attitude
seen
in
some
public-facing
organisations
where
the
default
response
to
a
customer’s
request
is
to
check
with
information
stored
on
or
generated
by
a
computer
and
then
make
decisions
based
on
that
often
in
the
face
of
common
sense
there
may
also
be
an
element
of
deliberate
unhelpfulness
towards
customers
and
service-users
whereby
more
"could"
be
done
to
reach
a
mutually
satisfactory
outcome
but
is
not
the
name
gained
popularity
through
the
british
sketch
comedy
"little
britain"
in
"little
britain"
"computer
says
no"
is
the
catchphrase
of
the
character
carol
beer
(played
by
david
walliams)
a
bank
worker
and
later
holiday
rep
and
hospital
receptionist
who
always
responds
to
a
customer's
enquiry
by
typing
it
into
her
computer
and
responding
with
"computer
says
no"
to
even
the
most
reasonable
of
requests
when
asked
to
do
something
aside
from
asking
the
computer
she
would
shrug
and
remain
obstinate
in
her
unhelpfulness
and
ultimately
cough
in
the
customer's
face
the
phrase
was
also
used
in
the
australian
soap
opera
"neighbours"
in
2006
as
a
reference
to
"little
britain"
the
"computer
says
no"
attitude
often
comes
from
larger
companies
that
rely
on
information
stored
electronically
when
this
information
is
not
updated
it
can
often
lead
to
refusals
of
financial
products
or
incorrect
information
being
sent
out
to
customers
these
situations
can
often
be
resolved
by
an
employee
updating
the
information;
however
when
this
cannot
be
done
easily
the
"computer
says
no"
attitude
can
be
viewed
as
becoming
prevalent
when
there
is
unhelpfulness
as
a
result
this
attitude
can
also
occur
when
an
employee
fails
to
read
human
emotion
in
the
customer
and
reacts
according
to
his
or
her
professional
training
or
relies
upon
a
script
this
attitude
also
crops
up
when
larger
companies
rely
on
computer
credit
scores
and
do
not
meet
with
a
customer
to
discuss
his
or
her
individual
needs
instead
basing
a
decision
upon
information
stored
in
computers
some
organisations
attempt
to
offset
this
attitude
by
moving
away
from
reliance
on
electronic
information
and
using
a
human
approach
towards
requests
"computer
says
no"
happens
in
a
more
literal
sense
when
computer
systems
employ
filters
that
prevent
messages
being
passed
along
as
when
these
messages
are
perceived
to
include
obscenities
when
information
is
not
passed
through
to
the
person
operating
the
computer
decisions
may
be
made
without
seeing
the
whole
picture
computer
rage
computer
rage
refers
to
negative
psychological
responses
towards
a
computer
due
to
heightened
anger
or
frustration
examples
of
computer
rage
include
cursing
or
yelling
at
a
computer
slamming
or
throwing
keyboards
and
mice
and
assaulting
the
computer
or
monitor
with
an
object
or
weapon
in
april
2015
a
colorado
man
was
cited
for
firing
a
gun
within
a
residential
area
when
he
took
his
computer
into
a
back
alley
and
shot
it
eight
times
with
a
9mm
pistol
when
questioned
he
told
police
that
he
had
become
so
frustrated
with
his
computer
that
he
had
"reached
critical
mass"
and
stated
that
after
he
had
shot
his
computer
"the
angels
sung
on
high"
in
2007
a
german
man
threw
his
computer
out
the
window
in
the
middle
of
the
night
startling
his
neighbors
german
police
were
sympathetic
and
did
not
press
charges
stating
"who
hasn't
felt
like
doing
that?"
in
2006
the
staged
surveillance
video
"bad
day"
showing
a
man
assaulting
his
computer
at
work
became
a
viral
hit
on
the
internet
reaching
over
two
million
views
other
instances
of
reported
computer
rage
have
ranged
from
a
restaurant
owner
who
threw
his
laptop
into
a
deep
fryer
to
an
individual
who
threw
his
computer
out
the
window
but
forgot
that
the
window
was
closed
in
1999
it
was
speculated
that
computer
rage
had
become
more
common
than
road
rage
in
traffic
but
in
a
2015
study
it
was
found
that
reported
rates
of
anger
when
using
a
computer
were
lower
than
reported
rates
of
anger
while
driving
however
reports
of
anger
while
driving
or
using
computers
were
found
to
be
far
more
common
than
anger
in
other
situations
in
a
2013
survey
of
american
adults
36%
of
respondents
who
reported
experiencing
computer
issues
also
reported
that
they
had
screamed
yelled
cursed
or
physically
assaulted
their
computers
within
the
last
six
months
in
2009
a
survey
was
conducted
with
british
computer
users
about
their
experiences
with
computers
this
survey
found
that
54%
of
respondents
reported
verbally
abusing
their
computers
and
40%
reported
that
they
had
become
physically
violent
toward
their
computers
the
survey
also
found
that
most
users
experienced
computer
rage
three
to
four
times
a
month
differences
in
types
of
computer
rage
have
also
been
found
between
different
geographical
regions
for
example
one
survey
found
that
individuals
from
london
have
been
found
to
be
five
times
more
likely
to
physically
assault
their
computers
while
those
from
yorkshire
and
humberside
were
found
to
be
more
likely
to
yell
at
their
computers
differences
have
also
been
observed
for
age
groups
as
younger
adults
(18–24
years
old)
have
reported
more
abusive
behaviors
in
the
face
of
computer
frustration
when
compared
to
older
adults
(over
35
years
old)
individuals
with
less
computer
experience
in
particular
have
also
been
reported
to
experience
increased
feelings
of
anger
and
helplessness
when
it
comes
to
computers
but
other
research
has
argued
that
it
is
the
self-efficacy
beliefs
about
computers
that
are
predictive
of
computer
frustration
not
the
amount
of
computer
experience
or
use
in
1999
professor
robert
j
edelmann
a
chartered
clinical
forensic
and
health
psychologist
and
a
fellow
of
the
british
psychological
society
was
offering
a
special
helpline
in
the
uk
for
those
suffering
from
technology
related
anger
users
can
experience
computer
anger
and
frustration
for
a
number
of
reasons
american
adults
surveyed
in
2013
reported
that
almost
half
(46%)
of
their
computer
problems
were
due
to
malware
or
computer
viruses
followed
by
software
issues
(10%)
and
not
enough
memory
(8%)
in
another
survey
users
reported
email
word
processors
web
browsing
operating
system
crashes
inability
to
locate
features
and
program
crashes
as
frequent
initiators
of
computer
frustration
these
technical
issues
paired
with
tight
timelines
poor
work
progress
and
failure
to
complete
a
computer
task
can
create
heightened
computer
anger
and
frustration
when
this
anger
and
frustration
exceeds
a
person's
control
it
can
turn
into
rage
research
on
emotion
has
shown
that
anger
is
often
caused
by
interruptions
of
plans
and
expectations
especially
through
the
violation
of
social
norms
this
sense
of
anger
can
be
magnified
when
the
individual
does
not
understand
why
they
are
unable
to
meet
their
goal
or
task
at
hand
or
why
there
was
a
violation
of
social
norms
psychologists
have
argued
that
this
is
particularly
relevant
to
computer
rage
as
computer
users
interact
with
computers
in
a
similar
manner
that
they
interact
with
other
people
(for
more
information
see
the
media
equation)
thus
when
computers
fail
to
function
in
the
face
of
incoming
deadlines
or
an
important
task
to
accomplish
users
can
feel
betrayed
by
the
computer
in
the
same
way
they
can
feel
betrayed
by
other
people
specifically
when
users
fail
to
understand
why
their
computer
will
not
work
properly
often
in
the
times
they
need
it
to
the
most
it
can
invoke
a
sense
of
hostility
as
it
is
interpreted
as
a
breach
of
social
norms
or
a
personal
attack
consistent
with
this
finding
perceived
betrayal
by
the
computer
can
also
elicit
other
negative
emotions
one
survey
of
us
adults
reported
that
10%
of
users
who
experience
computer
issues
experienced
feeling
helplessness
and
4%
reported
feeling
victimized
in
the
same
survey
7%
adults
ages
18–34
reported
that
they
had
cried
over
their
computer
problems
within
the
previous
six
months
computer
rage
can
result
in
damaged
property
or
physical
injuries
as
well
as
psychological
harm
some
experts
have
suggested
that
venting
frustrations
on
the
computer
may
have
some
benefits
but
other
experts
disagree
for
example
yelling
at
the
computer
has
been
suggested
as
a
way
to
moderate
one's
anger
to
avoid
the
ill
effects
of
anger
suppression
but
new
research
has
suggested
that
yelling
can
negatively
affect
health
in
itself
alternatively
releasing
anger
on
a
computer
has
been
viewed
as
advantageous
as
it
directs
this
rage
at
an
object
as
opposed
to
another
person
and
can
make
individuals
feel
better
afterwards
in
response
to
computer
issues
that
invoke
frustration
some
experts
have
suggested
walking
away
from
the
computer
for
15
minutes
to
"cool
off"
other
methods
to
prevent
computer
rage
can
be
backing
up
computer
data
often
increasing
memory
of
the
computer
and
even
imagining
pleasant
images
such
as
petting
an
animal
adopting
a
goal
of
improving
computer
knowledge
may
also
be
beneficial
as
users
are
less
likely
to
report
computer
rage
when
they
view
the
issue
as
a
challenge
and
not
as
a
setback
if
computer
rage
cannot
be
avoided
guidelines
on
how
to
rage
with
minimal
consequences
such
as
wearing
safety
goggles
and
taking
frustration
out
on
older
equipment
can
be
followed
to
reduce
the
likelihood
of
injury
and
significant
property
loss
employers
of
staff
who
work
with
computers
often
in
situations
where
time
is
crucial
can
take
steps
to
prevent
computer
rage
such
as
making
sure
there
is
adequate
software
and
providing
employees
with
anger
management
strategies
some
computer
technician
companies
have
reported
that
to
reduce
computer
rage
their
technicians
are
trained
on
how
to
work
with
customers
in
sensitive
psychological
states
just
as
much
as
how
to
diagnose
and
fix
technical
issues
designing
computer
interfaces
to
display
more
emotional
support
when
errors
occur
or
provide
therapy
strategies
has
also
been
suggested
as
a
way
to
mitigate
computer
anger
and
rage
the
application
of
affective
computing
has
been
shown
to
effectively
mitigate
negative
emotions
connected
to
computer
use
one
study
found
that
an
interface
that
sought
the
user's
feelings
provided
empathy
and
validated
reported
emotional
states
significantly
reduced
negative
emotions
associated
with
computer
frustration
for
users
another
study
found
that
when
error
messages
contain
positive
wording
("great
that
the
computer
will
soon
work
again")
compared
to
negative
wording
("this
is
frustrating")
or
a
neutral
error
message
users
exhibited
more
signs
of
happiness
pcaas
pcaas
or
"personal
computer
as
a
service"
is
a
personal
computer
hardware
and
optionally
software
leasing
licensing
and
delivery
model
in
which
personal
computer
and
optionally
software
(particularly
installed
on
the
pc)
are
leased
and
licensed
on
a
subscription
basis
the
subscription
often
includes
services
such
as
staging
imaging
maintenance
fix
logistics
services
and
may
also
be
bundled
with
helpdesk
services
data
backup
and
recovery
there
are
several
vendors
that
have
pcaas
offerings
including
bizbang
dell
hp
(they
call
theirs
device
as
a
service)
and
lenovo
(in
australia
only
for
now)
eniac
day
eniac
day
or
the
world’s
first
computer
day
is
celebrated
on
15
february
on
february
10
2011
the
city
of
philadelphia
officially
declared
that
february
15
2011
-
the
65th
anniversary
of
the
unveiling
of
the
electronic
numerical
integrator
and
computer
(eniac)
the
world's
first
general-purpose
electronic
computer
developed
at
the
university
of
pennsylvania's
moore
school
of
electrical
engineering
-
would
that
year
and
henceforth
be
known
as
eniac
day
computer
addiction
computer
addiction
can
be
described
as
the
excessive
or
compulsive
use
of
the
computer
which
persists
despite
serious
negative
consequences
for
personal
social
or
occupational
function
another
clear
conceptualization
is
made
by
block
who
stated
that
"conceptually
the
diagnosis
is
a
compulsive-impulsive
spectrum
disorder
that
involves
online
and/or
offline
computer
usage
and
consists
of
at
least
three
subtypes:
excessive
gaming
sexual
preoccupations
and
e-mail/text
messaging"
while
it
was
expected
that
this
new
type
of
addiction
would
find
a
place
under
the
compulsive
disorders
in
the
dsm-5
the
current
edition
of
the
"diagnostic
and
statistical
manual
of
mental
disorders"
it
is
still
counted
as
an
unofficial
disorder
the
concept
of
computer
addiction
is
broadly
divided
into
two
types
namely
offline
computer
addiction
and
online
computer
addiction
the
term
offline
computer
addiction
is
normally
used
when
speaking
about
excessive
gaming
behavior
which
can
be
practiced
both
offline
and
online
online
computer
addiction
also
known
as
internet
addiction
gets
more
attention
in
general
from
scientific
research
than
offline
computer
addiction
mainly
because
most
cases
of
computer
addiction
are
related
to
the
excessive
use
of
the
internet
although
addiction
is
usually
used
to
describe
dependence
on
substances
addiction
can
also
be
used
to
describe
pathological
internet
use
experts
on
internet
addiction
have
described
this
syndrome
as
an
individual
being
intensely
working
on
the
internet
prolonged
use
of
the
internet
uncontrollable
use
of
the
internet
unable
to
use
the
internet
with
efficient
time
not
being
interested
in
the
outside
world
not
spending
time
with
people
from
the
outside
world
and
an
increase
in
their
loneliness
and
dejection
however
simply
working
long
hours
on
the
computer
does
not
necessarily
mean
someone
is
addicted
excessive
computer
use
may
result
in
or
occur
with:
kimberly
young
indicates
that
previous
research
links
internet/computer
addiction
with
existing
mental
health
issues
most
notably
depression
she
states
that
computer
addiction
has
significant
effects
socially
such
as
low
self-esteem
psychologically
and
occupationally
which
led
many
subjects
to
academic
failure
according
to
a
korean
study
on
internet/computer
addiction
pathological
use
of
the
internet
results
in
negative
life
impacts
such
as
job
loss
marriage
breakdown
financial
debt
and
academic
failure
70%
of
internet
users
in
korea
are
reported
to
play
online
games
18%
of
which
are
diagnosed
as
game
addicts
which
relates
to
internet/computer
addiction
the
authors
of
the
article
conducted
a
study
using
kimberly
young's
questionnaire
the
study
showed
that
the
majority
of
those
who
met
the
requirements
of
internet/computer
addiction
suffered
from
interpersonal
difficulties
and
stress
and
that
those
addicted
to
online
games
specifically
responded
that
they
hoped
to
avoid
reality
computers
nowadays
rely
almost
entirely
on
the
internet
and
thus
relevant
research
articles
relating
to
internet
addiction
may
also
be
relevant
to
computer
addiction
many
studies
and
surveys
are
being
conducted
to
measure
the
extent
of
this
type
of
addiction
kimberly
young
has
created
a
questionnaire
based
on
other
disorders
to
assess
the
level
of
addiction
it
is
called
the
internet
addict
diagnostic
questionnaire
or
iadq
the
questionnaire
asks
users
about
their
online
usage
habits
as
well
as
their
feelings
about
their
internet
usage
according
to
the
iadq
sample
internet
addiction
resembles
that
of
a
gambling
disorder
answering
positively
to
five
out
of
the
eight
questions
on
the
iadq
may
be
indicative
of
an
online
addiction
observations
about
the
addictiveness
of
computers
and
more
specifically
computer
games
date
back
at
least
to
the
mid
1970s
addiction
and
addictive
behavior
was
common
among
the
users
of
the
plato
system
at
the
university
of
illinois
british
e-learning
academic
nicholas
rushby
suggested
in
his
1979
book
"an
introduction
to
educational
computing"
that
people
can
be
addicted
to
computers
and
suffer
withdrawal
symptoms
the
term
was
also
used
by
m
shotton
in
1989
in
her
book
"computer
addiction"
however
shotton
concludes
that
the
'addicts'
are
not
truly
addicted
dependency
on
computers
she
argues
is
better
understood
as
a
challenging
and
exiting
pastime
that
can
also
lead
to
a
professional
career
in
the
field
computers
do
not
turn
gregarious
extroverted
people
into
recluses;
instead
they
offer
introverts
a
source
of
inspiration
excitement
and
intellectual
stimulation
shotton's
work
seriously
questions
the
legitimacy
of
the
claim
that
computers
cause
addiction
the
term
became
more
widespread
with
the
explosive
growth
of
the
internet
as
well
the
availability
of
the
personal
computer
computers
and
the
internet
both
started
to
take
shape
as
a
personal
and
comfortable
medium
which
could
be
used
by
anyone
who
wanted
to
make
use
of
it
with
that
explosive
growth
of
individuals
making
use
of
pcs
and
the
internet
the
question
started
to
arise
whether
or
not
misuse
or
excessive
use
of
these
new
technologies
could
be
possible
as
well
it
was
hypothesized
that
like
any
technology
aimed
specifically
at
human
consumption
and
use
that
abuse
could
have
severe
consequences
for
the
individual
in
the
short
term
and
for
the
society
in
the
long
term
in
the
late
nineties
people
who
made
use
of
pcs
and
the
internet
where
already
referred
to
the
term
webaholics
or
cyberholics
pratarelli
et
al
suggested
at
that
point
already
to
label
"a
cluster
of
behaviors
potentially
causing
problems"
as
computer
or
internet
addiction
there
are
other
examples
of
computer
overuse
that
date
back
to
the
earliest
computer
games
press
reports
have
furthermore
noted
that
some
finnish
defence
forces
conscripts
were
not
mature
enough
to
meet
the
demands
of
military
life
and
were
required
to
interrupt
or
postpone
military
service
for
a
year
one
reported
source
of
the
lack
of
needed
social
skills
is
overuse
of
computer
games
or
the
internet
forbes
termed
this
overuse
"web
fixations"
and
stated
that
they
were
responsible
for
12
such
interruptions
or
deferrals
over
the
5
years
from
2000–2005
works
cited
laser
50
the
laser
50
is
an
educational
portable
computer
that
ran
the
basic
programming
language
released
in
1984
the
laser
50
used
a
zilog
z80
central
processing
unit
running
at
35
mhz
2
kb
to
18
kb
of
ram
a
12
kb
rom
and
a
80x7
dots
lcd
screen
computer
maintenance
computer
maintenance
is
the
practice
of
keeping
computers
in
a
good
state
of
repair
a
computer
containing
accumulated
dust
and
debris
may
not
run
properly
dust
and
debris
will
accumulate
as
a
result
of
air
cooling
any
filters
used
to
mitigate
this
need
regular
service
and
changes
if
the
cooling
system
is
not
filtered
then
regular
computer
cleaning
may
prevent
short
circuits
and
overheating
the
crumbs
dust
and
other
particulate
that
fall
between
the
keys
and
build
up
underneath
are
loosened
by
spraying
pressurized
air
into
the
keyboard
then
removed
with
a
low-pressure
vacuum
cleaner
a
plastic-cleaning
agent
applied
to
the
surface
of
the
keys
with
a
cloth
is
used
to
remove
the
accumulation
of
oil
and
dirt
from
repeated
contact
with
a
user's
fingertips
if
this
is
not
sufficient
for
a
more
severely
dirty
keyboard
keys
are
physically
removed
for
more
focused
individual
cleaning
or
for
better
access
to
the
area
beneath
finally
the
surface
is
wiped
with
a
disinfectant
a
monitor
displays
information
in
visual
form
using
text
and
graphics
the
portion
of
the
monitor
that
displays
the
information
is
called
the
screen
like
a
television
screen
a
computer
screen
can
show
still
or
moving
pictures
and
it’s
a
part
of
output
devices
fingerprints
water
spots
and
dust
are
removed
from
the
screen
with
a
cleaning
wipe
specialized
for
the
screen
type
(crt
lcd
etc)
a
general
plastic-cleaning
agent
is
used
on
the
outer
casing
which
requires
a
less
gentle
cleanser
but
may
need
more
focused
attention
to
unusual
buildups
of
dust
grime
pen
marks
etc
idiosyncratic
to
the
user
and
environment
the
top
surface
of
the
mouse
is
wiped
with
a
plastic
cleanser
to
remove
the
dirt
that
accumulates
from
contact
with
the
hand
as
on
the
keyboard
the
bottom
surface
is
also
cleaned
to
ensure
that
it
can
slide
freely
if
it
is
a
mechanical
mouse
the
trackball
is
taken
out
not
only
to
clean
the
ball
itself
but
to
scrape
dirt
from
the
runners
that
sense
the
ball's
movement
and
can
become
jittery
or
stuck
if
impeded
by
grime
internal
components
accumulate
dust
brought
in
by
the
airflow
maintained
by
fans
to
keep
the
pc
from
overheating
a
soft
brush
may
remove
loose
dirt;
the
remainder
is
dislodged
with
compressed
air
and
removed
with
a
low-pressure
vacuum
the
case
is
wiped
down
with
a
cleaning
agent
a
pressurized
blower
or
gas
duster
can
remove
dust
that
cannot
be
reached
with
a
brush
important
data
stored
on
computers
may
be
copied
and
archived
securely
so
that
in
the
event
of
failure
the
data
and
systems
may
be
reconstructed
when
major
maintenance
such
as
patching
is
performed
a
backup
is
recommended
as
the
first
step
in
case
the
update
fails
and
reversion
is
required
disk
cleanup
may
be
performed
as
regular
maintenance
to
remove
these
files
may
become
fragmented
and
so
slow
the
performance
of
the
computer
"disk
defragmentation"
may
be
performed
to
combine
these
fragments
and
so
improve
performance
in
the
usa
the
digital
millennium
copyright
act
specifically
exempts
computer-maintenance
activities
so
copies
of
copyright
files
may
be
made
in
the
course
of
maintenance
provided
that
they
are
destroyed
afterwards
operating-system
files
such
as
the
windows
registry
may
require
maintenance
a
utility
such
as
a
registry
cleaner
may
be
used
for
this
also
inbuilt
disk
defragmenter
will
also
help
software
packages
and
operating
systems
may
require
regular
updates
to
correct
software
bugs
and
to
address
security
weaknesses
maintaining
security
involves
vulnerability
management
and
installation
and
proper
operation
of
antivirus
softwares
like
kaspersky
avast
antivirus
mcafee
and
many
are
available
digital
electronic
computer
in
computer
science
a
digital
electronic
computer
is
a
computer
machine
which
is
both
an
electronic
computer
and
a
digital
computer
examples
of
a
digital
electronic
computers
include
the
ibm
pc
the
apple
macintosh
as
well
as
modern
smartphones
when
computers
that
were
both
digital
and
electronic
appeared
they
displaced
almost
all
other
kinds
of
computers
but
computation
has
historically
been
performed
in
various
non-digital
and
non-electronic
ways:
the
lehmer
sieve
is
an
example
of
a
digital
non-electronic
computer
while
analog
computers
are
examples
of
non-digital
computers
which
can
be
electronic
(with
analog
electronics)
and
mechanical
computers
are
examples
of
non-electronic
computers
(which
may
be
digital
or
not)
an
example
of
a
computer
which
is
both
non-digital
and
non-electronic
is
the
ancient
antikythera
mechanism
found
in
greece
all
kinds
of
computers
whether
they
are
digital
or
analog
and
electronic
or
non-electronic
can
be
turing
complete
if
they
have
sufficient
memory
a
digital
electronic
computer
is
not
necessarily
a
programmable
computer
a
stored
program
computer
or
a
general
purpose
computer
since
in
essence
a
digital
electronic
computer
can
be
built
for
one
specific
application
and
be
non-reprogrammable
as
of
2014
most
personal
computers
and
smartphones
in
people's
homes
that
use
multicore
central
processing
units
(such
as
amd
fx
intel
core
i7
or
the
multicore
varieties
of
arm-based
chips)
are
also
parallel
computers
using
the
mimd
(multiple
instructions
-
multiple
data)
paradigm
a
technology
previously
only
used
in
digital
electronic
supercomputers
as
of
2014
most
digital
electronic
supercomputers
are
also
cluster
computers
a
technology
that
can
be
used
at
home
in
the
form
of
small
beowulf
clusters
parallel
computation
is
also
possible
with
non-digital
or
non-electronic
computers
an
example
of
a
parallel
computation
system
using
the
abacus
would
be
a
group
of
human
computers
using
a
number
of
abacus
machines
for
computation
and
communicating
using
natural
language
a
digital
computer
can
perform
its
operations
in
the
decimal
system
in
binary
in
ternary
or
in
other
numeral
systems
as
of
2014
all
digital
electronic
computers
commonly
used
whether
personal
computers
or
supercomputers
are
working
in
the
binary
number
system
and
also
use
binary
logic
a
few
ternary
computers
using
ternary
logic
were
built
mainly
in
the
soviet
union
as
research
projects
a
digital
electronic
computer
is
not
necessarily
a
transistorized
computer:
before
the
advent
of
the
transistor
computers
used
vacuum
tubes
the
transistor
enabled
electronic
computers
to
become
much
more
powerful
and
recent
and
future
developments
in
digital
electronics
may
enable
humanity
to
build
even
more
powerful
electronic
computers
one
such
possible
development
is
the
memristor
people
living
in
the
beginning
of
the
21st
century
use
digital
electronic
computers
for
storing
data
such
as
photos
music
documents
and
for
performing
complex
mathematical
computations
or
for
communication
commonly
over
a
worldwide
computer
network
called
the
internet
which
connects
many
of
the
world's
computers
all
these
activities
made
possible
by
digital
electronic
computers
could
in
essence
be
performed
with
non-digital
or
non-electronic
computers
if
they
were
sufficiently
powerful
but
it
was
only
the
combination
of
electronics
technology
with
digital
computation
in
binary
that
enabled
humanity
to
reach
the
computation
power
necessary
for
today's
computing
advances
in
quantum
computing
dna
computing
optical
computing
or
other
technologies
could
lead
to
the
development
of
more
powerful
computers
in
the
future
digital
computers
are
inherently
best
described
by
discrete
mathematics
while
analog
computers
are
most
commonly
associated
with
continuous
mathematics
the
philosophy
of
digital
physics
views
the
universe
as
being
digital
konrad
zuse
wrote
a
book
known
as
"rechnender
raum"
in
which
he
described
the
whole
universe
as
one
all-encompassing
computer
octagon
systems
octagon
systems
corporation
is
an
industrial
computer
design
and
manufacturing
company
based
in
westminster
colorado
octagon
systems
designs
manufactures
sells
repairs
and
supports
its
line
of
industrial
mobile
and
rugged
computer
systems
for
industries
including
mining
military
transportation
and
others
the
company
has
international
representatives
in
africa
asia
europe
north
america
and
south
america
octagon
systems
was
founded
in
1981
and
introduced
an
embedded
computer
with
a
high
level
language
and
software
development
system
and
operating
systems
on
a
solid
state
disk
octagon’s
services
and
systems
grew
with
new
industrial
computer
system
solutions
including
use
in
the
std
bus
market
and
development
of
single-board
computers
octagon
systems
co-authored
the
epic
(form
factor)
epic™
embedded
computing
specification
that
became
a
world
standard
growing
applicational
use
of
octagon’s
products
led
to
use
in
areas
such
as
public
transportation
systems
rugged
computing
systems
for
mining
operations
as
well
as
others
octagon
systems
products
expanded
into
new
markets
as
the
use
of
industrial
transportation
and
rugged
computer
systems
became
increasingly
common
for
a
wide
array
of
applications
the
us
navy
chose
octagon’
products
for
a
major
long
term
contract
to
support
amphibious
warfare
computing
needs
and
octagon
products
have
been
deployed
in
more
than
100
mines
worldwide
octagon
systems’
xmb
mobile
servers
won
the
flagship
product
award
from
cots
journal
-
the
journal
of
military
electronics
computing
in
2006
octagon
systems
has
been
iso
certified
since
1993
octagon
systems
was
a
founding
member
of
the
small
form
factor
special
interest
group
in
2007
octagon
systems
co-authored
the
epic
(form
factor)
or
epic™
embedded
computing
specification
wdr
paper
computer
the
wdr
paper
computer
or
know-how
computer
was
a
"computer"
that
could
be
easily
assembled
from
a
sheet
of
paper
and
individual
matches
this
would
allow
anyone
interested
to
learn
how
to
program
without
having
an
electronic
computer
at
their
disposal
thus
this
"computer"
served
as
an
educational
aid
in
the
field
of
computer
science
the
know-how
computer
was
developed
by
wolfgang
back
and
ulrich
rohde
and
first
presented
in
the
television
program
wdr
computerclub
in
1983
he
was
also
published
in
german
magazines
mc
and
the
"computer"
worked
on
paper;
matches
were
used
as
information
units
only
five
commands
were
enough
to
represent
all
mathematical
functions
this
exercise
computer
on
paper
was
sent
in
over
400000
copies
and
belonged
at
that
time
to
the
computers
with
the
widest
circulation
an
implementation
as
a
computer
program
is
available
on
wolfgang
back's
website
the
method
of
operation
is
based
on
register
machine
but
is
more
to
the
approach
of
shepherdson
and
sturgis
a
derived
version
of
the
"paper
computer"
is
used
as
a
"know
how
computer"
in
namibian
school
education
wevolver
wevolver
provides
access
to
high
quality
engineering
projects
to
help
people
develop
better
technology
their
stated
mission
is
to
"enable
anyone
anywhere
to
develop
hardware
that
improves
life"
wevolver
10
was
a
project
repository
for
strictly
open
source
projects
but
in
20
it
has
been
expanded
to
include
a
version
control
system
and
is
available
for
use
by
private
teams
in
addition
to
open
source
projects
among
the
projects
using
their
version
control
system
are
openrov
plen
exiii
hackberry
and
inmoov
in
interviews
wevolver
team
members
have
said
the
strongest
part
of
their
platform
is
their
community
with
over
300k
followers
on
instagram
three
news
blogs
and
myriad
hosted
projects
they've
said
the
most
important
part
of
any
open-source
platform
is
the
people
using
it
watiac
watiac
was
a
virtual
computer
developed
for
teaching
the
principles
of
assembly
language
programming
to
undergraduates
watiac
and
the
watmap
assembly
language
that
ran
on
it
were
developed
in
1973
by
the
newly
founded
computer
systems
group
at
the
university
of
waterloo
under
the
direction
of
wes
graham
in
the
1970s
most
programming
was
conducted
through
batch
stream
processing
where
the
operating
systems
of
the
day
like
ibm`s
os-360
would
allow
a
single
program
to
use
all
the
resources
of
a
large
computer
for
a
limited
period
of
time
since
student
programs
were
only
run
a
few
times
possibly
only
once
after
they
had
been
successfully
written
and
debugged
efficient
running
of
those
programs
was
of
relatively
little
importance
compared
with
quick
compilation
and
relatively
good
error
messages
waterloo
had
been
a
leader
in
writing
single
pass
compile-and-go
teaching
compilers
with
first
its
watfor
fortran
compiler
and
its
watbol
cobol
compiler
watmap
was
developed
to
be
a
similar
compile-and-go
teaching
compiler
stanford
dash
stanford
dash
was
a
cache
coherent
multiprocessor
developed
in
the
late
1980s
by
a
group
led
by
anoop
gupta
john
l
hennessy
mark
horowitz
and
monica
s
lam
at
stanford
university
it
was
based
on
adding
a
pair
of
directory
boards
designed
at
stanford
to
up
to
16
sgi
iris
4d
power
series
machines
and
then
cabling
the
systems
in
a
mesh
topology
using
a
stanford-modified
version
of
the
torus
routing
chip
the
boards
designed
at
stanford
implemented
a
directory-based
cache
coherence
protocol
allowing
stanford
dash
to
support
distributed
shared
memory
for
up
to
64
processors
stanford
dash
was
also
notable
for
both
supporting
and
helping
to
formalize
weak
memory
consistency
models
including
release
consistency
because
stanford
dash
was
the
first
operational
machine
to
include
scalable
cache
coherence
it
influenced
subsequent
computer
science
research
as
well
as
the
commercially
available
sgi
origin
2000
stanford
dash
is
included
in
the
25th
anniversary
retrospective
of
selected
papers
from
the
international
symposium
on
computer
architecture
and
several
computer
science
books
has
been
simulated
by
the
university
of
edinburgh
and
is
used
as
a
case
study
in
contemporary
computer
science
classes
computer
programming
computer
programming
is
the
process
of
designing
and
building
an
executable
computer
program
for
accomplishing
a
specific
computing
task
programming
involves
tasks
such
as
analysis
generating
algorithms
profiling
algorithms'
accuracy
and
resource
consumption
and
the
implementation
of
algorithms
in
a
chosen
programming
language
(commonly
referred
to
as
coding)
the
source
code
of
a
program
is
written
in
one
or
more
programming
languages
the
purpose
of
programming
is
to
find
a
sequence
of
instructions
that
will
automate
the
performance
of
a
task
for
solving
a
given
problem
the
process
of
programming
thus
often
requires
expertise
in
several
different
subjects
including
knowledge
of
the
application
domain
specialized
algorithms
and
formal
logic
related
programming
tasks
include
testing
debugging
maintaining
a
program's
source
code
implementation
of
build
systems
and
management
of
derived
artifacts
such
as
machine
code
of
computer
programs
these
might
be
considered
part
of
the
programming
process
but
often
the
term
"software
development"
is
used
for
this
larger
process
with
the
term
"programming"
"implementation"
or
"coding"
reserved
for
the
actual
writing
of
source
code
software
engineering
combines
engineering
techniques
with
software
development
practices
programmable
devices
have
existed
at
least
as
far
back
as
1206
ad
when
the
automata
of
al-jazari
were
programmable
via
pegs
and
cams
to
play
various
rhythms
and
drum
patterns;
and
the
1801
jacquard
loom
could
produce
entirely
different
weaves
by
changing
the
"program"
-
a
series
of
pasteboard
cards
with
holes
punched
in
them
however
the
first
computer
program
is
generally
dated
to
1843
when
mathematician
ada
lovelace
published
an
algorithm
to
calculate
a
sequence
of
bernoulli
numbers
intended
to
be
carried
out
by
charles
babbage's
analytical
engine
women
would
continue
to
dominate
the
field
of
computer
programming
until
the
mid
1960s
in
the
1880s
herman
hollerith
invented
the
concept
of
storing
"data"
in
machine-readable
form
later
a
control
panel
(plugboard)
added
to
his
1906
type
i
tabulator
allowed
it
to
be
programmed
for
different
jobs
and
by
the
late
1940s
unit
record
equipment
such
as
the
ibm
602
and
ibm
604
were
programmed
by
control
panels
in
a
similar
way;
as
were
the
first
electronic
computers
however
with
the
concept
of
the
stored-program
computers
introduced
in
1949
both
programs
and
data
were
stored
and
manipulated
in
the
same
way
in
computer
memory
machine
code
was
the
language
of
early
programs
written
in
the
instruction
set
of
the
particular
machine
often
in
binary
notation
assembly
languages
were
soon
developed
that
let
the
programmer
specify
instruction
in
a
text
format
(eg
add
x
total)
with
abbreviations
for
each
operation
code
and
meaningful
names
for
specifying
addresses
however
because
an
assembly
language
is
little
more
than
a
different
notation
for
a
machine
language
any
two
machines
with
different
instruction
sets
also
have
different
assembly
languages
kathleen
booth
created
one
of
the
first
assembly
languages
in
1950
for
various
computers
at
birkbeck
college
high-level
languages
allow
the
programmer
to
write
programs
in
terms
that
are
syntactically
richer
and
more
capable
of
abstracting
the
code
making
it
targetable
to
varying
machine
instruction
sets
via
compilation
declarations
and
heuristics
the
first
compiler
for
a
programming
language
was
developed
by
grace
hopper
when
hopper
went
to
work
on
univac
in
1949
she
brought
the
idea
of
using
compilers
with
her
compilers
harness
the
power
of
computers
to
make
programming
easier
by
allowing
programmers
to
specify
calculations
by
entering
a
formula
using
infix
notation
(eg
)
for
example
fortran
the
first
widely
used
high-level
language
to
have
a
functional
implementation
which
permitted
the
abstraction
of
reusable
blocks
of
code
came
out
in
1957
in
1951
frances
e
holberton
developed
the
first
sort-merge
generator
which
ran
on
the
univac
i
another
woman
working
at
univac
adele
mildred
koss
developed
a
program
that
was
a
precursor
to
report
generators
in
russia
kateryna
yushchenko
developed
the
address
programming
language
for
the
mesm
in
1955
the
idea
for
the
creation
of
cobol
started
in
1959
when
mary
k
hawes
who
worked
for
burroughs
corporation
set
up
a
meeting
to
discuss
creating
a
common
business
language
she
invited
six
people
including
grace
hopper
hopper
was
involved
in
developing
cobol
as
a
business
language
and
creating
"self-documenting"
programming
hopper's
contribution
to
cobol
was
based
on
her
programming
language
called
flow-matic
in
1961
jean
e
sammet
developed
formac
and
also
published
"programming
languages:
history
and
fundamentals"
which
went
on
to
be
a
standard
work
on
programming
languages
programs
were
mostly
still
entered
using
punched
cards
or
paper
tape
see
computer
programming
in
the
punch
card
era
by
the
late
1960s
data
storage
devices
and
computer
terminals
became
inexpensive
enough
that
programs
could
be
created
by
typing
directly
into
the
computers
frances
holberton
created
a
code
to
allow
keyboard
inputs
while
she
worked
at
univac
text
editors
were
developed
that
allowed
changes
and
corrections
to
be
made
much
more
easily
than
with
punched
cards
sister
mary
kenneth
keller
worked
on
developing
the
programming
language
basic
which
she
was
a
graduate
student
at
dartmouth
in
the
1960s
one
of
the
first
object-oriented
programming
languages
smalltalk
was
developed
by
seven
programmers
including
adele
goldberg
in
the
1970s
in
1985
radia
perlman
developed
the
spinning
tree
protocol
in
order
to
route
packets
of
network
information
efficiently
whatever
the
approach
to
development
may
be
the
final
program
must
satisfy
some
fundamental
properties
the
following
properties
are
among
the
most
important:
in
computer
programming
readability
refers
to
the
ease
with
which
a
human
reader
can
comprehend
the
purpose
control
flow
and
operation
of
source
code
it
affects
the
aspects
of
quality
above
including
portability
usability
and
most
importantly
maintainability
readability
is
important
because
programmers
spend
the
majority
of
their
time
reading
trying
to
understand
and
modifying
existing
source
code
rather
than
writing
new
source
code
unreadable
code
often
leads
to
bugs
inefficiencies
and
duplicated
code
a
study
found
that
a
few
simple
readability
transformations
made
code
shorter
and
drastically
reduced
the
time
to
understand
it
following
a
consistent
programming
style
often
helps
readability
however
readability
is
more
than
just
programming
style
many
factors
having
little
or
nothing
to
do
with
the
ability
of
the
computer
to
efficiently
compile
and
execute
the
code
contribute
to
readability
some
of
these
factors
include:
the
presentation
aspects
of
this
(such
as
indents
line
breaks
color
highlighting
and
so
on)
are
often
handled
by
the
source
code
editor
but
the
content
aspects
reflect
the
programmer's
talent
and
skills
various
visual
programming
languages
have
also
been
developed
with
the
intent
to
resolve
readability
concerns
by
adopting
non-traditional
approaches
to
code
structure
and
display
integrated
development
environments
(ides)
aim
to
integrate
all
such
help
techniques
like
code
refactoring
can
enhance
readability
the
academic
field
and
the
engineering
practice
of
computer
programming
are
both
largely
concerned
with
discovering
and
implementing
the
most
efficient
algorithms
for
a
given
class
of
problem
for
this
purpose
algorithms
are
classified
into
"orders"
using
so-called
big
o
notation
which
expresses
resource
use
such
as
execution
time
or
memory
consumption
in
terms
of
the
size
of
an
input
expert
programmers
are
familiar
with
a
variety
of
well-established
algorithms
and
their
respective
complexities
and
use
this
knowledge
to
choose
algorithms
that
are
best
suited
to
the
circumstances
"programming
a
computer
for
playing
chess"
was
a
1950
paper
that
evaluated
a
"minimax"
algorithm
that
is
part
of
the
history
of
algorithmic
complexity;
a
course
on
ibm's
deep
blue
(chess
computer)
is
part
of
the
computer
science
curriculum
at
stanford
university
the
first
step
in
most
formal
software
development
processes
is
requirements
analysis
followed
by
testing
to
determine
value
modeling
implementation
and
failure
elimination
(debugging)
there
exist
a
lot
of
differing
approaches
for
each
of
those
tasks
one
approach
popular
for
requirements
analysis
is
use
case
analysis
many
programmers
use
forms
of
agile
software
development
where
the
various
stages
of
formal
software
development
are
more
integrated
together
into
short
cycles
that
take
a
few
weeks
rather
than
years
there
are
many
approaches
to
the
software
development
process
popular
modeling
techniques
include
object-oriented
analysis
and
design
(ooad)
and
model-driven
architecture
(mda)
the
unified
modeling
language
(uml)
is
a
notation
used
for
both
the
ooad
and
mda
a
similar
technique
used
for
database
design
is
entity-relationship
modeling
(er
modeling)
implementation
techniques
include
imperative
languages
(object-oriented
or
procedural)
functional
languages
and
logic
languages
it
is
very
difficult
to
determine
what
are
the
most
popular
of
modern
programming
languages
methods
of
measuring
programming
language
popularity
include:
counting
the
number
of
job
advertisements
that
mention
the
language
the
number
of
books
sold
and
courses
teaching
the
language
(this
overestimates
the
importance
of
newer
languages)
and
estimates
of
the
number
of
existing
lines
of
code
written
in
the
language
(this
underestimates
the
number
of
users
of
business
languages
such
as
cobol)
some
languages
are
very
popular
for
particular
kinds
of
applications
while
some
languages
are
regularly
used
to
write
many
different
kinds
of
applications
for
example
cobol
is
still
strong
in
corporate
data
centers
often
on
large
mainframe
computers
fortran
in
engineering
applications
scripting
languages
in
web
development
and
c
in
embedded
software
many
applications
use
a
mix
of
several
languages
in
their
construction
and
use
new
languages
are
generally
designed
around
the
syntax
of
a
prior
language
with
new
functionality
added
(for
example
c++
adds
object-orientation
to
c
and
java
adds
memory
management
and
bytecode
to
c++
but
as
a
result
loses
efficiency
and
the
ability
for
low-level
manipulation)
debugging
is
a
very
important
task
in
the
software
development
process
since
having
defects
in
a
program
can
have
significant
consequences
for
its
users
some
languages
are
more
prone
to
some
kinds
of
faults
because
their
specification
does
not
require
compilers
to
perform
as
much
checking
as
other
languages
use
of
a
static
code
analysis
tool
can
help
detect
some
possible
problems
normally
the
first
step
in
debugging
is
to
attempt
to
reproduce
the
problem
this
can
be
a
non-trivial
task
for
example
as
with
parallel
processes
or
some
unusual
software
bugs
also
specific
user
environment
and
usage
history
can
make
it
difficult
to
reproduce
the
problem
after
the
bug
is
reproduced
the
input
of
the
program
may
need
to
be
simplified
to
make
it
easier
to
debug
for
example
a
bug
in
a
compiler
can
make
it
crash
when
parsing
some
large
source
file
however
after
simplification
of
the
test
case
only
few
lines
from
the
original
source
file
can
be
sufficient
to
reproduce
the
same
crash
such
simplification
can
be
done
manually
using
a
divide-and-conquer
approach
the
programmer
will
try
to
remove
some
parts
of
original
test
case
and
check
if
the
problem
still
exists
when
debugging
the
problem
in
a
gui
the
programmer
can
try
to
skip
some
user
interaction
from
the
original
problem
description
and
check
if
remaining
actions
are
sufficient
for
bugs
to
appear
debugging
is
often
done
with
ides
like
eclipse
visual
studio
xcode
kdevelop
netbeans
and
standalone
debuggers
like
gdb
are
also
used
and
these
often
provide
less
of
a
visual
environment
usually
using
a
command
line
some
text
editors
such
as
emacs
allow
gdb
to
be
invoked
through
them
to
provide
a
visual
environment
different
programming
languages
support
different
styles
of
programming
(called
"programming
paradigms")
the
choice
of
language
used
is
subject
to
many
considerations
such
as
company
policy
suitability
to
task
availability
of
third-party
packages
or
individual
preference
ideally
the
programming
language
best
suited
for
the
task
at
hand
will
be
selected
trade-offs
from
this
ideal
involve
finding
enough
programmers
who
know
the
language
to
build
a
team
the
availability
of
compilers
for
that
language
and
the
efficiency
with
which
programs
written
in
a
given
language
execute
languages
form
an
approximate
spectrum
from
"low-level"
to
"high-level";
"low-level"
languages
are
typically
more
machine-oriented
and
faster
to
execute
whereas
"high-level"
languages
are
more
abstract
and
easier
to
use
but
execute
less
quickly
it
is
usually
easier
to
code
in
"high-level"
languages
than
in
"low-level"
ones
allen
downey
in
his
book
"how
to
think
like
a
computer
scientist"
writes:
many
computer
languages
provide
a
mechanism
to
call
functions
provided
by
shared
libraries
provided
the
functions
in
a
library
follow
the
appropriate
run-time
conventions
(eg
method
of
passing
arguments)
then
these
functions
may
be
written
in
any
other
language
computer
programmers
are
those
who
write
computer
software
their
jobs
usually
involve:
information
technology
information
technology
(it)
is
the
use
of
computers
to
store
retrieve
transmit
and
manipulate
data
or
information
often
in
the
context
of
a
business
or
other
enterprise
it
is
considered
to
be
a
subset
of
information
and
communications
technology
(ict)
an
information
technology
system
(it
system)
is
generally
an
information
system
a
communications
system
or
more
specifically
speaking
a
computer
system
–
including
all
hardware
software
and
peripheral
equipment
–
operated
by
a
limited
group
of
users
humans
have
been
storing
retrieving
manipulating
and
communicating
information
since
the
sumerians
in
mesopotamia
developed
writing
in
about
3000 bc
but
the
term
"information
technology"
in
its
modern
sense
first
appeared
in
a
1958
article
published
in
the
"harvard
business
review";
authors
harold
j
leavitt
and
thomas
l
whisler
commented
that
"the
new
technology
does
not
yet
have
a
single
established
name
we
shall
call
it
information
technology
(it)"
their
definition
consists
of
three
categories:
techniques
for
processing
the
application
of
statistical
and
mathematical
methods
to
decision-making
and
the
simulation
of
higher-order
thinking
through
computer
programs
the
term
is
commonly
used
as
a
synonym
for
computers
and
computer
networks
but
it
also
encompasses
other
information
distribution
technologies
such
as
television
and
telephones
several
products
or
services
within
an
economy
are
associated
with
information
technology
including
computer
hardware
software
electronics
semiconductors
internet
telecom
equipment
and
e-commerce
based
on
the
storage
and
processing
technologies
employed
it
is
possible
to
distinguish
four
distinct
phases
of
it
development:
pre-mechanical
(3000 bc –
1450 ad)
mechanical
(1450–1840)
electromechanical
(1840–1940)
and
electronic
(1940–present)
this
article
focuses
on
the
most
recent
period
(electronic)
which
began
in
about
1940
devices
have
been
used
to
aid
computation
for
thousands
of
years
probably
initially
in
the
form
of
a
tally
stick
the
antikythera
mechanism
dating
from
about
the
beginning
of
the
first
century
bc
is
generally
considered
to
be
the
earliest
known
mechanical
analog
computer
and
the
earliest
known
geared
mechanism
comparable
geared
devices
did
not
emerge
in
europe
until
the
16th
century
and
it
was
not
until
1645
that
the
first
mechanical
calculator
capable
of
performing
the
four
basic
arithmetical
operations
was
developed
electronic
computers
using
either
relays
or
valves
began
to
appear
in
the
early
1940s
the
electromechanical
zuse
z3
completed
in
1941
was
the
world's
first
programmable
computer
and
by
modern
standards
one
of
the
first
machines
that
could
be
considered
a
complete
computing
machine
colossus
developed
during
the
second
world
war
to
decrypt
german
messages
was
the
first
electronic
digital
computer
although
it
was
programmable
it
was
not
general-purpose
being
designed
to
perform
only
a
single
task
it
also
lacked
the
ability
to
store
its
program
in
memory;
programming
was
carried
out
using
plugs
and
switches
to
alter
the
internal
wiring
the
first
recognisably
modern
electronic
digital
stored-program
computer
was
the
manchester
baby
which
ran
its
first
program
on
21
june
1948
the
development
of
transistors
in
the
late
1940s
at
bell
laboratories
allowed
a
new
generation
of
computers
to
be
designed
with
greatly
reduced
power
consumption
the
first
commercially
available
stored-program
computer
the
ferranti
mark
i
contained
4050
valves
and
had
a
power
consumption
of
25
kilowatts
by
comparison
the
first
transistorised
computer
developed
at
the
university
of
manchester
and
operational
by
november
1953
consumed
only
150
watts
in
its
final
version
early
electronic
computers
such
as
colossus
made
use
of
punched
tape
a
long
strip
of
paper
on
which
data
was
represented
by
a
series
of
holes
a
technology
now
obsolete
electronic
data
storage
which
is
used
in
modern
computers
dates
from
world
war
ii
when
a
form
of
delay
line
memory
was
developed
to
remove
the
clutter
from
radar
signals
the
first
practical
application
of
which
was
the
mercury
delay
line
the
first
random-access
digital
storage
device
was
the
williams
tube
based
on
a
standard
cathode
ray
tube
but
the
information
stored
in
it
and
delay
line
memory
was
volatile
in
that
it
had
to
be
continuously
refreshed
and
thus
was
lost
once
power
was
removed
the
earliest
form
of
non-volatile
computer
storage
was
the
magnetic
drum
invented
in
1932
and
used
in
the
ferranti
mark
1
the
world's
first
commercially
available
general-purpose
electronic
computer
ibm
introduced
the
first
hard
disk
drive
in
1956
as
a
component
of
their
305
ramac
computer
system
most
digital
data
today
is
still
stored
magnetically
on
hard
disks
or
optically
on
media
such
as
cd-roms
until
2002
most
information
was
stored
on
analog
devices
but
that
year
digital
storage
capacity
exceeded
analog
for
the
first
time
as
of
2007
almost
94%
of
the
data
stored
worldwide
was
held
digitally:
52%
on
hard
disks
28%
on
optical
devices
and
11%
on
digital
magnetic
tape
it
has
been
estimated
that
the
worldwide
capacity
to
store
information
on
electronic
devices
grew
from
less
than
3
exabytes
in
1986
to
295
exabytes
in
2007
doubling
roughly
every
3
years
database
management
systems
emerged
in
the
1960s
to
address
the
problem
of
storing
and
retrieving
large
amounts
of
data
accurately
and
quickly
one
of
the
earliest
such
systems
was
ibm's
information
management
system
(ims)
which
is
still
widely
deployed
more
than
50
years
later
ims
stores
data
hierarchically
but
in
the
1970s
ted
codd
proposed
an
alternative
relational
storage
model
based
on
set
theory
and
predicate
logic
and
the
familiar
concepts
of
tables
rows
and
columns
the
first
commercially
available
relational
database
management
system
(rdbms)
was
available
from
oracle
in
1981
all
database
management
systems
consist
of
a
number
of
components
that
together
allow
the
data
they
store
to
be
accessed
simultaneously
by
many
users
while
maintaining
its
integrity
a
characteristic
of
all
databases
is
that
the
structure
of
the
data
they
contain
is
defined
and
stored
separately
from
the
data
itself
in
a
database
schema
the
extensible
markup
language
(xml)
has
become
a
popular
format
for
data
representation
in
recent
years
although
xml
data
can
be
stored
in
normal
file
systems
it
is
commonly
held
in
relational
databases
to
take
advantage
of
their
"robust
implementation
verified
by
years
of
both
theoretical
and
practical
effort"
as
an
evolution
of
the
standard
generalized
markup
language
(sgml)
xml's
text-based
structure
offers
the
advantage
of
being
both
machine
and
human-readable
the
relational
database
model
introduced
a
programming-language
independent
structured
query
language
(sql)
based
on
relational
algebra
the
terms
"data"
and
"information"
are
not
synonymous
anything
stored
is
data
but
it
only
becomes
information
when
it
is
organized
and
presented
meaningfully
most
of
the
world's
digital
data
is
unstructured
and
stored
in
a
variety
of
different
physical
formats
even
within
a
single
organization
data
warehouses
began
to
be
developed
in
the
1980s
to
integrate
these
disparate
stores
they
typically
contain
data
extracted
from
various
sources
including
external
sources
such
as
the
internet
organized
in
such
a
way
as
to
facilitate
decision
support
systems
(dss)
data
transmission
has
three
aspects:
transmission
propagation
and
reception
it
can
be
broadly
categorized
as
broadcasting
in
which
information
is
transmitted
unidirectionally
downstream
or
telecommunications
with
bidirectional
upstream
and
downstream
channels
xml
has
been
increasingly
employed
as
a
means
of
data
interchange
since
the
early
2000s
particularly
for
machine-oriented
interactions
such
as
those
involved
in
web-oriented
protocols
such
as
soap
describing
"data-in-transit
rather
than 
data-at-rest"
one
of
the
challenges
of
such
usage
is
converting
data
from
relational
databases
into
xml
document
object
model
(dom)
structures
hilbert
and
lopez
identify
the
exponential
pace
of
technological
change
(a
kind
of
moore's
law):
machines'
application-specific
capacity
to
compute
information
per
capita
roughly
doubled
every
14
months
between
1986
and
2007;
the
per
capita
capacity
of
the
world's
general-purpose
computers
doubled
every
18
months
during
the
same
two
decades;
the
global
telecommunication
capacity
per
capita
doubled
every
34
months;
the
world's
storage
capacity
per
capita
required
roughly
40
months
to
double
(every
3
years);
and
per
capita
broadcast
information
has
doubled
every
123
years
massive
amounts
of
data
are
stored
worldwide
every
day
but
unless
it
can
be
analysed
and
presented
effectively
it
essentially
resides
in
what
have
been
called
data
tombs:
"data
archives
that
are
seldom
visited"
to
address
that
issue
the
field
of
data
mining –
"the
process
of
discovering
interesting
patterns
and
knowledge
from
large
amounts
of
data" –
emerged
in
the
late
1980s
in
an
academic
context
the
association
for
computing
machinery
defines
it
as
"undergraduate
degree
programs
that
prepare
students
to
meet
the
computer
technology
needs
of
business
government
healthcare
schools
and
other
kinds
of
organizations 
it
specialists
assume
responsibility
for
selecting
hardware
and
software
products
appropriate
for
an
organization
integrating
those
products
with
organizational
needs
and
infrastructure
and
installing
customizing
and
maintaining
those
applications
for
the
organization’s
computer
users"
companies
in
the
information
technology
field
are
often
discussed
as
a
group
as
the
"tech
sector"
or
the
"tech
industry"
in
a
business
context
the
information
technology
association
of
america
has
defined
information
technology
as
"the
study
design
development
application
implementation
support
or
management
of
computer-based
information
systems"
the
responsibilities
of
those
working
in
the
field
include
network
administration
software
development
and
installation
and
the
planning
and
management
of
an
organization's
technology
life
cycle
by
which
hardware
and
software
are
maintained
upgraded
and
replaced
the
business
value
of
information
technology
lies
in
the
automation
of
business
processes
provision
of
information
for
decision
making
connecting
businesses
with
their
customers
and
the
provision
of
productivity
tools
to
increase
efficiency
the
field
of
information
ethics
was
established
by
mathematician
norbert
wiener
in
the
1940s
some
of
the
ethical
issues
associated
with
the
use
of
information
technology
include:
lithotope
a
lithotope
is
either
an
environment
in
which
a
sediment
was
deposited
or
an
area
of
uniform
sedimentation
1
surface
or
area
of
uniform
precipitation
2
sediment
having
a
relatively
homogeneous
sedimentation
environment
atmosphere
of
earth
the
atmosphere
of
earth
is
the
layer
of
gases
commonly
known
as
air
that
surrounds
the
planet
earth
and
is
retained
by
earth's
gravity
the
atmosphere
of
earth
protects
life
on
earth
by
creating
pressure
allowing
for
liquid
water
to
exist
on
the
earth's
surface
absorbing
ultraviolet
solar
radiation
warming
the
surface
through
heat
retention
(greenhouse
effect)
and
reducing
temperature
extremes
between
day
and
night
(the
diurnal
temperature
variation)
by
volume
dry
air
contains
7809%
nitrogen
2095%
oxygen
093%
argon
004%
carbon
dioxide
and
small
amounts
of
other
gases
air
also
contains
a
variable
amount
of
water
vapor
on
average
around
1%
at
sea
level
and
04%
over
the
entire
atmosphere
air
content
and
atmospheric
pressure
vary
at
different
layers
and
air
suitable
for
use
in
photosynthesis
by
terrestrial
plants
and
breathing
of
terrestrial
animals
is
found
only
in
earth's
troposphere
and
in
artificial
atmospheres
the
atmosphere
has
a
mass
of
about
515 kg
three
quarters
of
which
is
within
about
of
the
surface
the
atmosphere
becomes
thinner
and
thinner
with
increasing
altitude
with
no
definite
boundary
between
the
atmosphere
and
outer
space
the
kármán
line
at
or
157%
of
earth's
radius
is
often
used
as
the
border
between
the
atmosphere
and
outer
space
atmospheric
effects
become
noticeable
during
atmospheric
reentry
of
spacecraft
at
an
altitude
of
around
several
layers
can
be
distinguished
in
the
atmosphere
based
on
characteristics
such
as
temperature
and
composition
the
study
of
earth's
atmosphere
and
its
processes
is
called
atmospheric
science
(aerology)
early
pioneers
in
the
field
include
léon
teisserenc
de
bort
and
richard
assmann
the
three
major
constituents
of
earth's
atmosphere
are
nitrogen
oxygen
and
argon
water
vapor
accounts
for
roughly
025%
of
the
atmosphere
by
mass
the
concentration
of
water
vapor
(a
greenhouse
gas)
varies
significantly
from
around
10
ppm
by
volume
in
the
coldest
portions
of
the
atmosphere
to
as
much
as
5%
by
volume
in
hot
humid
air
masses
and
concentrations
of
other
atmospheric
gases
are
typically
quoted
in
terms
of
dry
air
(without
water
vapor)
the
remaining
gases
are
often
referred
to
as
trace
gases
among
which
are
the
greenhouse
gases
principally
carbon
dioxide
methane
nitrous
oxide
and
ozone
filtered
air
includes
trace
amounts
of
many
other
chemical
compounds
many
substances
of
natural
origin
may
be
present
in
locally
and
seasonally
variable
small
amounts
as
aerosols
in
an
unfiltered
air
sample
including
dust
of
mineral
and
organic
composition
pollen
and
spores
sea
spray
and
volcanic
ash
various
industrial
pollutants
also
may
be
present
as
gases
or
aerosols
such
as
chlorine
(elemental
or
in
compounds)
fluorine
compounds
and
elemental
mercury
vapor
sulfur
compounds
such
as
hydrogen
sulfide
and
sulfur
dioxide
(so)
may
be
derived
from
natural
sources
or
from
industrial
air
pollution
the
relative
concentration
of
gases
remains
constant
until
about
in
general
air
pressure
and
density
decrease
with
altitude
in
the
atmosphere
however
temperature
has
a
more
complicated
profile
with
altitude
and
may
remain
relatively
constant
or
even
increase
with
altitude
in
some
regions
(see
the
temperature
section
below)
because
the
general
pattern
of
the
temperature/altitude
profile
is
constant
and
measurable
by
means
of
instrumented
balloon
soundings
the
temperature
behavior
provides
a
useful
metric
to
distinguish
atmospheric
layers
in
this
way
earth's
atmosphere
can
be
divided
(called
atmospheric
stratification)
into
five
main
layers
excluding
the
exosphere
the
atmosphere
has
four
primary
layers
which
are
the
troposphere
stratosphere
mesosphere
and
thermosphere
from
highest
to
lowest
the
five
main
layers
are:
the
exosphere
is
the
outermost
layer
of
earth's
atmosphere
(ie
the
upper
limit
of
the
atmosphere)
it
extends
from
the
exobase
which
is
located
at
the
top
of
the
thermosphere
at
an
altitude
of
about
700 km
above
sea
level
to
about
10000 km
(6200 mi;
33000000 ft)
where
it
merges
into
the
solar
wind
this
layer
is
mainly
composed
of
extremely
low
densities
of
hydrogen
helium
and
several
heavier
molecules
including
nitrogen
oxygen
and
carbon
dioxide
closer
to
the
exobase
the
atoms
and
molecules
are
so
far
apart
that
they
can
travel
hundreds
of
kilometers
without
colliding
with
one
another
thus
the
exosphere
no
longer
behaves
like
a
gas
and
the
particles
constantly
escape
into
space
these
free-moving
particles
follow
ballistic
trajectories
and
may
migrate
in
and
out
of
the
magnetosphere
or
the
solar
wind
the
exosphere
is
located
too
far
above
earth
for
any
meteorological
phenomena
to
be
possible
however
the
aurora
borealis
and
aurora
australis
sometimes
occur
in
the
lower
part
of
the
exosphere
where
they
overlap
into
the
thermosphere
the
exosphere
contains
most
of
the
satellites
orbiting
earth
the
thermosphere
is
the
second-highest
layer
of
earth's
atmosphere
it
extends
from
the
mesopause
(which
separates
it
from
the
mesosphere)
at
an
altitude
of
about
up
to
the
thermopause
at
an
altitude
range
of
the
height
of
the
thermopause
varies
considerably
due
to
changes
in
solar
activity
because
the
thermopause
lies
at
the
lower
boundary
of
the
exosphere
it
is
also
referred
to
as
the
exobase
the
lower
part
of
the
thermosphere
from
above
earth's
surface
contains
the
ionosphere
the
temperature
of
the
thermosphere
gradually
increases
with
height
unlike
the
stratosphere
beneath
it
wherein
a
temperature
inversion
is
due
to
the
absorption
of
radiation
by
ozone
the
inversion
in
the
thermosphere
occurs
due
to
the
extremely
low
density
of
its
molecules
the
temperature
of
this
layer
can
rise
as
high
as
though
the
gas
molecules
are
so
far
apart
that
its
temperature
in
the
usual
sense
is
not
very
meaningful
the
air
is
so
rarefied
that
an
individual
molecule
(of
oxygen
for
example)
travels
an
average
of
between
collisions
with
other
molecules
although
the
thermosphere
has
a
high
proportion
of
molecules
with
high
energy
it
would
not
feel
hot
to
a
human
in
direct
contact
because
its
density
is
too
low
to
conduct
a
significant
amount
of
energy
to
or
from
the
skin
this
layer
is
completely
cloudless
and
free
of
water
vapor
however
non-hydrometeorological
phenomena
such
as
the
aurora
borealis
and
aurora
australis
are
occasionally
seen
in
the
thermosphere
the
international
space
station
orbits
in
this
layer
between
the
mesosphere
is
the
third
highest
layer
of
earth's
atmosphere
occupying
the
region
above
the
stratosphere
and
below
the
thermosphere
it
extends
from
the
stratopause
at
an
altitude
of
about
to
the
mesopause
at
above
sea
level
temperatures
drop
with
increasing
altitude
to
the
mesopause
that
marks
the
top
of
this
middle
layer
of
the
atmosphere
it
is
the
coldest
place
on
earth
and
has
an
average
temperature
around
just
below
the
mesopause
the
air
is
so
cold
that
even
the
very
scarce
water
vapor
at
this
altitude
can
be
sublimated
into
polar-mesospheric
noctilucent
clouds
these
are
the
highest
clouds
in
the
atmosphere
and
may
be
visible
to
the
naked
eye
if
sunlight
reflects
off
them
about
an
hour
or
two
after
sunset
or
a
similar
length
of
time
before
sunrise
they
are
most
readily
visible
when
the
sun
is
around
4
to
16
degrees
below
the
horizon
lightning-induced
discharges
known
as
transient
luminous
events
(tles)
occasionally
form
in
the
mesosphere
above
tropospheric
thunderclouds
the
mesosphere
is
also
the
layer
where
most
meteors
burn
up
upon
atmospheric
entrance
it
is
too
high
above
earth
to
be
accessible
to
jet-powered
aircraft
and
balloons
and
too
low
to
permit
orbital
spacecraft
the
mesosphere
is
mainly
accessed
by
sounding
rockets
and
rocket-powered
aircraft
the
stratosphere
is
the
second-lowest
layer
of
earth's
atmosphere
it
lies
above
the
troposphere
and
is
separated
from
it
by
the
tropopause
this
layer
extends
from
the
top
of
the
troposphere
at
roughly
above
earth's
surface
to
the
stratopause
at
an
altitude
of
about
the
atmospheric
pressure
at
the
top
of
the
stratosphere
is
roughly
1/1000
the
pressure
at
sea
level
it
contains
the
ozone
layer
which
is
the
part
of
earth's
atmosphere
that
contains
relatively
high
concentrations
of
that
gas
the
stratosphere
defines
a
layer
in
which
temperatures
rise
with
increasing
altitude
this
rise
in
temperature
is
caused
by
the
absorption
of
ultraviolet
radiation
(uv)
radiation
from
the
sun
by
the
ozone
layer
which
restricts
turbulence
and
mixing
although
the
temperature
may
be
at
the
tropopause
the
top
of
the
stratosphere
is
much
warmer
and
may
be
near
0 °c
the
stratospheric
temperature
profile
creates
very
stable
atmospheric
conditions
so
the
stratosphere
lacks
the
weather-producing
air
turbulence
that
is
so
prevalent
in
the
troposphere
consequently
the
stratosphere
is
almost
completely
free
of
clouds
and
other
forms
of
weather
however
polar
stratospheric
or
nacreous
clouds
are
occasionally
seen
in
the
lower
part
of
this
layer
of
the
atmosphere
where
the
air
is
coldest
the
stratosphere
is
the
highest
layer
that
can
be
accessed
by
jet-powered
aircraft
the
troposphere
is
the
lowest
layer
of
earth's
atmosphere
it
extends
from
earth's
surface
to
an
average
height
of
about
although
this
altitude
varies
from
about
at
the
geographic
poles
to
at
the
equator
with
some
variation
due
to
weather
the
troposphere
is
bounded
above
by
the
tropopause
a
boundary
marked
in
most
places
by
a
temperature
inversion
(ie
a
layer
of
relatively
warm
air
above
a
colder
one)
and
in
others
by
a
zone
which
is
isothermal
with
height
although
variations
do
occur
the
temperature
usually
declines
with
increasing
altitude
in
the
troposphere
because
the
troposphere
is
mostly
heated
through
energy
transfer
from
the
surface
thus
the
lowest
part
of
the
troposphere
(ie
earth's
surface)
is
typically
the
warmest
section
of
the
troposphere
this
promotes
vertical
mixing
(hence
the
origin
of
its
name
in
the
greek
word
τρόπος
"tropos"
meaning
"turn")
the
troposphere
contains
roughly
80%
of
the
mass
of
earth's
atmosphere
the
troposphere
is
denser
than
all
its
overlying
atmospheric
layers
because
a
larger
atmospheric
weight
sits
on
top
of
the
troposphere
and
causes
it
to
be
most
severely
compressed
fifty
percent
of
the
total
mass
of
the
atmosphere
is
located
in
the
lower
of
the
troposphere
nearly
all
atmospheric
water
vapor
or
moisture
is
found
in
the
troposphere
so
it
is
the
layer
where
most
of
earth's
weather
takes
place
it
has
basically
all
the
weather-associated
cloud
genus
types
generated
by
active
wind
circulation
although
very
tall
cumulonimbus
thunder
clouds
can
penetrate
the
tropopause
from
below
and
rise
into
the
lower
part
of
the
stratosphere
most
conventional
aviation
activity
takes
place
in
the
troposphere
and
it
is
the
only
layer
that
can
be
accessed
by
propeller-driven
aircraft
within
the
five
principal
layers
above
that
are
largely
determined
by
temperature
several
secondary
layers
may
be
distinguished
by
other
properties:
the
average
temperature
of
the
atmosphere
at
earth's
surface
is
or
depending
on
the
reference
the
average
atmospheric
pressure
at
sea
level
is
defined
by
the
international
standard
atmosphere
as
this
is
sometimes
referred
to
as
a
unit
of
standard
atmospheres
(atm)
total
atmospheric
mass
is
51480×10
kg
(1135×10
lb)
about
25%
less
than
would
be
inferred
from
the
average
sea
level
pressure
and
earth's
area
of
510072
megahectares
this
portion
being
displaced
by
earth's
mountainous
terrain
atmospheric
pressure
is
the
total
weight
of
the
air
above
unit
area
at
the
point
where
the
pressure
is
measured
thus
air
pressure
varies
with
location
and
weather
if
the
entire
mass
of
the
atmosphere
had
a
uniform
density
equal
to
sea
level
density
(about
12
kg
per
m)
from
sea
level
upwards
it
would
terminate
abruptly
at
an
altitude
of
it
actually
decreases
exponentially
with
altitude
dropping
by
half
every
or
by
a
factor
of
1/e
every
the
average
scale
height
of
the
atmosphere
below
however
the
atmosphere
is
more
accurately
modeled
with
a
customized
equation
for
each
layer
that
takes
gradients
of
temperature
molecular
composition
solar
radiation
and
gravity
into
account
in
summary
the
mass
of
earth's
atmosphere
is
distributed
approximately
as
follows:
by
comparison
the
summit
of
mt
everest
is
at
;
commercial
airliners
typically
cruise
between
where
the
thinner
air
improves
fuel
economy;
weather
balloons
reach
and
above;
and
the
highest
x-15
flight
in
1963
reached
even
above
the
kármán
line
significant
atmospheric
effects
such
as
auroras
still
occur
meteors
begin
to
glow
in
this
region
though
the
larger
ones
may
not
burn
up
until
they
penetrate
more
deeply
the
various
layers
of
earth's
ionosphere
important
to
hf
radio
propagation
begin
below
100 km
and
extend
beyond
500 km
by
comparison
the
international
space
station
and
space
shuttle
typically
orbit
at
350–400 km
within
the
f-layer
of
the
ionosphere
where
they
encounter
enough
atmospheric
drag
to
require
reboosts
every
few
months
depending
on
solar
activity
satellites
can
experience
noticeable
atmospheric
drag
at
altitudes
as
high
as
700–800 km
the
division
of
the
atmosphere
into
layers
mostly
by
reference
to
temperature
is
discussed
above
temperature
decreases
with
altitude
starting
at
sea
level
but
variations
in
this
trend
begin
above
11 km
where
the
temperature
stabilizes
through
a
large
vertical
distance
through
the
rest
of
the
troposphere
in
the
stratosphere
starting
above
about
20 km
the
temperature
increases
with
height
due
to
heating
within
the
ozone
layer
caused
by
capture
of
significant
ultraviolet
radiation
from
the
sun
by
the
dioxygen
and
ozone
gas
in
this
region
still
another
region
of
increasing
temperature
with
altitude
occurs
at
very
high
altitudes
in
the
aptly-named
thermosphere
above
90 km
because
in
an
ideal
gas
of
constant
composition
the
speed
of
sound
depends
only
on
temperature
and
not
on
the
gas
pressure
or
density
the
speed
of
sound
in
the
atmosphere
with
altitude
takes
on
the
form
of
the
complicated
temperature
profile
(see
illustration
to
the
right)
and
does
not
mirror
altitudinal
changes
in
density
or
pressure
the
density
of
air
at
sea
level
is
about
12 kg/m
(12 g/l
00012
g/cm)
density
is
not
measured
directly
but
is
calculated
from
measurements
of
temperature
pressure
and
humidity
using
the
equation
of
state
for
air
(a
form
of
the
ideal
gas
law)
atmospheric
density
decreases
as
the
altitude
increases
this
variation
can
be
approximately
modeled
using
the
barometric
formula
more
sophisticated
models
are
used
to
predict
orbital
decay
of
satellites
the
average
mass
of
the
atmosphere
is
about
5
quadrillion
(5)
tonnes
or
1/1200000
the
mass
of
earth
according
to
the
american
national
center
for
atmospheric
research
"the
total
mean
mass
of
the
atmosphere
is
51480 kg
with
an
annual
range
due
to
water
vapor
of
12
or
15 kg
depending
on
whether
surface
pressure
or
water
vapor
data
are
used;
somewhat
smaller
than
the
previous
estimate
the
mean
mass
of
water
vapor
is
estimated
as
127 kg
and
the
dry
air
mass
as
51352
±00003 kg"
solar
radiation
(or
sunlight)
is
the
energy
earth
receives
from
the
sun
earth
also
emits
radiation
back
into
space
but
at
longer
wavelengths
that
we
cannot
see
part
of
the
incoming
and
emitted
radiation
is
absorbed
or
reflected
by
the
atmosphere
in
may
2017
glints
of
light
seen
as
twinkling
from
an
orbiting
satellite
a
million
miles
away
were
found
to
be
reflected
light
from
ice
crystals
in
the
atmosphere
when
light
passes
through
earth's
atmosphere
photons
interact
with
it
through
"scattering"
if
the
light
does
not
interact
with
the
atmosphere
it
is
called
"direct
radiation"
and
is
what
you
see
if
you
were
to
look
directly
at
the
sun
"indirect
radiation"
is
light
that
has
been
scattered
in
the
atmosphere
for
example
on
an
overcast
day
when
you
cannot
see
your
shadow
there
is
no
direct
radiation
reaching
you
it
has
all
been
scattered
as
another
example
due
to
a
phenomenon
called
rayleigh
scattering
shorter
(blue)
wavelengths
scatter
more
easily
than
longer
(red)
wavelengths
this
is
why
the
sky
looks
blue;
you
are
seeing
scattered
blue
light
this
is
also
why
sunsets
are
red
because
the
sun
is
close
to
the
horizon
the
sun's
rays
pass
through
more
atmosphere
than
normal
to
reach
your
eye
much
of
the
blue
light
has
been
scattered
out
leaving
the
red
light
in
a
sunset
different
molecules
absorb
different
wavelengths
of
radiation
for
example
o
and
o
absorb
almost
all
wavelengths
shorter
than
300
nanometers
water
(ho)
absorbs
many
wavelengths
above
700 nm
when
a
molecule
absorbs
a
photon
it
increases
the
energy
of
the
molecule
this
heats
the
atmosphere
but
the
atmosphere
also
cools
by
emitting
radiation
as
discussed
below
the
combined
absorption
spectra
of
the
gases
in
the
atmosphere
leave
"windows"
of
low
opacity
allowing
the
transmission
of
only
certain
bands
of
light
the
optical
window
runs
from
around
300 nm
(ultraviolet-c)
up
into
the
range
humans
can
see
the
visible
spectrum
(commonly
called
light)
at
roughly
400–700 nm
and
continues
to
the
infrared
to
around
1100 nm
there
are
also
infrared
and
radio
windows
that
transmit
some
infrared
and
radio
waves
at
longer
wavelengths
for
example
the
radio
window
runs
from
about
one
centimeter
to
about
eleven-meter
waves
"emission"
is
the
opposite
of
absorption
it
is
when
an
object
emits
radiation
objects
tend
to
emit
amounts
and
wavelengths
of
radiation
depending
on
their
"black
body"
emission
curves
therefore
hotter
objects
tend
to
emit
more
radiation
with
shorter
wavelengths
colder
objects
emit
less
radiation
with
longer
wavelengths
for
example
the
sun
is
approximately
its
radiation
peaks
near
500 nm
and
is
visible
to
the
human
eye
earth
is
approximately
so
its
radiation
peaks
near
10000 nm
and
is
much
too
long
to
be
visible
to
humans
because
of
its
temperature
the
atmosphere
emits
infrared
radiation
for
example
on
clear
nights
earth's
surface
cools
down
faster
than
on
cloudy
nights
this
is
because
clouds
(ho)
are
strong
absorbers
and
emitters
of
infrared
radiation
this
is
also
why
it
becomes
colder
at
night
at
higher
elevations
the
greenhouse
effect
is
directly
related
to
this
absorption
and
emission
effect
some
gases
in
the
atmosphere
absorb
and
emit
infrared
radiation
but
do
not
interact
with
sunlight
in
the
visible
spectrum
common
examples
of
these
are
and
ho
the
refractive
index
of
air
is
close
to
but
just
greater
than
1
systematic
variations
in
refractive
index
can
lead
to
the
bending
of
light
rays
over
long
optical
paths
one
example
is
that
under
some
circumstances
observers
onboard
ships
can
see
other
vessels
just
over
the
horizon
because
light
is
refracted
in
the
same
direction
as
the
curvature
of
earth's
surface
the
refractive
index
of
air
depends
on
temperature
giving
rise
to
refraction
effects
when
the
temperature
gradient
is
large
an
example
of
such
effects
is
the
mirage
"atmospheric
circulation"
is
the
large-scale
movement
of
air
through
the
troposphere
and
the
means
(with
ocean
circulation)
by
which
heat
is
distributed
around
earth
the
large-scale
structure
of
the
atmospheric
circulation
varies
from
year
to
year
but
the
basic
structure
remains
fairly
constant
because
it
is
determined
by
earth's
rotation
rate
and
the
difference
in
solar
radiation
between
the
equator
and
poles
the
first
atmosphere
consisted
of
gases
in
the
solar
nebula
primarily
hydrogen
there
were
probably
simple
hydrides
such
as
those
now
found
in
the
gas
giants
(jupiter
and
saturn)
notably
water
vapor
methane
and
ammonia
outgassing
from
volcanism
supplemented
by
gases
produced
during
the
late
heavy
bombardment
of
earth
by
huge
asteroids
produced
the
next
atmosphere
consisting
largely
of
nitrogen
plus
carbon
dioxide
and
inert
gases
a
major
part
of
carbon-dioxide
emissions
dissolved
in
water
and
reacted
with
metals
such
as
calcium
and
magnesium
during
weathering
of
crustal
rocks
to
form
carbonates
that
were
deposited
as
sediments
water-related
sediments
have
been
found
that
date
from
as
early
as
38
billion
years
ago
about
34
billion
years
ago
nitrogen
formed
the
major
part
of
the
then
stable
"second
atmosphere"
the
influence
of
life
has
to
be
taken
into
account
rather
soon
in
the
history
of
the
atmosphere
because
hints
of
early
life-forms
appear
as
early
as
35
billion
years
ago
how
earth
at
that
time
maintained
a
climate
warm
enough
for
liquid
water
and
life
if
the
early
sun
put
out
30%
lower
solar
radiance
than
today
is
a
puzzle
known
as
the
"faint
young
sun
paradox"
the
geological
record
however
shows
a
continuous
relatively
warm
surface
during
the
complete
early
temperature
record
of
earth
–
with
the
exception
of
one
cold
glacial
phase
about
24
billion
years
ago
in
the
late
archean
eon
an
oxygen-containing
atmosphere
began
to
develop
apparently
produced
by
photosynthesizing
cyanobacteria
(see
great
oxygenation
event)
which
have
been
found
as
stromatolite
fossils
from
27
billion
years
ago
the
early
basic
carbon
isotopy
(isotope
ratio
proportions)
strongly
suggests
conditions
similar
to
the
current
and
that
the
fundamental
features
of
the
carbon
cycle
became
established
as
early
as
4
billion
years
ago
ancient
sediments
in
the
gabon
dating
from
between
about
2150
and
2080
million
years
ago
provide
a
record
of
earth's
dynamic
oxygenation
evolution
these
fluctuations
in
oxygenation
were
likely
driven
by
the
lomagundi
carbon
isotope
excursion
the
constant
re-arrangement
of
continents
by
plate
tectonics
influences
the
long-term
evolution
of
the
atmosphere
by
transferring
carbon
dioxide
to
and
from
large
continental
carbonate
stores
free
oxygen
did
not
exist
in
the
atmosphere
until
about
24
billion
years
ago
during
the
great
oxygenation
event
and
its
appearance
is
indicated
by
the
end
of
the
banded
iron
formations
before
this
time
any
oxygen
produced
by
photosynthesis
was
consumed
by
oxidation
of
reduced
materials
notably
iron
molecules
of
free
oxygen
did
not
start
to
accumulate
in
the
atmosphere
until
the
rate
of
production
of
oxygen
began
to
exceed
the
availability
of
reducing
materials
that
removed
oxygen
this
point
signifies
a
shift
from
a
reducing
atmosphere
to
an
oxidizing
atmosphere
o
showed
major
variations
until
reaching
a
steady
state
of
more
than
15%
by
the
end
of
the
precambrian
the
following
time
span
from
541
million
years
ago
to
the
present
day
is
the
phanerozoic
eon
during
the
earliest
period
of
which
the
cambrian
oxygen-requiring
metazoan
life
forms
began
to
appear
the
amount
of
oxygen
in
the
atmosphere
has
fluctuated
over
the
last
600
million
years
reaching
a
peak
of
about
30%
around
280
million
years
ago
significantly
higher
than
today's
21%
two
main
processes
govern
changes
in
the
atmosphere:
plants
use
carbon
dioxide
from
the
atmosphere
releasing
oxygen
breakdown
of
pyrite
and
volcanic
eruptions
release
sulfur
into
the
atmosphere
which
oxidizes
and
hence
reduces
the
amount
of
oxygen
in
the
atmosphere
however
volcanic
eruptions
also
release
carbon
dioxide
which
plants
can
convert
to
oxygen
the
exact
cause
of
the
variation
of
the
amount
of
oxygen
in
the
atmosphere
is
not
known
periods
with
much
oxygen
in
the
atmosphere
are
associated
with
rapid
development
of
animals
today's
atmosphere
contains
21%
oxygen
which
is
great
enough
for
this
rapid
development
of
animals
"air
pollution"
is
the
introduction
into
the
atmosphere
of
chemicals
particulate
matter
or
biological
materials
that
cause
harm
or
discomfort
to
organisms
stratospheric
ozone
depletion
is
caused
by
air
pollution
chiefly
from
chlorofluorocarbons
and
other
ozone-depleting
substances
the
scientific
consensus
is
that
the
anthropogenic
greenhouse
gases
currently
accumulating
in
the
atmosphere
are
the
main
cause
of
global
warming
on
october
19
2015
nasa
started
a
website
containing
daily
images
of
the
full
sunlit
side
of
earth
on
http://epicgsfcnasagov/
the
images
are
taken
from
the
deep
space
climate
observatory
(dscovr)
and
show
earth
as
it
rotates
during
a
day
underwater
underwater
refers
to
the
region
below
the
surface
of
water
where
the
water
exists
in
a
swimming
pool
or
a
natural
feature
(called
a
body
of
water)
such
as
an
ocean
sea
lake
pond
or
river
three
quarters
of
the
planet
earth
is
covered
by
water
a
majority
of
the
planet's
solid
surface
is
abyssal
plain
at
depths
between
below
the
surface
of
the
oceans
the
solid
surface
location
on
the
planet
closest
to
the
centre
of
the
orb
is
the
challenger
deep
located
in
the
mariana
trench
at
a
depth
of
although
a
number
of
human
activities
are
conducted
underwater—such
as
research
scuba
diving
for
work
or
recreation
or
even
underwater
warfare
with
submarines
this
very
extensive
environment
on
planet
earth
is
hostile
to
humans
in
many
ways
and
therefore
little
explored
but
it
can
be
explored
by
sonar
or
more
directly
via
manned
or
autonomous
submersibles
the
ocean
floors
have
been
surveyed
via
sonar
to
at
least
a
coarse
resolution;
particularly-strategic
areas
have
been
mapped
in
detail
in
the
name
of
detecting
enemy
submarines
or
aiding
friendly
ones
though
the
resulting
maps
may
still
be
classified
an
immediate
obstacle
to
human
activity
under
water
is
the
fact
that
human
lungs
cannot
naturally
function
in
this
environment
unlike
the
gills
of
fish
human
lungs
are
adapted
to
the
exchange
of
gases
at
atmospheric
pressure
not
liquids
aside
from
simply
having
insufficient
musculature
to
rapidly
move
water
in
and
out
of
the
lungs
a
more
significant
problem
for
all
air-breathing
animals
such
as
mammals
and
birds
is
that
water
contains
so
little
dissolved
oxygen
compared
with
atmospheric
air
air
is
around
21%
o;
water
typically
is
less
than
0001%
dissolved
oxygen
the
density
of
water
also
causes
problems
that
increase
dramatically
with
depth
the
atmospheric
pressure
at
the
surface
is
147
pounds
per
square
inch
or
around
100 kpa
a
comparable
water
pressure
occurs
at
a
depth
of
only
(
for
sea
water)
thus
at
about
10
m
below
the
surface
the
water
exerts
twice
the
pressure
(2
atmospheres
or
200 kpa)
on
the
body
as
air
at
surface
level
for
solids
and
liquids
like
bone
muscle
and
blood
this
added
pressure
is
not
much
of
a
problem;
but
it
is
a
problem
for
any
air-filled
spaces
like
the
mouth
ears
paranasal
sinuses
and
lungs
this
is
because
the
air
in
those
spaces
reduces
in
volume
when
under
pressure
and
so
does
not
provide
those
spaces
with
support
against
the
higher
outside
pressure
even
at
a
depth
of
underwater
an
inability
to
equalize
air
pressure
in
the
middle
ear
with
outside
water
pressure
can
cause
pain
and
the
tympanic
membrane
(eardrum)
can
rupture
at
depths
under
10 ft
(3 m)
the
danger
of
pressure
damage
is
greatest
in
shallow
water
because
the
ratio
of
pressure
change
is
greatest
near
the
surface
of
the
water
for
example
the
pressure
increase
between
the
surface
and
10 m
(33 ft)
is
100%
(100 kpa
to
200 kpa)
but
the
pressure
increase
from
30 m
(100 ft)
to
40 m
(130 ft)
is
only
25%
(400 kpa
to
500 kpa)
any
object
immersed
in
water
is
provided
with
a
buoyant
force
that
counters
the
force
of
gravity
appearing
to
make
the
object
less
heavy
if
the
overall
density
of
the
object
exceeds
the
density
of
water
the
object
sinks
if
the
overall
density
is
less
than
the
density
of
water
the
object
rises
until
it
floats
on
the
surface
with
increasing
depth
underwater
sunlight
is
absorbed
and
the
amount
of
visible
light
diminishes
because
absorption
is
greater
for
long
wavelengths
(red
end
of
the
visible
spectrum)
than
for
short
wavelengths
(blue
end
of
the
visible
spectrum)
the
colour
spectrum
is
rapidly
altered
with
increasing
depth
white
objects
at
the
surface
appear
bluish
underwater
and
red
objects
appear
dark
even
black
although
light
penetration
will
be
less
if
water
is
turbid
in
the
very
clear
water
of
the
open
ocean
less
than
25%
of
the
surface
light
reaches
a
depth
of
10 m
(33 feet)
at
100 m
(330 ft)
the
light
present
from
the
sun
is
normally
about
05%
of
that
at
the
surface
the
euphotic
depth
is
the
depth
at
which
light
intensity
falls
to
1%
of
the
value
at
the
surface
this
depth
is
dependent
upon
water
clarity
being
only
a
few
metres
underwater
in
a
turbid
estuary
but
may
reach
up
to
200
metres
in
the
open
ocean
at
the
euphotic
depth
plants
(such
as
phytoplankton)
have
no
net
energy
gain
from
photosynthesis
and
thus
cannot
grow
there
are
three
layers
of
ocean
temperature:
the
surface
layer
the
thermocline
and
the
deep
ocean
the
average
temperature
of
surface
layer
is
about
17 °c
about
90%
of
ocean's
water
is
below
the
thermocline
in
the
deep
ocean
where
most
of
the
water
is
below
4 °c
water
conducts
heat
around
25
times
more
efficiently
than
air
hypothermia
a
potentially
fatal
condition
occurs
when
the
human
body's
core
temperature
falls
below
35 °c
insulating
the
body's
warmth
from
water
is
the
main
purpose
of
diving
suits
and
exposure
suits
when
used
in
water
temperatures
below
25 °c
sound
is
transmitted
about
43
times
faster
in
water
(1484 m/s
in
fresh
water)
as
it
is
in
air
(343 m/s)
the
human
brain
can
determine
the
direction
of
sound
in
air
by
detecting
small
differences
in
the
time
it
takes
for
sound
waves
in
air
to
reach
each
of
the
two
ears
for
these
reasons
divers
find
it
difficult
to
determine
the
direction
of
sound
underwater
however
some
animals
have
adapted
to
this
difference
and
many
use
sound
to
navigate
underwater
universe
the
universe
is
all
of
space
and
time
and
their
contents
including
planets
stars
galaxies
and
all
other
forms
of
matter
and
energy
while
the
spatial
size
of
the
entire
universe
is
unknown
it
is
possible
to
measure
the
observable
universe
the
earliest
scientific
models
of
the
universe
were
developed
by
ancient
greek
and
indian
philosophers
and
were
geocentric
placing
earth
at
the
center
of
the
universe
over
the
centuries
more
precise
astronomical
observations
led
nicolaus
copernicus
to
develop
the
heliocentric
model
with
the
sun
at
the
center
of
the
solar
system
in
developing
the
law
of
universal
gravitation
isaac
newton
built
upon
copernicus'
work
as
well
as
observations
by
tycho
brahe
and
johannes
kepler's
laws
of
planetary
motion
further
observational
improvements
led
to
the
realization
that
the
sun
is
one
of
hundreds
of
billions
of
stars
in
the
milky
way
which
is
one
of
at
least
hundreds
of
billions
of
galaxies
in
the
universe
many
of
the
stars
in
our
galaxy
have
planets
at
the
largest
scale
galaxies
are
distributed
uniformly
and
the
same
in
all
directions
meaning
that
the
universe
has
neither
an
edge
nor
a
center
at
smaller
scales
galaxies
are
distributed
in
clusters
and
superclusters
which
form
immense
filaments
and
voids
in
space
creating
a
vast
foam-like
structure
discoveries
in
the
early
20th
century
have
suggested
that
the
universe
had
a
beginning
and
that
space
has
been
expanding
since
then
and
is
currently
still
expanding
at
an
increasing
rate
the
big
bang
theory
is
the
prevailing
cosmological
description
of
the
development
of
the
universe
under
this
theory
space
and
time
emerged
together
ago
with
a
fixed
amount
of
energy
and
matter
that
has
become
less
dense
as
the
universe
has
expanded
after
an
initial
accelerated
expansion
at
around
10
seconds
and
the
separation
of
the
four
known
fundamental
forces
the
universe
gradually
cooled
and
continued
to
expand
allowing
the
first
subatomic
particles
and
simple
atoms
to
form
dark
matter
gradually
gathered
forming
a
foam-like
structure
of
filaments
and
voids
under
the
influence
of
gravity
giant
clouds
of
hydrogen
and
helium
were
gradually
drawn
to
the
places
where
dark
matter
was
most
dense
forming
the
first
galaxies
stars
and
everything
else
seen
today
it
is
possible
to
see
objects
that
are
now
further
away
than
13799
billion
light-years
because
space
itself
has
expanded
and
it
is
still
expanding
today
this
means
that
objects
which
are
now
up
to
465
billion
light-years
away
can
still
be
seen
in
their
distant
past
because
in
the
past
when
their
light
was
emitted
they
were
much
closer
to
the
earth
from
studying
the
movement
of
galaxies
it
has
been
discovered
that
the
universe
contains
much
more
matter
than
is
accounted
for
by
visible
objects;
stars
galaxies
nebulas
and
interstellar
gas
this
unseen
matter
is
known
as
dark
matter
("dark"
means
that
there
is
a
wide
range
of
strong
indirect
evidence
that
it
exists
but
we
have
not
yet
detected
it
directly)
the
λcdm
model
is
the
most
widely
accepted
model
of
our
universe
it
suggests
that
about
[2015]
of
the
mass
and
energy
in
the
universe
is
a
cosmological
constant
(or
in
extensions
to
λcdm
other
forms
of
dark
energy
such
as
a
scalar
field)
which
is
responsible
for
the
current
expansion
of
space
and
about
[2015]
is
dark
matter
ordinary
("baryonic")
matter
is
therefore
only
49%
[2015]
of
the
physical
universe
stars
planets
and
visible
gas
clouds
only
form
about
6%
of
ordinary
matter
or
about
03%
of
the
entire
universe
there
are
many
competing
hypotheses
about
the
ultimate
fate
of
the
universe
and
about
what
if
anything
preceded
the
big
bang
while
other
physicists
and
philosophers
refuse
to
speculate
doubting
that
information
about
prior
states
will
ever
be
accessible
some
physicists
have
suggested
various
multiverse
hypotheses
in
which
the
universe
might
be
one
among
many
universes
that
likewise
exist
the
physical
universe
is
defined
as
all
of
space
and
time
(collectively
referred
to
as
spacetime)
and
their
contents
such
contents
comprise
all
of
energy
in
its
various
forms
including
electromagnetic
radiation
and
matter
and
therefore
planets
moons
stars
galaxies
and
the
contents
of
intergalactic
space
the
universe
also
includes
the
physical
laws
that
influence
energy
and
matter
such
as
conservation
laws
classical
mechanics
and
relativity
the
universe
is
often
defined
as
"the
totality
of
existence"
or
everything
that
exists
everything
that
has
existed
and
everything
that
will
exist
in
fact
some
philosophers
and
scientists
support
the
inclusion
of
ideas
and
abstract
concepts
–
such
as
mathematics
and
logic
–
in
the
definition
of
the
universe
the
word
"universe"
may
also
refer
to
concepts
such
as
"the
cosmos"
"the
world"
and
"nature"
the
word
"universe"
derives
from
the
old
french
word
"univers"
which
in
turn
derives
from
the
latin
word
"universum"
the
latin
word
was
used
by
cicero
and
later
latin
authors
in
many
of
the
same
senses
as
the
modern
english
word
is
used
a
term
for
"universe"
among
the
ancient
greek
philosophers
from
pythagoras
onwards
was
"tò
pân"
("the
all")
defined
as
all
matter
and
all
space
and
"tò
hólon"
("all
things")
which
did
not
necessarily
include
the
void
another
synonym
was
"ho
kósmos"
(meaning
the
world
the
cosmos)
synonyms
are
also
found
in
latin
authors
("totum"
"mundus"
"natura")
and
survive
in
modern
languages
eg
the
german
words
"das
all"
"weltall"
and
"natur"
for
"universe"
the
same
synonyms
are
found
in
english
such
as
everything
(as
in
the
theory
of
everything)
the
cosmos
(as
in
cosmology)
the
world
(as
in
the
many-worlds
interpretation)
and
nature
(as
in
natural
laws
or
natural
philosophy)
the
prevailing
model
for
the
evolution
of
the
universe
is
the
big
bang
theory
the
big
bang
model
states
that
the
earliest
state
of
the
universe
was
an
extremely
hot
and
dense
one
and
that
the
universe
subsequently
expanded
and
cooled
the
model
is
based
on
general
relativity
and
on
simplifying
assumptions
such
as
homogeneity
and
isotropy
of
space
a
version
of
the
model
with
a
cosmological
constant
(lambda)
and
cold
dark
matter
known
as
the
lambda-cdm
model
is
the
simplest
model
that
provides
a
reasonably
good
account
of
various
observations
about
the
universe
the
big
bang
model
accounts
for
observations
such
as
the
correlation
of
distance
and
redshift
of
galaxies
the
ratio
of
the
number
of
hydrogen
to
helium
atoms
and
the
microwave
radiation
background
the
initial
hot
dense
state
is
called
the
planck
epoch
a
brief
period
extending
from
time
zero
to
one
planck
time
unit
of
approximately
10
seconds
during
the
planck
epoch
all
types
of
matter
and
all
types
of
energy
were
concentrated
into
a
dense
state
and
gravity
-
currently
the
weakest
by
far
of
the
four
known
forces
-
is
believed
to
have
been
as
strong
as
the
other
fundamental
forces
and
all
the
forces
may
have
been
unified
since
the
planck
epoch
space
has
been
expanding
to
its
present
scale
with
a
very
short
but
intense
period
of
cosmic
inflation
believed
to
have
occurred
within
the
first
10
seconds
this
was
a
kind
of
expansion
different
from
those
we
can
see
around
us
today
objects
in
space
did
not
physically
move;
instead
the
metric
that
defines
space
itself
changed
although
objects
in
spacetime
cannot
move
faster
than
the
speed
of
light
this
limitation
does
not
apply
to
the
metric
governing
spacetime
itself
this
initial
period
of
inflation
is
believed
to
explain
why
space
appears
to
be
very
flat
and
much
larger
than
light
could
travel
since
the
start
of
the
universe
within
the
first
fraction
of
a
second
of
the
universe's
existence
the
four
fundamental
forces
had
separated
as
the
universe
continued
to
cool
down
from
its
inconceivably
hot
state
various
types
of
subatomic
particles
were
able
to
form
in
short
periods
of
time
known
as
the
quark
epoch
the
hadron
epoch
and
the
lepton
epoch
together
these
epochs
encompassed
less
than
10
seconds
of
time
following
the
big
bang
these
elementary
particles
associated
stably
into
ever
larger
combinations
including
stable
protons
and
neutrons
which
then
formed
more
complex
atomic
nuclei
through
nuclear
fusion
this
process
known
as
big
bang
nucleosynthesis
only
lasted
for
about
17
minutes
and
ended
about
20
minutes
after
the
big
bang
so
only
the
fastest
and
simplest
reactions
occurred
about
25%
of
the
protons
and
all
the
neutrons
in
the
universe
by
mass
were
converted
to
helium
with
small
amounts
of
deuterium
(a
form
of
hydrogen)
and
traces
of
lithium
any
other
element
was
only
formed
in
very
tiny
quantities
the
other
75%
of
the
protons
remained
unaffected
as
hydrogen
nuclei
after
nucleosynthesis
ended
the
universe
entered
a
period
known
as
the
photon
epoch
during
this
period
the
universe
was
still
far
too
hot
for
matter
to
form
neutral
atoms
so
it
contained
a
hot
dense
foggy
plasma
of
negatively
charged
electrons
neutral
neutrinos
and
positive
nuclei
after
about
377000
years
the
universe
had
cooled
enough
that
electrons
and
nuclei
could
form
the
first
stable
atoms
this
is
known
as
recombination
for
historical
reasons;
in
fact
electrons
and
nuclei
were
combining
for
the
first
time
unlike
plasma
neutral
atoms
are
transparent
to
many
wavelengths
of
light
so
for
the
first
time
the
universe
also
became
transparent
the
photons
released
("decoupled")
when
these
atoms
formed
can
still
be
seen
today;
they
form
the
cosmic
microwave
background
(cmb)
as
the
universe
expands
the
energy
density
of
electromagnetic
radiation
decreases
more
quickly
than
does
that
of
matter
because
the
energy
of
a
photon
decreases
with
its
wavelength
at
around
47000
years
the
energy
density
of
matter
became
larger
than
that
of
photons
and
neutrinos
and
began
to
dominate
the
large
scale
behavior
of
the
universe
this
marked
the
end
of
the
radiation-dominated
era
and
the
start
of
the
matter-dominated
era
in
the
earliest
stages
of
the
universe
tiny
fluctuations
within
the
universe's
density
led
to
concentrations
of
dark
matter
gradually
forming
ordinary
matter
attracted
to
these
by
gravity
formed
large
gas
clouds
and
eventually
stars
and
galaxies
where
the
dark
matter
was
most
dense
and
voids
where
it
was
least
dense
after
around
100
-
300
million
years
the
first
stars
formed
known
as
population
iii
stars
these
were
probably
very
massive
luminous
non
metallic
and
short-lived
they
were
responsible
for
the
gradual
reionization
of
the
universe
between
about
200-500
million
years
and
1
billion
years
and
also
for
seeding
the
universe
with
elements
heavier
than
helium
through
stellar
nucleosynthesis
the
universe
also
contains
a
mysterious
energy
-
possibly
a
scalar
field
-
called
dark
energy
the
density
of
which
does
not
change
over
time
after
about
98
billion
years
the
universe
had
expanded
sufficiently
so
that
the
density
of
matter
was
less
than
the
density
of
dark
energy
marking
the
beginning
of
the
present
dark-energy-dominated
era
in
this
era
the
expansion
of
the
universe
is
accelerating
due
to
dark
energy
of
the
four
fundamental
interactions
gravitation
is
the
dominant
at
astronomical
length
scales
gravity's
effects
are
cumulative;
by
contrast
the
effects
of
positive
and
negative
charges
tend
to
cancel
one
another
making
electromagnetism
relatively
insignificant
on
astronomical
length
scales
the
remaining
two
interactions
the
weak
and
strong
nuclear
forces
decline
very
rapidly
with
distance;
their
effects
are
confined
mainly
to
sub-atomic
length
scales
the
universe
appears
to
have
much
more
matter
than
antimatter
an
asymmetry
possibly
related
to
the
cp
violation
this
imbalance
between
matter
and
antimatter
is
partially
responsible
for
the
existence
of
all
matter
existing
today
since
matter
and
antimatter
if
equally
produced
at
the
big
bang
would
have
completely
annihilated
each
other
and
left
only
photons
as
a
result
of
their
interaction
the
universe
also
appears
to
have
neither
net
momentum
nor
angular
momentum
which
follows
accepted
physical
laws
if
the
universe
is
finite
these
laws
are
the
gauss's
law
and
the
non-divergence
of
the
stress-energy-momentum
pseudotensor
the
size
of
the
universe
is
somewhat
difficult
to
define
according
to
the
general
theory
of
relativity
far
regions
of
space
may
never
interact
with
ours
even
in
the
lifetime
of
the
universe
due
to
the
finite
speed
of
light
and
the
ongoing
expansion
of
space
for
example
radio
messages
sent
from
earth
may
never
reach
some
regions
of
space
even
if
the
universe
were
to
exist
forever:
space
may
expand
faster
than
light
can
traverse
it
distant
regions
of
space
are
assumed
to
exist
and
to
be
part
of
reality
as
much
as
we
are
even
though
we
can
never
interact
with
them
the
spatial
region
that
we
can
affect
and
be
affected
by
is
the
observable
universe
the
observable
universe
depends
on
the
location
of
the
observer
by
traveling
an
observer
can
come
into
contact
with
a
greater
region
of
spacetime
than
an
observer
who
remains
still
nevertheless
even
the
most
rapid
traveler
will
not
be
able
to
interact
with
all
of
space
typically
the
observable
universe
is
taken
to
mean
the
portion
of
the
universe
that
is
observable
from
our
vantage
point
in
the
milky
way
the
proper
distance—the
distance
as
would
be
measured
at
a
specific
time
including
the
present—between
earth
and
the
edge
of
the
observable
universe
is
46
billion
light-years
(14
billion
parsecs)
making
the
diameter
of
the
observable
universe
about
93
billion
light-years
(28
billion
parsecs)
the
distance
the
light
from
the
edge
of
the
observable
universe
has
travelled
is
very
close
to
the
age
of
the
universe
times
the
speed
of
light
but
this
does
not
represent
the
distance
at
any
given
time
because
the
edge
of
the
observable
universe
and
the
earth
have
since
moved
further
apart
for
comparison
the
diameter
of
a
typical
galaxy
is
30000
light-years
(9198
parsecs)
and
the
typical
distance
between
two
neighboring
galaxies
is
3
million
light-years
(9198
kiloparsecs)
as
an
example
the
milky
way
is
roughly
100000–180000
light-years
in
diameter
and
the
nearest
sister
galaxy
to
the
milky
way
the
andromeda
galaxy
is
located
roughly
25
million
light-years
away
because
we
cannot
observe
space
beyond
the
edge
of
the
observable
universe
it
is
unknown
whether
the
size
of
the
universe
in
its
totality
is
finite
or
infinite
estimates
for
the
total
size
of
the
universe
if
finite
reach
as
high
as
formula_1
megaparsecs
implied
by
one
resolution
of
the
no-boundary
proposal
astronomers
calculate
the
age
of
the
universe
by
assuming
that
the
lambda-cdm
model
accurately
describes
the
evolution
of
the
universe
from
a
very
uniform
hot
dense
primordial
state
to
its
present
state
and
measuring
the
cosmological
parameters
which
constitute
the
model
this
model
is
well
understood
theoretically
and
supported
by
recent
high-precision
astronomical
observations
such
as
wmap
and
planck
commonly
the
set
of
observations
fitted
includes
the
cosmic
microwave
background
anisotropy
the
brightness/redshift
relation
for
type
ia
supernovae
and
large-scale
galaxy
clustering
including
the
baryon
acoustic
oscillation
feature
other
observations
such
as
the
hubble
constant
the
abundance
of
galaxy
clusters
weak
gravitational
lensing
and
globular
cluster
ages
are
generally
consistent
with
these
providing
a
check
of
the
model
but
are
less
accurately
measured
at
present
assuming
that
the
lambda-cdm
model
is
correct
the
measurements
of
the
parameters
using
a
variety
of
techniques
by
numerous
experiments
yield
a
best
value
of
the
age
of
the
universe
as
of
2015
of
13799
±
0021
billion
years
over
time
the
universe
and
its
contents
have
evolved;
for
example
the
relative
population
of
quasars
and
galaxies
has
changed
and
space
itself
has
expanded
due
to
this
expansion
scientists
on
earth
can
observe
the
light
from
a
galaxy
30
billion
light-years
away
even
though
that
light
has
traveled
for
only
13
billion
years;
the
very
space
between
them
has
expanded
this
expansion
is
consistent
with
the
observation
that
the
light
from
distant
galaxies
has
been
redshifted;
the
photons
emitted
have
been
stretched
to
longer
wavelengths
and
lower
frequency
during
their
journey
analyses
of
type
ia
supernovae
indicate
that
the
spatial
expansion
is
accelerating
the
more
matter
there
is
in
the
universe
the
stronger
the
mutual
gravitational
pull
of
the
matter
if
the
universe
were
"too"
dense
then
it
would
re-collapse
into
a
gravitational
singularity
however
if
the
universe
contained
too
"little"
matter
then
the
self-gravity
would
be
too
weak
for
astronomical
structures
like
galaxies
or
planets
to
form
since
the
big
bang
the
universe
has
expanded
monotonically
perhaps
unsurprisingly
our
universe
has
just
the
right
mass-energy
density
equivalent
to
about
5
protons
per
cubic
meter
which
has
allowed
it
to
expand
for
the
last
138
billion
years
giving
time
to
form
the
universe
as
observed
today
there
are
dynamical
forces
acting
on
the
particles
in
the
universe
which
affect
the
expansion
rate
before
1998
it
was
expected
that
the
expansion
rate
would
be
decreasing
as
time
went
on
due
to
the
influence
of
gravitational
interactions
in
the
universe;
and
thus
there
is
an
additional
observable
quantity
in
the
universe
called
the
deceleration
parameter
which
most
cosmologists
expected
to
be
positive
and
related
to
the
matter
density
of
the
universe
in
1998
the
deceleration
parameter
was
measured
by
two
different
groups
to
be
negative
approximately
-055
which
technically
implies
that
the
second
derivative
of
the
cosmic
scale
factor
formula_2
has
been
positive
in
the
last
5-6
billion
years
this
acceleration
does
not
however
imply
that
the
hubble
parameter
is
currently
increasing;
see
deceleration
parameter
for
details
spacetimes
are
the
arenas
in
which
all
physical
events
take
place
the
basic
elements
of
spacetimes
are
events
in
any
given
spacetime
an
event
is
defined
as
a
unique
position
at
a
unique
time
a
spacetime
is
the
union
of
all
events
(in
the
same
way
that
a
line
is
the
union
of
all
of
its
points)
formally
organized
into
a
manifold
the
universe
appears
to
be
a
smooth
spacetime
continuum
consisting
of
three
spatial
dimensions
and
one
temporal
(time)
dimension
(an
event
in
the
spacetime
of
the
physical
universe
can
therefore
be
identified
by
a
set
of
four
coordinates:
("x"
"y"
"z"
"t")
)
on
the
average
space
is
observed
to
be
very
nearly
flat
(with
a
curvature
close
to
zero)
meaning
that
euclidean
geometry
is
empirically
true
with
high
accuracy
throughout
most
of
the
universe
spacetime
also
appears
to
have
a
simply
connected
topology
in
analogy
with
a
sphere
at
least
on
the
length-scale
of
the
observable
universe
however
present
observations
cannot
exclude
the
possibilities
that
the
universe
has
more
dimensions
(which
is
postulated
by
theories
such
as
the
string
theory)
and
that
its
spacetime
may
have
a
multiply
connected
global
topology
in
analogy
with
the
cylindrical
or
toroidal
topologies
of
two-dimensional
spaces
the
spacetime
of
the
universe
is
usually
interpreted
from
a
euclidean
perspective
with
space
as
consisting
of
three
dimensions
and
time
as
consisting
of
one
dimension
the
"fourth
dimension"
by
combining
space
and
time
into
a
single
manifold
called
minkowski
space
physicists
have
simplified
a
large
number
of
physical
theories
as
well
as
described
in
a
more
uniform
way
the
workings
of
the
universe
at
both
the
supergalactic
and
subatomic
levels
spacetime
events
are
not
absolutely
defined
spatially
and
temporally
but
rather
are
known
to
be
relative
to
the
motion
of
an
observer
minkowski
space
approximates
the
universe
without
gravity;
the
pseudo-riemannian
manifolds
of
general
relativity
describe
spacetime
with
matter
and
gravity
general
relativity
describes
how
spacetime
is
curved
and
bent
by
mass
and
energy
(gravity)
the
topology
or
geometry
of
the
universe
includes
both
local
geometry
in
the
observable
universe
and
global
geometry
cosmologists
often
work
with
a
given
space-like
slice
of
spacetime
called
the
comoving
coordinates
the
section
of
spacetime
which
can
be
observed
is
the
backward
light
cone
which
delimits
the
cosmological
horizon
the
cosmological
horizon
(also
called
the
particle
horizon
or
the
light
horizon)
is
the
maximum
distance
from
which
particles
can
have
traveled
to
the
observer
in
the
age
of
the
universe
this
horizon
represents
the
boundary
between
the
observable
and
the
unobservable
regions
of
the
universe
the
existence
properties
and
significance
of
a
cosmological
horizon
depend
on
the
particular
cosmological
model
an
important
parameter
determining
the
future
evolution
of
the
universe
theory
is
the
density
parameter
omega
(ω)
defined
as
the
average
matter
density
of
the
universe
divided
by
a
critical
value
of
that
density
this
selects
one
of
three
possible
geometries
depending
on
whether
ω
is
equal
to
less
than
or
greater
than
1
these
are
called
respectively
the
flat
open
and
closed
universes
observations
including
the
cosmic
background
explorer
(cobe)
wilkinson
microwave
anisotropy
probe
(wmap)
and
planck
maps
of
the
cmb
suggest
that
the
universe
is
infinite
in
extent
with
a
finite
age
as
described
by
the
friedmann–lemaître–robertson–walker
(flrw)
models
these
flrw
models
thus
support
inflationary
models
and
the
standard
model
of
cosmology
describing
a
flat
homogeneous
universe
presently
dominated
by
dark
matter
and
dark
energy
the
universe
may
be
"fine-tuned";
the
fine-tuned
universe
hypothesis
is
the
proposition
that
the
conditions
that
allow
the
existence
of
observable
life
in
the
universe
can
only
occur
when
certain
universal
fundamental
physical
constants
lie
within
a
very
narrow
range
of
values
so
that
if
any
of
several
fundamental
constants
were
only
slightly
different
the
universe
would
have
been
unlikely
to
be
conducive
to
the
establishment
and
development
of
matter
astronomical
structures
elemental
diversity
or
life
as
it
is
understood
the
proposition
is
discussed
among
philosophers
scientists
theologians
and
proponents
of
creationism
the
universe
is
composed
almost
completely
of
dark
energy
dark
matter
and
ordinary
matter
other
contents
are
electromagnetic
radiation
(estimated
to
constitute
from
0005%
to
close
to
001%
of
the
total
mass
of
the
universe)
and
antimatter
the
proportions
of
all
types
of
matter
and
energy
have
changed
over
the
history
of
the
universe
the
total
amount
of
electromagnetic
radiation
generated
within
the
universe
has
decreased
by
1/2
in
the
past
2
billion
years
today
ordinary
matter
which
includes
atoms
stars
galaxies
and
life
accounts
for
only
49%
of
the
contents
of
the
universe
the
present
overall
density
of
this
type
of
matter
is
very
low
roughly
45
×
10
grams
per
cubic
centimetre
corresponding
to
a
density
of
the
order
of
only
one
proton
for
every
four
cubic
meters
of
volume
the
nature
of
both
dark
energy
and
dark
matter
is
unknown
dark
matter
a
mysterious
form
of
matter
that
has
not
yet
been
identified
accounts
for
268%
of
the
cosmic
contents
dark
energy
which
is
the
energy
of
empty
space
and
is
causing
the
expansion
of
the
universe
to
accelerate
accounts
for
the
remaining
683%
of
the
contents
matter
dark
matter
and
dark
energy
are
distributed
homogeneously
throughout
the
universe
over
length
scales
longer
than
300
million
light-years
or
so
however
over
shorter
length-scales
matter
tends
to
clump
hierarchically;
many
atoms
are
condensed
into
stars
most
stars
into
galaxies
most
galaxies
into
clusters
superclusters
and
finally
large-scale
galactic
filaments
the
observable
universe
contains
approximately
300
sextillion
(3)
stars
and
more
than
100
billion
(10)
galaxies
typical
galaxies
range
from
dwarfs
with
as
few
as
ten
million
(10)
stars
up
to
giants
with
one
trillion
(10)
stars
between
the
larger
structures
are
voids
which
are
typically
10–150
mpc
(33
million–490
million
ly)
in
diameter
the
milky
way
is
in
the
local
group
of
galaxies
which
in
turn
is
in
the
laniakea
supercluster
this
supercluster
spans
over
500
million
light-years
while
the
local
group
spans
over
10
million
light-years
the
universe
also
has
vast
regions
of
relative
emptiness;
the
largest
known
void
measures
18
billion
ly
(550
mpc)
across
the
observable
universe
is
isotropic
on
scales
significantly
larger
than
superclusters
meaning
that
the
statistical
properties
of
the
universe
are
the
same
in
all
directions
as
observed
from
earth
the
universe
is
bathed
in
highly
isotropic
microwave
radiation
that
corresponds
to
a
thermal
equilibrium
blackbody
spectrum
of
roughly
272548
kelvins
the
hypothesis
that
the
large-scale
universe
is
homogeneous
and
isotropic
is
known
as
the
cosmological
principle
a
universe
that
is
both
homogeneous
and
isotropic
looks
the
same
from
all
vantage
points
and
has
no
center
an
explanation
for
why
the
expansion
of
the
universe
is
accelerating
remains
elusive
it
is
often
attributed
to
"dark
energy"
an
unknown
form
of
energy
that
is
hypothesized
to
permeate
space
on
a
mass–energy
equivalence
basis
the
density
of
dark
energy
(~
7
×
10
g/cm)
is
much
less
than
the
density
of
ordinary
matter
or
dark
matter
within
galaxies
however
in
the
present
dark-energy
era
it
dominates
the
mass–energy
of
the
universe
because
it
is
uniform
across
space
two
proposed
forms
for
dark
energy
are
the
cosmological
constant
a
"constant"
energy
density
filling
space
homogeneously
and
scalar
fields
such
as
quintessence
or
moduli
"dynamic"
quantities
whose
energy
density
can
vary
in
time
and
space
contributions
from
scalar
fields
that
are
constant
in
space
are
usually
also
included
in
the
cosmological
constant
the
cosmological
constant
can
be
formulated
to
be
equivalent
to
vacuum
energy
scalar
fields
having
only
a
slight
amount
of
spatial
inhomogeneity
would
be
difficult
to
distinguish
from
a
cosmological
constant
dark
matter
is
a
hypothetical
kind
of
matter
that
is
invisible
to
the
entire
electromagnetic
spectrum
but
which
accounts
for
most
of
the
matter
in
the
universe
the
existence
and
properties
of
dark
matter
are
inferred
from
its
gravitational
effects
on
visible
matter
radiation
and
the
large-scale
structure
of
the
universe
other
than
neutrinos
a
form
of
hot
dark
matter
dark
matter
has
not
been
detected
directly
making
it
one
of
the
greatest
mysteries
in
modern
astrophysics
dark
matter
neither
emits
nor
absorbs
light
or
any
other
electromagnetic
radiation
at
any
significant
level
dark
matter
is
estimated
to
constitute
268%
of
the
total
mass–energy
and
845%
of
the
total
matter
in
the
universe
the
remaining
49%
of
the
mass–energy
of
the
universe
is
ordinary
matter
that
is
atoms
ions
electrons
and
the
objects
they
form
this
matter
includes
stars
which
produce
nearly
all
of
the
light
we
see
from
galaxies
as
well
as
interstellar
gas
in
the
interstellar
and
intergalactic
media
planets
and
all
the
objects
from
everyday
life
that
we
can
bump
into
touch
or
squeeze
as
a
matter
of
fact
the
great
majority
of
ordinary
matter
in
the
universe
is
unseen
since
visible
stars
and
gas
inside
galaxies
and
clusters
account
for
less
than
10
per
cent
of
the
ordinary
matter
contribution
to
the
mass-energy
density
of
the
universe
ordinary
matter
commonly
exists
in
four
states
(or
phases):
solid
liquid
gas
and
plasma
however
advances
in
experimental
techniques
have
revealed
other
previously
theoretical
phases
such
as
bose–einstein
condensates
and
fermionic
condensates
ordinary
matter
is
composed
of
two
types
of
elementary
particles:
quarks
and
leptons
for
example
the
proton
is
formed
of
two
up
quarks
and
one
down
quark;
the
neutron
is
formed
of
two
down
quarks
and
one
up
quark;
and
the
electron
is
a
kind
of
lepton
an
atom
consists
of
an
atomic
nucleus
made
up
of
protons
and
neutrons
and
electrons
that
orbit
the
nucleus
because
most
of
the
mass
of
an
atom
is
concentrated
in
its
nucleus
which
is
made
up
of
baryons
astronomers
often
use
the
term
"baryonic
matter"
to
describe
ordinary
matter
although
a
small
fraction
of
this
"baryonic
matter"
is
electrons
soon
after
the
big
bang
primordial
protons
and
neutrons
formed
from
the
quark–gluon
plasma
of
the
early
universe
as
it
cooled
below
two
trillion
degrees
a
few
minutes
later
in
a
process
known
as
big
bang
nucleosynthesis
nuclei
formed
from
the
primordial
protons
and
neutrons
this
nucleosynthesis
formed
lighter
elements
those
with
small
atomic
numbers
up
to
lithium
and
beryllium
but
the
abundance
of
heavier
elements
dropped
off
sharply
with
increasing
atomic
number
some
boron
may
have
been
formed
at
this
time
but
the
next
heavier
element
carbon
was
not
formed
in
significant
amounts
big
bang
nucleosynthesis
shut
down
after
about
20
minutes
due
to
the
rapid
drop
in
temperature
and
density
of
the
expanding
universe
subsequent
formation
of
heavier
elements
resulted
from
stellar
nucleosynthesis
and
supernova
nucleosynthesis
ordinary
matter
and
the
forces
that
act
on
matter
can
be
described
in
terms
of
elementary
particles
these
particles
are
sometimes
described
as
being
fundamental
since
they
have
an
unknown
substructure
and
it
is
unknown
whether
or
not
they
are
composed
of
smaller
and
even
more
fundamental
particles
of
central
importance
is
the
standard
model
a
theory
that
is
concerned
with
electromagnetic
interactions
and
the
weak
and
strong
nuclear
interactions
the
standard
model
is
supported
by
the
experimental
confirmation
of
the
existence
of
particles
that
compose
matter:
quarks
and
leptons
and
their
corresponding
"antimatter"
duals
as
well
as
the
force
particles
that
mediate
interactions:
the
photon
the
w
and
z
bosons
and
the
gluon
the
standard
model
predicted
the
existence
of
the
recently
discovered
higgs
boson
a
particle
that
is
a
manifestation
of
a
field
within
the
universe
that
can
endow
particles
with
mass
because
of
its
success
in
explaining
a
wide
variety
of
experimental
results
the
standard
model
is
sometimes
regarded
as
a
"theory
of
almost
everything"
the
standard
model
does
not
however
accommodate
gravity
a
true
force-particle
"theory
of
everything"
has
not
been
attained
a
hadron
is
a
composite
particle
made
of
quarks
held
together
by
the
strong
force
hadrons
are
categorized
into
two
families:
baryons
(such
as
protons
and
neutrons)
made
of
three
quarks
and
mesons
(such
as
pions)
made
of
one
quark
and
one
antiquark
of
the
hadrons
protons
are
stable
and
neutrons
bound
within
atomic
nuclei
are
stable
other
hadrons
are
unstable
under
ordinary
conditions
and
are
thus
insignificant
constituents
of
the
modern
universe
from
approximately
10
seconds
after
the
big
bang
during
a
period
is
known
as
the
hadron
epoch
the
temperature
of
the
universe
had
fallen
sufficiently
to
allow
quarks
to
bind
together
into
hadrons
and
the
mass
of
the
universe
was
dominated
by
hadrons
initially
the
temperature
was
high
enough
to
allow
the
formation
of
hadron/anti-hadron
pairs
which
kept
matter
and
antimatter
in
thermal
equilibrium
however
as
the
temperature
of
the
universe
continued
to
fall
hadron/anti-hadron
pairs
were
no
longer
produced
most
of
the
hadrons
and
anti-hadrons
were
then
eliminated
in
particle-antiparticle
annihilation
reactions
leaving
a
small
residual
of
hadrons
by
the
time
the
universe
was
about
one
second
old
a
lepton
is
an
elementary
half-integer
spin
particle
that
does
not
undergo
strong
interactions
but
is
subject
to
the
pauli
exclusion
principle;
no
two
leptons
of
the
same
species
can
be
in
exactly
the
same
state
at
the
same
time
two
main
classes
of
leptons
exist:
charged
leptons
(also
known
as
the
"electron-like"
leptons)
and
neutral
leptons
(better
known
as
neutrinos)
electrons
are
stable
and
the
most
common
charged
lepton
in
the
universe
whereas
muons
and
taus
are
unstable
particle
that
quickly
decay
after
being
produced
in
high
energy
collisions
such
as
those
involving
cosmic
rays
or
carried
out
in
particle
accelerators
charged
leptons
can
combine
with
other
particles
to
form
various
composite
particles
such
as
atoms
and
positronium
the
electron
governs
nearly
all
of
chemistry
as
it
is
found
in
atoms
and
is
directly
tied
to
all
chemical
properties
neutrinos
rarely
interact
with
anything
and
are
consequently
rarely
observed
neutrinos
stream
throughout
the
universe
but
rarely
interact
with
normal
matter
the
lepton
epoch
was
the
period
in
the
evolution
of
the
early
universe
in
which
the
leptons
dominated
the
mass
of
the
universe
it
started
roughly
1
second
after
the
big
bang
after
the
majority
of
hadrons
and
anti-hadrons
annihilated
each
other
at
the
end
of
the
hadron
epoch
during
the
lepton
epoch
the
temperature
of
the
universe
was
still
high
enough
to
create
lepton/anti-lepton
pairs
so
leptons
and
anti-leptons
were
in
thermal
equilibrium
approximately
10
seconds
after
the
big
bang
the
temperature
of
the
universe
had
fallen
to
the
point
where
lepton/anti-lepton
pairs
were
no
longer
created
most
leptons
and
anti-leptons
were
then
eliminated
in
annihilation
reactions
leaving
a
small
residue
of
leptons
the
mass
of
the
universe
was
then
dominated
by
photons
as
it
entered
the
following
photon
epoch
a
photon
is
the
quantum
of
light
and
all
other
forms
of
electromagnetic
radiation
it
is
the
force
carrier
for
the
electromagnetic
force
even
when
static
via
virtual
photons
the
effects
of
this
force
are
easily
observable
at
the
microscopic
and
at
the
macroscopic
level
because
the
photon
has
zero
rest
mass;
this
allows
long
distance
interactions
like
all
elementary
particles
photons
are
currently
best
explained
by
quantum
mechanics
and
exhibit
wave–particle
duality
exhibiting
properties
of
waves
and
of
particles
the
photon
epoch
started
after
most
leptons
and
anti-leptons
were
annihilated
at
the
end
of
the
lepton
epoch
about
10
seconds
after
the
big
bang
atomic
nuclei
were
created
in
the
process
of
nucleosynthesis
which
occurred
during
the
first
few
minutes
of
the
photon
epoch
for
the
remainder
of
the
photon
epoch
the
universe
contained
a
hot
dense
plasma
of
nuclei
electrons
and
photons
about
380000
years
after
the
big
bang
the
temperature
of
the
universe
fell
to
the
point
where
nuclei
could
combine
with
electrons
to
create
neutral
atoms
as
a
result
photons
no
longer
interacted
frequently
with
matter
and
the
universe
became
transparent
the
highly
redshifted
photons
from
this
period
form
the
cosmic
microwave
background
tiny
variations
in
temperature
and
density
detectable
in
the
cmb
were
the
early
"seeds"
from
which
all
subsequent
structure
formation
took
place
general
relativity
is
the
geometric
theory
of
gravitation
published
by
albert
einstein
in
1915
and
the
current
description
of
gravitation
in
modern
physics
it
is
the
basis
of
current
cosmological
models
of
the
universe
general
relativity
generalizes
special
relativity
and
newton's
law
of
universal
gravitation
providing
a
unified
description
of
gravity
as
a
geometric
property
of
space
and
time
or
spacetime
in
particular
the
curvature
of
spacetime
is
directly
related
to
the
energy
and
momentum
of
whatever
matter
and
radiation
are
present
the
relation
is
specified
by
the
einstein
field
equations
a
system
of
partial
differential
equations
in
general
relativity
the
distribution
of
matter
and
energy
determines
the
geometry
of
spacetime
which
in
turn
describes
the
acceleration
of
matter
therefore
solutions
of
the
einstein
field
equations
describe
the
evolution
of
the
universe
combined
with
measurements
of
the
amount
type
and
distribution
of
matter
in
the
universe
the
equations
of
general
relativity
describe
the
evolution
of
the
universe
over
time
with
the
assumption
of
the
cosmological
principle
that
the
universe
is
homogeneous
and
isotropic
everywhere
a
specific
solution
of
the
field
equations
that
describes
the
universe
is
the
metric
tensor
called
the
friedmann–lemaître–robertson–walker
metric
where
("r"
θ
φ)
correspond
to
a
spherical
coordinate
system
this
metric
has
only
two
undetermined
parameters
an
overall
dimensionless
length
scale
factor
"r"
describes
the
size
scale
of
the
universe
as
a
function
of
time;
an
increase
in
"r"
is
the
expansion
of
the
universe
a
curvature
index
"k"
describes
the
geometry
the
index
"k"
is
defined
so
that
it
can
take
only
one
of
three
values:
0
corresponding
to
flat
euclidean
geometry;
1
corresponding
to
a
space
of
positive
curvature;
or
−1
corresponding
to
a
space
of
positive
or
negative
curvature
the
value
of
"r"
as
a
function
of
time
"t"
depends
upon
"k"
and
the
cosmological
constant
"λ"
the
cosmological
constant
represents
the
energy
density
of
the
vacuum
of
space
and
could
be
related
to
dark
energy
the
equation
describing
how
"r"
varies
with
time
is
known
as
the
friedmann
equation
after
its
inventor
alexander
friedmann
the
solutions
for
"r(t)"
depend
on
"k"
and
"λ"
but
some
qualitative
features
of
such
solutions
are
general
first
and
most
importantly
the
length
scale
"r"
of
the
universe
can
remain
constant
"only"
if
the
universe
is
perfectly
isotropic
with
positive
curvature
("k"=1)
and
has
one
precise
value
of
density
everywhere
as
first
noted
by
albert
einstein
however
this
equilibrium
is
unstable:
because
the
universe
is
known
to
be
inhomogeneous
on
smaller
scales
"r"
must
change
over
time
when
"r"
changes
all
the
spatial
distances
in
the
universe
change
in
tandem;
there
is
an
overall
expansion
or
contraction
of
space
itself
this
accounts
for
the
observation
that
galaxies
appear
to
be
flying
apart;
the
space
between
them
is
stretching
the
stretching
of
space
also
accounts
for
the
apparent
paradox
that
two
galaxies
can
be
40
billion
light-years
apart
although
they
started
from
the
same
point
138
billion
years
ago
and
never
moved
faster
than
the
speed
of
light
second
all
solutions
suggest
that
there
was
a
gravitational
singularity
in
the
past
when
"r"
went
to
zero
and
matter
and
energy
were
infinitely
dense
it
may
seem
that
this
conclusion
is
uncertain
because
it
is
based
on
the
questionable
assumptions
of
perfect
homogeneity
and
isotropy
(the
cosmological
principle)
and
that
only
the
gravitational
interaction
is
significant
however
the
penrose–hawking
singularity
theorems
show
that
a
singularity
should
exist
for
very
general
conditions
hence
according
to
einstein's
field
equations
"r"
grew
rapidly
from
an
unimaginably
hot
dense
state
that
existed
immediately
following
this
singularity
(when
"r"
had
a
small
finite
value);
this
is
the
essence
of
the
big
bang
model
of
the
universe
understanding
the
singularity
of
the
big
bang
likely
requires
a
quantum
theory
of
gravity
which
has
not
yet
been
formulated
third
the
curvature
index
"k"
determines
the
sign
of
the
mean
spatial
curvature
of
spacetime
averaged
over
sufficiently
large
length
scales
(greater
than
about
a
billion
light-years)
if
"k"=1
the
curvature
is
positive
and
the
universe
has
a
finite
volume
a
universe
with
positive
curvature
is
often
visualized
as
a
three-dimensional
sphere
embedded
in
a
four-dimensional
space
conversely
if
"k"
is
zero
or
negative
the
universe
has
an
infinite
volume
it
may
seem
counter-intuitive
that
an
infinite
and
yet
infinitely
dense
universe
could
be
created
in
a
single
instant
at
the
big
bang
when
"r"=0
but
exactly
that
is
predicted
mathematically
when
"k"
does
not
equal
1
by
analogy
an
infinite
plane
has
zero
curvature
but
infinite
area
whereas
an
infinite
cylinder
is
finite
in
one
direction
and
a
torus
is
finite
in
both
a
toroidal
universe
could
behave
like
a
normal
universe
with
periodic
boundary
conditions
the
ultimate
fate
of
the
universe
is
still
unknown
because
it
depends
critically
on
the
curvature
index
"k"
and
the
cosmological
constant
"λ"
if
the
universe
were
sufficiently
dense
"k"
would
equal
+1
meaning
that
its
average
curvature
throughout
is
positive
and
the
universe
will
eventually
recollapse
in
a
big
crunch
possibly
starting
a
new
universe
in
a
big
bounce
conversely
if
the
universe
were
insufficiently
dense
"k"
would
equal
0
or
−1
and
the
universe
would
expand
forever
cooling
off
and
eventually
reaching
the
big
freeze
and
the
heat
death
of
the
universe
modern
data
suggests
that
the
rate
of
expansion
of
the
universe
is
not
decreasing
as
originally
expected
but
increasing;
if
this
continues
indefinitely
the
universe
may
eventually
reach
a
big
rip
observationally
the
universe
appears
to
be
flat
("k"
=
0)
with
an
overall
density
that
is
very
close
to
the
critical
value
between
recollapse
and
eternal
expansion
some
speculative
theories
have
proposed
that
our
universe
is
but
one
of
a
set
of
disconnected
universes
collectively
denoted
as
the
multiverse
challenging
or
enhancing
more
limited
definitions
of
the
universe
scientific
multiverse
models
are
distinct
from
concepts
such
as
alternate
planes
of
consciousness
and
simulated
reality
max
tegmark
developed
a
four-part
classification
scheme
for
the
different
types
of
multiverses
that
scientists
have
suggested
in
response
to
various
physics
problems
an
example
of
such
multiverses
is
the
one
resulting
from
the
chaotic
inflation
model
of
the
early
universe
another
is
the
multiverse
resulting
from
the
many-worlds
interpretation
of
quantum
mechanics
in
this
interpretation
parallel
worlds
are
generated
in
a
manner
similar
to
quantum
superposition
and
decoherence
with
all
states
of
the
wave
functions
being
realized
in
separate
worlds
effectively
in
the
many-worlds
interpretation
the
multiverse
evolves
as
a
universal
wavefunction
if
the
big
bang
that
created
our
multiverse
created
an
ensemble
of
multiverses
the
wave
function
of
the
ensemble
would
be
entangled
in
this
sense
the
least
controversial
category
of
multiverse
in
tegmark's
scheme
is
the
multiverses
of
this
level
are
composed
by
distant
spacetime
events
"in
our
own
universe"
if
space
is
infinite
or
sufficiently
large
and
uniform
identical
instances
of
the
history
of
earth's
entire
hubble
volume
occur
every
so
often
simply
by
chance
tegmark
calculated
that
our
nearest
so-called
doppelgänger
is
10
meters
away
from
us
(a
double
exponential
function
larger
than
a
googolplex)
in
principle
it
would
be
impossible
to
scientifically
verify
the
existence
of
an
identical
hubble
volume
however
this
existence
does
follow
as
a
fairly
straightforward
consequence
it
is
possible
to
conceive
of
disconnected
spacetimes
each
existing
but
unable
to
interact
with
one
another
an
easily
visualized
metaphor
of
this
concept
is
a
group
of
separate
soap
bubbles
in
which
observers
living
on
one
soap
bubble
cannot
interact
with
those
on
other
soap
bubbles
even
in
principle
according
to
one
common
terminology
each
"soap
bubble"
of
spacetime
is
denoted
as
a
"universe"
whereas
our
particular
spacetime
is
denoted
as
"the
universe"
just
as
we
call
our
moon
"the
moon"
the
entire
collection
of
these
separate
spacetimes
is
denoted
as
the
multiverse
with
this
terminology
different
"universes"
are
not
causally
connected
to
each
other
in
principle
the
other
unconnected
"universes"
may
have
different
dimensionalities
and
topologies
of
spacetime
different
forms
of
matter
and
energy
and
different
physical
laws
and
physical
constants
although
such
possibilities
are
purely
speculative
others
consider
each
of
several
bubbles
created
as
part
of
chaotic
inflation
to
be
separate
"universes"
though
in
this
model
these
universes
all
share
a
causal
origin
historically
there
have
been
many
ideas
of
the
cosmos
(cosmologies)
and
its
origin
(cosmogonies)
theories
of
an
impersonal
universe
governed
by
physical
laws
were
first
proposed
by
the
greeks
and
indians
ancient
chinese
philosophy
encompassed
the
notion
of
the
universe
including
both
all
of
space
and
all
of
time
over
the
centuries
improvements
in
astronomical
observations
and
theories
of
motion
and
gravitation
led
to
ever
more
accurate
descriptions
of
the
universe
the
modern
era
of
cosmology
began
with
albert
einstein's
1915
general
theory
of
relativity
which
made
it
possible
to
quantitatively
predict
the
origin
evolution
and
conclusion
of
the
universe
as
a
whole
most
modern
accepted
theories
of
cosmology
are
based
on
general
relativity
and
more
specifically
the
predicted
big
bang
many
cultures
have
stories
describing
the
origin
of
the
world
and
universe
cultures
generally
regard
these
stories
as
having
some
truth
there
are
however
many
differing
beliefs
in
how
these
stories
apply
amongst
those
believing
in
a
supernatural
origin
ranging
from
a
god
directly
creating
the
universe
as
it
is
now
to
a
god
just
setting
the
"wheels
in
motion"
(for
example
via
mechanisms
such
as
the
big
bang
and
evolution)
ethnologists
and
anthropologists
who
study
myths
have
developed
various
classification
schemes
for
the
various
themes
that
appear
in
creation
stories
for
example
in
one
type
of
story
the
world
is
born
from
a
world
egg;
such
stories
include
the
finnish
epic
poem
"kalevala"
the
chinese
story
of
pangu
or
the
indian
brahmanda
purana
in
related
stories
the
universe
is
created
by
a
single
entity
emanating
or
producing
something
by
him-
or
herself
as
in
the
tibetan
buddhism
concept
of
adi-buddha
the
ancient
greek
story
of
gaia
(mother
earth)
the
aztec
goddess
coatlicue
myth
the
ancient
egyptian
god
atum
story
and
the
judeo-christian
genesis
creation
narrative
in
which
the
abrahamic
god
created
the
universe
in
another
type
of
story
the
universe
is
created
from
the
union
of
male
and
female
deities
as
in
the
maori
story
of
rangi
and
papa
in
other
stories
the
universe
is
created
by
crafting
it
from
pre-existing
materials
such
as
the
corpse
of
a
dead
god —
as
from
tiamat
in
the
babylonian
epic
"enuma
elish"
or
from
the
giant
ymir
in
norse
mythology –
or
from
chaotic
materials
as
in
izanagi
and
izanami
in
japanese
mythology
in
other
stories
the
universe
emanates
from
fundamental
principles
such
as
brahman
and
prakrti
the
creation
myth
of
the
serers
or
the
yin
and
yang
of
the
tao
the
pre-socratic
greek
philosophers
and
indian
philosophers
developed
some
of
the
earliest
philosophical
concepts
of
the
universe
the
earliest
greek
philosophers
noted
that
appearances
can
be
deceiving
and
sought
to
understand
the
underlying
reality
behind
the
appearances
in
particular
they
noted
the
ability
of
matter
to
change
forms
(eg
ice
to
water
to
steam)
and
several
philosophers
proposed
that
all
the
physical
materials
in
the
world
are
different
forms
of
a
single
primordial
material
or
"arche"
the
first
to
do
so
was
thales
who
proposed
this
material
to
be
water
thales'
student
anaximander
proposed
that
everything
came
from
the
limitless
"apeiron"
anaximenes
proposed
the
primordial
material
to
be
air
on
account
of
its
perceived
attractive
and
repulsive
qualities
that
cause
the
"arche"
to
condense
or
dissociate
into
different
forms
anaxagoras
proposed
the
principle
of
"nous"
(mind)
while
heraclitus
proposed
fire
(and
spoke
of
"logos")
empedocles
proposed
the
elements
to
be
earth
water
air
and
fire
his
four-element
model
became
very
popular
like
pythagoras
plato
believed
that
all
things
were
composed
of
number
with
empedocles'
elements
taking
the
form
of
the
platonic
solids
democritus
and
later
philosophers—most
notably
leucippus—proposed
that
the
universe
is
composed
of
indivisible
atoms
moving
through
a
void
(vacuum)
although
aristotle
did
not
believe
that
to
be
feasible
because
air
like
water
offers
resistance
to
motion
air
will
immediately
rush
in
to
fill
a
void
and
moreover
without
resistance
it
would
do
so
indefinitely
fast
although
heraclitus
argued
for
eternal
change
his
contemporary
parmenides
made
the
radical
suggestion
that
all
change
is
an
illusion
that
the
true
underlying
reality
is
eternally
unchanging
and
of
a
single
nature
parmenides
denoted
this
reality
as
(the
one)
parmenides'
idea
seemed
implausible
to
many
greeks
but
his
student
zeno
of
elea
challenged
them
with
several
famous
paradoxes
aristotle
responded
to
these
paradoxes
by
developing
the
notion
of
a
potential
countable
infinity
as
well
as
the
infinitely
divisible
continuum
unlike
the
eternal
and
unchanging
cycles
of
time
he
believed
that
the
world
is
bounded
by
the
celestial
spheres
and
that
cumulative
stellar
magnitude
is
only
finitely
multiplicative
the
indian
philosopher
kanada
founder
of
the
vaisheshika
school
developed
a
notion
of
atomism
and
proposed
that
light
and
heat
were
varieties
of
the
same
substance
in
the
5th
century
ad
the
buddhist
atomist
philosopher
dignāga
proposed
atoms
to
be
point-sized
durationless
and
made
of
energy
they
denied
the
existence
of
substantial
matter
and
proposed
that
movement
consisted
of
momentary
flashes
of
a
stream
of
energy
the
notion
of
temporal
finitism
was
inspired
by
the
doctrine
of
creation
shared
by
the
three
abrahamic
religions:
judaism
christianity
and
islam
the
christian
philosopher
john
philoponus
presented
the
philosophical
arguments
against
the
ancient
greek
notion
of
an
infinite
past
and
future
philoponus'
arguments
against
an
infinite
past
were
used
by
the
early
muslim
philosopher
al-kindi
(alkindus);
the
jewish
philosopher
saadia
gaon
(saadia
ben
joseph);
and
the
muslim
theologian
al-ghazali
(algazel)
astronomical
models
of
the
universe
were
proposed
soon
after
astronomy
began
with
the
babylonian
astronomers
who
viewed
the
universe
as
a
flat
disk
floating
in
the
ocean
and
this
forms
the
premise
for
early
greek
maps
like
those
of
anaximander
and
hecataeus
of
miletus
later
greek
philosophers
observing
the
motions
of
the
heavenly
bodies
were
concerned
with
developing
models
of
the
universe-based
more
profoundly
on
empirical
evidence
the
first
coherent
model
was
proposed
by
eudoxus
of
cnidos
according
to
aristotle's
physical
interpretation
of
the
model
celestial
spheres
eternally
rotate
with
uniform
motion
around
a
stationary
earth
normal
matter
is
entirely
contained
within
the
terrestrial
sphere
"de
mundo"
(composed
before
250
bc
or
between
350
and
200
bc)
stated
"five
elements
situated
in
spheres
in
five
regions
the
less
being
in
each
case
surrounded
by
the
greater—namely
earth
surrounded
by
water
water
by
air
air
by
fire
and
fire
by
ether—make
up
the
whole
universe"
this
model
was
also
refined
by
callippus
and
after
concentric
spheres
were
abandoned
it
was
brought
into
nearly
perfect
agreement
with
astronomical
observations
by
ptolemy
the
success
of
such
a
model
is
largely
due
to
the
mathematical
fact
that
any
function
(such
as
the
position
of
a
planet)
can
be
decomposed
into
a
set
of
circular
functions
(the
fourier
modes)
other
greek
scientists
such
as
the
pythagorean
philosopher
philolaus
postulated
(according
to
stobaeus
account)
that
at
the
center
of
the
universe
was
a
"central
fire"
around
which
the
earth
sun
moon
and
planets
revolved
in
uniform
circular
motion
the
greek
astronomer
aristarchus
of
samos
was
the
first
known
individual
to
propose
a
heliocentric
model
of
the
universe
though
the
original
text
has
been
lost
a
reference
in
archimedes'
book
"the
sand
reckoner"
describes
aristarchus's
heliocentric
model
archimedes
wrote:
you
king
gelon
are
aware
the
universe
is
the
name
given
by
most
astronomers
to
the
sphere
the
center
of
which
is
the
center
of
the
earth
while
its
radius
is
equal
to
the
straight
line
between
the
center
of
the
sun
and
the
center
of
the
earth
this
is
the
common
account
as
you
have
heard
from
astronomers
but
aristarchus
has
brought
out
a
book
consisting
of
certain
hypotheses
wherein
it
appears
as
a
consequence
of
the
assumptions
made
that
the
universe
is
many
times
greater
than
the
universe
just
mentioned
his
hypotheses
are
that
the
fixed
stars
and
the
sun
remain
unmoved
that
the
earth
revolves
about
the
sun
on
the
circumference
of
a
circle
the
sun
lying
in
the
middle
of
the
orbit
and
that
the
sphere
of
fixed
stars
situated
about
the
same
center
as
the
sun
is
so
great
that
the
circle
in
which
he
supposes
the
earth
to
revolve
bears
such
a
proportion
to
the
distance
of
the
fixed
stars
as
the
center
of
the
sphere
bears
to
its
surface
aristarchus
thus
believed
the
stars
to
be
very
far
away
and
saw
this
as
the
reason
why
stellar
parallax
had
not
been
observed
that
is
the
stars
had
not
been
observed
to
move
relative
each
other
as
the
earth
moved
around
the
sun
the
stars
are
in
fact
much
farther
away
than
the
distance
that
was
generally
assumed
in
ancient
times
which
is
why
stellar
parallax
is
only
detectable
with
precision
instruments
the
geocentric
model
consistent
with
planetary
parallax
was
assumed
to
be
an
explanation
for
the
unobservability
of
the
parallel
phenomenon
stellar
parallax
the
rejection
of
the
heliocentric
view
was
apparently
quite
strong
as
the
following
passage
from
plutarch
suggests
("on
the
apparent
face
in
the
orb
of
the
moon"):
cleanthes
[a
contemporary
of
aristarchus
and
head
of
the
stoics]
thought
it
was
the
duty
of
the
greeks
to
indict
aristarchus
of
samos
on
the
charge
of
impiety
for
putting
in
motion
the
hearth
of
the
universe
[ie
the
earth]
supposing
the
heaven
to
remain
at
rest
and
the
earth
to
revolve
in
an
oblique
circle
while
it
rotates
at
the
same
time
about
its
own
axis
the
only
other
astronomer
from
antiquity
known
by
name
who
supported
aristarchus's
heliocentric
model
was
seleucus
of
seleucia
a
hellenistic
astronomer
who
lived
a
century
after
aristarchus
according
to
plutarch
seleucus
was
the
first
to
prove
the
heliocentric
system
through
reasoning
but
it
is
not
known
what
arguments
he
used
seleucus'
arguments
for
a
heliocentric
cosmology
were
probably
related
to
the
phenomenon
of
tides
according
to
strabo
(119)
seleucus
was
the
first
to
state
that
the
tides
are
due
to
the
attraction
of
the
moon
and
that
the
height
of
the
tides
depends
on
the
moon's
position
relative
to
the
sun
alternatively
he
may
have
proved
heliocentricity
by
determining
the
constants
of
a
geometric
model
for
it
and
by
developing
methods
to
compute
planetary
positions
using
this
model
like
what
nicolaus
copernicus
later
did
in
the
16th
century
during
the
middle
ages
heliocentric
models
were
also
proposed
by
the
indian
astronomer
aryabhata
and
by
the
persian
astronomers
albumasar
and
al-sijzi
the
aristotelian
model
was
accepted
in
the
western
world
for
roughly
two
millennia
until
copernicus
revived
aristarchus's
perspective
that
the
astronomical
data
could
be
explained
more
plausibly
if
the
earth
rotated
on
its
axis
and
if
the
sun
were
placed
at
the
center
of
the
universe
as
noted
by
copernicus
himself
the
notion
that
the
earth
rotates
is
very
old
dating
at
least
to
philolaus
(c
450
bc)
heraclides
ponticus
(c
350
bc)
and
ecphantus
the
pythagorean
roughly
a
century
before
copernicus
the
christian
scholar
nicholas
of
cusa
also
proposed
that
the
earth
rotates
on
its
axis
in
his
book
"on
learned
ignorance"
(1440)
al-sijzi
also
proposed
that
the
earth
rotates
on
its
axis
empirical
evidence
for
the
earth's
rotation
on
its
axis
using
the
phenomenon
of
comets
was
given
by
tusi
(1201–1274)
and
ali
qushji
(1403–1474)
this
cosmology
was
accepted
by
isaac
newton
christiaan
huygens
and
later
scientists
edmund
halley
(1720)
and
jean-philippe
de
chéseaux
(1744)
noted
independently
that
the
assumption
of
an
infinite
space
filled
uniformly
with
stars
would
lead
to
the
prediction
that
the
nighttime
sky
would
be
as
bright
as
the
sun
itself;
this
became
known
as
olbers'
paradox
in
the
19th
century
newton
believed
that
an
infinite
space
uniformly
filled
with
matter
would
cause
infinite
forces
and
instabilities
causing
the
matter
to
be
crushed
inwards
under
its
own
gravity
this
instability
was
clarified
in
1902
by
the
jeans
instability
criterion
one
solution
to
these
paradoxes
is
the
charlier
universe
in
which
the
matter
is
arranged
hierarchically
(systems
of
orbiting
bodies
that
are
themselves
orbiting
in
a
larger
system
"ad
infinitum")
in
a
fractal
way
such
that
the
universe
has
a
negligibly
small
overall
density;
such
a
cosmological
model
had
also
been
proposed
earlier
in
1761
by
johann
heinrich
lambert
a
significant
astronomical
advance
of
the
18th
century
was
the
realization
by
thomas
wright
immanuel
kant
and
others
of
nebulae
in
1919
when
hooker
telescope
was
completed
the
prevailing
view
still
was
that
the
universe
consisted
entirely
of
the
milky
way
galaxy
using
the
hooker
telescope
edwin
hubble
identified
cepheid
variables
in
several
spiral
nebulae
and
in
1922–1923
proved
conclusively
that
andromeda
nebula
and
triangulum
among
others
were
entire
galaxies
outside
our
own
thus
proving
that
universe
consists
of
multitude
of
galaxies
the
modern
era
of
physical
cosmology
began
in
1917
when
albert
einstein
first
applied
his
general
theory
of
relativity
to
model
the
structure
and
dynamics
of
the
universe
outer
space
outer
space
or
just
space
is
the
expanse
that
exists
beyond
the
earth
and
outside
of
any
astronomical
object
outer
space
is
not
completely
empty—it
is
a
hard
vacuum
containing
a
low
density
of
particles
predominantly
a
plasma
of
hydrogen
and
helium
as
well
as
electromagnetic
radiation
magnetic
fields
neutrinos
dust
and
cosmic
rays
the
baseline
temperature
as
set
by
the
background
radiation
from
the
big
bang
is
the
plasma
between
galaxies
accounts
for
about
half
of
the
baryonic
(ordinary)
matter
in
the
universe;
it
has
a
number
density
of
less
than
one
hydrogen
atom
per
cubic
metre
and
a
temperature
of
millions
of
kelvins;
local
concentrations
of
this
plasma
have
condensed
into
stars
and
galaxies
studies
indicate
that
90%
of
the
mass
in
most
galaxies
is
in
an
unknown
form
called
dark
matter
which
interacts
with
other
matter
through
gravitational
but
not
electromagnetic
forces
observations
suggest
that
the
majority
of
the
mass-energy
in
the
observable
universe
is
a
poorly
understood
vacuum
energy
of
space
which
astronomers
label
"dark
energy"
intergalactic
space
takes
up
most
of
the
volume
of
the
universe
but
even
galaxies
and
star
systems
consist
almost
entirely
of
empty
space
outer
space
does
not
begin
at
a
definite
altitude
above
the
earth's
surface
however
the
kármán
line
an
altitude
of
above
sea
level
is
conventionally
used
as
the
start
of
outer
space
in
space
treaties
and
for
aerospace
records
keeping
the
framework
for
international
space
law
was
established
by
the
outer
space
treaty
which
entered
into
force
on
10
october
1967
this
treaty
precludes
any
claims
of
national
sovereignty
and
permits
all
states
to
freely
explore
outer
space
despite
the
drafting
of
un
resolutions
for
the
peaceful
uses
of
outer
space
anti-satellite
weapons
have
been
tested
in
earth
orbit
humans
began
the
physical
exploration
of
space
during
the
20th
century
with
the
advent
of
high-altitude
balloon
flights
followed
by
manned
rocket
launches
earth
orbit
was
first
achieved
by
yuri
gagarin
of
the
soviet
union
in
1961
and
unmanned
spacecraft
have
since
reached
all
of
the
known
planets
in
the
solar
system
due
to
the
high
cost
of
getting
into
space
manned
spaceflight
has
been
limited
to
low
earth
orbit
and
the
moon
outer
space
represents
a
challenging
environment
for
human
exploration
because
of
the
hazards
of
vacuum
and
radiation
microgravity
also
has
a
negative
effect
on
human
physiology
that
causes
both
muscle
atrophy
and
bone
loss
in
addition
to
these
health
and
environmental
issues
the
economic
cost
of
putting
objects
including
humans
into
space
is
very
high
in
350
bce
greek
philosopher
aristotle
suggested
that
"nature
abhors
a
vacuum"
a
principle
that
became
known
as
the
"horror
vacui"
this
concept
built
upon
a
5th-century
bce
ontological
argument
by
the
greek
philosopher
parmenides
who
denied
the
possible
existence
of
a
void
in
space
based
on
this
idea
that
a
vacuum
could
not
exist
in
the
west
it
was
widely
held
for
many
centuries
that
space
could
not
be
empty
as
late
as
the
17th
century
the
french
philosopher
rené
descartes
argued
that
the
entirety
of
space
must
be
filled
in
ancient
china
the
2nd-century
astronomer
zhang
heng
became
convinced
that
space
must
be
infinite
extending
well
beyond
the
mechanism
that
supported
the
sun
and
the
stars
the
surviving
books
of
the
hsüan
yeh
school
said
that
the
heavens
were
boundless
"empty
and
void
of
substance"
likewise
the
"sun
moon
and
the
company
of
stars
float
in
the
empty
space
moving
or
standing
still"
the
italian
scientist
galileo
galilei
knew
that
air
had
mass
and
so
was
subject
to
gravity
in
1640
he
demonstrated
that
an
established
force
resisted
the
formation
of
a
vacuum
however
it
would
remain
for
his
pupil
evangelista
torricelli
to
create
an
apparatus
that
would
produce
a
partial
vacuum
in
1643
this
experiment
resulted
in
the
first
mercury
barometer
and
created
a
scientific
sensation
in
europe
the
french
mathematician
blaise
pascal
reasoned
that
if
the
column
of
mercury
was
supported
by
air
then
the
column
ought
to
be
shorter
at
higher
altitude
where
the
air
pressure
is
lower
in
1648
his
brother-in-law
florin
périer
repeated
the
experiment
on
the
puy
de
dôme
mountain
in
central
france
and
found
that
the
column
was
shorter
by
three
inches
this
decrease
in
pressure
was
further
demonstrated
by
carrying
a
half-full
balloon
up
a
mountain
and
watching
it
gradually
expand
then
contract
upon
descent
in
1650
german
scientist
otto
von
guericke
constructed
the
first
vacuum
pump:
a
device
that
would
further
refute
the
principle
of
"horror
vacui"
he
correctly
noted
that
the
atmosphere
of
the
earth
surrounds
the
planet
like
a
shell
with
the
density
gradually
declining
with
altitude
he
concluded
that
there
must
be
a
vacuum
between
the
earth
and
the
moon
back
in
the
15th
century
german
theologian
nicolaus
cusanus
speculated
that
the
universe
lacked
a
center
and
a
circumference
he
believed
that
the
universe
while
not
infinite
could
not
be
held
as
finite
as
it
lacked
any
bounds
within
which
it
could
be
contained
these
ideas
led
to
speculations
as
to
the
infinite
dimension
of
space
by
the
italian
philosopher
giordano
bruno
in
the
16th
century
he
extended
the
copernican
heliocentric
cosmology
to
the
concept
of
an
infinite
universe
filled
with
a
substance
he
called
aether
which
did
not
resist
the
motion
of
heavenly
bodies
english
philosopher
william
gilbert
arrived
at
a
similar
conclusion
arguing
that
the
stars
are
visible
to
us
only
because
they
are
surrounded
by
a
thin
aether
or
a
void
this
concept
of
an
aether
originated
with
ancient
greek
philosophers
including
aristotle
who
conceived
of
it
as
the
medium
through
which
the
heavenly
bodies
move
the
concept
of
a
universe
filled
with
a
luminiferous
aether
retained
support
among
some
scientists
until
the
early
20th
century
this
form
of
aether
was
viewed
as
the
medium
through
which
light
could
propagate
in
1887
the
michelson–morley
experiment
tried
to
detect
the
earth's
motion
through
this
medium
by
looking
for
changes
in
the
speed
of
light
depending
on
the
direction
of
the
planet's
motion
however
the
null
result
indicated
something
was
wrong
with
the
concept
the
idea
of
the
luminiferous
aether
was
then
abandoned
it
was
replaced
by
albert
einstein's
theory
of
special
relativity
which
holds
that
the
speed
of
light
in
a
vacuum
is
a
fixed
constant
independent
of
the
observer's
motion
or
frame
of
reference
the
first
professional
astronomer
to
support
the
concept
of
an
infinite
universe
was
the
englishman
thomas
digges
in
1576
but
the
scale
of
the
universe
remained
unknown
until
the
first
successful
measurement
of
the
distance
to
a
nearby
star
in
1838
by
the
german
astronomer
friedrich
bessel
he
showed
that
the
star
61
cygni
had
a
parallax
of
just
031 arcseconds
(compared
to
the
modern
value
of
0287″)
this
corresponds
to
a
distance
of
over
10
light
years
in
1917
heber
curtis
noted
that
novae
in
spiral
nebulae
were
on
average
10
magnitudes
fainter
than
galactic
novae
suggesting
that
the
former
are
100
times
further
away
the
distance
to
the
andromeda
galaxy
was
determined
in
1923
by
american
astronomer
edwin
hubble
by
measuring
the
brightness
of
cepheid
variables
in
that
galaxy
a
new
technique
discovered
by
henrietta
leavitt
this
established
that
the
andromeda
galaxy
and
by
extension
all
galaxies
lay
well
outside
the
milky
way
the
modern
concept
of
outer
space
is
based
on
the
"big
bang"
cosmology
first
proposed
in
1931
by
the
belgian
physicist
georges
lemaître
this
theory
holds
that
the
universe
originated
from
a
very
dense
form
that
has
since
undergone
continuous
expansion
the
earliest
known
estimate
of
the
temperature
of
outer
space
was
by
the
swiss
physicist
charles
é
guillaume
in
1896
using
the
estimated
radiation
of
the
background
stars
he
concluded
that
space
must
be
heated
to
a
temperature
of
5–6 k
british
physicist
arthur
eddington
made
a
similar
calculation
to
derive
a
temperature
of
318 k
in
1926
german
physicist
erich
regener
used
the
total
measured
energy
of
cosmic
rays
to
estimate
an
intergalactic
temperature
of
28 k
in
1933
american
physicists
ralph
alpher
and
robert
herman
predicted
5 k
for
the
temperature
of
space
in
1948
based
on
the
gradual
decrease
in
background
energy
following
the
then-new
big
bang
theory
the
modern
measurement
of
the
cosmic
microwave
background
is
about
27k
the
term
"outward
space"
was
used
in
1842
by
the
english
poet
lady
emmeline
stuart-wortley
in
her
poem
"the
maiden
of
moscow"
the
expression
"outer
space"
was
used
as
an
astronomical
term
by
alexander
von
humboldt
in
1845
it
was
later
popularized
in
the
writings
of
h
g
wells
in
1901
the
shorter
term
"space"
is
older
first
used
to
mean
the
region
beyond
earth's
sky
in
john
milton's
"paradise
lost"
in
1667
according
to
the
big
bang
theory
the
very
early
universe
was
an
extremely
hot
and
dense
state
about
138 billion
years
ago
which
rapidly
expanded
about
380000
years
later
the
universe
had
cooled
sufficiently
to
allow
protons
and
electrons
to
combine
and
form
hydrogen—the
so-called
recombination
epoch
when
this
happened
matter
and
energy
became
decoupled
allowing
photons
to
travel
freely
through
the
continually
expanding
space
matter
that
remained
following
the
initial
expansion
has
since
undergone
gravitational
collapse
to
create
stars
galaxies
and
other
astronomical
objects
leaving
behind
a
deep
vacuum
that
forms
what
is
now
called
outer
space
as
light
has
a
finite
velocity
this
theory
also
constrains
the
size
of
the
directly
observable
universe
this
leaves
open
the
question
as
to
whether
the
universe
is
finite
or
infinite
the
present
day
shape
of
the
universe
has
been
determined
from
measurements
of
the
cosmic
microwave
background
using
satellites
like
the
wilkinson
microwave
anisotropy
probe
these
observations
indicate
that
the
spatial
geometry
of
the
observable
universe
is
"flat"
meaning
that
photons
on
parallel
paths
at
one
point
remain
parallel
as
they
travel
through
space
to
the
limit
of
the
observable
universe
except
for
local
gravity
the
flat
universe
combined
with
the
measured
mass
density
of
the
universe
and
the
accelerating
expansion
of
the
universe
indicates
that
space
has
a
non-zero
vacuum
energy
which
is
called
dark
energy
estimates
put
the
average
energy
density
of
the
present
day
universe
at
the
equivalent
of
59
protons
per
cubic
meter
including
dark
energy
dark
matter
and
baryonic
matter
(ordinary
matter
composed
of
atoms)
the
atoms
account
for
only
46%
of
the
total
energy
density
or
a
density
of
one
proton
per
four
cubic
meters
the
density
of
the
universe
however
is
clearly
not
uniform;
it
ranges
from
relatively
high
density
in
galaxies—including
very
high
density
in
structures
within
galaxies
such
as
planets
stars
and
black
holes—to
conditions
in
vast
voids
that
have
much
lower
density
at
least
in
terms
of
visible
matter
unlike
matter
and
dark
matter
dark
energy
seems
not
to
be
concentrated
in
galaxies:
although
dark
energy
may
account
for
a
majority
of
the
mass-energy
in
the
universe
dark
energy's
influence
is
5
orders
of
magnitude
smaller
than
the
influence
of
gravity
from
matter
and
dark
matter
within
the
milky
way
outer
space
is
the
closest
known
approximation
to
a
perfect
vacuum
it
has
effectively
no
friction
allowing
stars
planets
and
moons
to
move
freely
along
their
ideal
orbits
following
the
initial
formation
stage
however
even
the
deep
vacuum
of
intergalactic
space
is
not
devoid
of
matter
as
it
contains
a
few
hydrogen
atoms
per
cubic
meter
by
comparison
the
air
humans
breathe
contains
about
10
molecules
per
cubic
meter
the
low
density
of
matter
in
outer
space
means
that
electromagnetic
radiation
can
travel
great
distances
without
being
scattered:
the
mean
free
path
of
a
photon
in
intergalactic
space
is
about
10 km
or
10 billion
light
years
in
spite
of
this
extinction
which
is
the
absorption
and
scattering
of
photons
by
dust
and
gas
is
an
important
factor
in
galactic
and
intergalactic
astronomy
stars
planets
and
moons
retain
their
atmospheres
by
gravitational
attraction
atmospheres
have
no
clearly
delineated
upper
boundary:
the
density
of
atmospheric
gas
gradually
decreases
with
distance
from
the
object
until
it
becomes
indistinguishable
from
outer
space
the
earth's
atmospheric
pressure
drops
to
about
pa
at
of
altitude
compared
to
100000
pa
for
the
international
union
of
pure
and
applied
chemistry
(iupac)
definition
of
standard
pressure
above
this
altitude
isotropic
gas
pressure
rapidly
becomes
insignificant
when
compared
to
radiation
pressure
from
the
sun
and
the
dynamic
pressure
of
the
solar
wind
the
thermosphere
in
this
range
has
large
gradients
of
pressure
temperature
and
composition
and
varies
greatly
due
to
space
weather
the
temperature
of
outer
space
is
measured
in
terms
of
the
kinetic
activity
of
the
gas
as
it
is
on
earth
however
the
radiation
of
outer
space
has
a
different
temperature
than
the
kinetic
temperature
of
the
gas
meaning
that
the
gas
and
radiation
are
not
in
thermodynamic
equilibrium
all
of
the
observable
universe
is
filled
with
photons
that
were
created
during
the
big
bang
which
is
known
as
the
cosmic
microwave
background
radiation
(cmb)
(there
is
quite
likely
a
correspondingly
large
number
of
neutrinos
called
the
cosmic
neutrino
background)
the
current
black
body
temperature
of
the
background
radiation
is
about
the
gas
temperatures
in
outer
space
are
always
at
least
the
temperature
of
the
cmb
but
can
be
much
higher
for
example
the
corona
of
the
sun
reaches
temperatures
over
12–26 million k
magnetic
fields
have
been
detected
in
the
space
around
just
about
every
class
of
celestial
object
star
formation
in
spiral
galaxies
can
generate
small-scale
dynamos
creating
turbulent
magnetic
field
strengths
of
around
5–10 μg
the
davis–greenstein
effect
causes
elongated
dust
grains
to
align
themselves
with
a
galaxy's
magnetic
field
resulting
in
weak
optical
polarization
this
has
been
used
to
show
ordered
magnetic
fields
exist
in
several
nearby
galaxies
magneto-hydrodynamic
processes
in
active
elliptical
galaxies
produce
their
characteristic
jets
and
radio
lobes
non-thermal
radio
sources
have
been
detected
even
among
the
most
distant
high-z
sources
indicating
the
presence
of
magnetic
fields
outside
a
protective
atmosphere
and
magnetic
field
there
are
few
obstacles
to
the
passage
through
space
of
energetic
subatomic
particles
known
as
cosmic
rays
these
particles
have
energies
ranging
from
about
10 ev
up
to
an
extreme
10 ev
of
ultra-high-energy
cosmic
rays
the
peak
flux
of
cosmic
rays
occurs
at
energies
of
about
10 ev
with
approximately
87%
protons
12%
helium
nuclei
and
1%
heavier
nuclei
in
the
high
energy
range
the
flux
of
electrons
is
only
about
1%
of
that
of
protons
cosmic
rays
can
damage
electronic
components
and
pose
a
health
threat
to
space
travelers
according
to
astronauts
like
don
pettit
space
has
a
burned/metallic
odor
that
clings
to
their
suits
and
equipment
similar
to
the
scent
of
an
arc
welding
torch
despite
the
harsh
environment
several
life
forms
have
been
found
that
can
withstand
extreme
space
conditions
for
extended
periods
species
of
lichen
carried
on
the
esa
biopan
facility
survived
exposure
for
ten
days
in
2007
seeds
of
"arabidopsis
thaliana"
and
"nicotiana
tabacum"
germinated
after
being
exposed
to
space
for
15
years
a
strain
of
"bacillus
subtilis"
has
survived
559
days
when
exposed
to
low-earth
orbit
or
a
simulated
martian
environment
the
lithopanspermia
hypothesis
suggests
that
rocks
ejected
into
outer
space
from
life-harboring
planets
may
successfully
transport
life
forms
to
another
habitable
world
a
conjecture
is
that
just
such
a
scenario
occurred
early
in
the
history
of
the
solar
system
with
potentially
microorganism-bearing
rocks
being
exchanged
between
venus
earth
and
mars
even
at
relatively
low
altitudes
in
the
earth's
atmosphere
conditions
are
hostile
to
the
human
body
the
altitude
where
atmospheric
pressure
matches
the
vapor
pressure
of
water
at
the
temperature
of
the
human
body
is
called
the
armstrong
line
named
after
american
physician
harry
g
armstrong
it
is
located
at
an
altitude
of
around
at
or
above
the
armstrong
line
fluids
in
the
throat
and
lungs
boil
away
more
specifically
exposed
bodily
liquids
such
as
saliva
tears
and
liquids
in
the
lungs
boil
away
hence
at
this
altitude
human
survival
requires
a
pressure
suit
or
a
pressurized
capsule
once
in
space
sudden
exposure
of
unprotected
humans
to
very
low
pressure
such
as
during
a
rapid
decompression
can
cause
pulmonary
barotrauma—a
rupture
of
the
lungs
due
to
the
large
pressure
differential
between
inside
and
outside
the
chest
even
if
the
subject's
airway
is
fully
open
the
flow
of
air
through
the
windpipe
may
be
too
slow
to
prevent
the
rupture
rapid
decompression
can
rupture
eardrums
and
sinuses
bruising
and
blood
seep
can
occur
in
soft
tissues
and
shock
can
cause
an
increase
in
oxygen
consumption
that
leads
to
hypoxia
as
a
consequence
of
rapid
decompression
oxygen
dissolved
in
the
blood
empties
into
the
lungs
to
try
to
equalize
the
partial
pressure
gradient
once
the
deoxygenated
blood
arrives
at
the
brain
humans
lose
consciousness
after
a
few
seconds
and
die
of
hypoxia
within
minutes
blood
and
other
body
fluids
boil
when
the
pressure
drops
below
63 kpa
and
this
condition
is
called
ebullism
the
steam
may
bloat
the
body
to
twice
its
normal
size
and
slow
circulation
but
tissues
are
elastic
and
porous
enough
to
prevent
rupture
ebullism
is
slowed
by
the
pressure
containment
of
blood
vessels
so
some
blood
remains
liquid
swelling
and
ebullism
can
be
reduced
by
containment
in
a
pressure
suit
the
crew
altitude
protection
suit
(caps)
a
fitted
elastic
garment
designed
in
the
1960s
for
astronauts
prevents
ebullism
at
pressures
as
low
as
2
kpa
supplemental
oxygen
is
needed
at
to
provide
enough
oxygen
for
breathing
and
to
prevent
water
loss
while
above
pressure
suits
are
essential
to
prevent
ebullism
most
space
suits
use
around
30–39 kpa
of
pure
oxygen
about
the
same
as
on
the
earth's
surface
this
pressure
is
high
enough
to
prevent
ebullism
but
evaporation
of
nitrogen
dissolved
in
the
blood
could
still
cause
decompression
sickness
and
gas
embolisms
if
not
managed
humans
evolved
for
life
in
earth
gravity
and
exposure
to
weightlessness
has
been
shown
to
have
deleterious
effects
on
human
health
initially
more
than
50%
of
astronauts
experience
space
motion
sickness
this
can
cause
nausea
and
vomiting
vertigo
headaches
lethargy
and
overall
malaise
the
duration
of
space
sickness
varies
but
it
typically
lasts
for
1–3
days
after
which
the
body
adjusts
to
the
new
environment
longer-term
exposure
to
weightlessness
results
in
muscle
atrophy
and
deterioration
of
the
skeleton
or
spaceflight
osteopenia
these
effects
can
be
minimized
through
a
regimen
of
exercise
other
effects
include
fluid
redistribution
slowing
of
the
cardiovascular
system
decreased
production
of
red
blood
cells
balance
disorders
and
a
weakening
of
the
immune
system
lesser
symptoms
include
loss
of
body
mass
nasal
congestion
sleep
disturbance
and
puffiness
of
the
face
for
long-duration
space
travel
radiation
can
pose
an
acute
health
hazard
exposure
to
high-energy
ionizing
cosmic
rays
can
result
in
fatigue
nausea
vomiting
as
well
as
damage
to
the
immune
system
and
changes
to
the
white
blood
cell
count
over
longer
durations
symptoms
include
an
increased
risk
of
cancer
plus
damage
to
the
eyes
nervous
system
lungs
and
the
gastrointestinal
tract
on
a
round-trip
mars
mission
lasting
three
years
a
large
fraction
of
the
cells
in
an
astronaut's
body
would
be
traversed
and
potentially
damaged
by
high
energy
nuclei
the
energy
of
such
particles
is
significantly
diminished
by
the
shielding
provided
by
the
walls
of
a
spacecraft
and
can
be
further
diminished
by
water
containers
and
other
barriers
however
the
impact
of
the
cosmic
rays
upon
the
shielding
produces
additional
radiation
that
can
affect
the
crew
further
research
is
needed
to
assess
the
radiation
hazards
and
determine
suitable
countermeasures
there
is
no
clear
boundary
between
earth's
atmosphere
and
space
as
the
density
of
the
atmosphere
gradually
decreases
as
the
altitude
increases
there
are
several
standard
boundary
designations
namely:
in
2009
scientists
reported
detailed
measurements
with
a
supra-thermal
ion
imager
(an
instrument
that
measures
the
direction
and
speed
of
ions)
which
allowed
them
to
establish
a
boundary
at
above
earth
the
boundary
represents
the
midpoint
of
a
gradual
transition
over
tens
of
kilometers
from
the
relatively
gentle
winds
of
the
earth's
atmosphere
to
the
more
violent
flows
of
charged
particles
in
space
which
can
reach
speeds
well
over
the
outer
space
treaty
provides
the
basic
framework
for
international
space
law
it
covers
the
legal
use
of
outer
space
by
nation
states
and
includes
in
its
definition
of
"outer
space"
the
moon
and
other
celestial
bodies
the
treaty
states
that
outer
space
is
free
for
all
nation
states
to
explore
and
is
not
subject
to
claims
of
national
sovereignty
it
also
prohibits
the
deployment
of
nuclear
weapons
in
outer
space
the
treaty
was
passed
by
the
united
nations
general
assembly
in
1963
and
signed
in
1967
by
the
ussr
the
united
states
of
america
and
the
united
kingdom
as
of
2017
105
state
parties
have
either
ratified
or
acceded
to
the
treaty
an
additional
25
states
signed
the
treaty
without
ratifying
it
since
1958
outer
space
has
been
the
subject
of
multiple
united
nations
resolutions
of
these
more
than
50
have
been
concerning
the
international
co-operation
in
the
peaceful
uses
of
outer
space
and
preventing
an
arms
race
in
space
four
additional
space
law
treaties
have
been
negotiated
and
drafted
by
the
un's
committee
on
the
peaceful
uses
of
outer
space
still
there
remains
no
legal
prohibition
against
deploying
conventional
weapons
in
space
and
anti-satellite
weapons
have
been
successfully
tested
by
the
us
ussr
and
china
the
1979
moon
treaty
turned
the
jurisdiction
of
all
heavenly
bodies
(including
the
orbits
around
such
bodies)
over
to
the
international
community
however
this
treaty
has
not
been
ratified
by
any
nation
that
currently
practices
manned
spaceflight
in
1976
eight
equatorial
states
(ecuador
colombia
brazil
congo
zaire
uganda
kenya
and
indonesia)
met
in
bogotá
colombia
with
their
"declaration
of
the
first
meeting
of
equatorial
countries"
or
"the
bogotá
declaration"
they
claimed
control
of
the
segment
of
the
geosynchronous
orbital
path
corresponding
to
each
country
these
claims
are
not
internationally
accepted
a
spacecraft
enters
orbit
when
its
centripetal
acceleration
due
to
gravity
is
less
than
or
equal
to
the
centrifugal
acceleration
due
to
the
horizontal
component
of
its
velocity
for
a
low
earth
orbit
this
velocity
is
about
;
by
contrast
the
fastest
manned
airplane
speed
ever
achieved
(excluding
speeds
achieved
by
deorbiting
spacecraft)
was
in
1967
by
the
north
american
x-15
to
achieve
an
orbit
a
spacecraft
must
travel
faster
than
a
sub-orbital
spaceflight
the
energy
required
to
reach
earth
orbital
velocity
at
an
altitude
of
is
about
36 mj/kg
which
is
six
times
the
energy
needed
merely
to
climb
to
the
corresponding
altitude
spacecraft
with
a
perigee
below
about
are
subject
to
drag
from
the
earth's
atmosphere
which
decreases
the
orbital
altitude
the
rate
of
orbital
decay
depends
on
the
satellite's
cross-sectional
area
and
mass
as
well
as
variations
in
the
air
density
of
the
upper
atmosphere
below
about
decay
becomes
more
rapid
with
lifetimes
measured
in
days
once
a
satellite
descends
to
it
has
only
hours
before
it
vaporizes
in
the
atmosphere
the
escape
velocity
required
to
pull
free
of
earth's
gravitational
field
altogether
and
move
into
interplanetary
space
is
about
space
is
a
partial
vacuum:
its
different
regions
are
defined
by
the
various
atmospheres
and
"winds"
that
dominate
within
them
and
extend
to
the
point
at
which
those
winds
give
way
to
those
beyond
geospace
extends
from
earth's
atmosphere
to
the
outer
reaches
of
earth's
magnetic
field
whereupon
it
gives
way
to
the
solar
wind
of
interplanetary
space
interplanetary
space
extends
to
the
heliopause
whereupon
the
solar
wind
gives
way
to
the
winds
of
the
interstellar
medium
interstellar
space
then
continues
to
the
edges
of
the
galaxy
where
it
fades
into
the
intergalactic
void
geospace
is
the
region
of
outer
space
near
earth
including
the
upper
atmosphere
and
magnetosphere
the
van
allen
radiation
belts
lie
within
the
geospace
the
outer
boundary
of
geospace
is
the
magnetopause
which
forms
an
interface
between
the
earth's
magnetosphere
and
the
solar
wind
the
inner
boundary
is
the
ionosphere
the
variable
space-weather
conditions
of
geospace
are
affected
by
the
behavior
of
the
sun
and
the
solar
wind;
the
subject
of
geospace
is
interlinked
with
heliophysics—the
study
of
the
sun
and
its
impact
on
the
planets
of
the
solar
system
the
day-side
magnetopause
is
compressed
by
solar-wind
pressure—the
subsolar
distance
from
the
center
of
the
earth
is
typically
10
earth
radii
on
the
night
side
the
solar
wind
stretches
the
magnetosphere
to
form
a
magnetotail
that
sometimes
extends
out
to
more
than
100–200
earth
radii
for
roughly
four
days
of
each
month
the
lunar
surface
is
shielded
from
the
solar
wind
as
the
moon
passes
through
the
magnetotail
geospace
is
populated
by
electrically
charged
particles
at
very
low
densities
the
motions
of
which
are
controlled
by
the
earth's
magnetic
field
these
plasmas
form
a
medium
from
which
storm-like
disturbances
powered
by
the
solar
wind
can
drive
electrical
currents
into
the
earth's
upper
atmosphere
geomagnetic
storms
can
disturb
two
regions
of
geospace
the
radiation
belts
and
the
ionosphere
these
storms
increase
fluxes
of
energetic
electrons
that
can
permanently
damage
satellite
electronics
interfering
with
shortwave
radio
communication
and
gps
location
and
timing
magnetic
storms
can
also
be
a
hazard
to
astronauts
even
in
low
earth
orbit
they
also
create
aurorae
seen
at
high
latitudes
in
an
oval
surrounding
the
geomagnetic
poles
although
it
meets
the
definition
of
outer
space
the
atmospheric
density
within
the
first
few
hundred
kilometers
above
the
kármán
line
is
still
sufficient
to
produce
significant
drag
on
satellites
this
region
contains
material
left
over
from
previous
manned
and
unmanned
launches
that
are
a
potential
hazard
to
spacecraft
some
of
this
debris
re-enters
earth's
atmosphere
periodically
earth's
gravity
keeps
the
moon
in
orbit
at
an
average
distance
of
the
region
outside
earth's
atmosphere
and
extending
out
to
just
beyond
the
moon's
orbit
including
the
lagrangian
points
is
sometimes
referred
to
as
cislunar
space
the
region
of
space
where
earth's
gravity
remains
dominant
against
gravitational
perturbations
from
the
sun
is
called
the
hill
sphere
this
extends
well
out
into
translunar
space
to
a
distance
of
roughly
1%
of
the
mean
distance
from
earth
to
the
sun
or
deep
space
has
different
definitions
as
to
where
it
starts
it
has
been
defined
by
the
united
states
government
and
others
as
any
region
beyond
cislunar
space
the
international
telecommunication
union
responsible
for
radio
communication
(including
satellites)
defines
the
beginning
of
deep
space
at
about
5
times
that
distance
()
interplanetary
space
is
defined
by
the
solar
wind
a
continuous
stream
of
charged
particles
emanating
from
the
sun
that
creates
a
very
tenuous
atmosphere
(the
heliosphere)
for
billions
of
kilometers
into
space
this
wind
has
a
particle
density
of
5–10
protons/cm
and
is
moving
at
a
velocity
of
interplanetary
space
extends
out
to
the
heliopause
where
the
influence
of
the
galactic
environment
starts
to
dominate
over
the
magnetic
field
and
particle
flux
from
the
sun
the
distance
and
strength
of
the
heliopause
varies
depending
on
the
activity
level
of
the
solar
wind
the
heliopause
in
turn
deflects
away
low-energy
galactic
cosmic
rays
with
this
modulation
effect
peaking
during
solar
maximum
the
volume
of
interplanetary
space
is
a
nearly
total
vacuum
with
a
mean
free
path
of
about
one
astronomical
unit
at
the
orbital
distance
of
the
earth
however
this
space
is
not
completely
empty
and
is
sparsely
filled
with
cosmic
rays
which
include
ionized
atomic
nuclei
and
various
subatomic
particles
there
is
also
gas
plasma
and
dust
small
meteors
and
several
dozen
types
of
organic
molecules
discovered
to
date
by
microwave
spectroscopy
a
cloud
of
interplanetary
dust
is
visible
at
night
as
a
faint
band
called
the
zodiacal
light
interplanetary
space
contains
the
magnetic
field
generated
by
the
sun
there
are
also
magnetospheres
generated
by
planets
such
as
jupiter
saturn
mercury
and
the
earth
that
have
their
own
magnetic
fields
these
are
shaped
by
the
influence
of
the
solar
wind
into
the
approximation
of
a
teardrop
shape
with
the
long
tail
extending
outward
behind
the
planet
these
magnetic
fields
can
trap
particles
from
the
solar
wind
and
other
sources
creating
belts
of
charged
particles
such
as
the
van
allen
radiation
belts
planets
without
magnetic
fields
such
as
mars
have
their
atmospheres
gradually
eroded
by
the
solar
wind
interstellar
space
is
the
physical
space
within
a
galaxy
beyond
the
influence
each
star
has
upon
the
encompassed
plasma
the
contents
of
interstellar
space
are
called
the
interstellar
medium
approximately
70%
of
the
mass
of
the
interstellar
medium
consists
of
lone
hydrogen
atoms;
most
of
the
remainder
consists
of
helium
atoms
this
is
enriched
with
trace
amounts
of
heavier
atoms
formed
through
stellar
nucleosynthesis
these
atoms
are
ejected
into
the
interstellar
medium
by
stellar
winds
or
when
evolved
stars
begin
to
shed
their
outer
envelopes
such
as
during
the
formation
of
a
planetary
nebula
the
cataclysmic
explosion
of
a
supernova
generates
an
expanding
shock
wave
consisting
of
ejected
materials
that
further
enrich
the
medium
the
density
of
matter
in
the
interstellar
medium
can
vary
considerably:
the
average
is
around
10
particles
per
m
but
cold
molecular
clouds
can
hold
10–10
per
m
a
number
of
molecules
exist
in
interstellar
space
as
can
tiny
01 μm
dust
particles
the
tally
of
molecules
discovered
through
radio
astronomy
is
steadily
increasing
at
the
rate
of
about
four
new
species
per
year
large
regions
of
higher
density
matter
known
as
molecular
clouds
allow
chemical
reactions
to
occur
including
the
formation
of
organic
polyatomic
species
much
of
this
chemistry
is
driven
by
collisions
energetic
cosmic
rays
penetrate
the
cold
dense
clouds
and
ionize
hydrogen
and
helium
resulting
for
example
in
the
trihydrogen
cation
an
ionized
helium
atom
can
then
split
relatively
abundant
carbon
monoxide
to
produce
ionized
carbon
which
in
turn
can
lead
to
organic
chemical
reactions
the
local
interstellar
medium
is
a
region
of
space
within
100 parsecs
(pc)
of
the
sun
which
is
of
interest
both
for
its
proximity
and
for
its
interaction
with
the
solar
system
this
volume
nearly
coincides
with
a
region
of
space
known
as
the
local
bubble
which
is
characterized
by
a
lack
of
dense
cold
clouds
it
forms
a
cavity
in
the
orion
arm
of
the
milky
way
galaxy
with
dense
molecular
clouds
lying
along
the
borders
such
as
those
in
the
constellations
of
ophiuchus
and
taurus
(the
actual
distance
to
the
border
of
this
cavity
varies
from
60
to
250 pc
or
more)
this
volume
contains
about
10–10
stars
and
the
local
interstellar
gas
counterbalances
the
astrospheres
that
surround
these
stars
with
the
volume
of
each
sphere
varying
depending
on
the
local
density
of
the
interstellar
medium
the
local
bubble
contains
dozens
of
warm
interstellar
clouds
with
temperatures
of
up
to
7000 k
and
radii
of
05–5 pc
when
stars
are
moving
at
sufficiently
high
peculiar
velocities
their
astrospheres
can
generate
bow
shocks
as
they
collide
with
the
interstellar
medium
for
decades
it
was
assumed
that
the
sun
had
a
bow
shock
in
2012
data
from
interstellar
boundary
explorer
(ibex)
and
nasa's
voyager
probes
showed
that
the
sun's
bow
shock
does
not
exist
instead
these
authors
argue
that
a
subsonic
bow
wave
defines
the
transition
from
the
solar
wind
flow
to
the
interstellar
medium
a
bow
shock
is
the
third
boundary
of
an
astrosphere
after
the
termination
shock
and
the
astropause
(called
the
heliopause
in
the
solar
system)
intergalactic
space
is
the
physical
space
between
galaxies
studies
of
the
large
scale
distribution
of
galaxies
show
that
the
universe
has
a
foam-like
structure
with
clusters
and
groups
of
galaxies
lying
along
filaments
that
occupy
about
a
tenth
of
the
total
space
the
remainder
forms
huge
voids
that
are
mostly
empty
of
galaxies
typically
a
void
spans
a
distance
of
(10–40)
"h"
mpc
where
"h"
is
the
hubble
constant
in
units
of
surrounding
and
stretching
between
galaxies
there
is
a
rarefied
plasma
that
is
organized
in
a
galactic
filamentary
structure
this
material
is
called
the
intergalactic
medium
(igm)
the
density
of
the
igm
is
5–200
times
the
average
density
of
the
universe
it
consists
mostly
of
ionized
hydrogen;
ie
a
plasma
consisting
of
equal
numbers
of
electrons
and
protons
as
gas
falls
into
the
intergalactic
medium
from
the
voids
it
heats
up
to
temperatures
of
10 k
to
10 k
which
is
high
enough
so
that
collisions
between
atoms
have
enough
energy
to
cause
the
bound
electrons
to
escape
from
the
hydrogen
nuclei;
this
is
why
the
igm
is
ionized
at
these
temperatures
it
is
called
the
warm–hot
intergalactic
medium
(whim)
(although
the
plasma
is
very
hot
by
terrestrial
standards
10
k
is
often
called
"warm"
in
astrophysics)
computer
simulations
and
observations
indicate
that
up
to
half
of
the
atomic
matter
in
the
universe
might
exist
in
this
warm–hot
rarefied
state
when
gas
falls
from
the
filamentary
structures
of
the
whim
into
the
galaxy
clusters
at
the
intersections
of
the
cosmic
filaments
it
can
heat
up
even
more
reaching
temperatures
of
10 k
and
above
in
the
so-called
intracluster
medium
for
the
majority
of
human
history
space
was
explored
by
observations
made
from
the
earth's
surface—initially
with
the
unaided
eye
and
then
with
the
telescope
prior
to
the
advent
of
reliable
rocket
technology
the
closest
that
humans
had
come
to
reaching
outer
space
was
through
the
use
of
balloon
flights
in
1935
the
us
"explorer
ii"
manned
balloon
flight
had
reached
an
altitude
of
this
was
greatly
exceeded
in
1942
when
the
third
launch
of
the
german
a-4
rocket
climbed
to
an
altitude
of
about
in
1957
the
unmanned
satellite
sputnik
1
was
launched
by
a
russian
r-7
rocket
achieving
earth
orbit
at
an
altitude
of
this
was
followed
by
the
first
human
spaceflight
in
1961
when
yuri
gagarin
was
sent
into
orbit
on
vostok
1
the
first
humans
to
escape
low-earth
orbit
were
frank
borman
jim
lovell
and
william
anders
in
1968
on
board
the
us
apollo
8
which
achieved
lunar
orbit
and
reached
a
maximum
distance
of
from
the
earth
the
first
spacecraft
to
reach
escape
velocity
was
the
soviet
luna
1
which
performed
a
fly-by
of
the
moon
in
1959
in
1961
venera
1
became
the
first
planetary
probe
it
revealed
the
presence
of
the
solar
wind
and
performed
the
first
fly-by
of
venus
although
contact
was
lost
before
reaching
venus
the
first
successful
planetary
mission
was
the
1962
fly-by
of
venus
by
mariner
2
the
first
fly-by
of
mars
was
by
mariner
4
in
1964
since
that
time
unmanned
spacecraft
have
successfully
examined
each
of
the
solar
system's
planets
as
well
their
moons
and
many
minor
planets
and
comets
they
remain
a
fundamental
tool
for
the
exploration
of
outer
space
as
well
as
observation
of
the
earth
in
august
2012
"voyager
1"
became
the
first
man-made
object
to
leave
the
solar
system
and
enter
interstellar
space
the
absence
of
air
makes
outer
space
an
ideal
location
for
astronomy
at
all
wavelengths
of
the
electromagnetic
spectrum
this
is
evidenced
by
the
spectacular
pictures
sent
back
by
the
hubble
space
telescope
allowing
light
from
more
than
13 billion
years
ago—almost
to
the
time
of
the
big
bang—to
be
observed
however
not
every
location
in
space
is
ideal
for
a
telescope
the
interplanetary
zodiacal
dust
emits
a
diffuse
near-infrared
radiation
that
can
mask
the
emission
of
faint
sources
such
as
extrasolar
planets
moving
an
infrared
telescope
out
past
the
dust
increases
its
effectiveness
likewise
a
site
like
the
daedalus
crater
on
the
far
side
of
the
moon
could
shield
a
radio
telescope
from
the
radio
frequency
interference
that
hampers
earth-based
observations
unmanned
spacecraft
in
earth
orbit
are
an
essential
technology
of
modern
civilization
they
allow
direct
monitoring
of
weather
conditions
relay
long-range
communications
like
television
provide
a
means
of
precise
navigation
and
allow
remote
sensing
of
the
earth
the
latter
role
serves
a
wide
variety
of
purposes
including
tracking
soil
moisture
for
agriculture
prediction
of
water
outflow
from
seasonal
snow
packs
detection
of
diseases
in
plants
and
trees
and
surveillance
of
military
activities
the
deep
vacuum
of
space
could
make
it
an
attractive
environment
for
certain
industrial
processes
such
as
those
requiring
ultraclean
surfaces
however
like
asteroid
mining
space
manufacturing
requires
significant
investment
with
little
prospect
of
immediate
return
an
important
factor
in
the
total
expense
is
the
high
cost
of
placing
mass
into
earth
orbit:
$–
per
kg
in
inflation-adjusted
dollars
according
to
a
2006
estimate
proposed
concepts
for
addressing
this
issue
include
non-rocket
spacelaunch
momentum
exchange
tethers
and
space
elevators
interstellar
travel
for
a
human
crew
remains
at
present
only
a
theoretical
possibility
the
distances
to
the
nearest
stars
will
require
new
technological
developments
and
the
ability
to
safely
sustain
crews
for
journeys
lasting
several
decades
for
example
the
daedalus
project
study
which
proposed
a
spacecraft
powered
by
the
fusion
of
deuterium
and
he
would
require
36
years
to
reach
the
nearby
alpha
centauri
system
other
proposed
interstellar
propulsion
systems
include
light
sails
ramjets
and
beam-powered
propulsion
more
advanced
propulsion
systems
could
use
antimatter
as
a
fuel
potentially
reaching
relativistic
velocities
biophysical
environment
a
biophysical
environment
is
a
biotic
and
abiotic
surrounding
of
an
organism
or
population
and
consequently
includes
the
factors
that
have
an
influence
in
their
survival
development
and
evolution
a
biophysical
environment
can
vary
in
scale
from
microscopic
to
global
in
extent
it
can
also
be
subdivided
according
to
its
attributes
examples
include
the
marine
environment
the
atmospheric
environment
and
the
terrestrial
environment
the
number
of
biophysical
environments
is
countless
given
that
each
living
organism
has
its
own
environment
the
term
"environment"
can
refer
to
a
singular
global
environment
in
relation
to
humanity
or
a
local
biophysical
environment
eg
the
uk's
environment
agency
all
life
that
has
survived
must
have
adapted
to
conditions
of
its
environment
temperature
light
humidity
soil
nutrients
etc
all
influence
any
species
within
any
environment
however
life
in
turn
modifies
in
various
forms
its
conditions
some
long
term
modifications
along
the
history
of
our
planet
have
been
significant
such
as
the
incorporation
of
oxygen
to
the
atmosphere
this
process
consisted
in
the
breakdown
of
carbon
dioxide
by
anaerobic
microorganisms
that
used
the
carbon
in
their
metabolism
and
released
the
oxygen
to
the
atmosphere
this
led
to
the
existence
of
oxygen-based
plant
and
animal
life
the
great
oxygenation
event
other
interactions
are
more
immediate
and
simple
such
as
the
smoothing
effect
that
forests
have
on
the
temperature
cycle
compared
to
neighboring
unforested
areas
environmental
science
is
the
study
of
the
interactions
within
the
biophysical
environment
part
of
this
scientific
discipline
is
the
investigation
of
the
effect
of
human
activity
on
the
environment
ecology
a
sub-discipline
of
biology
and
a
part
of
environmental
sciences
is
often
mistaken
as
a
study
of
human
induced
effects
on
the
environment
environmental
studies
is
a
broader
academic
discipline
that
is
the
systematic
study
of
interaction
of
humans
with
their
environment
it
is
a
broad
field
of
study
that
includes
the
natural
environment
built
environments
and
social
environments
environmentalism
is
a
broad
social
and
philosophical
movement
that
in
a
large
part
seeks
to
minimise
and
compensate
the
negative
effect
of
human
activity
on
the
biophysical
environment
the
issues
of
concern
for
environmentalists
usually
relate
to
the
natural
environment
with
the
more
important
ones
being
climate
change
species
extinction
pollution
and
old
growth
forest
loss
one
of
the
studies
related
include
employing
geographic
information
science
to
study
the
biophysical
environment
regional
policy
regional
policy
aims
to
improve
economic
conditions
in
regions
of
relative
disadvantage
either
within
a
nation
or
within
a
supranational
grouping
such
as
the
european
union
although
the
european
union
is
one
of
the
richest
parts
of
the
world
there
are
large
internal
disparities
of
income
and
opportunity
between
its
regions
the
may
2004
enlargement
followed
by
accession
of
bulgaria
and
romania
in
january
2007
has
widened
these
gaps
regional
policy
transfers
resources
from
richer
to
poorer
regions
the
argument
for
regional
policy
is
that
it
is
both
an
instrument
of
financial
solidarity
and
a
powerful
force
for
economic
integration
the
major
italian
experience
of
regional
policy
is
the
cassa
per
il
mezzogiorno
set
up
in
the
mid-1950s
to
foster
economic
development
in
southern
italy
originally
intended
to
last
for
six
months
it
survived
until
1984
new
roads
irrigation
projects
and
developments
in
infrastructure
were
built
in
an
area
where
local
communities
had
suffered
seriously
from
poverty
de-population
and
high
levels
of
emigration
tourism
projects
attempted
to
exploit
calabria’s
beaches
uk
regional
policy
was
born
during
the
economic
depression
of
the
1930s
when
heavy
industries
in
the
north
were
devastated
"assisted
areas"
were
established
within
which
companies
could
acquire
grants
or
capital
allowances
–
known
as
regional
selective
assistance
(rsa)
–
in
return
for
protecting
jobs
the
overall
pattern
of
policy
changed
little
in
the
next
forty
years
despite
criticism
by
a
1970s
royal
commission
that
it
was
"empiricism
run
mad;
a
game
of
hit
and
miss
played
with
more
enthusiasm
than
success"
governments
of
both
parties
maintained
assisted
areas
under
the
1980s
thatcher
government
regional
policy
was
significantly
rolled
back
with
assisted
areas
substantially
reduced
in
size
the
post-1997
labour
administration
reorganised
regional
policy
with
rsa
replaced
by
selective
finance
for
investment
in
england
and
scotland
uk
policy
has
been
subject
to
eu
regional
policy
framework
with
its
strong
injunctions
against
unfair
competition
(generally
meaning
state
aid)
ws-policy
ws-policy
is
a
specification
that
allows
web
services
to
use
xml
to
advertise
their
policies
(on
security
quality
of
service
etc)
and
for
web
service
consumers
to
specify
their
policy
requirements
ws-policy
is
a
w3c
recommendation
as
of
september
2007
ws-policy
represents
a
set
of
specifications
that
describe
the
capabilities
and
constraints
of
the
security
(and
other
business)
policies
on
intermediaries
and
end
points
(for
example
required
security
tokens
supported
encryption
algorithms
and
privacy
rules)
and
how
to
associate
policies
with
services
and
end
points
assertions
can
either
be
requirements
put
upon
a
web
service
or
an
advertisement
of
the
policies
of
a
web
service
two
"operators"
(xml
tags)
are
used
to
make
statements
about
policy
combinations:
logically
an
empty
"wsp:all"
tag
makes
no
assertions
if
both
provider
and
consumer
specify
a
policy
an
effective
policy
will
be
computed
which
usually
consists
of
the
intersection
of
both
policies
the
new
policy
contains
those
assertions
made
by
both
sides
which
do
not
contradict
each
other
however
synonymous
assertions
are
considered
incompatible
by
a
policy
intersection
this
can
easily
be
explained
by
the
fact
that
policy
intersection
is
a
syntactic
approach
which
does
not
incorporate
the
semantics
of
the
assertions
furthermore
it
ignores
the
assertion
parameters
opposed
to
what
the
name
might
suggest
a
policy
intersection
is
(although
quite
similar)
not
a
set-intersection
haldane
principle
in
british
research
policy
the
haldane
principle
is
the
idea
that
decisions
about
what
to
spend
research
funds
on
should
be
made
by
researchers
rather
than
politicians
it
is
named
after
richard
burdon
haldane
who
in
1904
and
from
1909
to
1918
chaired
committees
and
commissions
which
recommended
this
policy
the
1904
committee
recommended
the
creation
of
the
university
grants
committee
which
has
evolved
via
the
universities
funding
council
into
the
current
higher
education
funding
councils:
research
councils
uk
higher
education
funding
council
for
england
scottish
funding
council
and
higher
education
funding
council
for
wales
in
1918
haldane's
committee
produced
the
"haldane
report"
the
report
suggested
that
research
required
by
government
departments
could
be
separated
into
that
required
by
specific
departments
and
that
which
was
more
general
it
recommended
that
departments
should
oversee
the
specific
research
but
the
general
research
should
be
under
the
control
of
autonomous
research
councils
which
would
be
free
from
political
and
administrative
pressures
that
might
discourage
research
in
certain
areas
the
principle
of
the
autonomy
of
the
research
councils
is
now
referred
to
as
the
haldane
principle
the
first
research
council
to
be
created
as
a
result
of
the
haldane
report
was
the
medical
research
council
the
principle
has
remained
enshrined
in
british
government
policy
but
has
been
criticised
and
altered
over
the
years
in
1939
jd
bernal
argued
that
social
good
was
more
important
than
researchers'
freedom
in
deciding
the
direction
of
research
solly
zuckerman
criticised
it
in
1971
for
its
artificial
separation
of
basic
and
applied
science
and
the
consequent
elevation
of
the
status
of
the
former
a
major
revision
to
the
application
of
the
haldane
principle
in
british
research
funding
came
in
the
early
1970s
with
the
rothschild
report
of
1971
and
its
implementation
which
transferred
about
25%
of
the
then
research
council
funds
and
the
decisions
on
the
research
to
be
funded
with
them
back
to
government
departments
a
move
later
undone
by
margaret
thatcher's
government
there
is
currently
a
debate
about
the
extent
to
which
the
principle
is
still
applied
in
practice
the
higher
education
and
research
act
2017
which
merged
the
research
councils
and
the
research
part
of
the
higher
education
funding
council
for
england
into
uk
research
and
innovation
enacted
the
haldane
principle
as
section
103(3):
"the
“haldane
principle”
is
the
principle
that
decisions
on
individual
research
proposals
are
best
taken
following
an
evaluation
of
the
quality
and
likely
impact
of
the
proposals
(such
as
a
peer
review
process)"
eightfold
path
(policy
analysis)
the
eightfold
path
is
a
method
of
policy
analysis
assembled
by
eugene
bardach
a
professor
at
the
goldman
school
of
public
policy
at
the
university
of
california
berkeley
it
is
outlined
in
his
book
"a
practical
guide
for
policy
analysis:
the
eightfold
path
to
more
effective
problem
solving"
which
is
now
in
its
fourth
edition
the
book
is
commonly
referenced
in
public
policy
and
public
administration
scholarship
bardach's
procedure
is
as
follows:
a
possible
ninth
step
based
on
bardach's
own
writing
might
be
"repeat
steps
1
-
8
as
necessary"
the
new
york
taxi
driver
test
is
a
technique
for
evaluating
the
effectiveness
of
communication
between
policy
makers
and
analysts
bardach
contends
that
policy
explanations
must
be
clear
and
down-to-earth
enough
for
a
taxi
driver
to
be
able
to
understand
the
premise
during
a
trip
through
city
streets
the
new
york
taxi
driver
is
presumed
to
be
both
a
non-specialist
and
a
tough
customer
multifunctionality
in
agriculture
multifunctionality
in
agriculture
(often
simply
"multifunctionality")
refers
to
the
numerous
benefits
that
agricultural
policies
may
provide
for
a
country
or
region
generally
speaking
multifunctionality
refers
to
the
"non-trade"
benefits
of
agriculture
that
is
benefits
other
than
commerce
and
food
production
these
include
in
the
wto
definition
of
multifunctionality
environmental
protection
landscape
preservation
rural
employment
and
food
security
these
can
be
broadly
classified
as
benefits
to
society
culture
a
national
economy
as
a
whole
national
security
and
other
concerns
for
example
in
addition
to
providing
food
and
plant-derived
products
for
the
population
agriculture
may
also
provide
jobs
for
rural
people
and
contribute
to
the
viability
of
the
area
create
a
more
stable
food
supply
and
provide
other
desired
environmental
and
rural
outputs
the
numerous
externalities
both
positive
and
negative
which
are
associated
with
agriculture
are
important
considerations
for
policy
makers
sometimes
current
agricultural
practices
and
markets
produce
too
much
of
an
undesired
effect
or
not
enough
of
a
desired
one
governments
may
step
in
to
correct
such
market
failures
with
policies
designed
to
either
encourage
or
discourage
a
certain
practice
however
individual
policies
may
carry
consequences
for
other
policies
and
for
other
countries
such
policies
are
therefore
a
major
topic
of
discussion
in
the
international
community
removing
protectionist
policies
on
agriculture
is
one
step
that
may
need
to
be
taken
for
a
country
to
maximize
positive
externalities
minimize
negative
ones
and
make
sure
that
the
mixture
of
outputs
derived
from
agriculture
corresponds
to
the
needs
of
society
however
removing
agricultural
supports
is
often
cause
for
consternation
among
public
officials
who
may
predict
the
loss
of
certain
positive
externalities
of
the
policies
already
in
place
at
the
same
time
officials
may
fear
the
implementation
of
new
market
protections
in
other
countries
which
are
trying
to
promote
the
production
of
such
outputs
of
agriculture
in
such
cases
advocates
for
free
trade
such
as
oecd
recommend
that
countries
reduce
as
much
as
possible
their
agricultural
protections
and
institute
policies
which
specifically
target
the
production
of
the
positive
non-commodity
outputs
to
help
countries
formulate
their
agricultural
policies
oecd
has
established
a
framework
for
analyzing
non-commodity
outputs
of
agricultural
activities
when
analyzing
the
multifunctionality
of
agriculture
and
the
appropriate
policies
to
implement
there
are
several
concepts
that
need
to
be
considered
the
first
of
these
is
jointness
or
the
extent
to
which
the
intended
agricultural
product
and
the
incidental
non-commodity
outputs
of
agricultural
activity
are
linked
the
production
of
some
non-commodity
outputs
may
be
inseparable
from
agricultural
commodity
outputs
while
others
may
be
produced
independently
of
agricultural
activity
the
goal
is
to
separate
agricultural
commodities
and
non-commodity
outputs
as
much
as
possible
the
next
issue
to
be
addressed
is
whether
or
not
the
production
or
non-production
of
the
non-commodity
output
in
question
constitutes
a
market
failure
if
there
is
no
market
failure
there
is
no
need
for
a
policy
to
correct
it
finally
policy
makers
should
examine
the
characteristics
of
the
output
in
question
since
it
may
have
both
a
degree
of
market
failure
and
jointness
associated
with
it
after
considering
the
matter
from
these
three
perspectives
policy
makers
may
find
non-governmental
ways
of
addressing
dealing
with
non-commodity
outputs
or
make
changes
in
their
agricultural
policies
in
agricultural
trade
discussions
in
the
wto
the
eu
and
japan
among
others
argue
that
multifunctionality
justifies
continued
protection
and
subsidization
of
agriculture
the
united
states
and
the
cairns
group
argue
that
support
of
multifunctionality
should
be
specific
targeted
and
provided
in
a
non-trade
distorting
manner
courtesy
resolution
courtesy
resolution
is
a
non-controversial
resolution
in
the
nature
of
congratulations
on
the
birth
of
a
child
celebration
of
a
wedding
anniversary
congratulations
of
an
outstanding
citizen
achievement
or
a
similar
event
it
is
"a
resolution
expressing
thanks
for
assistance
or
commending
meritorious
accomplishments"
an
example
of
a
courtesy
resolution
is
the
resolution
at
the
end
of
the
political
convention
thanking
everyone
for
their
time
for
a
courtesy
resolution
only
the
affirmative
vote
is
taken
and
this
is
usually
a
voice
vote
association
for
public
policy
analysis
and
management
the
association
for
public
policy
analysis
and
management
(appam)
is
an
american
organization
whose
focus
is
improving
public
policy
and
management
by
fostering
excellence
in
research
analysis
and
education
appam
founded
the
"journal
of
policy
analysis
and
management"
("jpam")
in
1981
the
president
is
brendan
sullivan
health
administration
health
administration
or
healthcare
administration
is
the
field
relating
to
leadership
management
and
administration
of
public
health
systems
health
care
systems
hospitals
and
hospital
networks
health
systems
management
or
health
care
systems
management
describes
the
leadership
and
general
management
of
hospitals
hospital
networks
and/or
health
care
systems
in
international
use
the
term
refers
to
management
at
all
levels
in
the
united
states
management
of
a
single
institution
(eg
a
hospital)
is
also
referred
to
as
"medical
and
health
services
management"
"healthcare
management"
or
"health
administration"
health
systems
management
ensures
that
specific
outcomes
are
attained
that
departments
within
a
health
facility
are
running
smoothly
that
the
right
people
are
in
the
right
jobs
that
people
know
what
is
expected
of
them
that
resources
are
used
efficiently
and
that
all
departments
are
working
towards
a
common
goal
hospital
administrators
are
individuals
or
groups
of
people
who
act
as
the
central
point
of
control
within
hospitals
these
individuals
may
be
previous
or
current
clinicians
or
individuals
with
other
backgrounds
there
are
two
types
of
administrators
generalists
and
specialists
generalists
are
individuals
who
are
responsible
for
managing
or
helping
to
manage
an
entire
facility
specialists
are
individuals
who
are
responsible
for
the
efficient
operations
of
a
specific
department
such
as
policy
analysis
finance
accounting
budgeting
human
resources
or
marketing
it
was
reported
in
september
2014
that
the
united
states
spends
roughly
$218
billion
per
year
on
hospital's
administration
costs
which
is
equivalent
to
143
percent
of
the
total
us
economy
hospital
administration
has
grown
as
a
percent
of
the
us
economy
from
9
percent
in
2000
to
143
percent
in
2012
according
to
"health
affairs"
in
11
different
countries
hospitals
allocate
approximately
12
percent
of
their
budget
toward
administrative
costs
in
the
united
states
hospitals
spend
25
percent
on
administrative
costs
nchl
competencies
that
require
to
engage
with
credibility
creativity
and
motivation
in
complex
and
dynamic
health
care
environments
health
care
management
is
usually
studied
through
healthcare
administration
or
healthcare
management
programs
in
a
business
school
or
in
some
institutions
in
a
school
of
public
health
although
many
colleges
and
universities
are
offering
a
bachelor's
degree
in
healthcare
administration
or
human
resources
a
master's
degree
is
considered
the
"standard
credential"
for
most
health
administrators
in
the
united
states
research
and
academic-based
doctorate
level
degrees
such
as
the
doctor
of
philosophy
(phd)
in
health
administration
and
the
doctor
of
health
administration
(dha)
degree
prepare
health
care
professionals
to
turn
their
clinical
or
administrative
experiences
into
opportunities
to
develop
new
knowledge
and
practice
teach
shape
public
policy
and/or
lead
complex
organizations
there
are
multiple
recognized
degree
types
that
are
considered
equivalent
from
the
perspective
of
professional
preparation
the
commission
on
the
accreditation
of
healthcare
management
education
(cahme)
is
the
accrediting
body
overseeing
master's-level
programs
in
the
united
states
and
canada
on
behalf
of
the
united
states
department
of
education
it
accredits
several
degree
program
types
including
master
of
hospital
administration
(mha)
master
of
health
services
administration
(mhsa)
master
of
business
administration
in
hospital
management
(mba-hm)
master
of
health
administration
(mha)
master
of
public
health
(mph
msph
mshpm)
master
of
science
(ms-hsm
ms-ha)
and
master
of
public
administration
(mpa)
health
care
management
study
is
a
new
discipline
in
nepal
pokhara
university
offers
a
hospital
management
course
national
open
college
launched
a
four-year
bachelor's
level
(bhcm)
course
in
september
2000
with
an
enrolment
of
40
students
and
the
next
year
it
also
started
a
one-year
postgraduate
diploma
(pgdhcm)
and
a
two-year
master's
course
(mhcm)
in
health
care
management
nobel
college
at
sinamangal
has
also
been
offering
a
bachelor’s
level
(bhcm)
course
since
2006
md
hospital
administration
(mdha)
and
master
in
hospital
management
(mhm)
are
being
started
from
2013
it
is
uncertain
how
many
citizens
of
nepal
are
gaining
healthcare
management
qualifications
in
other
countries
there
is
an
absence
of
professional
organization
and
regulation
in
the
health
care
management
profession
in
nepal
there
are
a
variety
of
different
professional
associations
related
to
health
systems
management
which
can
be
subcategorized
as
either
personal
or
institutional
membership
groups
personal
membership
groups
are
joined
by
individuals
and
typically
have
individual
skills
and
career
development
as
their
focus
larger
personal
membership
groups
include
the
healthcare
financial
management
association
and
the
healthcare
information
and
management
systems
society
institutional
membership
groups
are
joined
by
organizations;
whereas
they
typically
focus
on
organizational
effectiveness
and
may
also
include
data-sharing
agreements
and
other
medical
related
or
administrative
practice
sharing
vehicles
for
member
organizations
prominent
examples
include
the
american
hospital
association
and
the
university
healthsystems
consortium
early
hospital
administrators
were
called
patient
directors
or
superintendents
at
the
time
many
were
nurses
who
had
taken
on
administrative
responsibilities
over
half
of
the
members
of
the
american
hospital
association
were
graduate
nurses
in
1916
other
superintendents
were
medical
doctors
laymen
and
members
of
the
clergy
in
the
united
states
the
first
degree
granting
program
in
the
united
states
was
established
at
marquette
university
in
milwaukee
wisconsin
by
1927
the
first
two
students
received
their
degrees
the
original
idea
is
credited
to
father
moulinier
associated
with
the
catholic
hospital
association
the
first
modern
health
systems
management
program
was
established
in
1934
at
the
university
of
chicago
at
the
time
programs
were
completed
in
two
years
–
one
year
of
formal
graduate
study
and
one
year
of
practicing
internship
in
1958
the
sloan
program
at
cornell
university
began
offering
a
special
program
requiring
two
years
of
formal
study
which
remains
the
dominant
structure
in
the
united
states
and
canada
today
(see
also
"academic
preparation")
health
systems
management
has
been
described
as
a
"hidden"
health
profession
because
of
the
relatively
low-profile
role
managers
take
in
health
systems
in
comparison
to
direct-care
professions
such
as
nursing
and
medicine
however
the
visibility
of
the
management
profession
within
healthcare
has
been
rising
in
recent
years
due
largely
to
the
widespread
problems
developed
countries
are
having
in
balancing
cost
access
and
quality
in
their
hospitals
and
health
systems
overton
window
the
overton
window
is
the
range
of
ideas
tolerated
in
public
discourse
also
known
as
the
window
of
discourse
the
term
is
named
after
political
scientist
joseph
p
overton
who
claimed
that
an
idea's
political
viability
depends
mainly
on
whether
it
falls
within
a
range
acceptable
to
the
public
rather
than
on
politicians'
individual
preferences
according
to
overton
the
window
contains
the
range
of
policies
that
a
politician
can
recommend
without
appearing
too
extreme
to
gain
or
keep
public
office
in
the
current
climate
of
public
opinion
overton
described
a
spectrum
from
"more
free"
to
"less
free"
with
regard
to
government
intervention
oriented
vertically
on
an
axis
to
avoid
comparison
with
the
left-right
political
spectrum
as
the
spectrum
moves
or
expands
an
idea
at
a
given
location
may
become
more
or
less
politically
acceptable
political
commentator
joshua
treviño
postulated
that
the
degrees
of
acceptance
of
public
ideas
are
roughly:
the
overton
window
is
an
approach
to
identifying
which
ideas
define
the
domain
of
acceptability
within
a
democracy's
possible
governmental
policies
proponents
of
policies
outside
the
window
seek
to
convince
or
persuade
the
public
in
order
to
move
and/or
expand
the
window
proponents
of
current
policies
or
similar
ones
within
the
window
seek
to
convince
people
that
policies
outside
it
should
be
deemed
unacceptable
after
overton's
death
others
have
examined
the
concept
of
adjusting
the
window
by
the
deliberate
promotion
of
ideas
outside
of
it
or
"outer
fringe"
ideas
with
the
intention
of
making
less
fringe
ideas
acceptable
by
comparison
the
"door-in-the-face"
technique
of
persuasion
is
similar
the
idea
echoes
several
earlier
expressions
the
most
recent
and
similarly
academic
being
hallin's
spheres
in
his
1986
book
"the
uncensored
war"
communication
scholar
daniel
c
hallin
posits
three
areas
of
media
coverage
into
which
a
topic
may
fall
the
areas
are
diagrammed
as
concentric
circles
called
spheres
from
innermost
to
outermost
they
are
the
sphere
of
consensus
the
sphere
of
legitimate
controversy
and
the
sphere
of
deviance
proposals
and
positions
can
be
placed
at
varying
degrees
of
distance
from
the
metaphorical
center
and
political
actors
can
fight
over
and
help
change
these
positions
hallin's
theory
is
developed
and
applied
primarily
as
a
theory
that
explains
varying
levels
of
objectivity
in
media
coverage
but
it
also
accounts
for
the
ongoing
contest
among
media
and
other
political
actors
about
what
counts
as
legitimate
disagreement
potentially
leading
to
changes
in
the
boundaries
between
spheres
as
one
study
that
applies
hallin's
theory
explains
"the
borders
between
the
three
spheres
are
dynamic
depending
on
the
political
climate
and
on
the
editorial
line
of
the
various
media
outlets"
in
this
way
the
idea
also
captures
the
tug-of-war
over
the
boundaries
between
normal
and
deviant
political
discourse
an
idea
similar
to
the
overton
window
was
expressed
by
anthony
trollope
in
1868
in
his
novel
"phineas
finn":
in
his
"west
india
emancipation"
speech
at
canandaigua
new
york
in
1857
abolitionist
leader
frederick
douglass
described
how
public
opinion
limits
the
ability
of
those
in
power
to
act
with
impunity:
culture
change
culture
change
is
a
term
used
in
public
policy
making
that
emphasizes
the
influence
of
cultural
capital
on
individual
and
community
behavior
it
has
been
sometimes
called
repositioning
of
culture
which
means
the
reconstruction
of
the
cultural
concept
of
a
society
it
places
stress
on
the
social
and
cultural
capital
determinants
of
decision
making
and
the
manner
in
which
these
interact
with
other
factors
like
the
availability
of
information
or
the
financial
incentives
facing
individuals
to
drive
behavior
these
cultural
capital
influences
include
the
role
of
parenting
families
and
close
associates;
organizations
such
as
schools
and
workplaces;
communities
and
neighborhoods;
and
wider
social
influences
such
as
the
media
it
is
argued
that
this
cultural
capital
manifests
into
specific
values
attitudes
or
social
norms
which
in
turn
guide
the
behavioral
"intentions"
that
individuals
adopt
in
regard
to
particular
decisions
or
courses
of
action
these
behavioral
intentions
interact
with
other
factors
driving
behavior
such
as
financial
incentives
regulation
and
legislation
or
levels
of
information
to
drive
actual
behavior
and
ultimately
feed
back
into
underlying
cultural
capital
in
general
cultural
stereotypes
present
great
resistance
to
change
and
to
their
own
redefinition
culture
often
appears
fixed
to
the
observer
at
any
one
point
in
time
because
cultural
mutations
occur
incrementally
cultural
change
is
a
long-term
process
policymakers
need
to
make
a
great
effort
to
improve
some
basics
aspects
of
a
society’s
cultural
traits
the
term
is
used
by
knott
et
al
of
the
prime
minister's
strategy
unit
in
the
publication:
"achieving
culture
change:
a
policy
framework"
(knott
et
al
2008)
the
paper
sets
out
how
public
policy
can
achieve
social
and
cultural
change
through
'downstream'
interventions
including
fiscal
incentives
legislation
regulation
and
information
provision
and
also
'upstream'
interventions
such
as
parenting
peer
and
mentoring
programs
or
development
of
social
and
community
networks
the
key
concepts
the
paper
is
based
on
include:
knott
et
al
use
examples
from
a
range
of
policy
areas
to
demonstrate
how
the
culture
change
framework
can
be
applied
to
policymaking
for
example:
security
policy
security
policy
is
a
definition
of
what
it
means
to
"be
secure"
for
a
system
organization
or
other
entity
for
an
organization
it
addresses
the
constraints
on
behavior
of
its
members
as
well
as
constraints
imposed
on
adversaries
by
mechanisms
such
as
doors
locks
keys
and
walls
for
systems
the
security
policy
addresses
constraints
on
functions
and
flow
among
them
constraints
on
access
by
external
systems
and
adversaries
including
programs
and
access
to
data
by
people
if
it
is
important
to
be
secure
then
it
is
important
to
be
sure
all
of
the
security
policy
is
enforced
by
mechanisms
that
are
strong
enough
there
are
many
organized
methodologies
and
risk
assessment
strategies
to
assure
completeness
of
security
policies
and
assure
that
they
are
completely
enforced
in
complex
systems
such
as
information
systems
policies
can
be
decomposed
into
sub-policies
to
facilitate
the
allocation
of
security
mechanisms
to
enforce
sub-policies
however
this
practice
has
pitfalls
it
is
too
easy
to
simply
go
directly
to
the
sub-policies
which
are
essentially
the
rules
of
operation
and
dispense
with
the
top
level
policy
that
gives
the
false
sense
that
the
rules
of
operation
address
some
overall
definition
of
security
when
they
do
not
because
it
is
so
difficult
to
think
clearly
with
completeness
about
security
rules
of
operation
stated
as
"sub-policies"
with
no
"super-policy"
usually
turn
out
to
be
rambling
rules
that
fail
to
enforce
anything
with
completeness
consequently
a
top-level
security
policy
is
essential
to
any
serious
security
scheme
and
sub-policies
and
rules
of
operation
are
meaningless
without
it
science
policy
science
policy
is
concerned
with
the
allocation
of
resources
for
the
conduct
of
science
towards
the
goal
of
best
serving
the
public
interest
topics
include
the
funding
of
science
the
careers
of
scientists
and
the
translation
of
scientific
discoveries
into
technological
innovation
to
promote
commercial
product
development
competitiveness
economic
growth
and
economic
development
science
policy
focuses
on
knowledge
production
and
role
of
knowledge
networks
collaborations
and
the
complex
distributions
of
expertise
equipment
and
know-how
understanding
the
processes
and
organizational
context
of
generating
novel
and
innovative
science
and
engineering
ideas
is
a
core
concern
of
science
policy
science
policy
topics
include
weapons
development
health
care
and
environmental
monitoring
science
policy
thus
deals
with
the
entire
domain
of
issues
that
involve
science
a
large
and
complex
web
of
factors
influences
the
development
of
science
and
engineering
that
includes
government
science
policy
makers
private
firms
(including
both
national
and
multi-national
firms)
social
movements
media
non-governmental
organizations
universities
and
other
research
institutions
in
addition
science
policy
is
increasingly
international
as
defined
by
the
global
operations
of
firms
and
research
institutions
as
well
as
by
the
collaborative
networks
of
non-governmental
organizations
and
of
the
nature
of
scientific
inquiry
itself
state
policy
has
influenced
the
funding
of
public
works
and
science
for
thousands
of
years
dating
at
least
from
the
time
of
the
mohists
who
inspired
the
study
of
logic
during
the
period
of
the
hundred
schools
of
thought
and
the
study
of
defensive
fortifications
during
the
warring
states
period
in
china
general
levies
of
labor
and
grain
were
collected
to
fund
great
public
works
in
china
including
the
accumulation
of
grain
for
distribution
in
times
of
famine
for
the
building
of
levees
to
control
flooding
by
the
great
rivers
of
china
for
the
building
of
canals
and
locks
to
connect
rivers
of
china
some
of
which
flowed
in
opposite
directions
to
each
other
and
for
the
building
of
bridges
across
these
rivers
these
projects
required
a
civil
service
the
scholars
some
of
whom
demonstrated
great
mastery
of
hydraulics
in
italy
galileo
noted
that
individual
taxation
of
minute
amounts
could
fund
large
sums
to
the
state
which
could
then
fund
his
research
on
the
trajectory
of
cannonballs
noting
that
"each
individual
soldier
was
being
paid
from
coin
collected
by
a
general
tax
of
pennies
and
farthings
while
even
a
million
of
gold
would
not
suffice
to
pay
the
entire
army"
in
great
britain
lord
chancellor
sir
francis
bacon
had
a
formative
effect
on
science
policy
with
his
identification
of
"experiments
of
light
more
penetrating
into
nature
[than
what
others
know]"
which
today
we
call
the
crucial
experiment
governmental
approval
of
the
royal
society
recognized
a
scientific
community
which
exists
to
this
day
british
prizes
for
research
spurred
the
development
of
an
accurate
portable
chronometer
which
directly
enabled
reliable
navigation
and
sailing
on
the
high
seas
and
also
funded
babbage's
computer
the
professionalization
of
science
begun
in
the
nineteenth
century
was
partly
enabled
by
the
creation
of
scientific
organizations
such
as
the
national
academy
of
sciences
the
kaiser
wilhelm
institute
and
state
funding
of
universities
of
their
respective
nations
in
the
united
states
a
member
of
the
national
academy
of
sciences
can
sponsor
a
direct
submission
for
publication
in
the
"proceedings
of
the
national
academy
of
sciences"
"pnas"
serves
as
a
channel
to
recognize
research
of
importance
to
at
least
one
member
of
the
national
academy
of
sciences
public
policy
can
directly
affect
the
funding
of
capital
equipment
intellectual
infrastructure
for
industrial
research
by
providing
tax
incentives
to
those
organizations
who
fund
research
vannevar
bush
director
of
the
office
of
scientific
research
and
development
for
the
us
government
in
july
1945
wrote
"science
is
a
proper
concern
of
government"
vannevar
bush
directed
the
forerunner
of
the
national
science
foundation
and
his
writings
directly
inspired
researchers
to
invent
the
hyperlink
and
the
computer
mouse
the
darpa
initiative
to
support
computing
was
the
impetus
for
the
internet
protocol
stack
in
the
same
way
that
scientific
consortiums
like
cern
for
high-energy
physics
have
a
commitment
to
public
knowledge
access
to
this
public
knowledge
in
physics
led
directly
to
cern's
sponsorship
of
development
of
the
world
wide
web
and
standard
internet
access
for
all
the
programs
that
are
funded
are
often
divided
into
four
basic
categories:
basic
research
applied
research
development
and
facilities
and
equipment
translational
research
is
a
newer
concept
that
seeks
to
bridge
the
gap
between
basic
science
and
practical
applications
basic
science
attempts
to
stimulate
breakthroughs
breakthroughs
often
lead
to
an
explosion
of
new
technologies
and
approaches
once
the
basic
result
is
developed
it
is
widely
published;
however
conversion
into
a
practical
product
is
left
for
the
free
market
however
many
governments
have
developed
risk-taking
research
and
development
organizations
to
take
basic
theoretical
research
over
the
edge
into
practical
engineering
in
the
us
this
function
is
performed
by
darpa
on
the
other
hand
technology
development
is
a
policy
in
which
engineering
the
application
of
science
is
supported
rather
than
basic
science
the
emphasis
is
usually
given
to
projects
that
increase
important
strategic
or
commercial
engineering
knowledge
the
most
extreme
success
story
is
doubtless
the
manhattan
project
that
developed
nuclear
weapons
another
remarkable
success
story
was
the
"x-vehicle"
studies
that
gave
the
us
a
lasting
lead
in
aerospace
technologies
these
exemplify
two
disparate
approaches:
the
manhattan
project
was
huge
and
spent
unblinkingly
on
the
most
risky
alternative
approaches
the
project
members
believed
that
failure
would
result
in
their
enslavement
or
destruction
by
nazi
germany
each
x-project
built
an
aircraft
whose
only
purpose
was
to
develop
a
particular
technology
the
plan
was
to
build
a
few
cheap
aircraft
of
each
type
fly
a
test
series
often
to
the
destruction
of
an
aircraft
and
never
design
an
aircraft
for
a
practical
mission
the
only
mission
was
technology
development
a
number
of
high-profile
technology
developments
have
failed
the
us
space
shuttle
failed
to
meet
its
cost
or
flight
schedule
goals
most
observers
explain
the
project
as
over
constrained:
the
cost
goals
too
aggressive
the
technology
and
mission
too
underpowered
and
undefined
the
japanese
fifth
generation
computer
systems
project
met
every
technological
goal
but
failed
to
produce
commercially
important
artificial
intelligence
many
observers
believe
that
the
japanese
tried
to
force
engineering
beyond
available
science
by
brute
investment
half
the
amount
spent
on
basic
research
rather
might
have
produced
ten
times
the
result
utilitarian
policies
prioritize
scientific
projects
that
significantly
reduce
suffering
for
larger
numbers
of
people
this
approach
would
mainly
consider
the
numbers
of
people
that
can
be
helped
by
a
research
policy
research
is
more
likely
to
be
supported
when
it
costs
less
and
has
greater
benefits
utilitarian
research
often
pursues
incremental
improvements
rather
than
dramatic
advancements
in
knowledge
or
break-through
solutions
which
are
more
commercially
viable/feasible
in
contrast
monumental
science
is
a
policy
in
which
science
is
supported
for
the
sake
of
a
greater
understanding
of
the
universe
rather
than
for
specific
short-term
practical
goals
this
designation
covers
both
large
projects
often
with
large
facilities
and
smaller
research
that
does
not
have
obvious
practical
applications
and
are
often
overlooked
while
these
projects
may
not
always
have
obvious
practical
outcomes
they
provide
education
of
future
scientists
and
advancement
of
scientific
knowledge
of
lasting
worth
about
the
basic
building
blocks
of
science
practical
outcomes
do
result
from
many
of
these
"monumental"
science
programs
sometimes
these
practical
outcomes
are
foreseeable
and
sometimes
they
are
not
a
classic
example
of
a
monumental
science
program
focused
towards
a
practical
outcome
is
the
manhattan
project
an
example
of
a
monumental
science
program
that
produces
unexpected
practical
outcome
is
the
laser
coherent
light
the
principle
behind
lasing
was
first
predicted
by
einstein
in
1916
but
not
created
until
1954
by
charles
h
townes
with
the
maser
the
breakthrough
with
the
maser
led
to
the
creation
of
the
laser
in
1960
by
theodore
maiman
the
delay
between
the
theory
of
coherent
light
and
the
production
of
the
laser
was
partially
due
to
the
assumption
that
it
would
be
of
no
practical
use
this
policy
approach
prioritizes
efficiently
teaching
all
available
science
to
those
who
can
use
it
rather
than
investing
in
new
science
in
particular
the
goal
is
not
to
"lose"
any
existing
knowledge
and
to
find
new
practical
ways
to
apply
the
available
knowledge
the
classic
success
stories
of
this
method
occurred
in
the
19th
century
us
land-grant
universities
which
established
a
strong
tradition
of
research
in
practical
agricultural
and
engineering
methods
more
recently
the
green
revolution
prevented
mass
famine
over
the
last
thirty
years
the
focus
unsurprisingly
is
usually
on
developing
a
robust
curriculum
and
inexpensive
practical
methods
to
meet
local
needs
most
developed
countries
usually
have
a
specific
national
body
overseeing
national
science
(including
technology
and
innovation)
policy
in
the
case
developing
countries
many
follow
the
same
fashion
many
governments
of
developed
countries
provide
considerable
funds
(primarily
to
universities)
for
scientific
research
(in
fields
such
as
physics
and
geology)
as
well
as
social
science
research
(in
fields
such
as
economics
and
history)
much
of
this
is
not
intended
to
provide
concrete
results
that
may
be
commercialisable
although
research
in
scientific
fields
may
lead
to
results
that
have
such
potential
most
university
research
is
aimed
at
gaining
publication
in
peer
reviewed
academic
journals
a
funding
body
is
an
organisation
that
provides
research
funding
in
the
form
of
research
grants
or
scholarships
research
councils
are
the
funding
bodies
that
are
government-funded
agencies
engaged
in
the
support
of
research
in
different
disciplines
and
postgraduate
funding
funding
from
research
councils
is
typically
competitive
as
a
general
rule
more
funding
is
available
in
science
and
engineering
disciplines
than
in
the
arts
and
social
sciences
in
australia
the
two
main
research
councils
are
the
australian
research
council
and
the
national
health
and
medical
research
council
in
canada
the
three
main
research
councils
("tri-council")
are
the
social
sciences
and
humanities
research
council
(sshrc)
the
natural
sciences
and
engineering
research
council
(nserc)
and
the
canadian
institutes
of
health
research
(cihr)
additional
research
funding
agencies
include
the
canada
foundation
for
innovation
genome
canada
sustainable
development
technology
canada
and
several
tri-council
supported
networks
of
centres
of
excellence
in
brazil
two
important
research
agencies
are
the
national
council
for
scientific
and
technological
development
(cnpq
portuguese:
conselho
nacional
de
desenvolvimento
científico
e
tecnológico)
an
organization
of
the
brazilian
federal
government
under
the
ministry
of
science
and
technology
and
são
paulo
research
foundation
(fapesp
portuguese:
fundação
de
amparo
à
pesquisa
do
estado
de
são
paulo)
a
public
foundation
located
in
the
state
of
são
paulo
brazil
the
science
policy
of
the
european
union
is
carried
out
through
the
european
research
area
which
is
a
system
that
integrates
the
scientific
resources
of
member
nations
and
acts
as
a
"common
market"
for
research
and
innovation
purpose
the
european
union's
executive
body
the
european
commission
has
a
directorate-general
for
research;
which
is
responsible
for
the
union's
science
policy
in
addition
the
joint
research
centre
provides
independent
scientific
and
technical
advice
to
the
european
commission
and
member
states
of
the
european
union
(eu)
in
support
of
eu
policies
there
is
also
the
recently
established
european
research
council
the
first
european
union
funding
body
set
up
to
support
investigator-driven
research
there
are
also
european
science
agencies
that
operate
independently
of
the
european
union
such
as
the
european
science
foundation
european
space
agency
and
the
european
higher
education
area;
which
are
created
by
the
bologna
process
on
science
policy
and
on
the
european
research
area
is
grounded
the
european
environmental
research
and
innovation
policy
which
addresses
global
challenges
of
pivotal
importance
for
the
well-being
of
the
european
citizens
within
the
context
of
sustainable
development
and
environmental
protection
research
and
innovation
in
europe
is
financially
supported
by
the
programme
horizon
2020
which
is
also
open
to
participation
worldwide
german
research
funding
agencies
include
the
deutsche
forschungsgemeinschaft
which
covers
both
science
and
humanities
research
funding
by
the
government
of
india
comes
from
a
number
of
sources
for
basic
science
and
technology
research
these
include
the
council
for
scientific
and
industrial
research
(csir)
department
of
science
and
technology
(dst)
and
university
grants
commission
(ugc)
for
medical
research
these
include
the
indian
council
for
medical
research
(icmr)
csir
dst
and
department
of
biotechnology
(dbt)
for
applied
research
these
include
the
csir
dbt
and
science
and
engineering
research
council
(serc)
other
funding
authorities
are
the
defence
research
development
organisation
(drdo)
the
indian
council
of
agricultural
research
(icar)
the
indian
space
research
organisation
(isro)
the
department
of
ocean
development
(dod)
the
indian
council
for
social
science
research
(icssr)
and
the
ministry
of
environment
and
forests
(mef)
etc
irish
funding
councils
include
the
irish
research
council
(irc)
and
the
science
foundation
ireland
the
prior
irish
research
council
for
science
engineering
and
technology
(ircset)
and
the
irish
research
council
for
the
humanities
and
social
sciences
(irchss)
were
merged
to
form
the
irc
in
march
2012
dutch
research
funding
agencies
include
nederlandse
organisatie
voor
wetenschappelijk
onderzoek
(nwo)
and
agentschap
nl
the
government
of
pakistan
has
mandated
that
a
certain
percentage
of
gross
revenue
generated
by
all
telecom
service
providers
be
allocated
to
development
and
research
of
information
and
communication
technologies
the
national
ict
rd
fund
was
established
in
january
2007
under
the
soviet
union
much
research
was
routinely
suppressed
now
science
in
russia
is
supported
by
state
and
private
funds
from
the
state:
the
russian
humanitarian
scientific
foundation
(http://wwwrfhru)
the
russian
foundation
for
basic
research
(wwwrfbrru)
the
russian
science
foundation
(http://rscfru)
swiss
research
funding
agencies
include
the
swiss
national
science
foundation
(snsf)
the
innovation
promotion
agency
cti
(cti/kti)
ressortforschung
des
bundes
and
eidgenössische
stiftungsaufsicht
in
the
united
kingdom
the
haldane
principle
that
decisions
about
what
to
spend
research
funds
on
should
be
made
by
researchers
rather
than
politicians
is
still
influential
in
research
policy
there
are
several
university
departments
with
a
focus
on
science
policy
such
as
the
science
policy
research
unit
there
are
seven
grant-awarding
research
councils:
the
united
states
has
a
long
history
of
government
support
specially
for
science
and
technology
science
policy
in
the
united
states
is
the
responsibility
of
many
organizations
throughout
the
federal
government
much
of
the
large-scale
policy
is
made
through
the
legislative
budget
process
of
enacting
the
yearly
federal
budget
further
decisions
are
made
by
the
various
federal
agencies
which
spend
the
funds
allocated
by
congress
either
on
in-house
research
or
by
granting
funds
to
outside
organizations
and
researchers
research
funding
agencies
in
the
united
states
are
spread
among
many
different
departments
which
include:
public
policy
of
the
united
states
public
policy
decisions
are
often
decided
by
a
group
of
individuals
with
different
beliefs
and
interests
the
policies
of
the
united
states
of
america
comprise
all
actions
taken
by
its
federal
government
the
executive
branch
is
the
primary
entity
through
which
policies
are
enacted
however
the
policies
are
derived
from
a
collection
of
laws
executive
decisions
and
legal
precedents
the
policies
of
the
united
states
the
almanac
of
policy
issues
which
provides
background
information
archived
documents
and
links
to
major
us
public
policy
issues
organized
the
public
policy
of
the
united
states
into
nine
categories
the
following
lists
these
categories
followed
by
a
few
examples
of
specific
respective
policies:
agricultural
policy
of
the
united
states
is
the
governing
policy
for
agriculture
in
the
united
states
and
is
composed
primarily
of
the
periodically
renewed
federal
us
farm
bills
in
"a
new
agricultural
policy
for
the
united
states"
authors
dennis
keeney
and
long
kemp
summarize
the
agricultural
policy
of
the
united
states
as
follows:
"because
of
its
unique
geography
weather
history
and
policies
the
united
states
has
an
agriculture
that
has
been
dominated
by
production
of
commodity
crops
for
use
in
animal
industrial
and
export
enterprises
over
time
agricultural
policies
evolved
to
support
an
industrialized
commodity-based
agriculture
this
evolution
resulted
in
farmers
leaving
the
land
with
agriculture
moving
to
an
industrial
structure"
in
parallel
with
the
industrialization
of
agriculture
in
the
united
states
the
federal
government
also
developed
the
dietary
guidelines
for
americans
which
emphasize
consumption
of
foods
that
are
produced
by
large-scale
farming
the
drug
policy
of
the
united
states
is
established
by
the
office
of
national
drug
control
policy
a
former
cabinet-level
component
of
the
executive
office
of
the
president
of
the
united
states
which
was
established
by
the
anti-drug
abuse
act
of
1988
its
stated
goal
is
to
establish
policies
priorities
and
objectives
to
eradicate
illicit
drug
use
drug
manufacturing
and
trafficking
drug-related
crime
and
violence
and
drug-related
health
consequences
in
the
us
the
office
of
national
drug
control
policy's
two
current
specific
goals
are
to
"curtail
illicit
drug
consumption
in
america"
and
to
"improve
the
public
health
and
public
safety
of
the
american
people
by
reducing
the
consequences
of
drug
abuse"
they
plan
to
achieve
these
goals
by
taking
the
following
actions:
the
energy
policy
of
the
united
states
addresses
issues
of
energy
production
distribution
and
consumption
such
as
building
codes
and
gas
mileage
standards
the
united
states
department
of
energy
plays
a
major
role
and
its
mission
is
"to
ensure
america's
security
and
prosperity
by
addressing
its
energy
environmental
and
nuclear
challenges
through
transformative
science
and
technology
solutions"
moreover
the
white
house
provides
a
summary
of
the
united
states'
current
condition
regarding
its
energy
policy:
"for
decades
it
has
been
clear
that
the
way
americans
produce
and
consume
energy
is
not
sustainable
our
addiction
to
foreign
oil
and
fossil
fuels
puts
our
economy
our
national
security
and
our
environment
at
risk
to
take
this
country
in
a
new
direction
the
president
is
working
with
congress
to
pass
comprehensive
energy
and
climate
legislation
to
protect
our
nation
from
the
serious
economic
and
strategic
risks
associated
with
our
reliance
on
foreign
oil
to
create
jobs
and
to
cut
down
on
the
carbon
pollution
that
contributes
to
the
destabilizing
effects
of
climate
change"
the
following
is
a
snapshot
of
the
united
states'
current
energy
policy
goals:
the
environmental
policy
of
the
united
states
addresses
and
regulates
activities
that
impact
the
environment
its
general
goal
is
to
protect
the
environment
for
the
welfare
of
future
generations
the
environmental
policy
goals
are
detailed
below:
the
foreign
policy
of
the
united
states
defines
how
the
united
states
interacts
with
foreign
nations
it
only
addresses
the
security
of
the
american
people
and
promotes
international
order
the
following
are
the
most
prominent
foreign
policies
of
the
united
states:
the
federal
reserve
treasury
and
securities
and
exchange
commission
took
several
steps
on
september
19
to
intervene
in
the
crisis
to
stop
the
potential
run
on
money
market
mutual
funds
the
treasury
also
announced
on
september
19
a
new
$50
billion
program
to
insure
the
investments
similar
to
the
federal
deposit
insurance
corporation
(fdic)
program
part
of
the
announcements
included
temporary
exceptions
to
section
23a
and
23b
(regulation
w)
allowing
financial
groups
to
more
easily
share
funds
within
their
group
the
exceptions
would
expire
on
january
30
2009
unless
extended
by
the
federal
reserve
board
the
securities
and
exchange
commission
announced
termination
of
short-selling
of
799
financial
stocks
as
well
as
action
against
naked
short
selling
as
part
of
its
reaction
to
the
mortgage
crisis
policy
alienation
policy
alienation
refers
to
a
framework
which
examines
the
experiences
of
governmental
employees
with
new
policies
they
have
to
implement
it
has
been
used
to
describe
the
experiences
of
front-line
public
professionals
with
new
policies
it
is
defined
"as
a
general
cognitive
state
of
psychological
disconnection
from
the
policy
programme
being
implemented"
a
number
of
examples
can
clarify
the
concept
of
policy
alienation
for
example
bottery
(1998:40)
examining
the
pressures
on
professionals
stemming
from
new
policies
in
education
and
health
care
in
great
britain
cites
a
teacher
arguing
that:
“the
changes
have
been
outrageous
and
have
produced
a
culture
of
meritocracy
and
high
flyers
there’s
massive
paperwork
because
the
politicians
don’t
believe
teachers
are
to
be
trusted”
this
indicates
that
professionals
had
difficulties
identifying
with
the
policies
they
had
to
implement
a
second
example
refers
to
the
introduction
of
a
new
reimbursement
policy
in
mental
healthcare
in
the
netherlands
in
one
large-scale
survey
as
many
as
nine
out
of
ten
professionals
wanted
to
abandon
this
new
policy
(palm
et
al
2008)
psychologists
even
went
as
far
as
to
openly
demonstrate
on
the
street
against
this
policy
a
major
reason
for
this
was
that
many
could
not
align
their
professional
values
with
the
content
of
the
policy
as
one
professional
noted:
""within
the
new
healthcare
system
economic
values
are
dominant
too
little
attention
is
being
paid
to
the
content:
professionals
helping
patients
the
result
is
that
professionals
become
more
aware
of
the
costs
and
revenues
of
their
behavior
this
comes
at
the
expense
of
acting
according
to
professional
standards”"
overall
a
number
of
studies
show
an
increasing
discontent
among
public
professionals
toward
public
policies
(see
also
hebson
et
al
2003;
white
1996)
although
more
positive
experiences
can
also
be
found
(ruiter
2007)
the
policy
alienation
framework
was
developed
to
better
understand
the
experiences
of
front-line
public
professionals
with
new
policies
currently
there
is
an
intense
debate
concerning
professionals
in
the
public
sector
many
of
the
pressures
that
professionals
face
are
related
to
the
difficulties
they
have
with
the
policies
they
have
to
implement
when
implementers
are
unable
to
identify
with
a
policy
this
can
negatively
influence
policy
effectiveness
furthermore
a
high
degree
of
policy
alienation
can
affect
the
quality
of
interactions
between
professionals
and
citizens
which
may
eventually
influence
the
output
legitimacy
of
government
the
policy
alienation
framework
is
used
to
analyze
this
topic
it
has
been
shown
that
policy
alienation
increases
resistance
to
a
new
policy
lowers
behavioral
support
for
the
policy
and
decreases
job
satisfaction
of
public
professionals
hence
it
has
both
influences
on
the
individual
professional
as
well
as
on
policy
effectiveness
alienation
broadly
refers
to
a
sense
of
social
estrangement
an
absence
of
social
support
or
meaningful
social
connection
sociologists
public
administration
scholars
and
other
social
scientists
have
used
the
alienation
concept
in
various
studies
as
a
result
a
number
of
meanings
have
been
attributed
to
the
term
in
an
attempt
to
provide
clarity
seeman
broke
these
meanings
down
into
five
alienation
dimensions:
powerlessness
meaninglessness
normlessness
social
isolation
and
self-estrangement
many
scholars
have
used
these
dimensions
to
devise
operational
measures
for
alienation
so
that
they
can
examine
the
concept
in
a
range
of
settings
mau
for
example
used
four
dimensions
in
examining
student
alienation
rayce
et
al
when
investigating
adolescent
alienation
used
three
of
the
five
dimensions
further
many
other
researchers
have
used
seeman’s
classification
in
examining
the
concept
of
work
alienation
blauner
devised
operational
measures
for
three
of
the
dimensions:
powerlessness
meaninglessness
and
social
isolation
the
policy
alienation
framework
was
conceptualized
based
on
the
works
of
sociologists
such
as
hegel
marx
seeman
and
blauner
furthermore
works
of
public
administration
scholars
were
used
particularly
on
lipsky
(street-level
bureaucracy)
like
work
alienation
policy
alienation
is
multidimensional
consisting
of
policy
powerlessness
and
policy
meaninglessness
dimensions
in
the
work
alienation
literature
the
dimensions
of
powerlessness
and
meaninglessness
are
also
considered
very
important
in
essence
powerlessness
is
a
person's
lack
of
control
over
events
in
their
life
in
the
realm
of
policy
formulation
and
implementation
policy
powerlessness
relates
to
the
degree
of
influence
public
professionals
have
over
shaping
a
policy
program
powerlessness
can
occur
when
a
new
policy
is
drafted
without
the
help
of
the
professionals
by
for
example
not
consulting
their
professionals
associations
or
labor
unions
furthermore
on
an
operational
level
professionals
can
feel
powerless
when
they
have
to
adhere
to
tight
procedures
and
rules
when
implementing
a
policy
(see
also
lipsky)
this
kind
of
powerlessness
may
be
particularly
pronounced
in
professionals
whose
expectations
of
discretion
and
autonomy
contradict
notions
of
bureaucratic
control
(see
also
profession)
the
second
dimension
of
policy
alienation
is
meaninglessness
in
the
realm
of
policy
making
and
implementation
policy
meaninglessness
refers
to
a
professional’s
perception
of
the
contribution
that
the
policy
makes
to
a
greater
purpose
most
notably
to
society
or
to
their
own
clients
for
instance
a
professional
can
feel
that
implementing
a
policy
is
meaningless
if
it
does
not
deliver
any
apparent
beneficial
outcomes
for
society
such
as
more
safety
on
the
streets
to
make
the
dimensions
more
specific
five
sub-dimensions
were
identified:
strategic
tactical
and
operational
powerlessness
societal
and
client
meaninglessness
this
is
shown
in
the
table
below
five
sub-dimensions
of
policy
alienation
nosokinetics
nosokinetics
is
the
science/subject
of
measuring
and
modelling
the
process
of
care
in
health
and
social
care
systems"
nosokinetics"
brings
together
the
greek
words
for
"noso":
disease
and
"kinetics":
movement
black
box
models
are
currently
used
to
plan
changes
in
health
and
social
care
systems
these
input-output
models
overlook
the
process
of
inpatient
care
as
a
result
suboptimal
decisions
are
made
nosokinetics
(analogous
to
pharmacokinetics)
seeks
to
develop
dynamic
methods
which
measure
and
model
the
process
of
inpatient
care
the
aim
is
to
develop
a
scientific
base
to
underpin
the
planning
of
sustainable
health
and
social
care
systems
nosokinetics
is
a
new
"science"
that
was
established
in
the
uk
in
the
early
1990s
by
prof
peter
h
millard
after
publishing
his
phd
thesis
in
2004
nosokinetics
group
newsletter
was
established
prof
peter
h
millard
writes
about
nosokinetics
:
"if
the
random
forces
of
wind
and
tide
can
make
such
a
beautiful
statue
(referring
to
an
iceberg)
how
much
better
could
mankind
do
if
a
new
science
was
developed
which
explains
the
complex
processes
of
health
and
social
care
until
new
methods
of
planning
health
and
social
care
services
to
meet
the
needs
of
an
ageing
population
are
introduced
service
delivery
will
stumble
on
from
crisis
to
crisis
the
world
population
is
ageing
and
sustainable
systems
of
health
care
need
to
be
developed"
he
has
established
the
nosokinetics
group
of
interested
researchers
the
group
collaborates
to
organize
conferences
and
disseminates
news
of
nosokinetics
and
other
researchers'
research
and
practical
use
of
modelling
to
enhance
decision
making
in
health
and
social
care
systems
the
nosokinetics
group
has
succeeded
in
attracting
a
lot
of
researchers
nosokinetics
interested
people
are
present
in
many
countries
including
australia
uk
egypt
they
are
from
different
disciplines
ranging
from
health
care
providers
to
management
scientists
the
news
related
to
nosokinetics
is
shared
to
the
network
through
the
bimonthly
newsletter
"nosokinetics
news"
which
helps
to
communicate
papers
conferences
and
events
of
interest
to
the
nosokinetics
network
policy
studies
policy
studies
is
a
subdisicipline
of
political
science
that
includes
the
analysis
of
the
process
of
policymaking
(the
policy
process)
and
the
contents
of
policy
(policy
analysis)
policy
analysis
includes
substantive
area
research
(such
as
health
or
education
policy)
program
evaluation
and
impact
studies
and
policy
design
it
"involves
systematically
studying
the
nature
causes
and
effects
of
alternative
public
policies
with
particular
emphasis
on
determining
the
policies
that
will
achieve
given
goals"
it
emerged
in
the
united
states
in
the
1960s
and
1970s
policy
studies
also
examines
the
conflicts
and
conflict
resolution
that
arise
from
the
making
of
policies
in
civil
society
the
private
sector
or
more
commonly
in
the
public
sector
(eg
government)
it
frequently
focuses
on
the
public
sector
but
is
equally
applicable
to
other
kinds
of
organizations
(eg
the
not-for-profit
sector)
some
policy
study
experts
graduate
from
public
policy
schools
with
public
policy
degrees
alternatively
experts
may
have
backgrounds
in
policy
analysis
program
evaluation
sociology
psychology
philosophy
economics
anthropology
geography
law
political
science
social
work
environmental
planning
and
public
administration
traditionally
the
field
of
policy
studies
focused
on
domestic
policy
with
the
notable
exceptions
of
foreign
and
defense
policies
however
the
wave
of
economic
globalization
which
ensued
in
the
late
20th
and
early
21st
centuries
created
a
need
for
a
subset
of
policy
studies
that
focuses
on
global
governance
especially
as
it
relates
to
issues
that
transcend
national
borders
such
as
climate
change
terrorism
nuclear
proliferation
and
economic
development
this
subset
of
policy
studies
which
is
often
referred
to
as
international
policy
studies
typically
requires
mastery
of
a
second
language
and
attention
to
cross-cultural
issues
in
order
to
address
national
and
cultural
biases
for
example
the
monterey
institute
of
international
studies
at
middlebury
college
offers
master
of
arts
programs
that
focus
exclusively
on
international
policy
through
a
mix
of
interdisciplinary
and
cross-cultural
analysis
called
the
"monterey
way"
examples
of
academic
programs
in
policy
studies
include
the
harvard
kennedy
school
and
the
lbj
school
of
public
affairs
blanket
policy
blanket
policy
is
a
policy
which
behaves
similarly
to
a
variety
of
things
based
on
webster's
dictionary
it
"covers
a
group
or
class
of
things
or
properties
instead
of
one
or
more
things
mentioned
individually
as
where
a
mortgage
secures
various
debts
as
a
group
or
subjects
a
group
or
class
of
different
pieces
of
property
to
one
general
lien"
webster
1913
suppl
open
educational
resources
policy
open
educational
resource
policies
(oer
policies)
are
principles
or
tenets
adopted
by
governing
bodies
in
support
of
the
use
of
open
content—specifically
open
educational
resources
(oer)
--
and
practices
in
educational
institutions
such
policies
are
emerging
increasingly
at
the
national
state/province
and
local
levels
creative
commons
defines
(oer)
policies
as
"legislation
institutional
policies
and/or
funder
mandates
that
lead
to
the
creation
increased
use
and/or
support
for
improving
oer"
oer
are
learning
materials
that
reside
in
the
public
domain
or
have
been
released
under
an
intellectual
property
license
that
permits
their
free
use
and
re-purposing
by
others
creative
commons
hosts
an
open
educational
resources
policy
registry
which
lists
112
current
and
proposed
open
education
policies
from
around
the
world
another
resource
for
finding
oer
policies
is
the
open
educational
quality
initiative
opal
best
practice
clearing
house
the
opal
initiative
is
a
partnership
between
seven
organizations
including
the
international
council
for
open
and
distance
education
(icde)
unesco
european
foundation
for
quality
the
open
university
uk
aalto
university
and
the
catholic
university
portugal
led
by
the
university
of
duisburg-essen
in
germany
it
is
partly
funded
by
the
european
commission
on
friday
22
june
2012
the
unesco
world
open
educational
resources
(oer)
congress
released
the
2012
paris
oer
declaration
which
called
on
governments
to
openly
license
publicly
funded
educational
materials
unesco
member
states
unanimously
approved
the
declaration
which
highlights
the
importance
of
open
educational
resources
and
gives
recommendations
to
governments
and
institutions
around
the
globe
on
january
17
2014
the
council
on
higher
education
in
south
africa
published
a
"white
paper
for
post-school
education
and
training"
this
paper
emphasized
open
learning
principles
and
set
the
stage
for
supporting
national
efforts
to
design
and
develop
high-quality
open
educational
resources
in
response
the
university
of
south
africa
(unisa)—one
of
the
founding
partners
of
the
oeru
network
and
a
member
of
the
2012
unesco
oer
conference
in
paris—approved
an
open
educational
resource
(oer)
strategy
in
march
2014
an
open-access
policy
enacted
by
the
faculty
of
a
research
university
can
empower
them
in
choosing
how
to
distribute
their
own
scholarly
work
if
a
faculty
member
wishes
to
grant
exclusive
rights
to
a
publisher
they
would
first
need
to
request
a
waiver
from
their
faculty
governance
body
some
reasons
to
implement
this
kind
of
policy
institution-wide
are
to:
this
kind
of
blanket
policy
provides
support
to
those
whose
research
is
not
part
of
a
project
that
requires
open
access
to
the
research
done
for
example
since
the
february
2013
directive
from
the
united
states
office
of
science
and
technology
policy
us
federal
agencies
have
been
developing
their
own
policies
on
making
research
freely
available
within
a
year
of
publication
sparc
the
scholarly
publishing
and
academic
resources
coalition
led
the
collaborative
and
open
effort
to
create
an
"open
access
spectrum"
that
demonstrates
a
more
sophisticated
approach
is
needed
in
discussions
about
the
concept
of
openness
in
research
communications
the
"howopenisit?
guide
(as
well
as
an
faq
document
and
slide
deck)
is
available
for
download
on
the
sparc
website
another
useful
guide
has
been
developed
by
members
of
the
harvard
office
for
scholarly
communication
the
harvard
open
access
project
and
the
berkman
center
for
internet
and
society
this
online
guide
"good
practices
for
university
open-access
policies"
is
built
on
a
wiki
and
is
designed
to
evolve
over
time
according
to
the
co-authors:
emily
kilcer
stuart
shieber
and
peter
suber
on
june
10
2013
the
faculty
board
of
the
california
institute
of
technology
(caltech)
created
an
institution-wide
open
access
policy
the
ruling
stated
that
as
of
january
1
2014
all
caltech
faculty
must
agree
to
grant
nonexclusive
rights
to
caltech
to
disseminate
their
scholarly
papers
either
via
the
authors'
own
sites
or
to
caltech
authors
the
online
repository
the
goal
is
to
encourage
wider
distribution
of
their
work
and
to
simplify
the
copyright
process
when
posting
research
on
faculty
or
institutional
web
sites
the
initiative
was
put
in
place
to
prevent
publishers
of
those
journals
from
threatening
legal
action
or
issuing
takedown
notices
to
authors
who
have
posted
their
content
on
their
own
sites
or
to
caltechauthors
an
online
repository
for
research
papers
authored
by
caltech
faculty
and
other
researchers
at
caltech
on
march
21
2010
the
duke
university
academic
council
voted
to
support
the
university
library's
new
data
repository
dukespace
with
a
blanket
policy
to
provide
open
access
to
their
scholarly
writings
the
policy
allows
for
faculty
members
to
opt
out
at
any
time
and
it
is
regularly
reviewed
to
determine
its
effectiveness
duke
also
in
2010
joined
the
compact
for
open-access
publishing
equity
(cope)
and
established
a
fund
to
help
duke
faculty
members
to
cover
any
author
fees
required
to
publish
in
open
access
journals
on
february
12
2008
the
faculty
of
arts
and
sciences
of
harvard
university
approved
their
open
access
policy
granting
to
the
president
and
fellows
of
harvard
to
"make
available
his
or
her
scholarly
articles
and
to
exercise
the
copyright
in
those
articles
in
a
nonexclusive
irrevocable
paid-up
worldwide
license"
since
then
several
other
schools
within
the
university
now
participate
in
the
open
access
policies
supported
by
the
office
for
scholarly
communication:
the
graduate
school
of
design
the
school
of
education
the
business
school
the
law
school
the
kennedy
school
of
government
the
divinity
school
and
the
school
of
public
health
the
university's
open-access
repository
is
called
dash
(digital
access
to
scholarship
at
harvard)
which
is
where
the
faculty
upload
their
scholarly
articles
for
access
by
all
adopted
by
a
unanimous
vote
on
march
18
2009
the
massachusetts
institute
of
technology
(mit)
faculty
adopted
an
open
access
policy
the
policy
applies
to
"all
scholarly
articles
written
while
the
person
is
a
member
of
the
faculty
except
for
any
articles
completed
before
the
adoption
of
this
policy
and
any
articles
for
which
the
faculty
member
entered
into
an
incompatible
licensing
or
assignment
agreement
before
the
adoption
of
this
policy"
the
mit
online
repository
is
called
dspace@mit
and
it
was
designed
to
work
seamlessly
with
google
scholar
the
faculty
revised
and
updated
the
policy
in
2010
to
take
into
consideration
the
various
issues
associated
with
the
mit
librarians'
discussions
with
publishers
in
2010
the
dean
of
the
faculty
of
princeton
university
appointed
an
ad-hoc
committee
of
faculty
and
the
university
librarian
to
study
the
question
of
open
access
to
faculty
publications
-
and
in
march
2011
the
committee
recommended
several
changes
to
the
faculty
rules
to
allow
for
a
blanket
policy
for
open
access
to
princeton
faculty
scholarship
the
faculty
approved
an
open
access
policy
on
september
19
2011
which
was
last
revised
in
january
2012
on
june
26
2008
the
stanford
university
graduate
school
of
education
(gse)
were
the
first
in
that
school
to
grant
permission
to
the
university
to
make
their
scholarly
articles
publicly
accessible
and
to
exercise
the
copyright
in
a
"nonexclusive
irrevocable
worldwide
license
provided
that
the
articles
are
properly
attributed
to
the
authors
not
sold
for
a
profit"
the
gse
open
archive
houses
and
makes
publicly
available
the
gse
authors'
working
papers
as
well
as
published
articles
between
may
21-24th
2013
the
stanford
gse
doctoral
students
voted
in
favor
of
a
motion
to
enact
an
open
access
policy
at
this
time
however
despite
the
strong
case
made
by
professors
john
willinsky
and
juan
pablo
alperin
no
other
stanford
academic
units
have
stepped
forward
on
july
24
2013
the
academic
senate
of
the
university
of
california
(uc)
approved
the
uc
open
access
policy
for
all
8000
plus
faculty
at
their
ten
campuses
some
confusion
at
the
local
campuses
led
to
online
postings
of
journal
articles
whose
copyright
was
already
owned
by
publishers
for
example
in
december
2013
the
academic
publishing
company
elsevier
sent
several
uc
faculty
notices
to
take
down
certain
journal
articles
posted
openly
on
their
campus
webpages
eg
on
the
department
websites
or
faculty
profiles
the
uc
open
access
policy
protected
those
faculty
who
had
correctly
uploaded
their
articles
to
the
uc
escholarship
repository
in
another
case
of
misunderstanding
by
the
faculty
about
open
access
in
march
2014
the
university
received
a
digital
millennium
copyright
act
(dmca)
takedown
notice
for
nine
articles
owned
by
the
american
society
for
civil
engineers
(asce)
the
uc
faculty
authors
had
uploaded
to
escholarship
the
publisher-formatted
articles
between
2004
and
2008
before
the
uc
open
access
policy
had
been
enacted
and
in
violation
of
the
publisher's
agreement
with
the
authors
when
they
gave
their
copyrights
to
the
asce
in
2014
the
faculty
assembly
of
the
university
of
colorado
boulder
approved
the
cu
boulder
open
access
policy
"in
order
to
allow
for
broad
dissemination
of
their
research"
they
granted
to
the
regents
of
the
university
of
colorado
"a
nonexclusive
irrevocable
worldwide
license
to
exercise
any
and
all
rights
under
copyright
relating
to
their
scholarly
work
as
long
as
the
works
are
properly
attributed
to
the
authors
and
not
used
for
commercial
purposes"—and
that
the
individual
faculty
would
retain
full
ownership
of
the
material
authors
at
uc
boulder
are
expected
to
inform
publishers
about
the
university's
policy
and
that
they
"have
granted
a
pre-existing
license"
the
digital
repository
cu
scholar
is
maintained
by
the
university
libraries
and
functions
under
a
set
of
policies
derived
from
the
open
access
policy
contributions
from
the
cu
boulder
community
can
include
working
papers
and
technical
reports
published
scholarly
research
articles
completed
manuscripts
digital
art
or
multimedia
conference
papers
and
proceedings
theses
and
dissertations
undergraduate
honors
theses
journals
published
on
campus
faculty
course-related
output
primarily
of
scholarly
interest
and
data
sets
the
chancellor's
executive
committee
recently
approved
the
new
policy
following
the
lead
of
the
council
of
deans
and
the
office
of
the
provost
and
executive
vice
chancellor
in
2005
the
university
of
kansas
(ku)
created
ku
scholarworks
a
digital
repository
for
scholarly
work
created
by
ku
faculty
and
staff
faculty
senate
president
lisa
wolf-wendel
professor
of
education
leadership
and
policy
studies
approved
a
new
policy
"open
access
policy
for
university
of
kansas
scholarship"
on
april
30
2009
in
order
to
provide
the
broadest
possible
access
to
the
journal
literature
authored
by
ku
faculty"
in
june
2009
under
a
faculty-initiated
policy
approved
by
chancellor
robert
hemenway
ku
became
the
first
us
public
university
to
implement
an
open
access
policy
unless
a
ku
author
sought
a
waiver
all
articles
must
be
submitted
to
ku
scholarworks
"processes
to
implement
the
ku
open
access
policy"
were
endorsed
by
the
faculty
senate
in
february
2010
theses
and
dissertations
at
the
university
of
kansas
are
also
openly
available
however
in
2010
ku
graduate
studies
established
a
policy
that
a
student
may
request
permission
to
embargo
its
publication
for
six
months
one
year
or
two
years
graduates
earning
the
ku
master
of
fine
arts
in
creative
writing
or
phd
in
english
(literature
and
creative
writing
track)
may
request
a
permanent
embargo
in
the
united
kingdom
the
higher
education
funding
council
for
england
(hefce)
subsidized
the
jisc
academy
open
educational
resources
programme
jisc
refers
to
a
membership
organization
that
provides
digital
solutions
for
united
kingdom
education
and
research
initiatives
the
jisc/he
oer
programme
(phase
3
from
october
2011
–
october
2012)
was
meant
to
build
on
sustainable
procedure
indicated
in
the
first
two
phases
eventually
expanding
in
new
directions
that
connect
open
educational
resources
to
other
fields
of
work
this
third
phase
involved
important
stakeholders
emphasizing
fresh
challenges
and
insights
about
the
effect
of
oer
and
open
educational
practice
during
this
stage
the
concept
of
electronic
books
and
massive
open
online
courses
(mooc)
also
emerged
moocs
offer
courses
at
the
university
level
without
having
to
finish
the
whole
programme
many
students
get
the
chance
to
study
premium
courses
online
frequently
at
no
cost
hefce
made
significant
investments
through
the
jisc
and
academy
from
2009
until
2012
the
objective
was
to
encourage
sharing
and
reusing
of
resources
which
provide
benefits
to
higher
education
in
the
united
kingdom
more
than
80
projects
obtained
funding
during
the
uk
oer
programme
substantial
investments
were
channelled
towards
the
development
of
open
educational
resources
even
as
the
benefits
for
stakeholders
have
not
been
explained
properly
sufficient
evidence
is
needed
to
prove
this
point
one
criticism
is
that
many
such
programmes
are
not
technically
and
educationally
accessible
to
a
worldwide
audience
policy
transfer
policy
transfer
policy
transfer
is
a
process
in
which
knowledge
about
policies
administrative
arrangements
institutions
and
ideas
in
one
political
setting
(past
or
present)
is
used
in
the
development
of
policies
administrative
arrangements
institutions
and
ideas
in
another
political
setting
policy
transfer
has
a
long
history
with
a
variety
of
policies
such
as
zero
tolerance
policing
welfare-to-work
and
business
improvement
districts
moving
between
different
nation
states
policy
transfer
has
been
the
subject
of
considerable
academic
research
led
primarily
by
political
scientists
since
the
late
1990s
since
the
mid-2000s
geographers
have
also
played
an
important
role
in
these
debates
(often
use
the
term
"policy
mobilities"
instead
of
policy
transfer)
since
david
dolowitz
and
david
marsh's
(2000)
paper
'learning
from
abroad:
the
role
of
policy
transfer
in
contemporary
policy-making'
academic
research
has
focused
on
the
issues
of
who
is
involved
in
policy
transfer
what
is
transferred
from
and
to
where
policy
is
transferred
the
degrees
of
and
constraints
on
transfer
and
its
success
once
transferred
more
recently
there
have
been
attempts
to
explicate
the
role
of
two
way
communication
and
particularly
feedback
from
policy
stakeholders
for
successful
policy
transfer
along
with
efforts
to
acknowledge
the
"indigenization"
of
policies
as
they
are
modified
and
adapted
to
context
the
debates
in
geography
have
focused
on
the
technologies
and
methods
through
which
policies
and
ideas
circulate
–
such
as
study
tours
conferences
and
best
practice
guides
–
as
well
as
looking
at
how
and
why
the
policies
change
form
as
they
circulate
asia-pacific
network
for
global
change
research
the
asia-pacific
network
for
global
change
research
(apn)
is
an
intergovernmental
network
that
promotes
policy-oriented
research
and
capacity-building
activities
related
to
global
change
in
the
region
apn
receives
financial
contribution
from
the
governments
of
the
united
states
japan
republic
of
korea
and
new
zealand
with
in-kind
contribution
from
all
it
22
member
countries
the
apn
secretariat
is
based
in
kobe
japan
hosted
by
the
hyogo
prefectural
government
the
history
of
apn
dates
back
to
the
1990
white
house
conference
on
science
and
economics
research
related
to
global
change
17–18
april
1990
at
which
then
us
president
george
bush
invited
countries
of
the
world
to
join
the
united
states
in
creating
regional
networks
for
north-south
scientific
cooperation
at
the
intergovernmental
level
to
deal
with
global
environmental
change
research
later
in
1992
president
bush
and
then
prime
minister
of
japan
kiichi
miyazawa
signed
the
1992
us-japan
global
partnership
agreement
which
among
other
things
reaffirmed
and
strengthened
japan-us
commitment
to
global
change
research
discussions
along
these
lines
ultimately
resulted
in
the
establishment
of
three
global
change
research
networks:
enrich
for
europe
and
africa
apn
for
asia
and
the
pacific
and
iai
for
the
americas
apn
was
formally
launched
in
1996
at
its
first
intergovernmental
meeting
held
at
chiang
mai
thailand
in
1997
a
competitive
process
was
in
place
open
to
funding
applications
for
scientific
research
projects
relating
to
global
environmental
change
starting
from
12
countries
in
1996
apn
membership
has
grown
to
22
as
of
april
2013
in
addition
to
the
22
full
members
institutions
and
individuals
from
a
number
of
“approved
countries”
are
eligible
for
apn
funding
policy
monitoring
policy
monitoring
comes
a
range
of
activities
describing
and
analyzing
the
development
and
implementation
of
policies
identifying
potential
gaps
in
the
process
outlining
areas
for
improvement
and
holding
policy
implementers
accountable
for
their
activities
monitoring
policy
development
and
implementation
is
an
integral
component
of
the
policy
cycle
and
can
be
applied
in
sectors
including
agriculture
health
education
and
finance
policy
monitoring
can
improve
policy
informatioation
among
stakeholders
and
the
use
of
evaluation
techniques
to
provide
feedback
to
reframe
and
revise
policies
waterman
and
wood
derived
policy
monitoring
from
agency
theory
describing
a
process
where
policymakers
monitor
the
actions
of
their
bureaucratic
agents
who
implement
and
enforce
policies
this
monitoring
allows
policymakers
to
compensate
for
their
agents’
greater
knowledge
of
the
policy
process
and
enables
them
to
be
well-informed
decision
makers
thus
policy
monitoring
allows
policymakers
and
interested
actors
to
systematically
examine
the
process
of
creating
a
policy
implementing
it
and
evaluating
its
effects
policy
monitoring
activities
can
be
used
to
collect
and
analyze
data
related
to
the
development
and
implementation
of
specific
policies
it
can
also
help
link
policies
to
specific
outcomes
and
help
identify
and
evaluate
policy
impacts
policy
impacts
can
include
specific
changes
in
behavior
(eg
increased
number
of
people
wearing
seatbelts)
finances
(eg
increased
tax
revenue)
health
status
or
epidemiology
(eg
reduced
number
of
new
hiv
infections)
or
other
social
indicators
(eg
reduced
crime
rates
reduced
levels
of
pollution)
data
from
policy
monitoring
can
be
used
to
support
advocacy
efforts
and
guide
the
development
of
new
timely
and
relevant
policies
policy
monitoring
should
also
include
the
identification
of
operational
policy
barriers
that
can
be
addressed
through
policy
and
program
reform
and
findings
can
support
improved
implementation
of
existing
policies
numerous
actors
and
stakeholders
can
influence
the
movement
of
policy
from
inception
to
implementation
well-maintained
documentation
and
review
of
all
key
stakeholders
involved
in
a
policy
can
help
advocates
for
a
given
policy—such
as
military
reform
water
rights
or
disability
legislation—prepare
to
address
different
ideologies
capacities
or
interests
of
key
actors
limiting
stakeholder
analysis
only
to
government
and
official
policymakers
may
ignore
major
groups
that
can
support
policy
development
policy
monitoring
coalitions
should
agree
on
what
they
are
monitoring
and
be
succinct
in
their
recommendations
to
policymakers
policy
initiatives
themselves
are
often
controversial
and
policy
monitoring
can
be
contentious
because
it
shows
how
well
policy
implementers
and
enforcers
are
doing
their
jobs
those
conducting
policy
monitoring
should
be
thorough
in
their
data
collection
and
unbiased
in
their
presentation
of
facts
robust
trainings
on
policy
monitoring
work
can
help
organizations
be
systematic
and
effective
in
their
policy
monitoring
efforts
the
united
states
president’s
emergency
plan
for
aids
relief
(pepfar)’s
monitoring
policy
reform
tool
outlines
the
progression
of
policy
development
related
to
hiv
from
problem
identification
to
monitoring
and
evaluation
the
tool
supports
a
relatively
simple
and
uniform
monitoring
process
which
can
be
applied
to
any
policy
area
this
tool
can
guide
policy
monitoring
efforts
throughout
the
policy
reform
process
the
fao’s
food
and
agriculture
policy
decision
analysis
(fapda)
is
a
policy
monitoring
tool
that
provides
a
working
cycle
technique
to
identify
policy
problems
and
improve
analysis
of
policy
issues
by
incorporating
fapda
outputs
such
as
a
web-based
tool
country
policy
review
and
policy
analysis
report
policy
dialogue
can
be
more
systematic
and
encompass
different
actors
interested
in
fapda
data
the
world
health
organization
has
started
to
develop
dedicated
monitoring
systems
for
policy
interventions
on
the
social
determinants
of
health
that
improve
health
equity
such
as
social
protection
and
gender
equity
policies
policy
monitoring
can
be
performed
through
different
issue-driven
lenses
such
as
gender
sensitivity
or
gender
equality
gender-sensitive
policy
monitoring
analyzes
any
gender
aspects
of
a
policy
or
policy
issue
and
considers
the
impact
of
the
policy
on
both
men
and
women
as
well
as
its
impact
on
gender
relations
for
example
a
policy
that
is
shown
to
have
improved
the
welfare
of
a
household
may
not
necessarily
affect
all
household
members
positively
or
equally
and
may
have
even
exacerbated
gender
inequity
gender-sensitive
policy
monitoring
can
help
advance
gender
equity
and
improve
policy
implementation
civil
society
and
other
stakeholders
can
use
policy
monitoring
techniques
to
systematically
gather
data
on
the
gender
aspects
of
policies
and
use
these
data
to
influence
policymakers
to
favor
gender-equitable
health
policies¬¬¬—these
processes
are
essential
to
facilitating
gender
mainstreaming
a
2012
study
analyzed
planned
policy
interventions
across
the
22
publicly
accessible
pepfar
partnership
frameworks
to
understand
how
the
interventions
are
related
to
pepfar
and
country
or
regional
priorities
the
study
found
that
“policy
monitoring
by
donors
partner
country
governments
and
civil
society
stakeholders
can
help
measure
whether
policy
interventions
are
occurring
as
planned
in
order
to
further
hiv
prevention
care
and
treatment
and
health
system
goals
and
if
not
can
point
to
needed
changes"
mandate
(politics)
in
politics
a
mandate
is
the
authority
granted
by
a
constituency
to
act
as
its
representative
the
concept
of
a
government
having
a
legitimate
mandate
to
govern
via
the
fair
winning
of
a
democratic
election
is
a
central
idea
of
representative
democracy
new
governments
who
attempt
to
introduce
policies
that
they
did
not
make
public
during
an
election
campaign
are
said
not
to
have
a
legitimate
"mandate"
to
implement
such
policies
elections
especially
ones
with
a
large
margin
of
victory
and
are
often
said
to
give
the
newly
elected
government
or
elected
official
an
implicit
mandate
to
put
into
effect
certain
policies
when
a
government
seeks
re-election
they
may
introduce
new
policies
as
part
of
the
campaign
and
are
hoping
for
approval
from
the
voters
and
say
they
are
seeking
a
"new
mandate"
in
some
languages
a
'mandate'
can
mean
a
parliamentary
seat
won
in
an
election
rather
than
the
electoral
victory
itself
in
case
such
a
mandate
is
bound
to
the
wishes
of
the
electorate
it
is
an
imperative
mandate
otherwise
it
is
called
"free"
political
philosophy:
policy
analysis
policy
analysis
is
a
technique
used
in
public
administration
to
enable
civil
servants
activists
and
others
to
examine
and
evaluate
the
available
options
to
implement
the
goals
of
laws
and
elected
officials
the
process
is
also
used
in
the
administration
of
large
organizations
with
complex
policies
it
has
been
defined
as
the
process
of
"determining
which
of
various
policies
will
achieve
a
given
set
of
goals
in
light
of
the
relations
between
the
policies
and
the
goals"
policy
analysis
can
be
divided
into
two
major
fields:
the
areas
of
interest
and
the
purpose
of
analysis
determine
what
types
of
analysis
are
conducted
a
combination
of
two
kinds
of
policy
analyses
together
with
program
evaluation
is
defined
as
"policy
studies"
policy
analysis
is
frequently
deployed
in
the
public
sector
but
is
equally
applicable
elsewhere
such
as
nonprofit
organizations
and
non-governmental
organizations
policy
analysis
has
its
roots
in
systems
analysis
an
approach
used
by
united
states
secretary
of
defense
robert
mcnamara
in
the
1960s
various
approaches
to
policy
analysis
exist
the
analysis
"for"
policy
(and
analysis
"of"
policy)
is
the
central
approach
in
social
science
and
educational
policy
studies
it
is
linked
to
two
different
traditions
of
policy
analysis
and
research
frameworks
the
approach
of
analysis
"for"
policy
refers
to
research
conducted
for
actual
policy
development
often
commissioned
by
policymakers
inside
the
bureaucracy
(eg
senior
civil
servants)
within
which
the
policy
is
developed
analysis
"of"
policy
is
more
of
an
academic
exercise
conducted
by
academic
researchers
professors
and
think
tank
researchers
who
are
often
seeking
to
understand
why
a
particular
policy
was
developed
at
a
particular
time
and
assess
the
effects
intended
or
otherwise
of
that
policy
when
it
was
implemented
there
are
three
approaches
that
can
be
distinguished:
the
analysis-centric
the
policy
process
and
the
meta-policy
approach
the
analysis-centric
(or
"analycentric")
approach
focuses
on
individual
problems
and
their
solutions
its
scope
is
the
micro-scale
and
its
problem
interpretation
or
problem
resolution
usually
involves
a
technical
solution
the
primary
aim
is
to
identify
the
most
effective
and
efficient
solution
in
technical
and
economic
terms
(eg
the
most
efficient
allocation
of
resources)
the
policy
process
approach
puts
its
focal
point
onto
political
processes
and
involved
stakeholders;
its
scope
is
the
broader
meso-scale
and
it
interprets
problems
using
a
political
lens
(ie
the
interests
and
goals
of
elected
officials)
it
aims
at
determining
what
processes
means
and
policy
instruments
(eg
regulation
legislation
subsidy)
are
used
as
well
it
tries
to
explain
the
role
and
influence
of
stakeholders
within
the
policy
process
in
the
2010s
"stakeholders"
is
defined
broadly
to
include
citizens
community
groups
non-governmental
organizations
businesses
and
even
opposing
political
parties
by
changing
the
relative
power
and
influence
of
certain
groups
(eg
enhancing
public
participation
and
consultation)
solutions
to
problems
may
be
identified
that
have
more
"buy
in"
from
a
wider
group
one
way
of
doing
this
followed
a
heuristic
model
called
the
"policy
cycle"
in
its
simplest
form
the
policy
cycle
which
is
often
depicted
visually
as
a
loop
or
circle
starts
with
the
identification
of
the
problem
proceeds
to
an
examination
of
the
different
policy
tools
that
could
be
used
to
respond
to
that
problem
then
goes
on
to
the
implementation
stage
in
which
one
or
more
policies
are
put
into
practice
(eg
a
new
regulation
or
subsidy
is
set
in
place)
and
then
finally
once
the
policy
has
been
implemented
and
run
for
a
certain
period
the
policy
is
evaluated
a
number
of
different
viewpoints
can
be
used
during
evaluation
including
looking
at
a
policy's
effectiveness
cost-effectiveness
value
for
money
outcomes
or
outputs
the
meta-policy
approach
is
a
systems
and
context
approach;
ie
its
scope
is
the
macro-scale
and
its
problem
interpretation
is
usually
of
a
structural
nature
it
aims
at
explaining
the
contextual
factors
of
the
policy
process;
ie
what
the
political
economic
and
socio-cultural
factors
are
that
influence
it
as
problems
may
result
because
of
structural
factors
(eg
a
certain
economic
system
or
political
institution)
solutions
may
entail
changing
the
structure
itself
policy
analysis
uses
both
qualitative
methods
and
quantitative
methods
qualitative
research
includes
case
studies
and
interviews
with
community
members
quantitative
research
includes
survey
research
statistical
analysis
(also
called
"data
analysis")
and
model
building
a
common
practice
is
to
define
the
problem
and
evaluation
criteria;
identify
and
evaluate
alternatives;
and
recommend
a
certain
policy
accordingly
promotion
of
the
best
agendas
are
the
product
of
careful
"back-room"
analysis
of
policies
by
"a
priori"
assessment
and
"a
posteriori"
evaluation
there
are
six
dimensions
to
policy
analysis
categorized
as
the
effects
and
implementation
of
the
policy
across
a
period
of
time
also
collectively
known
as
"durability"
of
the
policy
which
means
the
capacity
in
content
of
the
policy
to
produce
visible
effective
compatible
change
or
results
over
time
with
robustness
effects
implementation
the
strategic
effects
dimensions
can
pose
certain
limitations
due
to
data
collection
however
the
analytical
dimensions
of
effects
directly
influences
acceptability
the
degree
of
acceptability
is
based
upon
the
plausible
definitions
of
actors
involved
in
feasibility
if
the
feasibility
dimension
is
compromised
it
will
put
the
implementation
at
risk
which
will
entail
additional
costs
finally
implementation
dimensions
collectively
influence
a
policy's
ability
to
produce
results
or
impacts
one
model
of
policy
analysis
is
the
"five-e
approach"
which
consists
of
examining
a
policy
in
terms
of:
policies
are
considered
as
frameworks
that
can
optimize
the
general
well-being
these
are
commonly
analyzed
by
legislative
bodies
and
lobbyists
every
policy
analysis
is
intended
to
bring
an
evaluative
outcome
a
systemic
policy
analysis
is
meant
for
in
depth
study
for
addressing
a
social
problem
following
are
steps
in
a
policy
analysis:
many
models
exist
to
analyze
the
development
and
implementation
of
public
policy
analysts
use
these
models
to
identify
important
aspects
of
policy
as
well
as
explain
and
predict
policy
and
its
consequences
each
of
these
models
are
based
upon
the
types
of
policies
some
evidence
supported
models
are:
public
policy
is
determined
by
a
range
of
political
institutions
which
give
policy
legitimacy
to
policy
measures
in
general
the
government
applies
policy
to
all
citizens
and
monopolizes
the
use
of
force
in
applying
or
implementing
policy
(through
government
control
of
law
enforcement
court
systems
imprisonment
and
armed
forces)
the
legislature
executive
and
judicial
branches
of
government
are
examples
of
institutions
that
give
policy
legitimacy
many
countries
also
have
independent
quasi-independent
or
arm's
length
bodies
which
while
funded
by
government
are
independent
from
elected
officials
and
political
leaders
these
organizations
may
include
government
commissions
tribunals
regulatory
agencies
and
electoral
commissions
policy
creation
is
a
process
that
typically
follows
a
sequence
of
steps
or
stages:
this
model
however
has
been
criticized
for
being
overly
linear
and
simplistic
in
reality
stages
of
the
policy
process
may
overlap
or
never
happen
also
this
model
fails
to
take
into
account
the
multiple
factors
attempting
to
influence
the
process
itself
as
well
as
each
other
and
the
complexity
this
entails
one
of
the
most
widely
used
model
for
public
institutions
are
of
herbert
a
simon
the
father
of
rational
models
it
is
also
used
by
private
corporations
however
many
criticise
the
model
due
to
characteristics
of
the
model
being
impractical
and
relying
on
unrealistic
assumptions
for
instance
it
is
a
difficult
model
to
apply
in
the
public
sector
because
social
problems
can
be
very
complex
ill-defined
and
interdependent
the
problem
lies
in
the
thinking
procedure
implied
by
the
model
which
is
linear
and
can
face
difficulties
in
extraordinary
problems
or
social
problems
which
have
no
sequences
of
happenings
the
rational
model
of
decision-making
is
a
process
for
making
sound
decisions
in
policy-making
in
the
public
sector
rationality
is
defined
as
“a
style
of
behavior
that
is
appropriate
to
the
achievement
of
given
goals
within
the
limits
imposed
by
given
conditions
and
constraints”
it
is
important
to
note
the
model
makes
a
series
of
assumptions
such
as:
'the
model
must
be
applied
in
a
system
that
is
stable';
'the
government
is
a
rational
and
unitary
actor
and
that
its
actions
are
perceived
as
rational
choices';
'the
policy
problem
is
unambiguous';
'there
are
no
limitations
of
time
or
cost'
furthermore
in
the
context
of
the
public
sector
policy
models
are
intended
to
achieve
maximum
social
gain
simon
identifies
an
outline
of
a
step
by
step
mode
of
analysis
to
achieve
rational
decisions
ian
thomas
describes
simon's
steps
as
follows:
the
model
of
rational
decision-making
has
also
proven
to
be
very
useful
to
several
decision
making
processes
in
industries
outside
the
public
sphere
nonetheless
there
are
some
who
criticize
the
rational
model
due
to
the
major
problems
which
can
be
faced
which
tend
to
arise
in
practice
because
social
and
environmental
values
can
be
difficult
to
quantify
and
forge
consensus
around
furthermore
the
assumptions
stated
by
simon
are
never
fully
valid
in
a
real
world
context
further
criticism
of
the
rational
model
include:
leaving
a
gap
between
planning
and
implementation
ignoring
of
the
role
of
people
entrepreneurs
leadership
etc
the
insufficiency
of
technical
competence
(ie
ignoring
the
human
factor)
reflecting
too
mechanical
an
approach
(ie
the
organic
nature
of
organizations)
requiring
of
multidimensional
and
complex
models
generation
of
predictions
which
are
often
wrong
(ie
simple
solutions
may
be
overlooked)
incurring
of
cost
(ie
costs
of
rational-comprehensive
planning
may
outweigh
the
cost
savings
of
the
policy)
however
thomas
r
dye
the
president
of
the
lincoln
center
for
public
service
states
the
rational
model
provides
a
good
perspective
since
in
modern
society
rationality
plays
a
central
role
and
everything
that
is
rational
tends
to
be
prized
thus
it
does
not
seem
strange
that
“we
ought
to
be
trying
for
rational
decision-making”
an
incremental
policy
model
relies
on
features
of
incremental
decision-making
such
as:
satisfying
organizational
drift
bounded
rationality
and
limited
cognition
among
others
such
policies
are
often
called
"muddling
through"
represent
a
conservative
tendency:
new
policies
are
only
slightly
different
from
old
policies
policy-makers
are
too
short
on
time
resources
and
brains
to
make
totally
new
policies;
as
such
past
policies
are
accepted
as
having
some
legitimacy
when
existing
policies
have
sunk
costs
which
discourage
innovation
incrementalism
is
an
easier
approach
than
rationalism
and
the
policies
are
more
politically
expedient
because
they
don't
necessitate
any
radical
redistribution
of
values
such
models
necessarily
struggle
to
improve
the
acceptability
of
public
policy
criticisms
of
such
a
policy
approach
include:
challenges
to
bargaining
(ie
not
successful
with
limited
resources)
downplaying
useful
quantitative
information
obscuring
real
relationships
between
political
entities
an
anti-intellectual
approach
to
problems
(ie
the
preclusion
of
imagination)
and
a
bias
towards
conservatism
(ie
bias
against
far-reaching
solutions)
there
are
many
contemporary
policies
relevant
to
gender
and
workplace
issues
actors
analyze
contemporary
gender-related
employment
issues
ranging
from
parental
leave
and
maternity
programs
sexual
harassment
and
work/life
balance
to
gender
mainstreaming
it
is
by
the
juxtaposition
of
a
variety
of
research
methodologies
focused
on
a
common
theme
the
richness
of
understanding
is
gained
this
integrates
what
are
usually
separate
bodies
of
evaluation
on
the
role
of
gender
in
welfare
state
developments
employment
transformations
workplace
policies
and
work
experience
this
policy
is
formed
as
a
result
of
forces
and
pressures
from
influential
groups
pressure
groups
are
informally
co-opted
into
the
policy
making
process
regulatory
agencies
are
captured
by
those
they
are
supposed
to
regulate
no
one
group
is
dominant
all
the
time
on
all
issues
the
group
is
the
bridge
between
the
individual
and
the
administration
the
executive
is
thus
pressured
by
interest
groups
the
task
of
the
system
is
to:
there
are
several
other
major
types
of
policy
analysis
broadly
groupable
into
competing
approaches:
the
success
of
a
policy
can
be
measured
by
changes
in
the
behavior
of
the
target
population
and
active
support
from
various
actors
and
institutions
involved
a
public
policy
is
an
authoritative
communication
prescribing
an
unambiguous
course
of
action
for
specified
individuals
or
groups
in
certain
situations
there
must
be
an
authority
or
leader
charged
with
the
implementation
and
monitoring
of
the
policy
with
a
sound
social
theory
underlying
the
program
and
the
target
group
evaluations
can
help
estimate
what
effects
will
be
produced
by
program
objectives/alternatives
however
claims
of
causality
can
only
be
made
with
randomized
control
trials
in
which
the
policy
change
is
applied
to
one
group
and
not
applied
to
a
control
group
and
individuals
are
randomly
assigned
to
these
groups
to
obtain
compliance
of
the
actors
involved
the
government
can
resort
to
positive
sanctions
such
as
favorable
publicity
price
supports
tax
credits
grants-in-aid
direct
services
or
benefits;
declarations;
rewards;
voluntary
standards;
mediation;
education;
demonstration
programs;
training
contracts;
subsidies;
loans;
general
expenditures;
informal
procedures
bargaining;
franchises;
sole-source
provider
awardsetc
policy
evaluation
is
used
to
examine
content
implementation
or
impact
of
the
policy
which
helps
to
understand
the
merit
worth
and
the
utility
of
the
policy
following
are
national
collaborating
centre
for
healthy
public
policy's
(ncchpp)
10
steps:
end
use
energy
demand
centres
the
end
use
energy
demand
(eued)
centres
carry
out
interdisciplinary
research
and
advise
policy
on
reducing
energy
demand
to
help
achieve
the
uk
government's
co2
emissions
targets
the
centres
are
a
£30m
investment
of
the
research
councils
uk
energy
programme
(with
additional
funding
from
industrial
partners)
that
run
from
2013-2018
the
six
large
centres
are
based
across
25
institutions
and
encompass
over
200
researchers
the
centres
are:
centre
for
energy
epidemiology
(cee)
-
based
at
university
college
london
and
directed
by
professor
tadj
oreszczyn
centre
on
innovation
and
energy
demand
(cied)
-
based
at
the
university
of
sussex
and
directed
by
professor
benjamin
sovacool
centre
for
industrial
energy
materials
and
products
(cie-map)
-
based
at
the
university
of
leeds
and
directed
by
professor
john
barrett
centre
for
sustainable
energy
use
in
food
chains
(csef)
-
based
at
brunel
university
and
led
by
professor
savvas
tassou
dynamics
of
energy
mobility
and
demand
(demand)
-
based
at
the
university
of
lancaster
and
led
by
professor
elizabeth
shove
interdisciplinary
centre
for
storage
transformation
and
upgrading
of
thermal
energy
(i-stute)
-
based
at
the
university
of
warwick
and
directed
by
professor
robert
critoph
the
uk
is
committed
to
achieving
large-scale
reductions
(80%)
in
greenhouse
gas
emissions
by
2050
(climate
change
act
(1990))
end
use
energy
demand
(eued)
research
is
about
reducing
energy
use
on
the
scale
needed
to
reach
this
target
whilst
maintaining
current
or
achieving
better
standards
of
living
and
economic
growth
the
eued
centres
were
funded
in
2013
following
a
2011
call
for
proposals
from
the
research
councils
uk
energy
programme
to
address
these
issues
centralisation
centralisation
(british)
or
centralization
(both
british
and
american)
is
the
process
by
which
the
activities
of
an
organization
particularly
those
regarding
planning
and
decision-making
become
concentrated
within
a
particular
geographical
location
group
this
moves
the
important
decision-making
and
planning
powers
within
the
center
of
the
organisation
the
term
has
a
variety
of
meanings
in
several
fields
in
political
science
centralisation
refers
to
the
concentration
of
a
government's
power—both
geographically
and
politically—into
a
centralized
government
"centralisation
of
authority"
is
defined
as
the
systematic
and
consistent
concentration
of
authority
at
a
central
point
or
in
a
person
within
the
organization
this
idea
was
first
introduced
in
the
qin
dynasty
of
china
the
qin
government
was
highly
bureaucratic
and
was
administered
by
a
hierarchy
of
officials
all
serving
the
first
emperor
qin
shi
huang
the
qin
dynasty
practised
all
the
things
that
han
feizi
taught
allowing
qin
shi
huang
to
own
and
control
all
his
territories
including
those
conquered
from
other
countries
zheng
and
his
advisers
ended
feudalism
in
china
by
setting
up
new
laws
and
regulations
under
a
centralized
and
bureaucratic
government
with
a
rigid
centralization
of
authority
under
this
system
both
the
military
and
government
thrived
this
was
because
talented
individuals
were
more
easily
identified
and
picked
out
to
be
trained
for
specialized
functions
the
acts
for
the
implementation
are
needed
after
delegation
therefore
the
authority
for
taking
the
decisions
can
be
spread
with
the
help
of
the
delegation
of
the
authority
the
centralisation
of
authority
can
be
done
immediately
if
complete
concentration
is
given
at
the
decision-making
stage
for
any
position
the
centralisation
can
be
done
with
a
position
or
at
a
level
in
an
organisation
ideally
the
decision-making
power
is
held
by
a
few
individuals
centralisation
of
authority
has
several
advantages
and
disadvantages
the
benefits
include:
disadvantages
on
the
other
hand
are
as
follows:
as
written
in
vi
lenin’s
book
"imperialism
the
highest
stage
of
capitalism"
"the
remarkably
rapid
concentration
of
production
in
ever-larger
enterprises
are
one
of
the
most
characteristic
features
of
capitalism"
he
researched
the
development
of
production
and
decided
to
develop
the
concept
of
production
as
a
centralised
framework
from
individual
and
scattered
small
workshops
into
large
factories
leading
the
capitalism
to
the
world
this
is
guided
by
the
idea
that
once
concentration
of
production
develops
into
a
particular
level
it
will
become
a
monopoly
like
party
organisations
of
cartel
syndicate
and
trust
most
businesses
deal
with
issues
relating
to
the
specifics
of
centralization
or
decentralization
of
decision-making
the
key
question
is
either
whether
the
authority
should
manage
all
the
things
at
the
centre
of
a
business
(centralised)
or
whether
it
should
be
delegated
far
away
from
the
centre
(decentralised)
the
choice
between
centralised
or
decentralised
varies
many
large
businesses
necessarily
involve
some
extent
of
decentralisation
and
some
extent
of
centralisation
when
it
begins
to
operate
from
several
places
or
any
new
units
and
markets
added
protected
area
downgrading
downsizing
and
degazettement
protected
area
downgrading
downsizing
and
degazettement
(paddd)
are
processes
that
change
the
legal
status
of
national
parks
and
other
protected
areas
"downgrading"
is
"a
decrease
in
legal
restrictions
on
the
number
magnitude
or
extent
of
human
activities
within
a
protected
area
(ie
legal
authorization
for
increased
human
use)"
"downsizing"
refers
to
a
"decrease
in
size
of
a
protected
area
as
a
result
of
excision
of
land
or
sea
area
through
a
legal
boundary
change"
"degazettement"
is
defined
as
a
loss
of
legal
protection
for
an
entire
national
park
or
other
protected
area
collectively
paddd
represents
legal
processes
that
temper
regulations
shrink
boundaries
or
eliminate
all
legal
protections
originally
associated
with
establishment
of
a
protected
area
paddd
is
a
phenomenon
that
has
recently
gained
attention
among
scientists
and
policymakers
scientific
publications
have
identified
more
than
600
enacted
paddd
events
in
57
countries
encompassing
a
total
of
more
than
550000
km2
of
protected
lands
paddd
was
a
topic
of
discussion
at
the
world
parks
congress
in
sydney
australia
in
november
2014
scientists
have
suggested
that
the
global
paddd
trend
could
be
combatted
via
a
systematic
programme
of
protected
area
"upgrading"
whereby
conserved
wild
areas
are
expanded
via
the
purchase
or
gazetting
of
surrounding
territory
successful
examples
of
protected-area
upgrading
include
gorongosa
national
park
in
mozambique
and
the
guanacaste
conservation
area
in
costa
rica
policy
learning
policy
learning
is
the
increased
understanding
that
occurs
when
policymakers
compare
one
set
of
policy
problems
to
others
within
their
own
or
in
other
jurisdictions
it
can
aid
in
understanding
why
a
policy
was
implemented
the
policy's
effects
and
how
the
policy
could
apply
to
the
policymakers'
jurisdiction
before
a
policy
is
adopted
it
goes
through
a
process
that
involves
various
combinations
of
elected
official(s)
political
parties
civil
servants
advocacy
groups
policy
experts
or
consultants
corporations
think
tanks
and
multiple
levels
of
government
policies
can
be
challenged
in
various
ways
including
questioning
its
legality
ideally
policymakers
develop
complete
knowledge
about
the
policy;
the
policy
should
achieve
its
intent
and
efficiently
use
resources
policy
learning
through
globalization
has
helped
government
organizations
become
more
competitive
policymakers
have
easy
access
to
global
policy
knowledge
through
the
internet
access
to
think
tanks
international
institutions
such
as
the
united
nations
international
monetary
fund
(imf)
or
the
world
bank
and
individual
experts
in
the
1960s
academics
started
to
study
how
policymakers
learn
about
policies
during
that
time
countries
were
experiencing
social
political
economic
and
technological
change
researchers
discovered
that
governments
in
different
countries
faced
similar
problems
in
policies
and
programs
amidst
uncertainty
on
how
to
handle
problems
in
financing
its
welfare
programs
policymakers
start
to
learn
about
policy
through
facts
first-hand
experiences
or
from
the
experiences
of
others
policy
instruments
and
policy
implementation
are
the
steps
to
policy
learning
policymakers
review
policy
objectives
tools
and
implementation
strategies
when
implementations
fail
reviews
look
for
the
cause(s)
adjustments
in
objectives
tools
and
implementation
are
considered
instrumental
policy
learning
is
the
acquisition
of
knowledge
about
the
effectiveness
of
various
policy
instruments
and
implementatiions
policymakers
must
make
choices
about
the
appropriate
policy
intervention
tool(s)
to
use
the
intent
is
to
discover
the
most
effective
tool(s)
that
consume
the
least
resources
policymakers
can
employ
seven
major
policy
instrument
types
after
choosing
a
policy
instrument
the
details
for
employing
the
instrument
must
be
arranged
implementation
carries
risks
of
failing
in
various
ways
such
as
ineffectiveness
unacceptable
delays
and
excessive
costs
practices
that
improve
success
rates
include
setting
reasonable
expectations
allowing
adequate
time
and
sufficient
resources
having
clear
communication
and
u
nderstanding
policy
objectives
minimizing
the
number
of
approvals
simplifying
management
structures
and
aligning
all
relevant
groups
around
the
implementation
along
with
mechanisms
to
adapt
the
implementation
in
accord
with
subsequent
experience
to
correct
problems
and
take
advantage
of
new
opportunities
the
top-down
approach
involves
allowing
high-level
policymakers
set
objectives
and
define
implementation
strategies
lower
level
implementers
carry
out
the
policy
objectives
must
be
clearly
defined
and
the
implementation
tools
must
be
selected
based
on
the
implementation
strategy
policy
designers
need
to
assess
the
commitment
of
policy
implementers
who
could
be
teachers
police
officers
social
workers
or
private
sector
workers
one
example
of
the
top-down
approach
was
in
1973
when
the
us
congress
passed
a
policy
limiting
the
driving
speed
to
55 mph
on
america’s
freeways
under
the
national
maximum
speed
law
the
policy
objective
was
to
reduce
gasoline
consumption
in
addition
to
increased
travel
times
a
side
effect
was
the
reduction
of
freeway
fatalities
the
bottom-up
approach
helps
policymakers
to
evaluate
whether
policy
goals
are
open
to
more
than
one
interpretation
does
the
policy
implement
a
statute
or
reflect
rules
practices
and/or
norms
such
as
energy
policy
or
criminal
procedure?
are
the
policy
goals
internally
consistent?
how
will
the
policy
affect
the
activities
of
workers
who
directly
provide
services?
bottom-up
approaches
require
policymakers
to
involve
both
service
providers
and
service
recipients
in
refining
goals
strategies
and
activities
this
bottom-up
approach
starts
from
consumer-facing
bureaucrats
and
moves
up
to
the
top
policymakers
should
the
policy
face
pushback
policymakers
must
be
open
to
negotiations
for
a
compromise
approach
the
bottom-up
approach
emphasizes
low
level
policy
implementers
but
policy
learners
must
not
attempt
to
frustrate
the
goals
of
top
policymakers
in
america
the
no
child
left
behind
act
(nclb)
adopted
policyies
that
would
have
benefited
from
bottom-up
perspectives
when
nclb
was
passed
many
states
struggled
to
figure
out
what
was
required
all
states
had
to
get
their
education
plans
approved
by
us
department
of
education
once
the
education
plan
was
approved
each
state
had
to
incorporate
nclb
into
the
state's
framework
of
educational
governance
and
to
use
the
legislation
to
achieve
the
state's
own
goals
if
the
us
federal
government
had
consulted
with
each
state
about
its
education
policies
performances
and
future
goals
teachers
and
the
government
would
have
had
a
better
understanding
on
what
policy
objectives
were
achievable
policy
learning
has
not
been
embraced
in
some
countries
some
countries
that
were
once
colonized
fear
that
embracing
policies
recommended
by
outsiders
will
allow
other
countries
to
exploit
their
resources
rockefeller
(1966)
claimed
that
in
latin
america
in
the
early
1960s
free-market
policies
were
in
competition
with
communist
propaganda
in
latin
american
countries
at
the
time
american
business
were
claimed
to
be
exploiting
the
people
and
their
resources
however
companies
such
as
chase
manhattan
bank
launched
a
program
in
panama
to
improve
cattle
raising
by
training
ranchersr
to
follow
the
scientific
advances
of
seeding
feeding
and
breeding
cattle
more
effectively
this
process
improved
the
quality
of
beef
which
encouraged
higher
meat
consumption
improved
dietary
standards
and
made
panama
a
beef
exporter
european
countries
created
the
euro
to
simplify
trading
between
european
union
countries
adopting
the
euro
would
remove
currency
risk
and
the
cost
of
currency
conversion
and
provide
a
common
monetary
policy
among
members
policy
learning
took
places
as
more
european
countries
learned
that
joining
the
eurozone
would
give
them
access
to
other
markets
citizens
of
eu
member
countries
could
travel
to
other
eu
countries
within
the
schengen
area
without
transiting
a
border
checkpoint
the
learning
did
not
reach
all
policy
sectors
some
eu
countries
kept
their
budgets
in
near
balance
amid
strong
growth
and
employment
while
others'
budgets
were
so
far
out
of
balance
that
their
overall
debt
created
fears
about
their
ability
to
make
their
payments
index
of
articles
related
to
terms
of
service
and
privacy
policies
this
is
a
list
of
articles
about
terms
of
service
and
privacy
policies
these
are
also
called
terms
of
use
and
are
rules
one
must
agree
to
in
order
to
use
a
service
the
articles
fall
in
two
main
categories:
descriptions
of
terms
used
for
specific
companies
or
products
and
descriptions
of
different
kinds
of
terms
in
general
articles
on
companies
vary
widely
in
the
amount
of
detail
they
give
on
terms
of
service
annotations
show
what
is
available
in
the
article
on
each
company
and
need
to
be
updated
as
those
articles
are
improved
terms
of
service
are
regularly
the
subject
of
news
articles
throughout
the
english-language
press
such
as
in
the
us
uk
africa
india
singapore
and
australia
terms
of
service
are
also
addressed
in
a
widely
reviewed
documentary
academic
research
and
legal
research
imperative
mandate
the
imperative
mandate
is
a
political
system
in
which
"representatives
enact
policies
in
accordance
with
mandates
and
can
be
recalled
by
people’s
assemblies"
it
requires
a
context
in
which
"power
is
not
monopolized
by
the
state
but
distributed
in
a
plurality
of
municipalities
and
assemblies
with
specific
political
authority"
the
imperative
mandate
goes
back
to
the
middle
ages
it
was
embraced
by
the
revolutionary
assemblies
in
paris
in
1793
but
then
banned
by
the
royalist
members
of
the
french
national
assembly
of
1789
to
block
greater
influence
by
the
people
it
was
also
rejected
in
the
american
revolution
it
was
embraced
in
the
paris
commune
and
by
the
council
communism
movement
the
imperative
mandate
has
been
used
by
the
united
democratic
front
and
abahlali
basemjondolo
in
south
africa
as
well
as
the
zapatistas
in
mexico
wicked
problem
a
wicked
problem
is
a
problem
that
is
difficult
or
impossible
to
solve
because
of
incomplete
contradictory
and
changing
requirements
that
are
often
difficult
to
recognize
the
use
of
the
term
"wicked"
here
has
come
to
denote
resistance
to
resolution
rather
than
evil
another
definition
is
"a
problem
whose
social
complexity
means
that
it
has
no
determinable
stopping
point"
moreover
because
of
complex
interdependencies
the
effort
to
solve
one
aspect
of
a
wicked
problem
may
reveal
or
create
other
problems
the
phrase
was
originally
used
in
social
planning
its
modern
sense
was
introduced
in
1967
by
c
west
churchman
in
a
guest
editorial
churchman
wrote
in
the
journal
"management
science"
responding
to
a
previous
use
of
the
term
by
horst
rittel
churchman
discussed
the
moral
responsibility
of
operations
research
"to
inform
the
manager
in
what
respect
our
'solutions'
have
failed
to
tame
his
wicked
problems"
rittel
and
melvin
m
webber
formally
described
the
concept
of
wicked
problems
in
a
1973
treatise
contrasting
"wicked"
problems
with
relatively
"tame"
soluble
problems
in
mathematics
chess
or
puzzle
solving
rittel
and
webber's
1973
formulation
of
wicked
problems
in
social
policy
planning
specified
ten
characteristics:
conklin
later
generalized
the
concept
of
problem
wickedness
to
areas
other
than
planning
and
policy;
conklin's
defining
characteristics
are:
classic
examples
of
wicked
problems
include
economic
environmental
and
political
issues
a
problem
whose
solution
requires
a
great
number
of
people
to
change
their
mindsets
and
behavior
is
likely
to
be
a
wicked
problem
therefore
many
standard
examples
of
wicked
problems
come
from
the
areas
of
public
planning
and
policy
these
include
global
climate
change
natural
hazards
healthcare
the
aids
epidemic
pandemic
influenza
international
drug
trafficking
nuclear
weapons
nuclear
energy
waste
and
social
injustice
in
recent
years
problems
in
many
areas
have
been
identified
as
exhibiting
elements
of
wickedness;
examples
range
from
aspects
of
design
decision
making
and
knowledge
management
to
business
strategy
rittel
and
webber
coined
the
term
in
the
context
of
problems
of
social
policy
an
arena
in
which
a
purely
scientific-engineering
approach
cannot
be
applied
because
of
the
lack
of
a
clear
problem
definition
and
differing
perspectives
of
stakeholders
in
their
words
thus
wicked
problems
are
also
characterised
by
the
following:
although
rittel
and
webber
framed
the
concept
in
terms
of
social
policy
and
planning
wicked
problems
occur
in
any
domain
involving
stakeholders
with
differing
perspectives
recognising
this
rittel
and
kunz
developed
a
technique
called
issue-based
information
system
(ibis)
which
facilitates
documentation
of
the
rationale
behind
a
group
decision
in
an
objective
manner
a
recurring
theme
in
research
and
industry
literature
is
the
connection
between
wicked
problems
and
design
design
problems
are
typically
wicked
because
they
are
often
ill
defined
(no
prescribed
way
forward)
involve
stakeholders
with
different
perspectives
and
have
no
"right"
or
"optimal"
solution
thus
wicked
problems
cannot
be
solved
by
the
application
of
standard
(or
known)
methods;
they
demand
creative
solutions
wicked
problems
cannot
be
tackled
by
the
traditional
approach
in
which
problems
are
defined
analysed
and
solved
in
sequential
steps
the
main
reason
for
this
is
that
there
is
no
clear
problem
definition
of
wicked
problems
in
a
paper
published
in
2000
nancy
roberts
identified
the
following
strategies
to
cope
with
wicked
problems:
in
his
1972
paper
rittel
hints
at
a
collaborative
approach;
one
which
attempts
"to
make
those
people
who
are
being
affected
into
participants
of
the
planning
process
they
are
not
merely
asked
but
actively
involved
in
the
planning
process"
a
disadvantage
of
this
approach
is
that
achieving
a
shared
understanding
and
commitment
to
solving
a
wicked
problem
is
a
time-consuming
process
another
difficulty
is
that
in
some
matters
at
least
one
group
of
people
may
hold
an
absolute
belief
that
necessarily
contradicts
other
absolute
beliefs
held
by
other
groups
collaboration
then
becomes
impossible
until
one
set
of
beliefs
is
relativized
or
abandoned
entirely
research
over
the
last
two
decades
has
shown
the
value
of
computer-assisted
argumentation
techniques
in
improving
the
effectiveness
of
cross-stakeholder
communication
the
technique
of
dialogue
mapping
has
been
used
in
tackling
wicked
problems
in
organizations
using
a
collaborative
approach
more
recently
in
a
four-year
study
of
interorganizational
collaboration
across
public
private
and
voluntary
sectors
steering
by
government
was
found
to
perversely
undermine
a
successful
collaboration
producing
an
organizational
crisis
which
led
to
the
collapse
of
a
national
initiative
in
"wholesome
design
for
wicked
problems"
robert
knapp
stated
that
there
are
ways
forward
in
dealing
with
wicked
problems:
examining
networks
designed
to
tackle
wicked
problems
in
health
care
such
as
caring
for
older
people
or
reducing
sexually
transmitted
infections
ferlie
and
colleagues
suggest
that
managed
networks
may
be
the
"least
bad"
way
of
"making
wicked
problems
governable"
a
range
of
approaches
called
"problem
structuring
methods"
(psms)
have
been
developed
in
operations
research
since
the
1970s
to
address
problems
involving
complexity
uncertainty
and
conflict
psms
are
usually
used
by
a
group
of
people
in
collaboration
(rather
than
by
a
solitary
individual)
to
create
a
consensus
about
or
at
least
to
facilitate
negotiations
about
what
needs
to
change
some
widely
adopted
psms
include
soft
systems
methodology
the
strategic
choice
approach
and
strategic
options
development
and
analysis
(soda)
russell
l
ackoff
wrote
about
complex
problems
as
messes:
"every
problem
interacts
with
other
problems
and
is
therefore
part
of
a
set
of
interrelated
problems
a
system
of
problems
i
choose
to
call
such
a
system
a
mess"
extending
ackoff
robert
horn
says
that
"a
social
mess
is
a
set
of
interrelated
problems
and
other
messes
complexity—systems
of
systems—is
among
the
factors
that
makes
social
messes
so
resistant
to
analysis
and
more
importantly
to
resolution"
according
to
horn
the
defining
characteristics
of
a
social
mess
are:
e
f
schumacher
distinguishes
between
"divergent
and
convergent
problems"
in
his
book
"a
guide
for
the
perplexed"
convergent
problems
are
those
for
which
attempted
solutions
gradually
converge
on
one
solution
or
answer
divergent
problems
are
those
for
which
different
answers
appear
to
increasingly
contradict
each
other
all
the
more
they
are
elaborated
requiring
a
different
approach
involving
faculties
of
a
higher
order
like
love
and
empathy
in
1990
degrace
and
stahl
introduced
the
concept
of
wicked
problems
to
software
development
in
the
last
decade
other
computer
scientists
have
pointed
out
that
software
development
shares
many
properties
with
other
design
practices
(particularly
that
people-
process-
and
technology-problems
have
to
be
considered
equally)
and
have
incorporated
rittel's
concepts
into
their
software
design
methodologies
the
design
and
integration
of
complex
software-defined
services
that
use
the
web
(web
services)
can
be
construed
as
an
evolution
from
previous
models
of
software
design
and
therefore
becomes
a
wicked
problem
also
kelly
levin
benjamin
cashore
graeme
auld
and
steven
bernstein
introduced
the
distinction
between
"wicked
problems"
and
"super
wicked
problems"
in
a
2007
conference
paper
which
was
followed
by
a
2012
journal
article
in
"policy
sciences"
in
their
discussion
of
global
climate
change
they
define
super
wicked
problems
as
having
the
following
additional
characteristics:
while
the
items
that
define
a
wicked
problem
relate
to
the
problem
itself
the
items
that
define
a
super
wicked
problem
relate
to
the
agent
trying
to
solve
it
global
warming
is
a
super
wicked
problem
and
the
need
to
intervene
to
tend
to
our
longer
term
interests
has
also
been
taken
up
by
others
including
richard
lazarus
sexuality
policy
watch
sexuality
policy
watch
(spw)
is
a
global
forum
of
researchers
and
activists
working
on
sexual
rights
issues
and
policies
across
the
world
the
forum
was
launched
in
2002
as
the
international
working
group
on
sexuality
and
social
policy
(iwgssp)
but
changed
its
name
to
sexuality
policy
watch
in
2006
since
its
inception
spw
has
conducted
research
on
trends
in
sexuality
advocated
to
prevent
violence
against
women
built
partnerships
with
sexual
rights
groups
and
published
key
policy
analyses
thus
together
with
the
latin
american
committee
for
the
rights
of
women
/brazil
(cladem)
the
commission
for
citizenship
and
reproduction
(ccr)
promsex
-
center
for
the
promotion
and
defense
of
sexual
and
reproductive
rights
and
the
national
rapporteurship
for
the
human
right
to
sexual
and
reproductive
health
in
brazil
spw
published
a
report
investigating
press
claims
in
2012
about
the
earlier
forced
sterilisation
campaigns
in
peru
spw
is
hosted
at
the
brazilian
interdisciplinary
aids
association
or
associação
brasileira
interdisciplinar
de
aids
(abia)
in
rio
de
janeiro
brazil
the
spw
co-chairs
are
sonia
corrêa
from
brazil
and
richard
parker
from
usa
policy
a
policy
is
a
deliberate
system
of
principles
to
guide
decisions
and
achieve
rational
outcomes
a
policy
is
a
statement
of
intent
and
is
implemented
as
a
procedure
or
protocol
policies
are
generally
adopted
by
a
governance
body
within
an
organization
policies
can
assist
in
both
"subjective"
and
"objective"
decision
making
policies
to
assist
in
subjective
decision
making
usually
assist
senior
management
with
decisions
that
must
be
based
on
the
relative
merits
of
a
number
of
factors
and
as
a
result
are
often
hard
to
test
objectively
eg
work-life
balance
policy
in
contrast
policies
to
assist
in
objective
decision
making
are
usually
operational
in
nature
and
can
be
objectively
tested
eg
password
policy
the
term
may
apply
to
government
private
sector
organizations
and
groups
as
well
as
individuals
presidential
executive
orders
corporate
privacy
policies
and
parliamentary
rules
of
order
are
all
examples
of
policy
policy
differs
from
rules
or
law
while
law
can
compel
or
prohibit
behaviors
(eg
a
law
requiring
the
payment
of
taxes
on
income)
policy
merely
guides
actions
toward
those
that
are
most
likely
to
achieve
a
desired
outcome
policy
or
policy
study
may
also
refer
to
the
process
of
making
important
organizational
decisions
including
the
identification
of
different
alternatives
such
as
programs
or
spending
priorities
and
choosing
among
them
on
the
basis
of
the
impact
they
will
have
policies
can
be
understood
as
political
managerial
financial
and
administrative
mechanisms
arranged
to
reach
explicit
goals
in
public
corporate
finance
a
critical
accounting
policy
is
a
policy
for
a
firm/company
or
an
industry
that
is
considered
to
have
a
notably
high
subjective
element
and
that
has
a
material
impact
on
the
financial
statements
the
intended
effects
of
a
policy
vary
widely
according
to
the
organization
and
the
context
in
which
they
are
made
broadly
policies
are
typically
instituted
to
avoid
some
negative
effect
that
has
been
noticed
in
the
organization
or
to
seek
some
positive
benefit
corporate
purchasing
policies
provide
an
example
of
how
organizations
attempt
to
avoid
negative
effects
many
large
companies
have
policies
that
all
purchases
above
a
certain
value
must
be
performed
through
a
purchasing
process
by
requiring
this
standard
purchasing
process
through
policy
the
organization
can
limit
waste
and
standardize
the
way
purchasing
is
done
the
state
of
california
provides
an
example
of
benefit-seeking
policy
in
recent
years
the
numbers
of
hybrid
cars
in
california
has
increased
dramatically
in
part
because
of
policy
changes
in
federal
law
that
provided
usd
$1500
in
tax
credits
(since
phased
out)
as
well
as
the
use
of
high-occupancy
vehicle
lanes
to
hybrid
owners
(no
loew
hybrid
vehicles)
in
this
case
the
organization
(state
and/or
federal
government)
created
an
effect
(increased
ownership
and
use
of
hybrid
vehicles)
through
policy
(tax
breaks
highway
lanes)
policies
frequently
have
side
effects
or
unintended
consequences
because
the
environments
that
policies
seek
to
influence
or
manipulate
are
typically
complex
adaptive
systems
(eg
governments
societies
large
companies)
making
a
policy
change
can
have
counterintuitive
results
for
example
a
government
may
make
a
policy
decision
to
raise
taxes
in
hopes
of
increasing
overall
tax
revenue
depending
on
the
size
of
the
tax
increase
this
may
have
the
overall
effect
of
reducing
tax
revenue
by
causing
capital
flight
or
by
creating
a
rate
so
high
that
citizens
are
deterred
from
earning
the
money
that
is
taxed
(see
the
laffer
curve)
the
policy
formulation
process
theoretically
includes
an
attempt
to
assess
as
many
areas
of
potential
policy
impact
as
possible
to
lessen
the
chances
that
a
given
policy
will
have
unexpected
or
unintended
consequences
in
political
science
the
policy
cycle
is
a
tool
used
for
the
analyzing
of
the
development
of
a
policy
item
it
can
also
be
referred
to
as
a
"stagist
approach"
"stages
heuristic"
or
"stages
approach"
it
is
thus
a
rule
of
thumb
rather
than
the
actual
reality
of
how
policy
is
created
but
has
been
influential
in
how
political
scientists
looked
at
policy
in
general
it
was
developed
as
a
theory
from
harold
lasswell's
work
one
version
by
james
e
anderson
in
his
"public
policy-making"
(1974)
has
the
following
stages:
an
eight
step
policy
cycle
is
developed
in
detail
in
"the
australian
policy
handbook"
by
peter
bridgman
and
glyn
davis:
(now
with
catherine
althaus
in
its
4th
and
5th
editions)
the
althaus
bridgman
davis
model
is
heuristic
and
iterative
it
is
and
not
meant
to
be
or
predictive
policy
cycles
are
typically
characterized
as
adopting
a
classical
approach
and
tend
to
describe
processes
from
the
perspective
of
policy
decision
makers
accordingly
some
postpositivist
academics
challenge
cyclical
models
as
unresponsive
and
unrealistic
preferring
systemic
and
more
complex
models
they
consider
a
broader
range
of
actors
involved
in
the
policy
space
that
includes
civil
society
organisations
the
media
intellectuals
think
tanks
or
policy
research
institutes
corporations
lobbyists
etc
policies
are
typically
promulgated
through
official
written
documents
policy
documents
often
come
with
the
endorsement
or
signature
of
the
executive
powers
within
an
organization
to
legitimize
the
policy
and
demonstrate
that
it
is
considered
in
force
such
documents
often
have
standard
formats
that
are
particular
to
the
organization
issuing
the
policy
while
such
formats
differ
in
form
policy
documents
usually
contain
certain
standard
components
including
:
some
policies
may
contain
additional
sections
including:
the
american
political
scientist
theodore
j
lowi
proposed
four
types
of
policy
namely
distributive
redistributive
regulatory
and
constituent
in
his
article
'four
systems
of
policy
politics
and
choice'
and
in
'american
business
public
policy
case
studies
and
political
theory'
policy
addresses
the
intent
of
the
organization
whether
government
business
professional
or
voluntary
policy
is
intended
to
affect
the
'real'
world
by
guiding
the
decisions
that
are
made
whether
they
are
formally
written
or
not
most
organizations
have
identified
policies
policies
may
be
classified
in
many
different
ways
the
following
is
a
sample
of
several
different
types
of
policies
broken
down
by
their
effect
on
members
of
the
organization
distributive
policies
extend
goods
and
services
to
members
of
an
organization
as
well
as
distributing
the
costs
of
the
goods/services
amongst
the
members
of
the
organization
examples
include
government
policies
that
impact
spending
for
welfare
public
education
highways
and
public
safety
or
a
professional
organization's
benefits
plan
regulatory
policies
or
mandates
limit
the
discretion
of
individuals
and
agencies
or
otherwise
compel
certain
types
of
behavior
these
policies
are
generally
thought
to
be
best
applied
when
good
behavior
can
be
easily
defined
and
bad
behavior
can
be
easily
regulated
and
punished
through
fines
or
sanctions
an
example
of
a
fairly
successful
public
regulatory
policy
is
that
of
a
highway
speed
limit
constituent
policies
create
executive
power
entities
or
deal
with
laws
constituent
policies
also
deal
with
fiscal
policy
in
some
circumstances
policies
are
dynamic;
they
are
not
just
static
lists
of
goals
or
laws
policy
blueprints
have
to
be
implemented
often
with
unexpected
results
social
policies
are
what
happens
'on
the
ground'
when
they
are
implemented
as
well
as
what
happens
at
the
decision
making
or
legislative
stage
when
the
term
policy
is
used
it
may
also
refer
to:
the
actions
the
organization
actually
takes
may
often
vary
significantly
from
stated
policy
this
difference
is
sometimes
caused
by
political
compromise
over
policy
while
in
other
situations
it
is
caused
by
lack
of
policy
implementation
and
enforcement
implementing
policy
may
have
unexpected
results
stemming
from
a
policy
whose
reach
extends
further
than
the
problem
it
was
originally
crafted
to
address
additionally
unpredictable
results
may
arise
from
selective
or
idiosyncratic
enforcement
of
policy
types
of
policy
analysis
include:
these
qualifiers
can
be
combined
so
one
could
for
example
have
a
stationary-memoryless-index
policy
media
policy
media
policy
/
m
politics
is
a
term
describing
all
legislation
and
political
action
directed
towards
regulating
the
media
especially
mass
media
and
the
media
industry
those
actions
will
usually
be
prompted
by
pressures
from
public
opinion
or
from
industry
interest
groups
print
media
public
radio
and
television
broadcasting
mobile
communications
all
converge
in
the
digital
infrastructure
this
digitalisation
produces
markets
that
still
lack
consistent
and
rigorous
regulation
in
instances
where
regulations
exist
technical
innovations
outpace
and
overtake
existing
rules
and
give
rise
to
illegal
activities
like
copyright
violations
this
has
to
be
dealt
with
to
defend
intellectual
property
rights
(see
eg
digital
economy
act
2010)
media
politics
is
the
subject
of
studies
in
media
research
and
cultural
studies
liberal
media
policy
is
adversely
affected
by
the
fact
that
political
success
itself
hinges
critically
on
favorable
comments
in
the
media
see
politico-media
complex
reshaping
cultural
policies
reshaping
cultural
policies
(styled
as
re|shaping
cultural
policies)
is
a
report
series
published
by
unesco
which
monitors
the
implementation
of
the
unesco
convention
on
the
protection
and
promotion
of
the
diversity
of
cultural
expressions
(2005)
the
2005
unesco
convention
encourages
its
146
parties
to
introduce
policies
for
culture
within
a
global
context
and
commitment
to
protect
and
promote
the
diversity
of
cultural
expressions
the
second
and
most
recent
report
(2018)
subtitled
“advancing
creativity
for
development”
follows
the
first
report
(2015)
with
the
subtitle
“a
decade
promoting
the
diversity
of
cultural
expressions
for
development”
primarily
the
report
series
draws
on
reports
of
all
parties
to
the
convention
submitted
every
four
years
in
which
they
present
and
describe
the
actions
they
have
taken
in
order
to
implement
the
convention
these
reports
are
called
quadrennial
periodic
reports
(qprs)
in
addition
the
report
series
includes
the
analysis
of
other
both
governmental
and
non-governmental
sources
in
general
the
report
investigates
how
implementing
the
convention
reshapes
cultural
policies
additionally
it
provides
evidence
of
how
the
implementation
process
contributes
to
attaining
the
united
nations
2030
sustainable
development
goals
(sdgs)
to
end
poverty
protect
the
planet
and
ensure
prosperity
for
every
human
being
the
report
series
also
analyses
trends
and
issues
concerning
the
creative
economy
which
currently
is
worth
$2250
billion
and
employs
30
million
people
worldwide
the
report
puts
forward
a
set
of
policy
recommendations
for
the
future
addressing
the
adaptation
of
cultural
policies
to
rapid
change
in
the
digital
environment
based
on
human
rights
and
fundamental
freedoms
of
expression
“each
report
is
not
an
end-result
but
a
tool
to
be
used
in
a
long-term
process
that
includes
the
forging
of
spaces
for
policy
dialogue
reinforcing
stakeholders’
capacities
to
work
together
to
generate
data
and
information
and
advocate
for
policy
innovation
both
nationally
and
globally”
the
reports
are
published
in
english
french
spanish
russian
portuguese
arabic
chinese
indonesian
vietnamese
and
german
unesco
is
the
lead
institutional
author
of
the
global
report
series
and
coordinates
a
broader
network
of
independent
experts
who
author
chapters
in
line
with
the
parties’
quadrennial
periodic
reporting
the
series
is
produced
every
four
years
the
first
cycle
spanned
the
years
2012-2015
and
the
second
runs
from
2016
to
2019
accordingly
the
third
publication
will
take
place
in
december
2021
the
2005
convention
is
an
international
standard
setting
instrument
providing
a
framework
for
the
governance
of
culture
in
this
context
governance
of
culture
refers
to
policies
and
measures
governments
establish
to
regulate
to
promote
and
to
protect
all
forms
of
creativity
and
artistic
expressions
the
most
recent
unesco
convention
in
the
field
of
culture
and
ratified
by
146
parties
it
is
the
first
international
legal
tool
to
encourage
governments
to
invest
in
creativity
it
frames
the
formulation
and
implementation
of
different
types
of
legislative
regulatory
institutional
and
financial
interventions
to
promote
the
emergence
of
dynamic
cultural
and
creative
industry
sectors
around
the
world
within
the
context
of
the
2005
convention
the
diversity
of
cultural
expressions
"″"refers
to
the
manifold
ways
in
which
the
cultures
of
groups
and
societies
find
expression
these
expressions
are
passed
on
within
and
among
groups
and
societies"″"
specifically
the
convention
understands
cultural
expressions
as
all
forms
of
creativity
and
artistic
expressions
such
as
in
cinema/audiovisual
arts
design
digital
arts
music
performing
arts
publishing
and
the
visual
arts
the
2005
convention
was
"since
its
beginnings
permeated
by
a
material
and
economic
perspective
of
cultural
expressions
focused
on
the
production
and
consumption
of
cultural
goods
and
services
with
a
view
to
promote
more
balanced
exchanges
and
sustainable
development
that
takes
into
account
cultural
diversity
concerns"
the
implementation
of
the
2005
convention
aims
to
contribute
to
achieving
several
sustainable
development
goals
(sdgs)
precisely
sdg
4
(quality
education)
sdg
5
(gender
equality)
sdg
8
(decent
work
and
economic
growth)
sdg
10
(reduced
inequalities)
sdg
16
(peace
justice
and
strong
institutions)
and
sdg
17
(partnerships
for
the
goals)
the
implementation
process
identifies
investing
in
creativity
as
a
priority
for
sustainable
development
at
the
global
level
the
convention
calls
for
countries
to
provide
financial
assistance
for
creativity
through
their
official
development
assistance
(oda)
by
investing
in
the
convention’s
international
fund
for
cultural
diversity
additionally
unesco
through
the
2005
convention
offers
technical
assistance
to
strengthen
human
and
institutional
capacities
in
developing
countries
the
director
general
of
unesco
audrey
azoulay
referring
to
the
unesco
general
conference's
conviction
that
cultural
activities
goods
and
services
have
both
an
economic
and
a
cultural
nature
stated:
″[c]ulture
is
not
a
commodity:
it
carries
values
and
identities
it
gives
markers
to
live
together
in
a
globalized
world
our
role
is
to
encourage
question
collect
data
to
understand
and
energize
creative
channels
to
encourage
the
mobility
of
artists
to
stimulate
a
rapidly
changing
sector
in
the
new
digital
environment″
annika
markovic
ambassador
and
permanent
delegate
of
sweden
to
unesco
in
2018
claimed
that
the
report
is
"“the
only
global
document
that
presents
an
overview
of
cultural
development
world-wide
and
monitors
state
action
to
protect
and
promote
the
diversity
of
cultural
expressions
at
all
levels”"
the
following
aspects
thematically
summarize
the
core
findings
identified
by
the
2018
report
with
regard
to
the
implementation
of
the
2005
unesco
convention
for
the
first
time
national
development
plans
and
strategies
integrate
culture
mainly
of
countries
in
the
global
south
as
a
result
cities
seem
to
invest
more
and
more
in
cultural
industries
for
development
the
un’s
2030
agenda
recognized
the
role
of
creativity
in
sustainable
development
in
the
implementation
of
the
sdgs
however
the
share
of
development
aid
spent
on
culture
today
is
the
lowest
it
has
been
in
over
10
years
according
to
the
report
digital
revenues
make
up
50%
of
the
recorded
music
market
growing
almost
18%
over
the
past
year
due
to
a
sharp
increase
in
the
share
of
streaming
revenues
the
report
states
that
the
internet
transforms
the
cultural
value
chain
into
a
network
platform
e-commerce
challenges
both
culture
and
trade
policies
that
intend
to
promote
the
diversity
of
cultural
expressions
it
articulates
the
urgency
to
improve
data
collection
on
revenues
generated
through
digital
channels
in
order
to
design
better
policies
and
negotiate
fair
trade
agreements
the
report
claims
that
monitoring
the
relationship
between
large
platforms
big
data
artificial
intelligence
and
the
diversity
of
cultural
expressions
is
crucial
to
ensure
that
a
variety
of
distribution
platforms
and
providers
promote
and
protect
future
artistic
creations
as
informed
by
the
report
attacks
against
artists
have
increased
in
the
past
years
including
in
the
digital
environment
where
surveillance
and
online
trolling
pose
new
threats
to
artistic
freedom
in
2016
430
cases
were
reported
around
the
world
(compared
to
340
in
2015
and
90
in
2014)
musicians
are
the
most
threatened
group
while
authors
also
often
become
a
target
in
2016
attacks
against
authors
occurred
most
often
in
the
asia-pacific
region
(80
cases)
the
middle
east
and
north
africa
(51
cases)
and
europe
(47
cases)
the
report
reveals
that
meanwhile
there
exists
an
increased
awareness
with
regard
to
such
threats
leading
to
a
larger
number
of
initiatives
to
support
the
social
and
economic
rights
of
artists
particularly
in
african
countries
while
there
exists
legal
action
to
affirm
the
freedom
of
expression
for
artists
other
laws
addressing
terrorism
and
state
security
repress
artistic
expressions
the
report
states
that
half
of
the
persons
working
in
the
cultural
and
creative
industries
are
female
however
a
gender
gap
persists
worldwide
concerning
equal
pay
access
to
funding
and
prices
charged
for
creative
works
consequently
women
remain
under-represented
in
key
creative
roles
and
are
outnumbered
in
decision-making
positions
women
make
up
only
34%
of
ministers
for
culture
(compared
to
24%
in
2005)
and
only
31%
of
national
arts
program
directors
generally
women
are
represented
in
specific
cultural
fields
such
as
arts
education
and
training
(60%)
book
publishing
and
press
(54%)
audiovisual
and
interactive
media
(26%)
as
well
as
design
and
creative
services
(33%)
the
report
demonstrates
that
predominantly
restrictions
in
terms
of
mobility
represent
great
challenges
to
persons
pursuing
careers
in
the
cultural
and
creative
industries
specifically
to
those
from
the
global
south
it
reveals
that
a
holder
of
a
german
passport
can
travel
to
176
countries
without
a
visa
while
a
holder
of
an
afghan
passport
can
only
travel
to
24
countries
without
a
visa
as
a
matter
of
fact
artists
and
cultural
professionals
need
to
travel
to
perform
to
reach
new
audiences
or
to
attend
a
residency
or
to
engage
in
networking
the
report
exposes
that
travel
restrictions
including
difficulties
in
obtaining
visas
oftentimes
impedes
artists
from
the
global
south
to
participate
in
art
biennales
or
film
festivals
even
when
invited
to
receive
an
award
or
to
promote
their
works
as
stated
in
the
report
the
2005
convention
provides
legitimacy
for
the
formulation
of
cultural
policies
and
their
adaptation
to
changing
circumstances
and
needs
the
report
underscores
that
collaborative
governance
and
multi-stakeholder
policy
making
have
progressed
notably
in
some
developing
countries
particularly
in
the
creative
economy
and
cultural
education
as
a
result
parties
to
the
convention
have
made
considerable
progress
in
fostering
digital
arts
creation
supporting
creative
entrepreneurship
accelerating
the
modernization
of
cultural
sectors
promoting
distribution
and
updating
copyright
legislation
however
the
report
also
reveals
a
lack
in
civil
society
participation
in
policy
making
it
underlines
the
urgency
for
more
effort
to
ensure
the
creation
of
open
transparent
and
participatory
policy
processes
in
order
to
involve
civil
society
participation
in
policy
making
in
accordance
with
the
report
the
2005
convention
formally
recognizes
that
cultural
goods
and
services
not
only
have
important
economic
value
but
also
convey
identities
meanings
and
values
as
a
consequence
at
least
eight
bilateral
and
regional
free
trade
agreements
concluded
between
2015
and
2017
have
introduced
cultural
clauses
or
list
of
commitments
that
promote
the
objectives
and
principles
of
the
2005
convention
despite
the
lack
of
the
promotion
of
the
objectives
and
principles
of
the
2005
convention
with
regard
to
the
negotiation
of
mega-regional
partnership
agreements
some
parties
to
the
trans
pacific
partnership
(ttp)
have
succeeded
in
introducing
important
cultural
reservations
to
protect
and
promote
the
diversity
of
cultural
expressions
the
report’s
primary
objective
is
“to
provide
key
actors
with
better
knowledge
on
how
to
support
evidence-based
policy
and
to
strengthen
informed
transparent
and
participatory
systems
of
governance
for
culture”
it
aims
to
motivate
governments
and
civil
society
actors
to
integrate
findings
and
recommendations
into
their
national
cultural
policy
and
development
strategies
and
frameworks
following
the
findings
presented
above
the
implementation
of
the
2005
convention
"introduce[s]
a
range
of
different
policy
strategies
for
integrating
culture
into
development
processes"
and
culture
is
increasingly
regarded
as
"an
economic
asset
in
pursuing
sustainable
development"
based
on
its
analysis
and
findings
the
global
report
of
2018
suggests
the
following
road
map
for
the
parties
to
the
2005
convention
accordingly
parties
could
tackle
major
challenges
in
the
implementation
of
the
2005
convention
by:
speaking
about
the
visibility
of
the
progress
in
cultural
policies
shown
by
the
report
series
bárbara
lovrinić
stated
that
"“[u]nfortunately
where
unesco
is
concerned
there
is
a
lack
of
promotion
in
the
media
in
general
in
the
long
term
the
report
could
have
a
positive
impact
on
these
issues
which
would
be
enhanced
if
the
public
were
made
more
aware
of
such
work”"
she
also
points
out
that
there
is
""a
risk
that
many
people
will
not
dwell
on
the
2005
convention
and
the
sustainable
development
goals
unless
they
are
already
somewhat
familiar
with
the
topic”"
with
reference
to
the
title
of
the
report
series
she
concludes
that
""cultural
policy-making
is
still
far
from
being
reshaped
for
it
takes
a
serious
amount
of
time
to
yield
valuable
results”"
outline
of
education
the
following
outline
is
provided
as
an
overview
of
and
topical
guide
to
education:
education
–
in
the
general
sense
is
any
act
or
experience
that
has
a
formative
effect
on
the
mind
character
or
physical
ability
of
an
individual
in
its
technical
sense
education
is
the
process
by
which
society
deliberately
transmits
its
accumulated
knowledge
skills
and
values
from
one
generation
to
another
education
can
also
be
defined
as
the
process
of
becoming
an
educated
person
history
of
education
education
education
is
the
process
of
facilitating
learning
or
the
acquisition
of
knowledge
skills
values
beliefs
and
habits
educational
methods
include
storytelling
discussion
teaching
training
and
directed
research
education
frequently
takes
place
under
the
guidance
of
educators
and
also
learners
may
also
educate
themselves
education
can
take
place
in
formal
or
informal
settings
and
any
experience
that
has
a
formative
effect
on
the
way
one
thinks
feels
or
acts
may
be
considered
educational
the
methodology
of
teaching
is
called
pedagogy
formal
education
is
commonly
divided
formally
into
such
stages
as
preschool
or
kindergarten
primary
school
secondary
school
and
then
college
university
or
apprenticeship
a
right
to
education
has
been
recognized
by
some
governments
and
the
united
nations
in
most
regions
education
is
compulsory
up
to
a
certain
age
etymologically
the
word
"education"
is
derived
from
the
latin
word
"ēducātiō"
("a
breeding
a
bringing
up
a
rearing")
from
"ēducō"
("i
educate
i
train")
which
is
related
to
the
homonym
"ēdūcō"
("i
lead
forth
i
take
out;
i
raise
up
i
erect")
from
"ē-"
("from
out
of")
and
"dūcō"
("i
lead
i
conduct")
education
began
in
prehistory
as
adults
trained
the
young
in
the
knowledge
and
skills
deemed
necessary
in
their
society
in
pre-literate
societies
this
was
achieved
orally
and
through
imitation
story-telling
passed
knowledge
values
and
skills
from
one
generation
to
the
next
as
cultures
began
to
extend
their
knowledge
beyond
skills
that
could
be
readily
learned
through
imitation
formal
education
developed
schools
existed
in
egypt
at
the
time
of
the
middle
kingdom
plato
founded
the
academy
in
athens
the
first
institution
of
higher
learning
in
europe
the
city
of
alexandria
in
egypt
established
in
330
bce
became
the
successor
to
athens
as
the
intellectual
cradle
of
ancient
greece
there
the
great
library
of
alexandria
was
built
in
the
3rd
century
bce
european
civilizations
suffered
a
collapse
of
literacy
and
organization
following
the
fall
of
rome
in
ce
476
in
china
confucius
(551–479
bce)
of
the
state
of
lu
was
the
country's
most
influential
ancient
philosopher
whose
educational
outlook
continues
to
influence
the
societies
of
china
and
neighbours
like
korea
japan
and
vietnam
confucius
gathered
disciples
and
searched
in
vain
for
a
ruler
who
would
adopt
his
ideals
for
good
governance
but
his
analects
were
written
down
by
followers
and
have
continued
to
influence
education
in
east
asia
into
the
modern
era
the
aztecs
also
had
a
well-developed
theory
about
education
which
has
an
equivalent
word
in
nahuatl
called
"tlacahuapahualiztli"
it
means
"the
art
of
raising
or
educating
a
person"
or
"the
art
of
strengthening
or
bringing
up
men"
this
was
a
broad
conceptualization
of
education
which
prescribed
that
it
begins
at
home
supported
by
formal
schooling
and
reinforced
by
community
living
historians
cite
that
formal
education
was
mandatory
for
everyone
regardless
of
social
class
and
gender
there
was
also
the
word
"neixtlamachiliztli"
which
is
"the
act
of
giving
wisdom
to
the
face"
these
concepts
underscore
a
complex
set
of
educational
practices
which
was
oriented
towards
communicating
to
the
next
generation
the
experience
and
intellectual
heritage
of
the
past
for
the
purpose
of
individual
development
and
his
integration
into
the
community
after
the
fall
of
rome
the
catholic
church
became
the
sole
preserver
of
literate
scholarship
in
western
europe
the
church
established
cathedral
schools
in
the
early
middle
ages
as
centres
of
advanced
education
some
of
these
establishments
ultimately
evolved
into
medieval
universities
and
forebears
of
many
of
europe's
modern
universities
during
the
high
middle
ages
chartres
cathedral
operated
the
famous
and
influential
chartres
cathedral
school
the
medieval
universities
of
western
christendom
were
well-integrated
across
all
of
western
europe
encouraged
freedom
of
inquiry
and
produced
a
great
variety
of
fine
scholars
and
natural
philosophers
including
thomas
aquinas
of
the
university
of
naples
robert
grosseteste
of
the
university
of
oxford
an
early
expositor
of
a
systematic
method
of
scientific
experimentation
and
saint
albert
the
great
a
pioneer
of
biological
field
research
founded
in
1088
the
university
of
bologne
is
considered
the
first
and
the
oldest
continually
operating
university
elsewhere
during
the
middle
ages
islamic
science
and
mathematics
flourished
under
the
islamic
caliphate
which
was
established
across
the
middle
east
extending
from
the
iberian
peninsula
in
the
west
to
the
indus
in
the
east
and
to
the
almoravid
dynasty
and
mali
empire
in
the
south
the
renaissance
in
europe
ushered
in
a
new
age
of
scientific
and
intellectual
inquiry
and
appreciation
of
ancient
greek
and
roman
civilizations
around
1450
johannes
gutenberg
developed
a
printing
press
which
allowed
works
of
literature
to
spread
more
quickly
the
european
age
of
empires
saw
european
ideas
of
education
in
philosophy
religion
arts
and
sciences
spread
out
across
the
globe
missionaries
and
scholars
also
brought
back
new
ideas
from
other
civilizations –
as
with
the
jesuit
china
missions
who
played
a
significant
role
in
the
transmission
of
knowledge
science
and
culture
between
china
and
europe
translating
works
from
europe
like
euclid's
elements
for
chinese
scholars
and
the
thoughts
of
confucius
for
european
audiences
the
enlightenment
saw
the
emergence
of
a
more
secular
educational
outlook
in
europe
in
most
countries
today
full-time
education
whether
at
school
or
otherwise
is
compulsory
for
all
children
up
to
a
certain
age
due
to
this
the
proliferation
of
compulsory
education
combined
with
population
growth
unesco
has
calculated
that
in
the
next
30 years
more
people
will
receive
formal
education
than
in
all
of
human
history
thus
far
formal
education
occurs
in
a
structured
environment
whose
explicit
purpose
is
teaching
students
usually
formal
education
takes
place
in
a
school
environment
with
classrooms
of
multiple
students
learning
together
with
a
trained
certified
teacher
of
the
subject
most
school
systems
are
designed
around
a
set
of
values
or
ideals
that
govern
all
educational
choices
in
that
system
such
choices
include
curriculum
organizational
models
design
of
the
physical
learning
spaces
(eg
classrooms)
student-teacher
interactions
methods
of
assessment
class
size
educational
activities
and
more
preschools
provide
education
from
ages
approximately
three
to
seven
depending
on
the
country
when
children
enter
primary
education
these
are
also
known
as
nursery
schools
and
as
kindergarten
except
in
the
us
where
kindergarten
is
a
term
often
used
to
describe
the
earliest
levels
of
primary
education
kindergarten
"provide[s]
a
child-centred
preschool
curriculum
for
three-
to
seven-year-old
children
that
aim[s]
at
unfolding
the
child's
physical
intellectual
and
moral
nature
with
balanced
emphasis
on
each
of
them"
primary
(or
elementary)
education
consists
of
the
first
five
to
seven
years
of
formal
structured
education
in
general
primary
education
consists
of
six
to
eight
years
of
schooling
starting
at
the
age
of
five
or
six
although
this
varies
between
and
sometimes
within
countries
globally
around
89%
of
children
aged
six
to
twelve
are
enrolled
in
primary
education
and
this
proportion
is
rising
under
the
education
for
all
programs
driven
by
unesco
most
countries
have
committed
to
achieving
universal
enrollment
in
primary
education
by
2015
and
in
many
countries
it
is
compulsory
the
division
between
primary
and
secondary
education
is
somewhat
arbitrary
but
it
generally
occurs
at
about
eleven
or
twelve
years
of
age
some
education
systems
have
separate
middle
schools
with
the
transition
to
the
final
stage
of
secondary
education
taking
place
at
around
the
age
of
fourteen
schools
that
provide
primary
education
are
mostly
referred
to
as
"primary
schools
"or
"elementary
schools"
primary
schools
are
often
subdivided
into
infant
schools
and
junior
school
in
india
for
example
compulsory
education
spans
over
twelve
years
with
eight
years
of
elementary
education
five
years
of
primary
schooling
and
three
years
of
upper
primary
schooling
various
states
in
the
republic
of
india
provide
12
years
of
compulsory
school
education
based
on
a
national
curriculum
framework
designed
by
the
national
council
of
educational
research
and
training
in
most
contemporary
educational
systems
of
the
world
secondary
education
comprises
the
formal
education
that
occurs
during
adolescence
it
is
characterized
by
transition
from
the
typically
compulsory
comprehensive
primary
education
for
minors
to
the
optional
selective
tertiary
"postsecondary"
or
"higher"
education
(eg
university
vocational
school)
for
adults
depending
on
the
system
schools
for
this
period
or
a
part
of
it
may
be
called
secondary
or
high
schools
gymnasiums
lyceums
middle
schools
colleges
or
vocational
schools
the
exact
meaning
of
any
of
these
terms
varies
from
one
system
to
another
the
exact
boundary
between
primary
and
secondary
education
also
varies
from
country
to
country
and
even
within
them
but
is
generally
around
the
seventh
to
the
tenth
year
of
schooling
secondary
education
occurs
mainly
during
the
teenage
years
in
the
united
states
canada
and
australia
primary
and
secondary
education
together
are
sometimes
referred
to
as
k-12
education
and
in
new
zealand
year
1–13
is
used
the
purpose
of
secondary
education
can
be
to
give
common
knowledge
to
prepare
for
higher
education
or
to
train
directly
in
a
profession
secondary
education
in
the
united
states
did
not
emerge
until
1910
with
the
rise
of
large
corporations
and
advancing
technology
in
factories
which
required
skilled
workers
in
order
to
meet
this
new
job
demand
high
schools
were
created
with
a
curriculum
focused
on
practical
job
skills
that
would
better
prepare
students
for
white
collar
or
skilled
blue
collar
work
this
proved
beneficial
for
both
employers
and
employees
since
the
improved
human
capital
lowered
costs
for
the
employer
while
skilled
employees
received
higher
wages
secondary
education
has
a
longer
history
in
europe
where
grammar
schools
or
academies
date
from
as
early
as
the
16th
century
in
the
form
of
public
schools
fee-paying
schools
or
charitable
educational
foundations
which
themselves
date
even
further
back
community
colleges
offer
another
option
at
this
transitional
stage
of
education
they
provide
nonresidential
junior
college
courses
to
people
living
in
a
particular
area
higher
education
also
called
tertiary
third
stage
or
postsecondary
education
is
the
non-compulsory
educational
level
that
follows
the
completion
of
a
school
such
as
a
high
school
or
secondary
school
tertiary
education
is
normally
taken
to
include
undergraduate
and
postgraduate
education
as
well
as
vocational
education
and
training
colleges
and
universities
mainly
provide
tertiary
education
collectively
these
are
sometimes
known
as
tertiary
institutions
individuals
who
complete
tertiary
education
generally
receive
certificates
diplomas
or
academic
degrees
higher
education
typically
involves
work
towards
a
degree-level
or
foundation
degree
qualification
in
most
developed
countries
a
high
proportion
of
the
population
(up
to
50%)
now
enter
higher
education
at
some
time
in
their
lives
higher
education
is
therefore
very
important
to
national
economies
both
as
a
significant
industry
in
its
own
right
and
as
a
source
of
trained
and
educated
personnel
for
the
rest
of
the
economy
university
education
includes
teaching
research
and
social
services
activities
and
it
includes
both
the
undergraduate
level
(sometimes
referred
to
as
tertiary
education)
and
the
graduate
(or
postgraduate)
level
(sometimes
referred
to
as
graduate
school)
some
universities
are
composed
of
several
colleges
one
type
of
university
education
is
a
liberal
arts
education
which
can
be
defined
as
a
"college
or
university
curriculum
aimed
at
imparting
broad
general
knowledge
and
developing
general
intellectual
capacities
in
contrast
to
a
professional
vocational
or
technical
curriculum"
although
what
is
known
today
as
liberal
arts
education
began
in
europe
the
term
"liberal
arts
college"
is
more
commonly
associated
with
institutions
in
the
united
states
such
as
williams
college
or
barnard
college
vocational
education
is
a
form
of
education
focused
on
direct
and
practical
training
for
a
specific
trade
or
craft
vocational
education
may
come
in
the
form
of
an
apprenticeship
or
internship
as
well
as
institutions
teaching
courses
such
as
carpentry
agriculture
engineering
medicine
architecture
and
the
arts
in
the
past
those
who
were
disabled
were
often
not
eligible
for
public
education
children
with
disabilities
were
repeatedly
denied
an
education
by
physicians
or
special
tutors
these
early
physicians
(people
like
itard
seguin
howe
gallaudet)
set
the
foundation
for
special
education
today
they
focused
on
individualized
instruction
and
functional
skills
in
its
early
years
special
education
was
only
provided
to
people
with
severe
disabilities
but
more
recently
it
has
been
opened
to
anyone
who
has
experienced
difficulty
learning
while
considered
"alternative"
today
most
alternative
systems
have
existed
since
ancient
times
after
the
public
school
system
was
widely
developed
beginning
in
the
19th
century
some
parents
found
reasons
to
be
discontented
with
the
new
system
alternative
education
developed
in
part
as
a
reaction
to
perceived
limitations
and
failings
of
traditional
education
a
broad
range
of
educational
approaches
emerged
including
alternative
schools
self
learning
homeschooling
and
unschooling
example
alternative
schools
include
montessori
schools
waldorf
schools
(or
steiner
schools)
friends
schools
sands
school
summerhill
school
walden's
path
the
peepal
grove
school
sudbury
valley
school
krishnamurti
schools
and
open
classroom
schools
charter
schools
are
another
example
of
alternative
education
which
have
in
the
recent
years
grown
in
numbers
in
the
us
and
gained
greater
importance
in
its
public
education
system
in
time
some
ideas
from
these
experiments
and
paradigm
challenges
may
be
adopted
as
the
norm
in
education
just
as
friedrich
fröbel's
approach
to
early
childhood
education
in
19th-century
germany
has
been
incorporated
into
contemporary
kindergarten
classrooms
other
influential
writers
and
thinkers
have
included
the
swiss
humanitarian
johann
heinrich
pestalozzi;
the
american
transcendentalists
amos
bronson
alcott
ralph
waldo
emerson
and
henry
david
thoreau;
the
founders
of
progressive
education
john
dewey
and
francis
parker;
and
educational
pioneers
such
as
maria
montessori
and
rudolf
steiner
and
more
recently
john
caldwell
holt
paul
goodman
frederick
mayer
george
dennison
and
ivan
illich
indigenous
education
refers
to
the
inclusion
of
indigenous
knowledge
models
methods
and
content
within
formal
and
non-formal
educational
systems
often
in
a
post-colonial
context
the
growing
recognition
and
use
of
indigenous
education
methods
can
be
a
response
to
the
erosion
and
loss
of
indigenous
knowledge
and
language
through
the
processes
of
colonialism
furthermore
it
can
enable
indigenous
communities
to
"reclaim
and
revalue
their
languages
and
cultures
and
in
so
doing
improve
the
educational
success
of
indigenous
students"
informal
learning
is
one
of
three
forms
of
learning
defined
by
the
organisation
for
economic
co-operation
and
development
(oecd)
informal
learning
occurs
in
a
variety
of
places
such
as
at
home
work
and
through
daily
interactions
and
shared
relationships
among
members
of
society
for
many
learners
this
includes
language
acquisition
cultural
norms
and
manners
in
informal
learning
there
is
often
a
reference
person
a
peer
or
expert
to
guide
the
learner
if
learners
have
a
personal
interest
in
what
they
are
informally
being
taught
learners
tend
to
expand
their
existing
knowledge
and
conceive
new
ideas
about
the
topic
being
learned
for
example
a
museum
is
traditionally
considered
an
informal
learning
environment
as
there
is
room
for
free
choice
a
diverse
and
potentially
non-standardized
range
of
topics
flexible
structures
socially
rich
interaction
and
no
externally
imposed
assessments
while
informal
learning
often
takes
place
outside
educational
establishments
and
does
not
follow
a
specified
curriculum
it
can
also
occur
within
educational
settings
and
even
during
formal
learning
situations
educators
can
structure
their
lessons
to
directly
utilize
their
students
informal
learning
skills
within
the
education
setting
in
the
late
19th
century
education
through
play
began
to
be
recognized
as
making
an
important
contribution
to
child
development
in
the
early
20th
century
the
concept
was
broadened
to
include
young
adults
but
the
emphasis
was
on
physical
activities
lp
jacks
also
an
early
proponent
of
lifelong
learning
described
education
through
recreation:
"a
master
in
the
art
of
living
draws
no
sharp
distinction
between
his
work
and
his
play
his
labour
and
his
leisure
his
mind
and
his
body
his
education
and
his
recreation
he
hardly
knows
which
is
which
he
simply
pursues
his
vision
of
excellence
through
whatever
he
is
doing
and
leaves
others
to
determine
whether
he
is
working
or
playing
to
himself
he
always
seems
to
be
doing
both
enough
for
him
that
he
does
it
well"
education
through
recreation
is
the
opportunity
to
learn
in
a
seamless
fashion
through
all
of
life's
activities
the
concept
has
been
revived
by
the
university
of
western
ontario
to
teach
anatomy
to
medical
students
autodidacticism
(also
autodidactism)
is
a
term
used
to
describe
self-directed
learning
one
may
become
an
autodidact
at
nearly
any
point
in
one's
life
notable
autodidacts
include
abraham
lincoln
(us
president)
srinivasa
ramanujan
(mathematician)
michael
faraday
(chemist
and
physicist)
charles
darwin
(naturalist)
thomas
alva
edison
(inventor)
tadao
ando
(architect)
george
bernard
shaw
(playwright)
frank
zappa
(composer
recording
engineer
film
director)
and
leonardo
da
vinci
(engineer
scientist
mathematician)
many
large
university
institutions
are
now
starting
to
offer
free
or
almost
free
full
courses
such
as
harvard
mit
and
berkeley
teaming
up
to
form
edx
other
universities
offering
open
education
are
prestigious
private
universities
such
as
stanford
princeton
duke
johns
hopkins
the
university
of
pennylvania
and
caltech
as
well
as
notable
public
universities
including
tsinghua
peking
edinburgh
university
of
michigan
and
university
of
virginia
open
education
has
been
called
the
biggest
change
in
the
way
people
learn
since
the
printing
press
despite
favourable
studies
on
effectiveness
many
people
may
still
desire
to
choose
traditional
campus
education
for
social
and
cultural
reasons
many
open
universities
are
working
to
have
the
ability
to
offer
students
standardized
testing
and
traditional
degrees
and
credentials
the
conventional
merit-system
degree
is
currently
not
as
common
in
open
education
as
it
is
in
campus
universities
although
some
open
universities
do
already
offer
conventional
degrees
such
as
the
open
university
in
the
united
kingdom
presently
many
of
the
major
open
education
sources
offer
their
own
form
of
certificate
due
to
the
popularity
of
open
education
these
new
kind
of
academic
certificates
are
gaining
more
respect
and
equal
"academic
value"
to
traditional
degrees
out
of
182
colleges
surveyed
in
2009
nearly
half
said
tuition
for
online
courses
was
higher
than
for
campus-based
ones
a
recent
meta-analysis
found
that
online
and
blended
educational
approaches
had
better
outcomes
than
methods
that
used
solely
face-to-face
interaction
the
education
sector
or
education
system
is
a
group
of
institutions
(ministries
of
education
local
educational
authorities
teacher
training
institutions
schools
universities
etc)
whose
primary
purpose
is
to
provide
education
to
children
and
young
people
in
educational
settings
it
involves
a
wide
range
of
people
(curriculum
developers
inspectors
school
principals
teachers
school
nurses
students
etc)
these
institutions
can
vary
according
to
different
contexts
schools
deliver
education
with
support
from
the
rest
of
the
education
system
through
various
elements
such
as
education
policies
and
guidelines
–
to
which
school
policies
can
refer
–
curricula
and
learning
materials
as
well
as
pre-
and
in-service
teacher
training
programmes
the
school
environment
–
both
physical
(infrastructures)
and
psychological
(school
climate)
–
is
also
guided
by
school
policies
that
should
ensure
the
well-being
of
students
when
they
are
in
school
the
organisation
for
economic
co-operation
and
development
has
found
that
schools
tend
to
perform
best
when
principals
have
full
authority
and
responsibility
for
ensuring
that
students
are
proficient
in
core
subjects
upon
graduation
they
must
also
seek
feedback
from
students
for
quality-assurance
and
improvement
governments
should
limit
themselves
to
monitoring
student
proficiency
the
education
sector
is
fully
integrated
into
society
through
interactions
with
a
large
number
of
stakeholders
and
other
sectors
these
include
parents
local
communities
religious
leaders
ngos
stakeholders
involved
in
health
child
protection
justice
and
law
enforcement
(police)
media
and
political
leadership
several
un
agencies
have
asserted
that
comprehensive
sexuality
education
should
be
integrated
into
school
curriculum
chimombo
pointed
out
education's
role
as
a
policy
instrument
capable
of
instilling
social
change
and
economic
advancement
in
developing
countries
by
giving
communities
the
opportunity
to
take
control
of
their
destinies
the
2030
agenda
for
sustainable
development
adopted
by
the
united
nations
(un)
general
assembly
in
september
2015
calls
for
a
new
vision
to
address
the
environmental
social
and
economic
concerns
facing
the
world
today
the
agenda
includes
17
sustainable
development
goals
(sdgs)
including
sdg
4
on
education
since
1909
the
ratio
of
children
in
the
developing
world
attending
school
has
increased
before
then
a
small
minority
of
boys
attended
school
by
the
start
of
the
21st
century
the
majority
of
all
children
in
most
regions
of
the
world
attended
school
universal
primary
education
is
one
of
the
eight
international
millennium
development
goals
towards
which
progress
has
been
made
in
the
past
decade
though
barriers
still
remain
securing
charitable
funding
from
prospective
donors
is
one
particularly
persistent
problem
researchers
at
the
overseas
development
institute
have
indicated
that
the
main
obstacles
to
funding
for
education
include
conflicting
donor
priorities
an
immature
aid
architecture
and
a
lack
of
evidence
and
advocacy
for
the
issue
additionally
transparency
international
has
identified
corruption
in
the
education
sector
as
a
major
stumbling
block
to
achieving
universal
primary
education
in
africa
furthermore
demand
in
the
developing
world
for
improved
educational
access
is
not
as
high
as
foreigners
have
expected
indigenous
governments
are
reluctant
to
take
on
the
ongoing
costs
involved
there
is
also
economic
pressure
from
some
parents
who
prefer
their
children
to
earn
money
in
the
short
term
rather
than
work
towards
the
long-term
benefits
of
education
a
study
conducted
by
the
unesco
international
institute
for
educational
planning
indicates
that
stronger
capacities
in
educational
planning
and
management
may
have
an
important
spill-over
effect
on
the
system
as
a
whole
sustainable
capacity
development
requires
complex
interventions
at
the
institutional
organizational
and
individual
levels
that
could
be
based
on
some
foundational
principles:
nearly
every
country
now
has
universal
primary
education
similarities
–
in
systems
or
even
in
ideas
–
that
schools
share
internationally
have
led
to
an
increase
in
international
student
exchanges
the
european
socrates-erasmus
program
facilitates
exchanges
across
european
universities
the
soros
foundation
provides
many
opportunities
for
students
from
central
asia
and
eastern
europe
programs
such
as
the
international
baccalaureate
have
contributed
to
the
internationalization
of
education
the
global
campus
online
led
by
american
universities
allows
free
access
to
class
materials
and
lecture
files
recorded
during
the
actual
classes
the
programme
for
international
student
assessment
and
the
international
association
for
the
evaluation
of
educational
achievement
objectively
monitor
and
compare
the
proficiency
of
students
from
a
wide
range
of
different
nations
the
internationalization
of
education
is
sometimes
equated
by
critics
with
the
westernization
of
education
these
critics
say
that
the
internationalization
of
education
leads
to
the
erosion
of
local
education
systems
and
indigenous
values
and
norms
which
are
replaced
with
western
systems
and
cultural
and
ideological
values
and
orientation
technology
plays
an
increasingly
significant
role
in
improving
access
to
education
for
people
living
in
impoverished
areas
and
developing
countries
however
lack
of
technological
advancement
is
still
causing
barriers
with
regards
to
quality
and
access
to
education
in
developing
countries
charities
like
one
laptop
per
child
are
dedicated
to
providing
infrastructures
through
which
the
disadvantaged
may
access
educational
materials
the
olpc
foundation
a
group
out
of
mit
media
lab
and
supported
by
several
major
corporations
has
a
stated
mission
to
develop
a
$100
laptop
for
delivering
educational
software
the
laptops
were
widely
available
as
of
2008
they
are
sold
at
cost
or
given
away
based
on
donations
in
africa
the
new
partnership
for
africa's
development
(nepad)
has
launched
an
"e-school
program"
to
provide
all
600000
primary
and
high
schools
with
computer
equipment
learning
materials
and
internet
access
within
10
years
an
international
development
agency
project
called
nabuurcom
started
with
the
support
of
former
american
president
bill
clinton
uses
the
internet
to
allow
co-operation
by
individuals
on
issues
of
social
development
india
is
developing
technologies
that
will
bypass
land-based
telephone
and
internet
infrastructure
to
deliver
distance
learning
directly
to
its
students
in
2004
the
indian
space
research
organisation
launched
edusat
a
communications
satellite
providing
access
to
educational
materials
that
can
reach
more
of
the
country's
population
at
a
greatly
reduced
cost
research
into
lcps
(low-cost
private
schools)
found
that
over
5
years
to
july
2013
debate
around
lcpss
to
achieving
education
for
all
(efa)
objectives
was
polarized
and
finding
growing
coverage
in
international
policy
the
polarization
was
due
to
disputes
around
whether
the
schools
are
affordable
for
the
poor
reach
disadvantaged
groups
provide
quality
education
support
or
undermine
equality
and
are
financially
sustainable
the
report
examined
the
main
challenges
encountered
by
development
organizations
which
support
lcpss
surveys
suggest
these
types
of
schools
are
expanding
across
africa
and
asia
this
success
is
attributed
to
excess
demand
these
surveys
found
concern
for:
the
report
showed
some
cases
of
successful
voucher
and
subsidy
programs;
evaluations
of
international
support
to
the
sector
are
not
widespread
addressing
regulatory
ineffectiveness
is
a
key
challenge
emerging
approaches
stress
the
importance
of
understanding
the
political
economy
of
the
market
for
lcps
specifically
how
relationships
of
power
and
accountability
between
users
government
and
private
providers
can
produce
better
education
outcomes
for
the
poor
educational
psychology
is
the
study
of
how
humans
learn
in
educational
settings
the
effectiveness
of
educational
interventions
the
psychology
of
teaching
and
the
social
psychology
of
schools
as
organizations
although
the
terms
"educational
psychology"
and
"school
psychology"
are
often
used
interchangeably
researchers
and
theorists
are
likely
to
be
identified
as
whereas
practitioners
in
schools
or
school-related
settings
are
identified
as
school
psychologists
educational
psychology
is
concerned
with
the
processes
of
educational
attainment
in
the
general
population
and
in
sub-populations
such
as
gifted
children
and
those
with
specific
disabilities
educational
psychology
can
in
part
be
understood
through
its
relationship
with
other
disciplines
it
is
informed
primarily
by
psychology
bearing
a
relationship
to
that
discipline
analogous
to
the
relationship
between
medicine
and
biology
educational
psychology
in
turn
informs
a
wide
range
of
specialties
within
educational
studies
including
instructional
design
educational
technology
curriculum
development
organizational
learning
special
education
and
classroom
management
educational
psychology
both
draws
from
and
contributes
to
cognitive
science
and
the
learning
sciences
in
universities
departments
of
educational
psychology
are
usually
housed
within
faculties
of
education
possibly
accounting
for
the
lack
of
representation
of
educational
psychology
content
in
introductory
psychology
textbooks
(lucas
blazek
raley
2006)
intelligence
is
an
important
factor
in
how
the
individual
responds
to
education
those
who
have
higher
intelligence
tend
to
perform
better
at
school
and
go
on
to
higher
levels
of
education
this
effect
is
also
observable
in
the
opposite
direction
in
that
education
increases
measurable
intelligence
studies
have
shown
that
while
educational
attainment
is
important
in
predicting
intelligence
in
later
life
intelligence
at
53
is
more
closely
correlated
to
intelligence
at
8
years
old
than
to
educational
attainment
there
has
been
much
interest
in
learning
modalities
and
styles
over
the
last
two
decades
the
most
commonly
employed
learning
modalities
are:
other
commonly
employed
modalities
include
musical
interpersonal
verbal
logical
and
intrapersonal
dunn
and
dunn
focused
on
identifying
relevant
stimuli
that
may
influence
learning
and
manipulating
the
school
environment
at
about
the
same
time
as
joseph
renzulli
recommended
varying
teaching
strategies
howard
gardner
identified
a
wide
range
of
modalities
in
his
multiple
intelligences
theories
the
myers-briggs
type
indicator
and
keirsey
temperament
sorter
based
on
the
works
of
jung
focus
on
understanding
how
people's
personality
affects
the
way
they
interact
personally
and
how
this
affects
the
way
individuals
respond
to
each
other
within
the
learning
environment
the
work
of
david
kolb
and
anthony
gregorc's
type
delineator
follows
a
similar
but
more
simplified
approach
some
theories
propose
that
all
individuals
benefit
from
a
variety
of
learning
modalities
while
others
suggest
that
individuals
may
have
preferred
learning
styles
learning
more
easily
through
visual
or
kinesthetic
experiences
a
consequence
of
the
latter
theory
is
that
effective
teaching
should
present
a
variety
of
teaching
methods
which
cover
all
three
learning
modalities
so
that
different
students
have
equal
opportunities
to
learn
in
a
way
that
is
effective
for
them
guy
claxton
has
questioned
the
extent
that
learning
styles
such
as
visual
auditory
and
kinesthetic(vak)
are
helpful
particularly
as
they
can
have
a
tendency
to
label
children
and
therefore
restrict
learning
recent
research
has
argued
"there
is
no
adequate
evidence
base
to
justify
incorporating
learning
styles
assessments
into
general
educational
practice"
educational
neuroscience
is
an
emerging
scientific
field
that
brings
together
researchers
in
cognitive
neuroscience
developmental
cognitive
neuroscience
educational
psychology
educational
technology
education
theory
and
other
related
disciplines
to
explore
the
interactions
between
biological
processes
and
education
researchers
in
educational
neuroscience
investigate
the
neural
mechanisms
of
reading
numerical
cognition
attention
and
their
attendant
difficulties
including
dyslexia
dyscalculia
and
adhd
as
they
relate
to
education
several
academic
institutions
around
the
world
are
beginning
to
devote
resources
to
the
establishment
of
educational
neuroscience
research
as
an
academic
field
philosophy
of
education
is
"the
philosophical
study
of
education
and
its
problems ()
its
central
subject
matter
is
education
and
its
methods
are
those
of
philosophy"
"the
philosophy
of
education
may
be
either
the
philosophy
of
the
process
of
education
or
the
philosophy
of
the
discipline
of
education
that
is
it
may
be
part
of
the
discipline
in
the
sense
of
being
concerned
with
the
aims
forms
methods
or
results
of
the
process
of
educating
or
being
educated;
or
it
may
be
metadisciplinary
in
the
sense
of
being
concerned
with
the
concepts
aims
and
methods
of
the
discipline"
as
such
it
is
both
part
of
the
field
of
education
and
a
field
of
applied
philosophy
drawing
from
fields
of
metaphysics
epistemology
axiology
and
the
philosophical
approaches
(speculative
prescriptive
or
analytic)
to
address
questions
in
and
about
pedagogy
education
policy
and
curriculum
as
well
as
the
process
of
learning
to
name
a
few
for
example
it
might
study
what
constitutes
upbringing
and
education
the
values
and
norms
revealed
through
upbringing
and
educational
practices
the
limits
and
legitimization
of
education
as
an
academic
discipline
and
the
relation
between
education
theory
and
practice
there
is
no
broad
consensus
as
to
what
education's
chief
aim
or
aims
are
or
should
be
different
places
and
at
different
times
have
used
educational
systems
for
different
purposes
the
prussian
education
system
in
the
19th
century
for
example
wanted
to
turn
boys
and
girls
into
adults
who
would
serve
the
state's
political
goals
some
authors
stress
its
value
to
the
individual
emphasizing
its
potential
for
positively
influencing
students'
personal
development
promoting
autonomy
forming
a
cultural
identity
or
establishing
a
career
or
occupation
other
authors
emphasize
education's
contributions
to
societal
purposes
including
good
citizenship
shaping
students
into
productive
members
of
society
thereby
promoting
society's
general
economic
development
and
preserving
cultural
values
the
purpose
of
education
in
a
given
time
and
place
affects
who
is
taught
what
is
taught
and
how
the
education
system
behaves
for
example
in
the
21st
century
many
countries
treat
education
as
a
positional
good
in
this
competitive
approach
people
want
their
own
students
to
get
a
better
education
than
other
students
this
approach
can
lead
to
unfair
treatment
of
some
students
especially
those
from
disadvantaged
or
marginalized
groups
for
example
in
this
system
a
city's
school
system
may
draw
school
district
boundaries
so
that
nearly
all
the
students
in
one
school
are
from
low-income
families
and
that
nearly
all
the
students
in
the
neighboring
schools
come
from
more
affluent
families
even
though
concentrating
low-income
students
in
one
school
results
in
worse
educational
achievement
for
the
entire
school
system
in
formal
education
a
curriculum
is
the
set
of
courses
and
their
content
offered
at
a
school
or
university
as
an
idea
curriculum
stems
from
the
latin
word
for
"race
course"
referring
to
the
course
of
deeds
and
experiences
through
which
children
grow
to
become
mature
adults
a
curriculum
is
prescriptive
and
is
based
on
a
more
general
syllabus
which
merely
specifies
what
topics
must
be
understood
and
to
what
level
to
achieve
a
particular
grade
or
standard
an
academic
discipline
is
a
branch
of
knowledge
which
is
formally
taught
either
at
the
university
–
or
via
some
other
such
method
each
discipline
usually
has
several
sub-disciplines
or
branches
and
distinguishing
lines
are
often
both
arbitrary
and
ambiguous
examples
of
broad
areas
of
academic
disciplines
include
the
natural
sciences
mathematics
computer
science
social
sciences
humanities
and
applied
sciences
educational
institutions
may
incorporate
fine
arts
as
part
of
k-12
grade
curricula
or
within
majors
at
colleges
and
universities
as
electives
the
various
types
of
fine
arts
are
music
dance
and
theatre
the
sudbury
valley
school
offers
a
model
of
education
without
a
curricula
instruction
is
the
facilitation
of
another's
learning
instructors
in
primary
and
secondary
institutions
are
often
called
teachers
and
they
direct
the
education
of
students
and
might
draw
on
many
subjects
like
reading
writing
mathematics
science
and
history
instructors
in
post-secondary
institutions
might
be
called
teachers
instructors
or
professors
depending
on
the
type
of
institution;
and
they
primarily
teach
only
their
specific
discipline
studies
from
the
united
states
suggest
that
the
quality
of
teachers
is
the
single
most
important
factor
affecting
student
performance
and
that
countries
which
score
highly
on
international
tests
have
multiple
policies
in
place
to
ensure
that
the
teachers
they
employ
are
as
effective
as
possible
with
the
passing
of
nclb
in
the
united
states
(no
child
left
behind)
teachers
must
be
highly
qualified
a
popular
way
to
gauge
teaching
performance
is
to
use
student
evaluations
of
teachers
(sets)
but
these
evaluations
have
been
criticized
for
being
counterproductive
to
learning
and
inaccurate
due
to
student
bias
college
basketball
coach
john
wooden
the
wizard
of
westwood
would
teach
through
quick
"this
not
that"
technique
he
would
show
(a)
the
correct
way
to
perform
an
action
(b)
the
incorrect
way
the
player
performed
it
and
again
(c)
the
correct
way
to
perform
an
action
this
helped
him
to
be
a
responsive
teacher
and
fix
errors
on
the
fly
also
less
communication
from
him
meant
more
time
that
the
player
could
practice
it
has
been
argued
that
high
rates
of
education
are
essential
for
countries
to
be
able
to
achieve
high
levels
of
economic
growth
empirical
analyses
tend
to
support
the
theoretical
prediction
that
poor
countries
should
grow
faster
than
rich
countries
because
they
can
adopt
cutting
edge
technologies
already
tried
and
tested
by
rich
countries
however
technology
transfer
requires
knowledgeable
managers
and
engineers
who
are
able
to
operate
new
machines
or
production
practices
borrowed
from
the
leader
in
order
to
close
the
gap
through
imitation
therefore
a
country's
ability
to
learn
from
the
leader
is
a
function
of
its
stock
of
"human
capital"
recent
study
of
the
determinants
of
aggregate
economic
growth
have
stressed
the
importance
of
fundamental
economic
institutions
and
the
role
of
cognitive
skills
at
the
level
of
the
individual
there
is
a
large
literature
generally
related
to
the
work
of
jacob
mincer
on
how
earnings
are
related
to
the
schooling
and
other
human
capital
this
work
has
motivated
a
large
number
of
studies
but
is
also
controversial
the
chief
controversies
revolve
around
how
to
interpret
the
impact
of
schooling
some
students
who
have
indicated
a
high
potential
for
learning
by
testing
with
a
high
intelligence
quotient
may
not
achieve
their
full
academic
potential
due
to
financial
difficulties
economists
samuel
bowles
and
herbert
gintis
argued
in
1976
that
there
was
a
fundamental
conflict
in
american
schooling
between
the
egalitarian
goal
of
democratic
participation
and
the
inequalities
implied
by
the
continued
profitability
of
capitalist
production
many
countries
are
now
drastically
changing
the
way
they
educate
their
citizens
the
world
is
changing
at
an
ever
quickening
rate
which
means
that
a
lot
of
knowledge
becomes
obsolete
and
inaccurate
more
quickly
the
emphasis
is
therefore
shifting
to
teaching
the
skills
of
learning:
to
picking
up
new
knowledge
quickly
and
in
as
agile
a
way
as
possible
finnish
schools
have
even
begun
to
move
away
from
the
regular
subject-focused
curricula
introducing
instead
developments
like
phenomenon-based
learning
where
students
study
concepts
like
climate
change
instead
there
are
also
active
educational
interventions
to
implement
programs
and
paths
specific
to
non-traditional
students
such
as
first
generation
students
education
is
also
becoming
a
commodity
no
longer
reserved
for
children
adults
need
it
too
some
governmental
bodies
like
the
finnish
innovation
fund
sitra
in
finland
have
even
proposed
compulsory
lifelong
education
united
states
military
veteran
suicide
united
states
military
veteran
suicide
is
an
ongoing
phenomenon
regarding
a
reportedly
high
rate
of
suicide
among
us
military
veterans
in
comparison
to
the
general
public
according
to
the
most
recent
report
published
by
the
united
states
department
of
veterans
affairs
(va)
in
2016
which
analyzed
55
million
veterans'
records
from
1979
to
2014
the
current
analysis
indicates
that
an
average
of
20
veterans
a
day
die
from
suicide
in
2012
alone
an
estimated
6500
former
military
personnel
died
by
suicide
more
active
duty
veterans
177
succumbed
to
suicide
that
year
than
were
killed
in
combat
176
the
army
suffered
52%
of
the
suicides
from
all
branches
in
2013
the
va
released
a
study
that
covered
suicides
from
1999
to
2010
which
showed
that
roughly
22
veterans
were
dying
by
suicide
per
day
or
one
every
65
minutes
some
sources
suggest
that
this
rate
may
be
undercounting
suicides
a
recent
analysis
found
a
suicide
rate
among
veterans
of
about
30
per
100000
population
per
year
compared
with
the
civilian
rate
of
14
per
100000
however
the
comparison
was
not
adjusted
for
age
and
sex
the
total
number
of
suicides
differs
by
age
group;
31%
of
these
suicides
were
by
veterans
49
and
younger
while
69%
were
by
veterans
aged
50
and
older
as
with
suicides
in
general
suicide
of
veterans
is
primarily
male
with
about
97
percent
of
the
suicides
being
male
in
the
states
that
reported
gender
in
2015
the
clay
hunt
veterans
suicide
prevention
act
passed
in
the
senate
and
was
then
enacted
as
on
february
12
2015
in
august
2016
the
va
released
a
new
report
which
consisted
of
the
nation's
largest
analysis
of
veteran
suicide
the
report
reviewed
more
than
55
million
veterans'
records
from
1979
to
2014
from
every
state
in
the
nation
the
previous
report
from
2012
was
primarily
limited
to
data
on
veterans
who
used
vha
health
services
or
from
mortality
records
obtained
directly
from
20
states
and
approximately
3
million
records
compared
to
the
data
from
the
2012
report
which
estimated
the
number
of
veteran
deaths
by
suicide
to
be
22
per
day
the
current
analysis
indicates
that
in
2014
an
average
of
20
veterans
a
day
died
from
suicide
the
first
suicide
prevention
center
in
the
united
states
was
opened
in
los
angeles
in
1958
with
funding
from
the
us
public
health
service
in
1966
the
center
for
studies
of
suicide
prevention
(later
the
suicide
research
unit)
was
established
at
the
national
institute
of
mental
health
(nimh)
of
the
national
institutes
of
health
(nih)
later
on
in
1970
the
nimh
pushed
in
phoenix
the
discussion
about
the
status
of
suicide
prevention
presented
relevant
findings
about
suicide
rate
and
identified
the
future
directions
and
priorities
of
the
topic
however
it
wasn't
until
mid-1990s
when
suicide
started
being
the
central
issue
of
the
political-social
agenda
of
the
united
states
survivors
from
suicide
began
to
mobilize
encouraging
the
development
of
a
national
strategy
for
suicide
prevention
finally
two
congressional
resolutions—s
res
84
and
h
res
212
of
the
105th
congress—recognized
suicide
as
a
national
problem
and
suicide
prevention
as
a
national
priority
as
recommended
in
the
un
guidelines
these
groups
set
out
to
establish
a
public
and
private
partnership
that
would
be
responsible
for
promoting
suicide
prevention
in
the
united
states
this
innovative
public-private
partnership
jointly
sponsored
a
national
consensus
conference
on
suicide
prevention
in
reno
nevada
which
developed
a
list
of
81
recommendations
one
of
the
most
important
laws
about
veterans'
suicide
prevention
is
the
joshua
omvig
veterans
suicide
prevention
act
(jovspa)
of
2007
supporting
the
creation
of
a
comprehensive
program
to
reduce
the
incidence
of
suicide
among
veterans
named
for
a
veteran
of
operation
iraqi
freedom
who
died
by
suicide
in
2005
the
act
directed
the
secretary
of
the
us
department
of
veterans
affairs
(va)
to
implement
a
comprehensive
suicide
prevention
program
for
veterans
components
include
staff
education
mental
health
assessments
as
part
of
overall
health
assessments
a
suicide
prevention
coordinator
at
each
va
medical
facility
research
efforts
24-hour
mental
health
care
a
toll-free
crisis
line
and
outreach
to
and
education
for
veterans
and
their
families
in
the
summer
of
2009
va
added
a
one-to-one
“chat
service”
for
veterans
who
prefer
to
reach
out
for
assistance
using
the
internet
in
2010
the
national
action
alliance
for
suicide
prevention
was
created
and
in
2012
the
national
strategy
was
revised
with
obama’s
administration
suicide
prevention
strategies
for
veterans
expanded
and
a
goal
was
formed
to
make
the
process
of
finding
and
obtaining
mental
health
resources
easier
for
veterans
work
to
retain
and
recruit
mental
health
professionals
and
make
the
government
programs
more
accountable
for
the
people
they
serve
in
2011
the
national
veterans
suicide
prevention
hotline
was
renamed
the
veterans
crisis
line
(vcl)
the
primary
mission
of
the
vcl
is
“to
provide
24/7
world-class
suicide
prevention
and
crisis
intervention
services
to
veterans
servicemembers
and
their
family
members”
the
vcl
faces
a
number
of
challenges
it
must
meet
the
operational
and
business
demands
of
responding
to
over
500000
calls
per
year
along
with
thousands
of
electronic
chats
and
text
messages
and
initiating
rescue
processes
when
indicated
it
must
also
train
staff
to
respond
to
veterans
and
their
family
members
in
individual
encounters
during
which
a
responder
must
make
an
accurate
assessment
of
the
needs
of
the
caller
under
stressful
time-sensitive
conditions
since
its
inception
in
july
2007
the
vcl
has
answered
over
3
million
calls
and
initiated
the
dispatch
of
emergency
services
to
callers
in
imminent
crisis
over
84000
times
since
launching
chat
in
2009
and
text
services
in
november
2011
the
vcl
has
answered
nearly
359000
and
nearly
78000
requests
for
chat
and
text
services
respectively
in
addition
staff
has
forwarded
more
than
504000
referrals
to
local
va
suicide
prevention
coordinators
on
behalf
of
veterans
to
ensure
continuity
of
care
with
veterans’
local
va
providers
for
fy
2016
more
than
51000
chats
and
17000
texts
were
answered
by
vcl
responders
for
fy
2017
nearly
54000
chats
and
nearly
16000
texts
were
answered
by
vcl
responders
emergency
services
were
dispatched
to
over
12000
callers
in
immediate
crisis
in
fy
2016
and
nearly
19000
callers
in
immediate
crisis
in
fy
2017
for
fy
2016
nearly
87000
referrals
were
made
to
local
suicide
prevention
coordinators
for
follow-up
care
and
over
95000
referrals
were
made
in
fy
2017
the
2018
federal
budget
expanded
mental
health
screenings
for
veterans
a
study
published
in
the
"cleveland
clinic
journal
of
medicine"
found
that
the
same
study
also
found
that
in
veterans
with
ptsd
related
to
combat
experience
combat-related
guilt
may
be
a
significant
predictor
of
suicidal
ideation
and
attempts
craig
bryan
of
the
university
of
utah
national
center
for
veterans
studies
said
that
veterans
have
the
same
risk
factors
for
suicide
as
the
general
population
including
feelings
of
depression
hopelessness
post-traumatic
stress
disorder
a
history
of
trauma
and
access
to
firearms
a
study
done
by
the
"department
of
veterans
affairs"
discovered
that
veterans
are
more
likely
to
develop
symptoms
of
ptsd
for
a
number
of
reasons
such
as:
the
"department
of
veterans
affairs"
also
discovered
that
where
you
were
deployed
and
which
branch
of
military
you
are
with
can
also
have
drastic
effects
on
your
mental
status
after
returning
from
service
as
in
most
combat
wars
your
experiences
will
vary
depending
on
where
you
are
stationed
critics
of
this
reporting
such
as
author
tim
worstall
in
feb
2013
claim
that
there
is
no
epidemic
when
comparing
similar
demographic
cohorts
in
the
civilian
population
he
points
out
that
since
vets
are
predominantly
male
the
suicide
rate
to
compare
to
is
not
the
general
civilian
rate
but
the
rate
for
males
veterans
have
difficulty
transitioning
from
the
military
to
civilian
life
many
choose
to
transition
by
utilizing
their
gi
bill
or
other
education
benefits
the
pursuit
of
education
often
facilitates
the
transition
to
civilian
life
the
pursuit
of
education
among
veterans
can
aggravate
post
service
conditions
that
are
linked
to
a
higher
likelihood
of
suicide
but
often
aids
in
the
transition
to
civilian
life
veterans
pursuing
education
especially
those
utilizing
the
post
9/11
gi
bill
are
more
likely
to
have
protective
factors
related
to
socialization
and
reintegration
than
those
who
are
not
although
higher
education
has
presented
many
difficulties
to
returning
veterans
research
supports
that
veterans
often
benefit
from
transitioning
from
the
military
into
higher
education
academic
life
often
requires
student
veterans
to
work
and
interact
with
other
classmates
most
academic
institutions
have
student
veteran
organizations
and
resources
centers
specifically
to
aid
military
veterans
military
education
benefits
primarily
the
post
9/11
gi
bill
pay
the
cost
of
tuition
and
provide
a
housing
stipend
to
student
veterans
education
benefits
often
give
veteran
students
an
income
a
goal
to
continue
to
work
towards
and
socialization
with
the
general
population
national
commission
on
teaching
and
america's
future
the
national
commission
on
teaching
and
america's
future
(nctaf)
is
a
non-profit
non-partisan
education
policy
advocacy
organization
based
in
washington
dc
founded
in
1994
by
then-north
carolina
governor
jim
hunt
and
stanford
university
professor
linda
darling-hammond
the
nctaf
focuses
its
research
on
improving
the
teaching
profession
through
recruitment
development
and
retention
of
skilled
teachers
in
2017
the
nctaf
announced
that
it
will
merge
with
learning
forward
and
will
operate
under
the
learning
forward
name
in
its
1996
report
"what
matters
most:
teaching
for
america's
future"
the
nctaf
issued
broad
recommendations
for
education
leaders
and
state
policymakers
to
among
other
things
overhaul
teacher
education
programs
establish
state
boards
of
professional
teaching
standards
strengthen
teacher
licensure
standards
implement
teacher
mentoring
programs
and
create
teacher
compensation
policies
that
reward
knowledge
and
expertise
the
report
had
wide-reaching
impact
with
seven
states
including
illinois
indiana
kentucky
maine
missouri
north
carolina
and
ohio
signing
on
to
be
partners
in
implementing
the
report's
recommendations
in
2001
the
nctaf
appointed
former
us
federal
election
official
tom
carrol
its
executive
director
carroll
announced
his
retirement
in
2014
he
was
succeeded
by
melinda
george
reach
every
reader
reach
every
reader
is
a
five-year
initiative
supported
by
a
$30
million
grant
from
chan
zuckerberg
initiative
co-founders
priscilla
chan
and
facebook
ceo
mark
zuckerberg
reach
every
reader
was
launched
by
faculty
at
the
harvard
graduate
school
of
education
and
massachusetts
institute
of
technology's
integrated
learning
initiative
and
involves
collaborators
at
the
florida
center
for
reading
research
and
florida
state
university
college
of
communication
and
information
and
the
charlotte-mecklenburg
school
district
in
north
carolina
the
collaboration
consists
of
five
projects:
reach
every
reader
will
develop
a
web-based
screening
tool
for
reading
difficulties
that
diagnoses
underlying
causes
the
diagnostic
screening
tool
will
identify
kindergarteners
who
are
at
high
risk
for
reading
difficulty
the
goal
is
to
make
this
type
of
screening
available
to
all
children
the
collaboration
will
also
examine
which
interventions
work
for
which
students
in
order
to
work
toward
the
development
of
personalized
interventions
researchers
will
work
with
schools
to
deliver
their
interventions
to
kindergarten
students
in
summer
programs
and
eventually
implement
them
in
the
school
curriculum
concern
has
been
expressed
that
the
project
involves
"crisis
talk"
that
creates
pressure
for
children
and
that
parents
may
be
concerned
about
the
tracking
of
their
children's
personal
information
state
college
of
florida
collegiate
school
state
college
of
florida
collegiate
school
(scfcs)
is
a
college
preparatory
school
located
on
state
college
of
florida's
bradenton
campus
it
is
based
on
a
school
in
sweden
with
similar
views
of
having
students
work
on
their
own
pace
classes
are
available
for
grades
6-10
the
school
is
largely
technology
based
utilizing
a
service
canvas
from
instructure
to
assign
and
turn
in
schoolwork
each
student
is
assigned
an
ipad
based
on
their
grade
level
and
apple
laptops
are
available
for
services
not
available
on
the
ipad
each
student
start
classes
on
the
college
campus
in
eleventh
grade
if
they
pass
an
enrollment
test
called
the
pert
and
have
at
least
a
30
gpa
after
completing
the
program
they
are
given
an
associate
degree
at
graduation
alongside
their
high
school
diploma
following
this
for
a
two-year
period
students
can
be
given
a
tuition-paid
scholarship
for
the
florida
gulf
coast
university
the
current
headmaster
is
kelly
monod
state
college
of
florida
collegiate
school
is
recognized
for
its
high
academic
record
and
high
quality
work
in
the
year
2011
scfcs
participated
in
the
florida
comprehensive
assessment
test
(fcat)
in
sixth
grade
reading
scfcs
students
scored
85
compared
to
manatee
county
public
school
district
63
and
all
florida
students’
composite
of
67
sixth
grade
math
scores
were
scfcs
at
72
manatee
county
48
and
florida
57
in
seventh
grade
reading
scfcs
students
scored
83
compared
to
manatee
county
65
and
florida
68
scfcs
scored
73
in
seventh
grade
math
compared
to
manatee
county
at
59
and
florida
at
62
many
scf
collegiate
students
regularly
participate
on
the
college
brain
bowl
team
often
with
great
success
scf
collegiate
students
carlyle
styer
and
christopher
medrano
were
a
part
of
the
"scf
fire
team"
which
won
the
2015
fcsaa
brain
bowl
state
championship
carlyle
styer
was
joined
by
fellow
scfcs
student
kara
stevens
as
well
as
four
other
scf
students
in
winning
the
2015
naqt
community
college
championship
it
is
believed
that
this
may
be
the
only
instance
in
which
high
school
students
have
played
a
significant
role
in
a
quiz
bowl
national
championship
at
the
collegiate
level
freethought
freethought
(or
"free
thought")
is
a
philosophical
viewpoint
which
holds
that
positions
regarding
truth
should
be
formed
on
the
basis
of
logic
reason
and
empiricism
rather
than
authority
tradition
revelation
or
dogma
according
to
the
oxford
english
dictionary
independent
working
class
education
independent
working
class
education
is
an
approach
to
education
particularly
adult
education
developed
by
labour
activists
whereby
the
education
of
working
class
people
is
seen
as
a
specifically
political
process
linked
to
other
aspects
of
class
struggle
the
term
abbreviated
to
(iwce)
is
particularly
linked
to
the
plebs'
league
skill
assessment
competence
assessment
is
a
process
in
which
evidence
is
gathered
by
the
assessor
and
evaluated
against
agreed
criteria
in
order
to
make
a
judgement
of
competence
skill
assessment
is
the
comparison
of
actual
performance
of
a
skill
with
the
specified
standard
for
performance
of
that
skill
under
the
circumstances
specified
by
the
standard
and
evaluation
of
whether
the
performance
meets
or
exceed
the
requirements
assessment
of
a
skill
should
comply
with
the
four
principles
of
validity
reliability
fairness
and
flexibility
formative
assessment
provides
feedback
for
remedial
work
and
coaching
while
summative
assessment
checks
whether
the
competence
has
been
achieved
at
the
end
of
training
assessment
of
combinations
of
skills
and
of
foundational
knowledge
may
provide
greater
efficiency
and
in
some
cases
competence
in
one
skill
my
imply
competence
in
other
skills
the
thoroughness
reqired
of
assessment
may
depend
on
the
consequences
of
occasional
poor
performance
validity
is
the
primary
requirement
if
the
assessment
is
not
valid
then
the
other
characteristics
are
irrelevant
validity
means
that
an
assessment
process
effectively
assesses
what
it
is
claimed
and
intended
to
assess
to
achieve
this
the
assessment
tools
must
address
all
requirements
of
the
standard
to
the
appropriate
depth
(neither
too
much
nor
too
little)
and
be
repeated
often
enough
to
ensure
that
the
required
performance
is
repeatable
the
training
standard
that
specifies
the
competency
is
the
benchmark
for
assessment
and
to
be
valid
the
assessment
must
comply
exactly
with
its
requirements
so
that
nothing
required
by
the
standard
is
omitted
and
nothing
that
is
not
required
is
included
the
assessment
tools
for
a
skill
therefore
need
to
be
designed
so
that
they
allow
the
skill
to
be
tested
in
compliance
with
the
requirements
of
the
standard
it
can
be
useful
to
map
the
assessment
tools
to
the
specific
competences
to
ensure
that
they
cover
the
full
scope
of
the
standard
there
may
be
a
requirement
for
periodical
validation
of
assessment
tools
this
process
generally
involves
mapping
the
tools
against
the
standard
and
checking
that
the
tools
comply
with
the
other
principles
of
assessment
and
the
rules
of
evidence
after
validity
reliability
is
essential
a
reliable
assessment
is
one
where
the
evidence
elicited
and
interpretation
of
evidence
is
consistent
with
the
skill
required
so
that
the
assessment
consistently
produces
outcomes
that
are
compliant
with
the
standard
the
assessment
decision
of
a
given
observed
performance
should
not
vary
for
different
assessors
the
same
evidence
should
lead
to
the
same
outcome
to
achieve
this
the
assessment
tool
must
provide
sufficient
guidance
for
the
assessor
in
practice
the
assessment
instrument
provided
to
the
candidate
should
be
paired
with
an
assessor
guide
which
provides
instructions
to
the
assessor
to
guide
their
judgement
of
satisfactory
performance
or
acceptable
answers
to
questions
to
be
fair
the
assessment
process
must
be
clearly
understood
by
the
candidates
and
there
must
be
agreement
by
both
assessors
and
candidates
that
candidates’
reasonable
needs
circumstances
are
addressed
the
assessment
tool
can
provide
evidence
that
the
process
is
understood
and
accepted
by
the
candidate
by
having
a
place
where
a
statement
to
this
effect
is
signed
by
the
candidate
at
the
start
of
the
assessment
a
further
statement
that
the
assessor
has
checked
with
the
candidate
for
any
special
circumstances
or
requirements
can
also
be
included
reasonable
adjustment
must
not
compromise
the
validity
or
reliability
of
the
assessment
flexibility
of
assessment
is
desirable
where
reasonably
practicable
this
is
a
feature
that
should
be
inherent
in
the
assessment
tools
for
the
skill
and
should
take
into
account
the
expected
variability
of
circumstances
including
variations
in
candidates
equipment
location
environmental
conditions
and
other
things
not
entirely
under
the
control
of
the
assessor
but
within
the
scope
of
the
competence
requirements
flexibility
does
not
imply
bending
the
rules
or
failing
to
comply
with
the
specifications
of
the
standards
all
performance
criteria
must
be
addressed
formative
assessments
are
formal
and
informal
tests
tasks
quizzes
discussions
or
observations
taken
during
the
learning
process
these
assessments
identify
strengths
and
weaknesses
and
provide
feedback
to
modify
the
consequent
learning
activities
to
facilitate
efficient
learning
and
skill
development
summative
assessments
evaluate
skills
at
or
after
the
end
of
an
instructional
unit
to
ensure
that
competence
has
been
achieved
at
this
point
remedial
work
may
no
longer
be
practicable
integrated
assessment
is
part
of
the
learning
and
teaching
process
and
can
take
place
at
various
stages
of
a
learning
programme
assessments
may
combine
assessment
of
theory
and
practice
some
skills
may
need
separate
and
specific
assessment
but
others
can
be
combined
for
efficiency
assessment
is
not
an
event
that
only
occurs
at
the
end
of
training
it
is
most
effective
when
continuous
and
when
providing
constant
feedback
on
progress
and
problems
allowing
timely
intervention
where
useful
in
many
cases
a
sample
of
evidence
is
sufficient
to
infer
competence
over
a
fairly
large
range
as
competence
in
a
skill
that
requires
competence
in
other
skills
may
be
a
proxy
for
those
more
foundational
skills
comprehensive
planning
is
usually
necessary
to
produce
robust
assessment
tools
that
suit
the
training
programme
and
do
justice
to
both
the
training
standard
and
the
learners
assessment
of
practical
skills
is
usually
best
done
by
direct
observation
of
performance
in
conditions
as
close
as
reasonably
practicable
to
the
circumstances
in
which
the
skill
would
normally
be
practiced
where
this
is
not
reasonably
practicable
simulations
may
be
appropriate
to
whatever
level
of
accuracy
is
available
assessment
of
realistic
combinations
of
skills
may
save
a
lot
of
time
and
scenarios
may
be
devised
that
allow
simultaneous
and
sequential
assessments
of
several
skill
in
one
assessment
session
the
number
of
repetitions
required
will
also
depend
on
how
critical
the
skill
is
considered
to
be
a
single
successful
demonstration
may
be
sufficient
to
show
that
the
candidate
can
perform
a
task
when
the
consequences
are
minor
several
sequential
faultless
performances
may
be
required
if
another
person's
life
will
depend
on
correct
performance
assessment
tools
for
practical
skills
may
describe
a
task
to
be
done
and
the
assessors
guide
should
generally
list
the
stages
of
the
task
and
the
details
the
assessor
should
check
off
as
they
are
done
where
order
is
important
this
should
be
mentioned
a
checklist
may
be
provided
as
permanent
record
or
a
video
may
be
taken
in
some
cases
there
will
be
a
product
which
can
be
retained
as
long
term
evidence
along
with
the
paperwork
or
database
records
peacejam
ghana
peacejam
ghana
started
in
2008
by
its
official
chapter
the
west
africa
center
for
peace
foundation
ghana
(wacpf)
peacejam
ghana
has
mentored
and
trained
over
5000
students
since
its
inception
peacejam
ghana
has
produced
many
scholars
some
of
whom
have
received
the
tpg
global
impact
youth
fellows
scholarship
to
pursue
higher
degrees
wisdom
addo
is
the
founder
and
executive
director
of
the
west
africa
center
for
peace
foundation
ghana
peacejam
is
an
annual
youth
leadership
conference
that
is
built
around
the
nobel
peace
prize
laureates
who
work
with
young
people
with
the
aim
of
imparting
their
skills
knowledge
and
wisdom
to
them
for
community
and
sustainable
development
the
conference
usually
draws
students
from
junior
high
and
senior
high
schools
across
the
country
who
are
usually
trained
and
mentored
by
mentors
on
diverse
areas
including
but
not
limited
to
commitment
to
justice
and
peace
social
responsibility
academic
excellence
and
sustainable
development
1
osu
presby
senior
high
school
2
kaneshie
senior
high
secondary
technical
-
accra
3
kraboa
presby
senior
high
technical
4
 half
assini
senior
high
school
5
annor
adjaye
senior
high
school
-
6
 accra
high
senior
high
school
–
accra
7
 st
mary's
senior
high
school
(ghana)
8
st
stephen’s
r/c
-
9
prince
of
peace
-
10 
star
of
the
sea
r/c
-
11 
mataheko
r/c
-
12 
st
kizito
-
13 
bennett
caulley
-
logic
logic
(from
the
)
is
the
systematic
study
of
the
form
of
valid
inference
and
the
most
general
laws
of
truth
a
valid
inference
is
one
where
there
is
a
specific
relation
of
logical
support
between
the
assumptions
of
the
inference
and
its
conclusion
in
ordinary
discourse
inferences
may
be
signified
by
words
such
as
"therefore"
"hence"
"ergo"
and
so
on
there
is
no
universal
agreement
as
to
the
exact
scope
and
subject
matter
of
logic
(see
below)
but
it
has
traditionally
included
the
classification
of
arguments
the
systematic
exposition
of
the
'logical
form'
common
to
all
valid
arguments
the
study
of
proof
and
inference
including
paradoxes
and
fallacies
and
the
study
of
syntax
and
semantics
historically
logic
has
been
studied
in
philosophy
(since
ancient
times)
and
mathematics
(since
the
mid-19th
century)
and
recently
logic
has
been
studied
in
computer
science
linguistics
psychology
and
other
fields
the
concept
of
logical
form
is
central
to
logic
the
validity
of
an
argument
is
determined
by
its
logical
form
not
by
its
content
traditional
aristotelian
syllogistic
logic
and
modern
symbolic
logic
are
examples
of
formal
logic
however
agreement
on
what
logic
is
has
remained
elusive
and
although
the
field
of
universal
logic
has
studied
the
common
structure
of
logics
in
2007
mossakowski
et
al
commented
that
"it
is
embarrassing
that
there
is
no
widely
acceptable
formal
definition
of
'a
logic'"
logic
is
generally
considered
formal
when
it
analyzes
and
represents
the
"form"
of
any
valid
argument
type
the
form
of
an
argument
is
displayed
by
representing
its
sentences
in
the
formal
grammar
and
symbolism
of
a
logical
language
to
make
its
content
usable
in
formal
inference
simply
put
to
formalize
simply
means
to
translate
english
sentences
into
the
language
of
logic
this
is
called
showing
the
"logical
form"
of
the
argument
it
is
necessary
because
indicative
sentences
of
ordinary
language
show
a
considerable
variety
of
form
and
complexity
that
makes
their
use
in
inference
impractical
it
requires
first
ignoring
those
grammatical
features
irrelevant
to
logic
(such
as
gender
and
declension
if
the
argument
is
in
latin)
replacing
conjunctions
irrelevant
to
logic
(such
as
"but")
with
logical
conjunctions
like
"and"
and
replacing
ambiguous
or
alternative
logical
expressions
("any"
"every"
etc)
with
expressions
of
a
standard
type
(such
as
"all"
or
the
universal
quantifier
∀)
second
certain
parts
of
the
sentence
must
be
replaced
with
schematic
letters
thus
for
example
the
expression
"all
ps
are
qs"
shows
the
logical
form
common
to
the
sentences
"all
men
are
mortals"
"all
cats
are
carnivores"
"all
greeks
are
philosophers"
and
so
on
the
schema
can
further
be
condensed
into
the
formula
"a(pq)"
where
the
letter
"a"
indicates
the
judgement
'all
-
are
-'
the
importance
of
form
was
recognised
from
ancient
times
aristotle
uses
variable
letters
to
represent
valid
inferences
in
"prior
analytics"
leading
jan
łukasiewicz
to
say
that
the
introduction
of
variables
was
"one
of
aristotle's
greatest
inventions"
according
to
the
followers
of
aristotle
(such
as
ammonius)
only
the
logical
principles
stated
in
schematic
terms
belong
to
logic
not
those
given
in
concrete
terms
the
concrete
terms
"man"
"mortal"
etc
are
analogous
to
the
substitution
values
of
the
schematic
placeholders
"p"
"q"
"r"
which
were
called
the
"matter"
(greek
"hyle")
of
the
inference
there
is
a
big
difference
between
the
kinds
of
formulas
seen
in
traditional
term
logic
and
the
predicate
calculus
that
is
the
fundamental
advance
of
modern
logic
the
formula
"a(pq)"
(all
ps
are
qs)
of
traditional
logic
corresponds
to
the
more
complex
formula
formula_1
in
predicate
logic
involving
the
logical
connectives
for
universal
quantification
and
implication
rather
than
just
the
predicate
letter
"a"
and
using
variable
arguments
formula_2
where
traditional
logic
uses
just
the
term
letter
"p"
with
the
complexity
comes
power
and
the
advent
of
the
predicate
calculus
inaugurated
revolutionary
growth
of
the
subject
the
validity
of
an
argument
depends
upon
the
meaning
or
"semantics"
of
the
sentences
that
make
it
up
aristotle's
organon
especially
"on
interpretation"
gives
a
cursory
outline
of
semantics
which
the
scholastic
logicians
particularly
in
the
thirteenth
and
fourteenth
century
developed
into
a
complex
and
sophisticated
theory
called
supposition
theory
this
showed
how
the
truth
of
simple
sentences
expressed
schematically
depend
on
how
the
terms
'supposit'
or
"stand
for"
certain
extra-linguistic
items
for
example
in
part
ii
of
his
summa
logicae
william
of
ockham
presents
a
comprehensive
account
of
the
necessary
and
sufficient
conditions
for
the
truth
of
simple
sentences
in
order
to
show
which
arguments
are
valid
and
which
are
not
thus
"every
a
is
b'
is
true
if
and
only
if
there
is
something
for
which
'a'
stands
and
there
is
nothing
for
which
'a'
stands
for
which
'b'
does
not
also
stand"
early
modern
logic
defined
semantics
purely
as
a
relation
between
ideas
antoine
arnauld
in
the
port
royal
logic
says
that
'after
conceiving
things
by
our
ideas
we
compare
these
ideas
and
finding
that
some
belong
together
and
some
do
not
we
unite
or
separate
them
this
is
called
"affirming"
or
"denying"
and
in
general
"judging"
thus
truth
and
falsity
are
no
more
than
the
agreement
or
disagreement
of
ideas
this
suggests
obvious
difficulties
leading
locke
to
distinguish
between
'real'
truth
when
our
ideas
have
'real
existence'
and
'imaginary'
or
'verbal'
truth
where
ideas
like
harpies
or
centaurs
exist
only
in
the
mind
this
view
(psychologism)
was
taken
to
the
extreme
in
the
nineteenth
century
and
is
generally
held
by
modern
logicians
to
signify
a
low
point
in
the
decline
of
logic
before
the
twentieth
century
modern
semantics
is
in
some
ways
closer
to
the
medieval
view
in
rejecting
such
psychological
truth-conditions
however
the
introduction
of
quantification
needed
to
solve
the
problem
of
multiple
generality
rendered
impossible
the
kind
of
subject-predicate
analysis
that
underlies
medieval
semantics
the
main
modern
approach
is
"model-theoretic
semantics"
based
on
alfred
tarski's
semantic
theory
of
truth
the
approach
assumes
that
the
meaning
of
the
various
parts
of
the
propositions
are
given
by
the
possible
ways
we
can
give
a
recursively
specified
group
of
interpretation
functions
from
them
to
some
predefined
domain
of
discourse:
an
interpretation
of
first-order
predicate
logic
is
given
by
a
mapping
from
terms
to
a
universe
of
individuals
and
a
mapping
from
propositions
to
the
truth
values
"true"
and
"false"
model-theoretic
semantics
is
one
of
the
fundamental
concepts
of
model
theory
modern
semantics
also
admits
rival
approaches
such
as
the
proof-theoretic
semantics
that
associates
the
meaning
of
propositions
with
the
roles
that
they
can
play
in
inferences
an
approach
that
ultimately
derives
from
the
work
of
gerhard
gentzen
on
structural
proof
theory
and
is
heavily
influenced
by
ludwig
wittgenstein's
later
philosophy
especially
his
aphorism
"meaning
is
use"
"inference"
is
not
to
be
confused
with
"implication"
an
implication
is
a
sentence
of
the
form
'if
p
then
q'
and
can
be
true
or
false
the
stoic
logician
philo
of
megara
was
the
first
to
define
the
truth
conditions
of
such
an
implication:
false
only
when
the
antecedent
p
is
true
and
the
consequent
q
is
false
in
all
other
cases
true
an
inference
on
the
other
hand
consists
of
two
separately
asserted
propositions
of
the
form
'p
therefore
q'
an
inference
is
not
true
or
false
but
valid
or
invalid
however
there
is
a
connection
between
implication
and
inference
as
follows:
if
the
implication
'if
p
then
q'
is
"true"
the
inference
'p
therefore
q'
is
"valid"
this
was
given
an
apparently
paradoxical
formulation
by
philo
who
said
that
the
implication
'if
it
is
day
it
is
night'
is
true
only
at
night
so
the
inference
'it
is
day
therefore
it
is
night'
is
valid
in
the
night
but
not
in
the
day
the
theory
of
inference
(or
'consequences')
was
systematically
developed
in
medieval
times
by
logicians
such
as
william
of
ockham
and
walter
burley
it
is
uniquely
medieval
though
it
has
its
origins
in
aristotle's
topics
and
boethius'
"de
syllogismis
hypotheticis"
this
is
why
many
terms
in
logic
are
latin
for
example
the
rule
that
licenses
the
move
from
the
implication
'if
p
then
q'
plus
the
assertion
of
its
antecedent
p
to
the
assertion
of
the
consequent
q
is
known
as
modus
ponens
(or
'mode
of
positing')
its
latin
formulation
is
'posito
antecedente
ponitur
consequens'
the
latin
formulations
of
many
other
rules
such
as
'ex
falso
quodlibet'
(anything
follows
from
a
falsehood)
'reductio
ad
absurdum'
(disproof
by
showing
the
consequence
is
absurd)
also
date
from
this
period
however
the
theory
of
consequences
or
of
the
so-called
'hypothetical
syllogism'
was
never
fully
integrated
into
the
theory
of
the
'categorical
syllogism'
this
was
partly
because
of
the
resistance
to
reducing
the
categorical
judgment
'every
s
is
p'
to
the
so-called
hypothetical
judgment
'if
anything
is
s
it
is
p'
the
first
was
thought
to
imply
'some
s
is
p'
the
second
was
not
and
as
late
as
1911
in
the
encyclopædia
britannica
article
on
logic
we
find
the
oxford
logician
th
case
arguing
against
sigwart's
and
brentano's
modern
analysis
of
the
universal
proposition
a
formal
system
is
an
organization
of
terms
used
for
the
analysis
of
deduction
it
consists
of
an
alphabet
a
language
over
the
alphabet
to
construct
sentences
and
a
rule
for
deriving
sentences
among
the
important
properties
that
logical
systems
can
have
are:
some
logical
systems
do
not
have
all
four
properties
as
an
example
kurt
gödel's
incompleteness
theorems
show
that
sufficiently
complex
formal
systems
of
arithmetic
cannot
be
consistent
and
complete;
however
first-order
predicate
logics
not
extended
by
specific
axioms
to
be
arithmetic
formal
systems
with
equality
can
be
complete
and
consistent
as
the
study
of
argument
is
of
clear
importance
to
the
reasons
that
we
hold
things
to
be
true
logic
is
of
essential
importance
to
rationality
here
we
have
defined
logic
to
be
"the
systematic
study
of
the
form
of
arguments";
the
reasoning
behind
argument
is
of
several
sorts
but
only
some
of
these
arguments
fall
under
the
aegis
of
logic
proper
deductive
reasoning
concerns
the
logical
consequence
of
given
premises
and
is
the
form
of
reasoning
most
closely
connected
to
logic
on
a
narrow
conception
of
logic
(see
below)
logic
concerns
just
deductive
reasoning
although
such
a
narrow
conception
controversially
excludes
most
of
what
is
called
informal
logic
from
the
discipline
there
are
other
forms
of
reasoning
that
are
rational
but
that
are
generally
not
taken
to
be
part
of
logic
these
include
inductive
reasoning
which
covers
forms
of
inference
that
move
from
collections
of
particular
judgements
to
universal
judgements
and
abductive
reasoning
which
is
a
form
of
inference
that
goes
from
observation
to
a
hypothesis
that
accounts
for
the
reliable
data
(observation)
and
seeks
to
explain
relevant
evidence
the
american
philosopher
charles
sanders
peirce
(1839–1914)
first
introduced
the
term
as
"guessing"
peirce
said
that
to
"abduce"
a
hypothetical
explanation
formula_3
from
an
observed
surprising
circumstance
formula_4
is
to
surmise
that
formula_3
may
be
true
because
then
formula_4
would
be
a
matter
of
course
thus
to
abduce
formula_3
from
formula_4
involves
determining
that
formula_3
is
sufficient
(or
nearly
sufficient)
but
not
necessary
for
formula_4
while
inductive
and
abductive
inference
are
not
part
of
logic
proper
the
methodology
of
logic
has
been
applied
to
them
with
some
degree
of
success
for
example
the
notion
of
deductive
validity
(where
an
inference
is
deductively
valid
if
and
only
if
there
is
no
possible
situation
in
which
all
the
premises
are
true
but
the
conclusion
false)
exists
in
an
analogy
to
the
notion
of
inductive
validity
or
"strength"
where
an
inference
is
inductively
strong
if
and
only
if
its
premises
give
some
degree
of
probability
to
its
conclusion
whereas
the
notion
of
deductive
validity
can
be
rigorously
stated
for
systems
of
formal
logic
in
terms
of
the
well-understood
notions
of
semantics
inductive
validity
requires
us
to
define
a
reliable
generalization
of
some
set
of
observations
the
task
of
providing
this
definition
may
be
approached
in
various
ways
some
less
formal
than
others;
some
of
these
definitions
may
use
logical
association
rule
induction
while
others
may
use
mathematical
models
of
probability
such
as
decision
trees
logic
arose
(see
below)
from
a
concern
with
correctness
of
argumentation
modern
logicians
usually
wish
to
ensure
that
logic
studies
just
those
arguments
that
arise
from
appropriately
general
forms
of
inference
for
example
thomas
hofweber
writes
in
the
"stanford
encyclopedia
of
philosophy"
that
logic
"does
not
however
cover
good
reasoning
as
a
whole
that
is
the
job
of
the
theory
of
rationality
rather
it
deals
with
inferences
whose
validity
can
be
traced
back
to
the
formal
features
of
the
representations
that
are
involved
in
that
inference
be
they
linguistic
mental
or
other
representations"
logic
has
been
defined
as
"the
study
of
arguments
correct
in
virtue
of
their
form"
this
has
not
been
the
definition
taken
in
this
article
but
the
idea
that
logic
treats
special
forms
of
argument
deductive
argument
rather
than
argument
in
general
has
a
history
in
logic
that
dates
back
at
least
to
logicism
in
mathematics
(19th
and
20th
centuries)
and
the
advent
of
the
influence
of
mathematical
logic
on
philosophy
a
consequence
of
taking
logic
to
treat
special
kinds
of
argument
is
that
it
leads
to
identification
of
special
kinds
of
truth
the
logical
truths
(with
logic
equivalently
being
the
study
of
logical
truth)
and
excludes
many
of
the
original
objects
of
study
of
logic
that
are
treated
as
informal
logic
robert
brandom
has
argued
against
the
idea
that
logic
is
the
study
of
a
special
kind
of
logical
truth
arguing
that
instead
one
can
talk
of
the
logic
of
material
inference
(in
the
terminology
of
wilfred
sellars)
with
logic
making
explicit
the
commitments
that
were
originally
implicit
in
informal
inference
logic
comes
from
the
greek
word
"logos"
originally
meaning
"the
word"
or
"what
is
spoken"
but
coming
to
mean
"thought"
or
"reason"
in
the
western
world
logic
was
first
developed
by
aristotle
who
called
the
subject
'analytics'
aristotelian
logic
became
widely
accepted
in
science
and
mathematics
and
remained
in
wide
use
in
the
west
until
the
early
19th century
aristotle's
system
of
logic
was
responsible
for
the
introduction
of
hypothetical
syllogism
temporal
modal
logic
and
inductive
logic
as
well
as
influential
vocabulary
such
as
terms
predicables
syllogisms
and
propositions
there
was
also
the
rival
stoic
logic
in
europe
during
the
later
medieval
period
major
efforts
were
made
to
show
that
aristotle's
ideas
were
compatible
with
christian
faith
during
the
high
middle
ages
logic
became
a
main
focus
of
philosophers
who
would
engage
in
critical
logical
analyses
of
philosophical
arguments
often
using
variations
of
the
methodology
of
scholasticism
in
1323
william
of
ockham's
influential
"summa
logicae"
was
released
by
the
18th
century
the
structured
approach
to
arguments
had
degenerated
and
fallen
out
of
favour
as
depicted
in
holberg's
satirical
play
"erasmus
montanus"
the
chinese
logical
philosopher
gongsun
long
()
proposed
the
paradox
"one
and
one
cannot
become
two
since
neither
becomes
two"
in
china
the
tradition
of
scholarly
investigation
into
logic
however
was
repressed
by
the
qin
dynasty
following
the
legalist
philosophy
of
han
feizi
in
india
the
anviksiki
school
of
logic
was
founded
by
medhatithi
gautama
(c
6th
century
bce)
innovations
in
the
scholastic
school
called
nyaya
continued
from
ancient
times
into
the
early
18th century
with
the
navya-nyaya
school
by
the
16th century
it
developed
theories
resembling
modern
logic
such
as
gottlob
frege's
"distinction
between
sense
and
reference
of
proper
names"
and
his
"definition
of
number"
as
well
as
the
theory
of
"restrictive
conditions
for
universals"
anticipating
some
of
the
developments
in
modern
set
theory
since
1824
indian
logic
attracted
the
attention
of
many
western
scholars
and
has
had
an
influence
on
important
19th-century
logicians
such
as
charles
babbage
augustus
de
morgan
and
george
boole
in
the
20th century
western
philosophers
like
stanislaw
schayer
and
klaus
glashoff
have
explored
indian
logic
more
extensively
the
syllogistic
logic
developed
by
aristotle
predominated
in
the
west
until
the
mid-19th century
when
interest
in
the
foundations
of
mathematics
stimulated
the
development
of
symbolic
logic
(now
called
mathematical
logic)
in
1854
george
boole
published
"an
investigation
of
the
laws
of
thought
on
which
are
founded
the
mathematical
theories
of
logic
and
probabilities"
introducing
symbolic
logic
and
the
principles
of
what
is
now
known
as
boolean
logic
in
1879
gottlob
frege
published
"begriffsschrift"
which
inaugurated
modern
logic
with
the
invention
of
quantifier
notation
reconciling
the
aristotelian
and
stoic
logics
in
a
broader
system
and
solving
such
problems
for
which
aristotelian
logic
was
impotent
such
as
the
problem
of
multiple
generality
from
1910
to
1913
alfred
north
whitehead
and
bertrand
russell
published
"principia
mathematica"
on
the
foundations
of
mathematics
attempting
to
derive
mathematical
truths
from
axioms
and
inference
rules
in
symbolic
logic
in
1931
gödel
raised
serious
problems
with
the
foundationalist
program
and
logic
ceased
to
focus
on
such
issues
the
development
of
logic
since
frege
russell
and
wittgenstein
had
a
profound
influence
on
the
practice
of
philosophy
and
the
perceived
nature
of
philosophical
problems
(see
analytic
philosophy)
and
philosophy
of
mathematics
logic
especially
sentential
logic
is
implemented
in
computer
logic
circuits
and
is
fundamental
to
computer
science
logic
is
commonly
taught
by
university
philosophy
departments
often
as
a
compulsory
discipline
the
"organon"
was
aristotle's
body
of
work
on
logic
with
the
"prior
analytics"
constituting
the
first
explicit
work
in
formal
logic
introducing
the
syllogistic
the
parts
of
syllogistic
logic
also
known
by
the
name
term
logic
are
the
analysis
of
the
judgements
into
propositions
consisting
of
two
terms
that
are
related
by
one
of
a
fixed
number
of
relations
and
the
expression
of
inferences
by
means
of
syllogisms
that
consist
of
two
propositions
sharing
a
common
term
as
premise
and
a
conclusion
that
is
a
proposition
involving
the
two
unrelated
terms
from
the
premises
aristotle's
work
was
regarded
in
classical
times
and
from
medieval
times
in
europe
and
the
middle
east
as
the
very
picture
of
a
fully
worked
out
system
however
it
was
not
alone:
the
stoics
proposed
a
system
of
propositional
logic
that
was
studied
by
medieval
logicians
also
the
problem
of
multiple
generality
was
recognized
in
medieval
times
nonetheless
problems
with
syllogistic
logic
were
not
seen
as
being
in
need
of
revolutionary
solutions
today
some
academics
claim
that
aristotle's
system
is
generally
seen
as
having
little
more
than
historical
value
(though
there
is
some
current
interest
in
extending
term
logics)
regarded
as
made
obsolete
by
the
advent
of
propositional
logic
and
the
predicate
calculus
others
use
aristotle
in
argumentation
theory
to
help
develop
and
critically
question
argumentation
schemes
that
are
used
in
artificial
intelligence
and
legal
arguments
a
propositional
calculus
or
logic
(also
a
sentential
calculus)
is
a
formal
system
in
which
formulae
representing
propositions
can
be
formed
by
combining
atomic
propositions
using
logical
connectives
and
in
which
a
system
of
formal
proof
rules
establishes
certain
formulae
as
"theorems"
an
example
of
a
theorem
of
propositional
logic
is
formula_11
which
says
that
if
a
holds
then
b
implies
a
predicate
logic
is
the
generic
term
for
symbolic
formal
systems
such
as
first-order
logic
second-order
logic
many-sorted
logic
and
infinitary
logic
it
provides
an
account
of
quantifiers
general
enough
to
express
a
wide
set
of
arguments
occurring
in
natural
language
for
example
bertrand
russell's
famous
barber
paradox
"there
is
a
man
who
shaves
all
and
only
men
who
do
not
shave
themselves"
can
be
formalised
by
the
sentence
formula_12
using
the
non-logical
predicate
formula_13
to
indicate
that
"x"
is
a
man
and
the
non-logical
relation
formula_14
to
indicate
that
"x"
shaves
"y";
all
other
symbols
of
the
formulae
are
logical
expressing
the
universal
and
existential
quantifiers
conjunction
implication
negation
and
biconditional
whilst
aristotelian
syllogistic
logic
specifies
a
small
number
of
forms
that
the
relevant
part
of
the
involved
judgements
may
take
predicate
logic
allows
sentences
to
be
analysed
into
subject
and
argument
in
several
additional
ways—allowing
predicate
logic
to
solve
the
problem
of
multiple
generality
that
had
perplexed
medieval
logicians
the
development
of
predicate
logic
is
usually
attributed
to
gottlob
frege
who
is
also
credited
as
one
of
the
founders
of
analytical
philosophy
but
the
formulation
of
predicate
logic
most
often
used
today
is
the
first-order
logic
presented
in
principles
of
mathematical
logic
by
david
hilbert
and
wilhelm
ackermann
in
1928
the
analytical
generality
of
predicate
logic
allowed
the
formalization
of
mathematics
drove
the
investigation
of
set
theory
and
allowed
the
development
of
alfred
tarski's
approach
to
model
theory
it
provides
the
foundation
of
modern
mathematical
logic
frege's
original
system
of
predicate
logic
was
second-order
rather
than
first-order
second-order
logic
is
most
prominently
defended
(against
the
criticism
of
willard
van
orman
quine
and
others)
by
george
boolos
and
stewart
shapiro
in
languages
modality
deals
with
the
phenomenon
that
sub-parts
of
a
sentence
may
have
their
semantics
modified
by
special
verbs
or
modal
particles
for
example
""we
go
to
the
games"
can
be
modified
to
give
"we
should
go
to
the
games"
and
"we
can
go
to
the
games"
and
perhaps
"we
will
go
to
the
games""
more
abstractly
we
might
say
that
modality
affects
the
circumstances
in
which
we
take
an
assertion
to
be
satisfied
confusing
modality
is
known
as
the
modal
fallacy
aristotle's
logic
is
in
large
parts
concerned
with
the
theory
of
non-modalized
logic
although
there
are
passages
in
his
work
such
as
the
famous
sea-battle
argument
in
"de
interpretatione"
§
9
that
are
now
seen
as
anticipations
of
modal
logic
and
its
connection
with
potentiality
and
time
the
earliest
formal
system
of
modal
logic
was
developed
by
avicenna
who
ultimately
developed
a
theory
of
"temporally
modalized"
syllogistic
while
the
study
of
necessity
and
possibility
remained
important
to
philosophers
little
logical
innovation
happened
until
the
landmark
investigations
of
clarence
irving
lewis
in
1918
who
formulated
a
family
of
rival
axiomatizations
of
the
alethic
modalities
his
work
unleashed
a
torrent
of
new
work
on
the
topic
expanding
the
kinds
of
modality
treated
to
include
deontic
logic
and
epistemic
logic
the
seminal
work
of
arthur
prior
applied
the
same
formal
language
to
treat
temporal
logic
and
paved
the
way
for
the
marriage
of
the
two
subjects
saul
kripke
discovered
(contemporaneously
with
rivals)
his
theory
of
frame
semantics
which
revolutionized
the
formal
technology
available
to
modal
logicians
and
gave
a
new
graph-theoretic
way
of
looking
at
modality
that
has
driven
many
applications
in
computational
linguistics
and
computer
science
such
as
dynamic
logic
the
motivation
for
the
study
of
logic
in
ancient
times
was
clear:
it
is
so
that
one
may
learn
to
distinguish
good
arguments
from
bad
arguments
and
so
become
more
effective
in
argument
and
oratory
and
perhaps
also
to
become
a
better
person
half
of
the
works
of
aristotle's
organon
treat
inference
as
it
occurs
in
an
informal
setting
side
by
side
with
the
development
of
the
syllogistic
and
in
the
aristotelian
school
these
informal
works
on
logic
were
seen
as
complementary
to
aristotle's
treatment
of
rhetoric
this
ancient
motivation
is
still
alive
although
it
no
longer
takes
centre
stage
in
the
picture
of
logic;
typically
dialectical
logic
forms
the
heart
of
a
course
in
critical
thinking
a
compulsory
course
at
many
universities
dialectic
has
been
linked
to
logic
since
ancient
times
but
it
has
not
been
until
recent
decades
that
european
and
american
logicians
have
attempted
to
provide
mathematical
foundations
for
logic
and
dialectic
by
formalising
dialectical
logic
dialectical
logic
is
also
the
name
given
to
the
special
treatment
of
dialectic
in
hegelian
and
marxist
thought
there
have
been
pre-formal
treatises
on
argument
and
dialectic
from
authors
such
as
stephen
toulmin
("the
uses
of
argument")
nicholas
rescher
("dialectics")
and
van
eemeren
and
grootendorst
(pragma-dialectics)
theories
of
defeasible
reasoning
can
provide
a
foundation
for
the
formalisation
of
dialectical
logic
and
dialectic
itself
can
be
formalised
as
moves
in
a
game
where
an
advocate
for
the
truth
of
a
proposition
and
an
opponent
argue
such
games
can
provide
a
formal
game
semantics
for
many
logics
argumentation
theory
is
the
study
and
research
of
informal
logic
fallacies
and
critical
questions
as
they
relate
to
every
day
and
practical
situations
specific
types
of
dialogue
can
be
analyzed
and
questioned
to
reveal
premises
conclusions
and
fallacies
argumentation
theory
is
now
applied
in
artificial
intelligence
and
law
mathematical
logic
comprises
two
distinct
areas
of
research:
the
first
is
the
application
of
the
techniques
of
formal
logic
to
mathematics
and
mathematical
reasoning
and
the
second
in
the
other
direction
the
application
of
mathematical
techniques
to
the
representation
and
analysis
of
formal
logic
the
earliest
use
of
mathematics
and
geometry
in
relation
to
logic
and
philosophy
goes
back
to
the
ancient
greeks
such
as
euclid
plato
and
aristotle
many
other
ancient
and
medieval
philosophers
applied
mathematical
ideas
and
methods
to
their
philosophical
claims
one
of
the
boldest
attempts
to
apply
logic
to
mathematics
was
the
logicism
pioneered
by
philosopher-logicians
such
as
gottlob
frege
and
bertrand
russell
mathematical
theories
were
supposed
to
be
logical
tautologies
and
the
programme
was
to
show
this
by
means
of
a
reduction
of
mathematics
to
logic
the
various
attempts
to
carry
this
out
met
with
failure
from
the
crippling
of
frege's
project
in
his
"grundgesetze"
by
russell's
paradox
to
the
defeat
of
hilbert's
program
by
gödel's
incompleteness
theorems
both
the
statement
of
hilbert's
program
and
its
refutation
by
gödel
depended
upon
their
work
establishing
the
second
area
of
mathematical
logic
the
application
of
mathematics
to
logic
in
the
form
of
proof
theory
despite
the
negative
nature
of
the
incompleteness
theorems
gödel's
completeness
theorem
a
result
in
model
theory
and
another
application
of
mathematics
to
logic
can
be
understood
as
showing
how
close
logicism
came
to
being
true:
every
rigorously
defined
mathematical
theory
can
be
exactly
captured
by
a
first-order
logical
theory;
frege's
proof
calculus
is
enough
to
"describe"
the
whole
of
mathematics
though
not
"equivalent"
to
it
if
proof
theory
and
model
theory
have
been
the
foundation
of
mathematical
logic
they
have
been
but
two
of
the
four
pillars
of
the
subject
set
theory
originated
in
the
study
of
the
infinite
by
georg
cantor
and
it
has
been
the
source
of
many
of
the
most
challenging
and
important
issues
in
mathematical
logic
from
cantor's
theorem
through
the
status
of
the
axiom
of
choice
and
the
question
of
the
independence
of
the
continuum
hypothesis
to
the
modern
debate
on
large
cardinal
axioms
recursion
theory
captures
the
idea
of
computation
in
logical
and
arithmetic
terms;
its
most
classical
achievements
are
the
undecidability
of
the
entscheidungsproblem
by
alan
turing
and
his
presentation
of
the
church–turing
thesis
today
recursion
theory
is
mostly
concerned
with
the
more
refined
problem
of
complexity
classes—when
is
a
problem
efficiently
solvable?—and
the
classification
of
degrees
of
unsolvability
philosophical
logic
deals
with
formal
descriptions
of
ordinary
non-specialist
("natural")
language
that
is
strictly
only
about
the
arguments
within
philosophy's
other
branches
most
philosophers
assume
that
the
bulk
of
everyday
reasoning
can
be
captured
in
logic
if
a
method
or
methods
to
translate
ordinary
language
into
that
logic
can
be
found
philosophical
logic
is
essentially
a
continuation
of
the
traditional
discipline
called
"logic"
before
the
invention
of
mathematical
logic
philosophical
logic
has
a
much
greater
concern
with
the
connection
between
natural
language
and
logic
as
a
result
philosophical
logicians
have
contributed
a
great
deal
to
the
development
of
non-standard
logics
(eg
free
logics
tense
logics)
as
well
as
various
extensions
of
classical
logic
(eg
modal
logics)
and
non-standard
semantics
for
such
logics
(eg
kripke's
supervaluationism
in
the
semantics
of
logic)
logic
and
the
philosophy
of
language
are
closely
related
philosophy
of
language
has
to
do
with
the
study
of
how
our
language
engages
and
interacts
with
our
thinking
logic
has
an
immediate
impact
on
other
areas
of
study
studying
logic
and
the
relationship
between
logic
and
ordinary
speech
can
help
a
person
better
structure
his
own
arguments
and
critique
the
arguments
of
others
many
popular
arguments
are
filled
with
errors
because
so
many
people
are
untrained
in
logic
and
unaware
of
how
to
formulate
an
argument
correctly
logic
cut
to
the
heart
of
computer
science
as
it
emerged
as
a
discipline:
alan
turing's
work
on
the
"entscheidungsproblem"
followed
from
kurt
gödel's
work
on
the
incompleteness
theorems
the
notion
of
the
general
purpose
computer
that
came
from
this
work
was
of
fundamental
importance
to
the
designers
of
the
computer
machinery
in
the
1940s
in
the
1950s
and
1960s
researchers
predicted
that
when
human
knowledge
could
be
expressed
using
logic
with
mathematical
notation
it
would
be
possible
to
create
a
machine
that
reasons
or
artificial
intelligence
this
was
more
difficult
than
expected
because
of
the
complexity
of
human
reasoning
in
logic
programming
a
program
consists
of
a
set
of
axioms
and
rules
logic
programming
systems
such
as
prolog
compute
the
consequences
of
the
axioms
and
rules
in
order
to
answer
a
query
today
logic
is
extensively
applied
in
the
fields
of
artificial
intelligence
and
computer
science
and
these
fields
provide
a
rich
source
of
problems
in
formal
and
informal
logic
argumentation
theory
is
one
good
example
of
how
logic
is
being
applied
to
artificial
intelligence
the
acm
computing
classification
system
in
particular
regards:
furthermore
computers
can
be
used
as
tools
for
logicians
for
example
in
symbolic
logic
and
mathematical
logic
proofs
by
humans
can
be
computer-assisted
using
automated
theorem
proving
the
machines
can
find
and
check
proofs
as
well
as
work
with
proofs
too
lengthy
to
write
out
by
hand
the
logics
discussed
above
are
all
"bivalent"
or
"two-valued";
that
is
they
are
most
naturally
understood
as
dividing
propositions
into
true
and
false
propositions
non-classical
logics
are
those
systems
that
reject
various
rules
of
classical
logic
hegel
developed
his
own
dialectic
logic
that
extended
kant's
transcendental
logic
but
also
brought
it
back
to
ground
by
assuring
us
that
"neither
in
heaven
nor
in
earth
neither
in
the
world
of
mind
nor
of
nature
is
there
anywhere
such
an
abstract
'either–or'
as
the
understanding
maintains
whatever
exists
is
concrete
with
difference
and
opposition
in
itself"
in
1910
nicolai
a
vasiliev
extended
the
law
of
excluded
middle
and
the
law
of
contradiction
and
proposed
the
law
of
excluded
fourth
and
logic
tolerant
to
contradiction
in
the
early
20th century
jan
łukasiewicz
investigated
the
extension
of
the
traditional
true/false
values
to
include
a
third
value
"possible"
so
inventing
ternary
logic
the
first
multi-valued
logic
in
the
western
tradition
logics
such
as
fuzzy
logic
have
since
been
devised
with
an
infinite
number
of
"degrees
of
truth"
represented
by
a
real
number
between
0
and
1
intuitionistic
logic
was
proposed
by
lej
brouwer
as
the
correct
logic
for
reasoning
about
mathematics
based
upon
his
rejection
of
the
law
of
the
excluded
middle
as
part
of
his
intuitionism
brouwer
rejected
formalization
in
mathematics
but
his
student
arend
heyting
studied
intuitionistic
logic
formally
as
did
gerhard
gentzen
intuitionistic
logic
is
of
great
interest
to
computer
scientists
as
it
is
a
constructive
logic
and
sees
many
applications
such
as
extracting
verified
programs
from
proofs
and
influencing
the
design
of
programming
languages
through
the
formulae-as-types
correspondence
modal
logic
is
not
truth
conditional
and
so
it
has
often
been
proposed
as
a
non-classical
logic
however
modal
logic
is
normally
formalized
with
the
principle
of
the
excluded
middle
and
its
relational
semantics
is
bivalent
so
this
inclusion
is
disputable
what
is
the
epistemological
status
of
the
laws
of
logic?
what
sort
of
argument
is
appropriate
for
criticizing
purported
principles
of
logic?
in
an
influential
paper
entitled
"is
logic
empirical?"
hilary
putnam
building
on
a
suggestion
of
w
v
quine
argued
that
in
general
the
facts
of
propositional
logic
have
a
similar
epistemological
status
as
facts
about
the
physical
universe
for
example
as
the
laws
of
mechanics
or
of
general
relativity
and
in
particular
that
what
physicists
have
learned
about
quantum
mechanics
provides
a
compelling
case
for
abandoning
certain
familiar
principles
of
classical
logic:
if
we
want
to
be
realists
about
the
physical
phenomena
described
by
quantum
theory
then
we
should
abandon
the
principle
of
distributivity
substituting
for
classical
logic
the
quantum
logic
proposed
by
garrett
birkhoff
and
john
von
neumann
another
paper
of
the
same
name
by
michael
dummett
argues
that
putnam's
desire
for
realism
mandates
the
law
of
distributivity
distributivity
of
logic
is
essential
for
the
realist's
understanding
of
how
propositions
are
true
of
the
world
in
just
the
same
way
as
he
has
argued
the
principle
of
bivalence
is
in
this
way
the
question
"is
logic
empirical?"
can
be
seen
to
lead
naturally
into
the
fundamental
controversy
in
metaphysics
on
realism
versus
anti-realism
the
notion
of
implication
formalized
in
classical
logic
does
not
comfortably
translate
into
natural
language
by
means
of
"if 
then "
due
to
a
number
of
problems
called
the
paradoxes
of
material
implication
the
first
class
of
paradoxes
involves
counterfactuals
such
as
"if
the
moon
is
made
of
green
cheese
then
2+2=5"
which
are
puzzling
because
natural
language
does
not
support
the
principle
of
explosion
eliminating
this
class
of
paradoxes
was
the
reason
for
ci
lewis's
formulation
of
strict
implication
which
eventually
led
to
more
radically
revisionist
logics
such
as
relevance
logic
the
second
class
of
paradoxes
involves
redundant
premises
falsely
suggesting
that
we
know
the
succedent
because
of
the
antecedent:
thus
"if
that
man
gets
elected
granny
will
die"
is
materially
true
since
granny
is
mortal
regardless
of
the
man's
election
prospects
such
sentences
violate
the
gricean
maxim
of
relevance
and
can
be
modelled
by
logics
that
reject
the
principle
of
monotonicity
of
entailment
such
as
relevance
logic
hegel
was
deeply
critical
of
any
simplified
notion
of
the
law
of
non-contradiction
it
was
based
on
gottfried
wilhelm
leibniz's
idea
that
this
law
of
logic
also
requires
a
sufficient
ground
to
specify
from
what
point
of
view
(or
time)
one
says
that
something
cannot
contradict
itself
a
building
for
example
both
moves
and
does
not
move;
the
ground
for
the
first
is
our
solar
system
and
for
the
second
the
earth
in
hegelian
dialectic
the
law
of
non-contradiction
of
identity
itself
relies
upon
difference
and
so
is
not
independently
assertable
closely
related
to
questions
arising
from
the
paradoxes
of
implication
comes
the
suggestion
that
logic
ought
to
tolerate
inconsistency
relevance
logic
and
paraconsistent
logic
are
the
most
important
approaches
here
though
the
concerns
are
different:
a
key
consequence
of
classical
logic
and
some
of
its
rivals
such
as
intuitionistic
logic
is
that
they
respect
the
principle
of
explosion
which
means
that
the
logic
collapses
if
it
is
capable
of
deriving
a
contradiction
graham
priest
the
main
proponent
of
dialetheism
has
argued
for
paraconsistency
on
the
grounds
that
there
are
in
fact
true
contradictions
the
philosophical
vein
of
various
kinds
of
skepticism
contains
many
kinds
of
doubt
and
rejection
of
the
various
bases
on
which
logic
rests
such
as
the
idea
of
logical
form
correct
inference
or
meaning
typically
leading
to
the
conclusion
that
there
are
no
logical
truths
this
is
in
contrast
with
the
usual
views
in
philosophical
skepticism
where
logic
directs
skeptical
enquiry
to
doubt
received
wisdoms
as
in
the
work
of
sextus
empiricus
friedrich
nietzsche
provides
a
strong
example
of
the
rejection
of
the
usual
basis
of
logic:
his
radical
rejection
of
idealization
led
him
to
reject
truth
as
a
" mobile
army
of
metaphors
metonyms
and
anthropomorphisms—in
short 
metaphors
which
are
worn
out
and
without
sensuous
power;
coins
which
have
lost
their
pictures
and
now
matter
only
as
metal
no
longer
as
coins"
his
rejection
of
truth
did
not
lead
him
to
reject
the
idea
of
either
inference
or
logic
completely
but
rather
suggested
that
"logic
[came]
into
existence
in
man's
head
[out]
of
illogic
whose
realm
originally
must
have
been
immense
innumerable
beings
who
made
inferences
in
a
way
different
from
ours
perished"
thus
there
is
the
idea
that
logical
inference
has
a
use
as
a
tool
for
human
survival
but
that
its
existence
does
not
support
the
existence
of
truth
nor
does
it
have
a
reality
beyond
the
instrumental:
"logic
too
also
rests
on
assumptions
that
do
not
correspond
to
anything
in
the
real
world"
this
position
held
by
nietzsche
however
has
come
under
extreme
scrutiny
for
several
reasons
some
philosophers
such
as
jürgen
habermas
claim
his
position
is
self-refuting—and
accuse
nietzsche
of
not
even
having
a
coherent
perspective
let
alone
a
theory
of
knowledge
georg
lukács
in
his
book
"the
destruction
of
reason"
asserts
that
"were
we
to
study
nietzsche's
statements
in
this
area
from
a
logico-philosophical
angle
we
would
be
confronted
by
a
dizzy
chaos
of
the
most
lurid
assertions
arbitrary
and
violently
incompatible"
bertrand
russell
described
nietzsche's
irrational
claims
with
"he
is
fond
of
expressing
himself
paradoxically
and
with
a
view
to
shocking
conventional
readers"
in
his
book
"a
history
of
western
philosophy"
state
college
of
florida
manatee–sarasota
state
college
of
florida
manatee-sarasota
(scf)
is
a
state
college
with
campuses
located
in
manatee
and
sarasota
county
florida
part
of
the
florida
college
system
it
is
designated
a
"state
college"
because
it
offers
a
greater
number
of
four-year
bachelor's
degrees
than
traditional
two-year
community
colleges
founded
in
1957
as
manatee
junior
college
it
was
known
as
manatee
community
college
from
1985
to
2009
today
scf
operates
three
campuses
in
bradenton
lakewood
ranch
and
venice
the
bradenton
campus
includes
the
family
heritage
house
museum
the
scf
collegiate
school
(scfcs)
the
neel
performing
arts
center
and
the
scf
dental
hygiene
clinic
which
provides
low
cost
dental
care
to
the
public
state
college
of
florida
was
established
on
september
17
1957
by
the
florida
board
of
education
as
manatee
junior
college
the
college
came
into
existence
under
a
plan
of
the
florida
board
of
education
to
provide
accessible
higher
education
to
florida's
population
the
first
classes
were
held
on
september
2
1958
in
what
was
formerly
a
senior
high
school;
enrollment
in
the
first
term
was
502
students
the
college
began
administering
classes
in
its
own
facilities
in
1959
where
the
100
acre
bradenton
campus
stands
today
the
venice
center
was
opened
in
1977
by
mjc's
board
of
trustees
during
this
period
the
center's
functions
were
funded
by
the
donations
of
residents
living
in
the
surrounding
communities
which
included
venice
north
port
and
englewood
it
was
not
until
1983
that
the
college
received
an
appropriation
from
the
florida
legislature
to
expand
the
venice
center
into
what
is
now
the
full-service
venice
campus
it
was
dedicated
on
march
30
1985
and
the
college's
name
was
changed
that
year
to
manatee
community
college
at
the
beginning
of
2003
mcc
opened
the
lakewood
ranch
campus
the
land
appropriated
for
this
was
donated
by
the
schroeder-manatee
ranch
the
lakewood
ranch
campus
offers
credit
and
non-credit
programs
of
study
as
well
as
technical
and
workforce
development
courses
in
2007
the
schroeder-manatee
ranch
donated
an
additional
to
the
lakewood
ranch
campus
mcc
obtained
supplementary
funding
from
the
florida
legislature
which
was
allocated
for
the
construction
of
a
new
classroom/laboratory
building
in
2009
mcc
received
approval
from
the
state
board
of
education
to
offer
baccalaureate
degrees
and
changed
its
name
to
state
college
of
florida
manatee-sarasota
to
reflect
its
new
status
as
a
four
year
state
institution
the
first
bachelor's
degree
offered
at
scf
was
a
bachelor
of
science
in
nursing
which
started
in
january
of
2010
several
other
bachelor's
degrees
are
now
available
students
can
attend
classes
on
campuses
located
in
bradenton
venice
and
lakewood
ranch
as
well
as
many
business
and
public-sector
sites
throughout
the
community
and
from
their
homes
via
online
classes
on
september
29th
2017
state
college
of
florida
purchased
of
land
to
build
a
brand
new
campus
in
parrish
that
will
provide
high
quality
education
to
those
living
north
of
the
manatee
river
the
college
president
is
dr
carol
f
probstfeld
who
was
inaugurated
as
the
sixth
president
of
state
college
of
florida
on
november
8th
2013
on
june
7th
2018
dr
todd
g
fritch
was
named
the
first
executive
vice
president
and
provost
of
scf
state
college
of
florida
is
accredited
by
the
commission
on
colleges
of
the
southern
association
of
colleges
and
schools
to
award
associate
and
baccalaureate
degrees
noncredit
education
is
offered
under
scf's
corporate
community
development
programs
more
than
50
percent
of
the
college-bound
high
school
students
in
manatee
and
sarasota
counties
attend
scf
each
year
with
a
current
enrollment
of
over
30000
students
scf
is
among
the
top
100
producers
of
associate
degrees
in
the
united
states
scf
has
over
50
different
clubs
and
organizations
for
students
to
participate
in
such
as
intramural
sports
phi
theta
kappa
and
a
circle
k
international
club
state
college
of
florida's
athletics
department
consist
of
five
intercollegiate
sports
teams
they
include:
men's
basketball
baseball
softball
women's
tennis
and
woman's
volleyball
scf's
athletic
teams
are
nicknamed
the
manatees
and
they
participate
in
the
suncoast
conference
of
the
florida
state
college
activities
association
(fscaa)
in
division
i
of
njcaa
region
viii
state
college
of
florida's
music
department
is
home
to
over
nine
different
performing
ensembles
consisting
of
the
bradenton
symphony
orchestra
symphonic
band
chamber
choir
concert
choir
jazz
ensemble
jazz
combo
guitar
ensemble
keyboard
studies
presidential
string
quartet
and
the
musical
theatre
ensemble
music
students
from
scf
perform
in
multiple
concerts
throughout
the
semester
as
well
as
various
community
and
state
events
such
as
the
fscaa
symposium
the
theatre
and
musical
theatre
department
at
state
college
of
florida
does
a
total
of
four
productions
per
year
or
two
per
semester
the
theatre
faculty
includes
dean
anthony
craig
smith
james
thaggard
(he
is
also
the
box
office
manager)
and
melodie
dickerson
most
theatre
graduates
have
gone
on
to
four-year
universities
to
receive
ba
or
bfa
degrees
in
theatre
and
performing
arts
the
brain
bowl
team
at
state
college
of
florida
currently
coached
by
christina
dwyer
has
achieved
state
and
national
recognition
for
being
one
of
the
top
quiz
bowl
programs
in
the
country
in
the
2014-2015
competition
season
scf's
"fire
team"
compiled
a
record
of
58-2
against
other
two-year
schools
going
on
to
win
championships
at
tournaments
such
as
the
2014
delta
burke
invitational
2015
fcsaa
west
central
regional
2015
fcsaa
brain
bowl
state
championship
2015
naqt
south
florida
community
college
sectionals
and
the
2015
naqt
community
college
championship
tournament
the
team
was
also
invited
to
compete
in
naqt's
intercollegiate
championship
tournament
(dii)
where
the
team
placed
25th
with
a
record
of
7-6
notably
defeating
four-year
schools
such
as
uc
berkeley
duke
university
university
of
alabama
and
claremont
colleges
in
the
process
the
2015
state
and
national
championship
teams
consisted
of
team
captain
and
club
president
michael
moore
jr
and
players
naimul
chowdhury
leon
hostetler
austin
goode
carlyle
styer
kara
stevens
and
christopher
medrano
in
individual
competition
moore
and
chowdhury
placed
third
and
seventh
in
the
nation
respectively
former
coaches
include
dr
hyun
kim
(co-coach
with
christina
dwyer
during
2014-2015
season)
and
dr
carole
cole
the
2016
state
and
national
championship
teams
consisted
of
team
captain
and
club
president
michael
friedman
and
players
david
espinal
former
championship
winner
austin
goode
paul
forester
and
haley
miller
they
won
first
place
at
state
and
second
at
nationals
scf
player
david
espinal
would
lead
the
2017
team
to
championships
and
states
with
players
lacey
anderson
damien
bobrek
and
nathanael
havlik
the
team
went
on
to
win
first
place
at
erik
korray
third
in
state
and
third
at
nationals
the
current
2018
team
is
lead
by
david
espinal
and
austin
goode
with
fellow
players
ariel
rodriguez
lacey
anderson
justin
reitwiesner
and
sierra
beeson
the
bradenton
campus
is
home
to
the
family
heritage
house
museum
a
gallery
and
resource
center
for
the
study
of
african-american
achievements
exhibits
include
a
timeline
of
significant
events
in
african-american
history
including
slavery
fights
for
freedom
community
building
and
education
the
harlem
renaissance
the
civil
rights
movement
kwanzaa
and
the
modern
era
in
south
africa
there
are
also
displays
about
the
underground
railroad
and
a
collection
of
african
masks
admission
is
free
alumni
status
is
open
to
all
graduates
of
state
college
of
florida
(formerly
manatee
community
college)
all
former
students
of
scf
who
regularly
matriculated
and
left
scf
in
good
standing
inspired
education
group
inspired
education
group
is
a
group
who
operates
and
builds
schools
in
europe
the
middle
east
africa
australia
and
latin
america
inspired
was
founded
by
nadim
m
nsouli
in
2013
when
his
group
acquired
reddam
house
in
south
africa
since
its
founding
reddam
house
has
acquired
and
built
9
schools
in
south
africa
along
with
the
group's
flagship
school
reddam
house
berkshire
in
wokingham
england
the
school
grounds
in
wokingham
were
taken
over
from
bearwood
college
in
2015
nsouli
-
the
founder
ceo
and
chairman
-
has
worked
as
a
lawyer
and
as
an
investment
banker
and
he
has
led
the
group
since
its
founding
inspired’s
president
is
graeme
crawford
a
south
african
educator
and
founder
of
reddam
house
dr
stephen
spurr
is
the
group
education
director
and
was
the
head
master
of
westminster
school
in
london
from
2004
to
2015
he
joined
inspired
in
2014
the
group’s
strategy
has
been
described
as
"buy
and
build"
involving
the
purchase
of
existing
schools
as
well
as
the
building
of
new
schools
it
has
offices
in
london
milan
auckland
bogota
johannesburgand
dubai
schools
in
other
european
countries
that
form
part
of
the
inspired
group
include
st
george's
international
school
in
switzerland
st
john's
international
school
in
belgium
st
louis
school
in
italy
and
sotogrande
international
school
in
spain
in
the
middle
east
the
group
has
acquired
british
school
of
bahrain
inspired's
latin
american
schools
include
blue
valley
school
in
costa
rica
colegio
san
mateo
in
colombia
and
cambridge
college
lima
in
peru
by
2017
inspired
operated
more
than
30
schools
and
as
of
2018
educating
over
35000
students
in
46
schools
inspired
acquired
part
of
new
zealand’s
biggest
private-education
provider
acg
education’s
schools
division
in
2018
from
pacific
equity
partners
for
about
$500
million
principle
a
principle
is
a
proposition
or
value
that
is
a
guide
for
behavior
or
evaluation
in
law
it
is
a
rule
that
has
to
be
or
usually
is
to
be
followed
or
can
be
desirably
followed
or
is
an
inevitable
consequence
of
something
such
as
the
laws
observed
in
nature
or
the
way
that
a
system
is
constructed
the
principles
of
such
a
system
are
understood
by
its
users
as
the
essential
characteristics
of
the
system
or
reflecting
system's
designed
purpose
and
the
effective
operation
or
use
of
which
would
be
impossible
if
any
one
of
the
principles
was
to
be
ignored
a
system
may
be
explicitly
based
on
and
implemented
from
a
document
of
principles
as
was
done
in
ibm's
360/370
"principles
of
operation"
examples
of
principles
are
entropy
in
a
number
of
fields
least
action
in
physics
those
in
descriptive
comprehensive
and
fundamental
law:
doctrines
or
assumptions
forming
normative
rules
of
conduct
separation
of
church
and
state
in
statecraft
the
central
dogma
of
molecular
biology
fairness
in
ethics
etc
in
common
english
it
is
a
substantive
and
collective
term
referring
to
rule
governance
the
absence
of
which
being
"unprincipled"
is
considered
a
character
defect
it
may
also
be
used
to
declare
that
a
reality
has
diverged
from
some
ideal
or
norm
as
when
something
is
said
to
be
true
only
"in
principle"
but
not
in
fact
a
principle
represents
values
that
orient
and
rule
the
conduct
of
persons
in
a
particular
society
to
"act
on
principle"
is
to
act
in
accordance
with
one's
moral
ideals
principles
are
absorbed
in
childhood
through
a
process
of
socialization
there
is
a
presumption
of
liberty
of
individuals
that
is
restrained
exemplary
principles
include
first
do
no
harm
the
golden
rule
and
the
doctrine
of
the
mean
it
represents
a
set
of
values
that
inspire
the
written
norms
that
organize
the
life
of
a
society
submitting
to
the
powers
of
an
authority
generally
the
state
the
law
establishes
a
legal
obligation
in
a
coercive
way;
it
therefore
acts
as
principle
conditioning
of
the
action
that
limits
the
liberty
of
the
individuals
see
for
examples
the
territorial
principle
homestead
principle
and
precautionary
principle
archimedes
principle
relating
buoyancy
to
the
weight
of
displaced
water
is
an
early
example
of
a
law
in
science
another
early
one
developed
by
malthus
is
the
"population
principle"
now
called
the
malthusian
principle
freud
also
wrote
on
principles
especially
the
reality
principle
necessary
to
keep
the
id
and
pleasure
principle
in
check
biologists
use
the
principle
of
priority
and
principle
of
binominal
nomenclature
for
precision
in
naming
species
there
are
many
principles
observed
in
physics
notably
in
cosmology
which
observes
the
mediocrity
principle
the
anthropic
principle
the
principle
of
relativity
and
the
cosmological
principle
other
well-known
principles
include
the
uncertainty
principle
in
quantum
mechanics
and
the
pigeonhole
principle
and
superposition
principle
in
mathematics
the
principle
states
that
every
event
has
a
rational
explanation
the
principle
has
a
variety
of
expressions
all
of
which
are
perhaps
best
summarized
by
the
following:
however
one
realizes
that
in
every
sentence
there
is
a
direct
relation
between
the
predicate
and
the
subject
to
say
that
"the
earth
is
round"
corresponds
to
a
direct
relation
between
the
subject
and
the
predicate
according
to
aristotle
“it
is
impossible
for
the
same
thing
to
belong
and
not
to
belong
at
the
same
time
to
the
same
thing
and
in
the
same
respect”
for
example
it
is
not
possible
that
in
exactly
the
same
moment
and
place
it
rains
and
doesn't
rain
the
principle
of
the
excluding
third
or
"principium
tertium
exclusum"
is
a
principle
of
the
traditional
logic
formulated
canonically
by
leibniz
as:
either
"a"
is
"b"
or
"a"
isn't
"b"
it
is
read
the
following
way:
either
"p"
is
true
or
its
denial
¬"p"
is
it
is
also
known
as
"tertium
non
datur"
('a
third
(thing)
is
not)
classically
it
is
considered
to
be
one
of
the
most
important
fundamental
principles
or
laws
of
thought
(along
with
the
principles
of
identity
no
contradiction
and
sufficient
reason)
preternatural
the
preternatural
or
praeternatural
is
that
which
appears
outside
or
beside
(latin
"")
the
natural
it
is
"suspended
between
the
mundane
and
the
miraculous"
in
theology
the
term
is
often
used
to
distinguish
marvels
or
deceptive
trickery
often
attributed
to
witchcraft
or
demons
from
the
purely
divine
power
of
the
genuinely
supernatural
to
violate
the
laws
of
nature
in
the
early
modern
period
the
term
was
used
by
scientists
to
refer
to
abnormalities
and
strange
phenomena
of
various
kinds
that
seemed
to
depart
from
the
norms
of
nature
medieval
theologians
made
a
clear
distinction
between
the
natural
the
preternatural
and
the
supernatural
thomas
aquinas
argued
that
the
supernatural
consists
in
"god’s
unmediated
actions";
the
natural
is
"what
happens
always
or
most
of
the
time";
and
the
preternatural
is
"what
happens
rarely
but
nonetheless
by
the
agency
of
created
beings 
marvels
belong
properly
speaking
to
the
realm
of
the
preternatural"
theologians
following
aquinas
argued
that
only
god
had
the
power
to
disregard
the
laws
of
nature
that
he
has
created
but
that
demons
could
manipulate
the
laws
of
nature
by
a
form
of
trickery
to
deceive
the
unwary
into
believing
they
had
experienced
real
miracles
according
to
historian
lorraine
daston
by
the
16th
century
the
term
"preternatural"
was
increasingly
used
to
refer
to
demonic
activity
comparable
to
the
use
of
magic
by
human
adepts:
the
devil
"being
a
natural
magician 
may
perform
many
acts
in
ways
above
our
knowledge
though
not
transcending
our
natural
power"
according
to
the
philosophy
of
the
time
preternatural
phenomena
were
not
contrary
to
divine
law
but
used
hidden
or
occult
powers
that
violated
the
"normal"
pattern
of
natural
phenomena
with
the
emergence
of
early
modern
science
the
concept
of
the
preternatural
increasingly
came
to
be
used
to
refer
to
strange
or
abnormal
phenomena
that
seemed
to
violate
the
normal
working
of
nature
but
which
were
not
associated
with
magic
and
witchcraft
this
was
a
development
of
the
idea
that
preternatural
phenomena
were
fake
miracles
as
daston
puts
it
"to
simplify
the
historical
sequence
somewhat:
first
preternatural
phenomena
were
demonized
and
thereby
incidentally
naturalized;
then
the
demons
were
deleted
leaving
only
the
natural
causes"
the
use
of
the
term
was
especially
common
in
medicine
for
example
in
john
brown's
"a
compleat
treatise
of
preternatural
tumours"
(1678)
or
william
smellie's
"a
collection
of
preternatural
cases
and
observations
in
midwifery"
(1754)
in
the
19th
century
the
term
was
appropriated
in
anthropology
to
refer
to
folk
beliefs
about
fairies
trolls
and
other
such
creatures
which
were
not
thought
of
as
demonic
but
which
were
perceived
to
affect
the
natural
world
in
unpredictable
ways
according
to
thorstein
veblen
such
preternatural
agents
were
often
thought
of
as
forces
somewhere
between
supernatural
beings
and
material
processes
"the
preternatural
agency
is
not
necessarily
conceived
to
be
a
personal
agent
in
the
full
sense
but
it
is
an
agency
which
partakes
of
the
attributes
of
personality
to
the
extent
of
somewhat
arbitrarily
influencing
the
outcome
of
any
enterprise
and
especially
of
any
contest"
the
linguistic
association
between
individual
agents
and
unexplained
or
unfortunate
circumstances
remains
many
people
attribute
occurrences
that
are
known
to
be
material
processes
such
as
"gremlins
in
the
engine"
a
"ghost
in
the
machine"
or
attributing
motives
to
objects:
"the
clouds
are
threatening"
the
anthropomorphism
in
our
daily
life
is
a
combination
of
the
above
cultural
stems
as
well
as
the
manifestation
of
our
pattern-projecting
minds
in
2011
penn
state
press
began
publishing
a
learned
journal
titled
"preternature:
critical
and
historical
studies
on
the
preternatural"
edited
by
kirsten
uszkalo
and
richard
raiswell
the
journal
is
dedicated
to
publishing
articles
reviews
and
short
editions
of
original
texts
that
deal
with
conceptions
and
perceptions
of
the
preternatural
in
any
culture
and
in
any
historical
period
the
journal
covers
"magics
witchcraft
spiritualism
occultism
prophecy
monstrophy
demonology
and
folklore"
nature
nature
in
the
broadest
sense
is
the
natural
physical
or
material
world
or
universe
"nature"
can
refer
to
the
phenomena
of
the
physical
world
and
also
to
life
in
general
the
study
of
nature
is
a
large
if
not
the
only
part
of
science
although
humans
are
part
of
nature
human
activity
is
often
understood
as
a
separate
category
from
other
natural
phenomena
the
word
"nature"
is
derived
from
the
latin
word
"natura"
or
"essential
qualities
innate
disposition"
and
in
ancient
times
literally
meant
"birth"
"natura"
is
a
latin
translation
of
the
greek
word
"physis"
(φύσις)
which
originally
related
to
the
intrinsic
characteristics
that
plants
animals
and
other
features
of
the
world
develop
of
their
own
accord
the
concept
of
nature
as
a
whole
the
physical
universe
is
one
of
several
expansions
of
the
original
notion;
it
began
with
certain
core
applications
of
the
word
φύσις
by
pre-socratic
philosophers
and
has
steadily
gained
currency
ever
since
this
usage
continued
during
the
advent
of
modern
scientific
method
in
the
last
several
centuries
within
the
various
uses
of
the
word
today
"nature"
often
refers
to
geology
and
wildlife
nature
can
refer
to
the
general
realm
of
living
plants
and
animals
and
in
some
cases
to
the
processes
associated
with
inanimate
objects—the
way
that
particular
types
of
things
exist
and
change
of
their
own
accord
such
as
the
weather
and
geology
of
the
earth
it
is
often
taken
to
mean
the
"natural
environment"
or
wilderness—wild
animals
rocks
forest
and
in
general
those
things
that
have
not
been
substantially
altered
by
human
intervention
or
which
persist
despite
human
intervention
for
example
manufactured
objects
and
human
interaction
generally
are
not
considered
part
of
nature
unless
qualified
as
for
example
"human
nature"
or
"the
whole
of
nature"
this
more
traditional
concept
of
natural
things
which
can
still
be
found
today
implies
a
distinction
between
the
natural
and
the
artificial
with
the
artificial
being
understood
as
that
which
has
been
brought
into
being
by
a
human
consciousness
or
a
human
mind
depending
on
the
particular
context
the
term
"natural"
might
also
be
distinguished
from
the
or
the
supernatural
earth
is
the
only
planet
known
to
support
life
and
its
natural
features
are
the
subject
of
many
fields
of
scientific
research
within
the
solar
system
it
is
third
closest
to
the
sun;
it
is
the
largest
terrestrial
planet
and
the
fifth
largest
overall
its
most
prominent
climatic
features
are
its
two
large
polar
regions
two
relatively
narrow
temperate
zones
and
a
wide
equatorial
tropical
to
subtropical
region
precipitation
varies
widely
with
location
from
several
metres
of
water
per
year
to
less
than
a
millimetre
71
percent
of
the
earth's
surface
is
covered
by
salt-water
oceans
the
remainder
consists
of
continents
and
islands
with
most
of
the
inhabited
land
in
the
northern
hemisphere
earth
has
evolved
through
geological
and
biological
processes
that
have
left
traces
of
the
original
conditions
the
outer
surface
is
divided
into
several
gradually
migrating
tectonic
plates
the
interior
remains
active
with
a
thick
layer
of
plastic
mantle
and
an
iron-filled
core
that
generates
a
magnetic
field
this
iron
core
is
composed
of
a
solid
inner
phase
and
a
fluid
outer
phase
convective
motion
in
the
core
generates
electric
currents
through
dynamo
action
and
these
in
turn
generate
the
geomagnetic
field
the
atmospheric
conditions
have
been
significantly
altered
from
the
original
conditions
by
the
presence
of
life-forms
which
create
an
ecological
balance
that
stabilizes
the
surface
conditions
despite
the
wide
regional
variations
in
climate
by
latitude
and
other
geographic
factors
the
long-term
average
global
climate
is
quite
stable
during
interglacial
periods
and
variations
of
a
degree
or
two
of
average
global
temperature
have
historically
had
major
effects
on
the
ecological
balance
and
on
the
actual
geography
of
the
earth
geology
is
the
science
and
study
of
the
solid
and
liquid
matter
that
constitutes
the
earth
the
field
of
geology
encompasses
the
study
of
the
composition
structure
physical
properties
dynamics
and
history
of
earth
materials
and
the
processes
by
which
they
are
formed
moved
and
changed
the
field
is
a
major
academic
discipline
and
is
also
important
for
mineral
and
hydrocarbon
extraction
knowledge
about
and
mitigation
of
natural
hazards
some
geotechnical
engineering
fields
and
understanding
past
climates
and
environments
the
geology
of
an
area
evolves
through
time
as
rock
units
are
deposited
and
inserted
and
deformational
processes
change
their
shapes
and
locations
rock
units
are
first
emplaced
either
by
deposition
onto
the
surface
or
intrude
into
the
overlying
rock
deposition
can
occur
when
sediments
settle
onto
the
surface
of
the
earth
and
later
lithify
into
sedimentary
rock
or
when
as
volcanic
material
such
as
volcanic
ash
or
lava
flows
blanket
the
surface
igneous
intrusions
such
as
batholiths
laccoliths
dikes
and
sills
push
upwards
into
the
overlying
rock
and
crystallize
as
they
intrude
after
the
initial
sequence
of
rocks
has
been
deposited
the
rock
units
can
be
deformed
and/or
metamorphosed
deformation
typically
occurs
as
a
result
of
horizontal
shortening
horizontal
extension
or
side-to-side
(strike-slip)
motion
these
structural
regimes
broadly
relate
to
convergent
boundaries
divergent
boundaries
and
transform
boundaries
respectively
between
tectonic
plates
earth
is
estimated
to
have
formed
454 billion
years
ago
from
the
solar
nebula
along
with
the
sun
and
other
planets
the
moon
formed
roughly
20 million
years
later
initially
molten
the
outer
layer
of
the
earth
cooled
resulting
in
the
solid
crust
outgassing
and
volcanic
activity
produced
the
primordial
atmosphere
condensing
water
vapor
most
or
all
of
which
came
from
ice
delivered
by
comets
produced
the
oceans
and
other
water
sources
the
highly
energetic
chemistry
is
believed
to
have
produced
a
self-replicating
molecule
around
4 billion
years
ago
continents
formed
then
broke
up
and
reformed
as
the
surface
of
earth
reshaped
over
hundreds
of
millions
of
years
occasionally
combining
to
make
a
supercontinent
roughly
750 million
years
ago
the
earliest
known
supercontinent
rodinia
began
to
break
apart
the
continents
later
recombined
to
form
pannotia
which
broke
apart
about
540 million
years
ago
then
finally
pangaea
which
broke
apart
about
180 million
years
ago
during
the
neoproterozoic
era
freezing
temperatures
covered
much
of
the
earth
in
glaciers
and
ice
sheets
this
hypothesis
has
been
termed
the
"snowball
earth"
and
it
is
of
particular
interest
as
it
precedes
the
cambrian
explosion
in
which
multicellular
life
forms
began
to
proliferate
about
530–540 million
years
ago
since
the
cambrian
explosion
there
have
been
five
distinctly
identifiable
mass
extinctions
the
last
mass
extinction
occurred
some
66
million
years
ago
when
a
meteorite
collision
probably
triggered
the
extinction
of
the
non-avian
dinosaurs
and
other
large
reptiles
but
spared
small
animals
such
as
mammals
over
the
past
66 million
years
mammalian
life
diversified
several
million
years
ago
a
species
of
small
african
ape
gained
the
ability
to
stand
upright
the
subsequent
advent
of
human
life
and
the
development
of
agriculture
and
further
civilization
allowed
humans
to
affect
the
earth
more
rapidly
than
any
previous
life
form
affecting
both
the
nature
and
quantity
of
other
organisms
as
well
as
global
climate
by
comparison
the
great
oxygenation
event
produced
by
the
proliferation
of
algae
during
the
siderian
period
required
about
300 million
years
to
culminate
the
present
era
is
classified
as
part
of
a
mass
extinction
event
the
holocene
extinction
event
the
fastest
ever
to
have
occurred
some
such
as
e
o
wilson
of
harvard
university
predict
that
human
destruction
of
the
biosphere
could
cause
the
extinction
of
one-half
of
all
species
in
the
next
100 years
the
extent
of
the
current
extinction
event
is
still
being
researched
debated
and
calculated
by
biologists
the
earth's
atmosphere
is
a
key
factor
in
sustaining
the
ecosystem
the
thin
layer
of
gases
that
envelops
the
earth
is
held
in
place
by
gravity
air
is
mostly
nitrogen
oxygen
water
vapor
with
much
smaller
amounts
of
carbon
dioxide
argon
etc
the
atmospheric
pressure
declines
steadily
with
altitude
the
ozone
layer
plays
an
important
role
in
depleting
the
amount
of
ultraviolet
(uv)
radiation
that
reaches
the
surface
as
dna
is
readily
damaged
by
uv
light
this
serves
to
protect
life
at
the
surface
the
atmosphere
also
retains
heat
during
the
night
thereby
reducing
the
daily
temperature
extremes
terrestrial
weather
occurs
almost
exclusively
in
the
lower
part
of
the
atmosphere
and
serves
as
a
convective
system
for
redistributing
heat
ocean
currents
are
another
important
factor
in
determining
climate
particularly
the
major
underwater
thermohaline
circulation
which
distributes
heat
energy
from
the
equatorial
oceans
to
the
polar
regions
these
currents
help
to
moderate
the
differences
in
temperature
between
winter
and
summer
in
the
temperate
zones
also
without
the
redistributions
of
heat
energy
by
the
ocean
currents
and
atmosphere
the
tropics
would
be
much
hotter
and
the
polar
regions
much
colder
weather
can
have
both
beneficial
and
harmful
effects
extremes
in
weather
such
as
tornadoes
or
hurricanes
and
cyclones
can
expend
large
amounts
of
energy
along
their
paths
and
produce
devastation
surface
vegetation
has
evolved
a
dependence
on
the
seasonal
variation
of
the
weather
and
sudden
changes
lasting
only
a
few
years
can
have
a
dramatic
effect
both
on
the
vegetation
and
on
the
animals
which
depend
on
its
growth
for
their
food
climate
is
a
measure
of
the
long-term
trends
in
the
weather
various
factors
are
known
to
influence
the
climate
including
ocean
currents
surface
albedo
greenhouse
gases
variations
in
the
solar
luminosity
and
changes
to
the
earth's
orbit
based
on
historical
records
the
earth
is
known
to
have
undergone
drastic
climate
changes
in
the
past
including
ice
ages
the
climate
of
a
region
depends
on
a
number
of
factors
especially
latitude
a
latitudinal
band
of
the
surface
with
similar
climatic
attributes
forms
a
climate
region
there
are
a
number
of
such
regions
ranging
from
the
tropical
climate
at
the
equator
to
the
polar
climate
in
the
northern
and
southern
extremes
weather
is
also
influenced
by
the
seasons
which
result
from
the
earth's
axis
being
tilted
relative
to
its
orbital
plane
thus
at
any
given
time
during
the
summer
or
winter
one
part
of
the
earth
is
more
directly
exposed
to
the
rays
of
the
sun
this
exposure
alternates
as
the
earth
revolves
in
its
orbit
at
any
given
time
regardless
of
season
the
northern
and
southern
hemispheres
experience
opposite
seasons
weather
is
a
chaotic
system
that
is
readily
modified
by
small
changes
to
the
environment
so
accurate
weather
forecasting
is
limited
to
only
a
few
days
overall
two
things
are
happening
worldwide:
(1)
temperature
is
increasing
on
the
average;
and
(2)
regional
climates
have
been
undergoing
noticeable
changes
water
is
a
chemical
substance
that
is
composed
of
hydrogen
and
oxygen
and
is
vital
for
all
known
forms
of
life
in
typical
usage
"water"
refers
only
to
its
liquid
form
or
state
but
the
substance
also
has
a
solid
state
ice
and
a
gaseous
state
water
vapor
or
steam
water
covers
71%
of
the
earth's
surface
on
earth
it
is
found
mostly
in
oceans
and
other
large
bodies
of
water
with
16%
of
water
below
ground
in
aquifers
and
0001%
in
the
air
as
vapor
clouds
and
precipitation
oceans
hold
97%
of
surface
water
glaciers
and
polar
ice
caps
24%
and
other
land
surface
water
such
as
rivers
lakes
and
ponds
06%
additionally
a
minute
amount
of
the
earth's
water
is
contained
within
biological
bodies
and
manufactured
products
an
ocean
is
a
major
body
of
saline
water
and
a
principal
component
of
the
hydrosphere
approximately
71%
of
the
earth's
surface
(an
area
of
some
361
million
square
kilometers)
is
covered
by
ocean
a
continuous
body
of
water
that
is
customarily
divided
into
several
principal
oceans
and
smaller
seas
more
than
half
of
this
area
is
over
deep
average
oceanic
salinity
is
around
35
parts
per
thousand
(ppt)
(35%)
and
nearly
all
seawater
has
a
salinity
in
the
range
of
30
to
38
ppt
though
generally
recognized
as
several
'separate'
oceans
these
waters
comprise
one
global
interconnected
body
of
salt
water
often
referred
to
as
the
world
ocean
or
global
ocean
this
concept
of
a
global
ocean
as
a
continuous
body
of
water
with
relatively
free
interchange
among
its
parts
is
of
fundamental
importance
to
oceanography
the
major
oceanic
divisions
are
defined
in
part
by
the
continents
various
archipelagos
and
other
criteria:
these
divisions
are
(in
descending
order
of
size)
the
pacific
ocean
the
atlantic
ocean
the
indian
ocean
the
southern
ocean
and
the
arctic
ocean
smaller
regions
of
the
oceans
are
called
seas
gulfs
bays
and
other
names
there
are
also
salt
lakes
which
are
smaller
bodies
of
landlocked
saltwater
that
are
not
interconnected
with
the
world
ocean
two
notable
examples
of
salt
lakes
are
the
aral
sea
and
the
great
salt
lake
a
lake
(from
latin
"lacus")
is
a
terrain
feature
(or
physical
feature)
a
body
of
liquid
on
the
surface
of
a
world
that
is
localized
to
the
bottom
of
basin
(another
type
of
landform
or
terrain
feature;
that
is
it
is
not
global)
and
moves
slowly
if
it
moves
at
all
on
earth
a
body
of
water
is
considered
a
lake
when
it
is
inland
not
part
of
the
ocean
is
larger
and
deeper
than
a
pond
and
is
fed
by
a
river
the
only
world
other
than
earth
known
to
harbor
lakes
is
titan
saturn's
largest
moon
which
has
lakes
of
ethane
most
likely
mixed
with
methane
it
is
not
known
if
titan's
lakes
are
fed
by
rivers
though
titan's
surface
is
carved
by
numerous
river
beds
natural
lakes
on
earth
are
generally
found
in
mountainous
areas
rift
zones
and
areas
with
ongoing
or
recent
glaciation
other
lakes
are
found
in
endorheic
basins
or
along
the
courses
of
mature
rivers
in
some
parts
of
the
world
there
are
many
lakes
because
of
chaotic
drainage
patterns
left
over
from
the
last
ice
age
all
lakes
are
temporary
over
geologic
time
scales
as
they
will
slowly
fill
in
with
sediments
or
spill
out
of
the
basin
containing
them
a
pond
is
a
body
of
standing
water
either
natural
or
man-made
that
is
usually
smaller
than
a
lake
a
wide
variety
of
man-made
bodies
of
water
are
classified
as
ponds
including
water
gardens
designed
for
aesthetic
ornamentation
fish
ponds
designed
for
commercial
fish
breeding
and
solar
ponds
designed
to
store
thermal
energy
ponds
and
lakes
are
distinguished
from
streams
via
current
speed
while
currents
in
streams
are
easily
observed
ponds
and
lakes
possess
thermally
driven
micro-currents
and
moderate
wind
driven
currents
these
features
distinguish
a
pond
from
many
other
aquatic
terrain
features
such
as
stream
pools
and
tide
pools
a
river
is
a
natural
watercourse
usually
freshwater
flowing
toward
an
ocean
a
lake
a
sea
or
another
river
in
a
few
cases
a
river
simply
flows
into
the
ground
or
dries
up
completely
before
reaching
another
body
of
water
small
rivers
may
also
be
called
by
several
other
names
including
stream
creek
brook
rivulet
and
rill;
there
is
no
general
rule
that
defines
what
can
be
called
a
river
many
names
for
small
rivers
are
specific
to
geographic
location;
one
example
is
"burn"
in
scotland
and
north-east
england
sometimes
a
river
is
said
to
be
larger
than
a
creek
but
this
is
not
always
the
case
due
to
vagueness
in
the
language
a
river
is
part
of
the
hydrological
cycle
water
within
a
river
is
generally
collected
from
precipitation
through
surface
runoff
groundwater
recharge
springs
and
the
release
of
stored
water
in
natural
ice
and
snowpacks
(ie
from
glaciers)
a
stream
is
a
flowing
body
of
water
with
a
current
confined
within
a
bed
and
stream
banks
in
the
united
states
a
stream
is
classified
as
a
watercourse
less
than
wide
streams
are
important
as
conduits
in
the
water
cycle
instruments
in
groundwater
recharge
and
they
serve
as
corridors
for
fish
and
wildlife
migration
the
biological
habitat
in
the
immediate
vicinity
of
a
stream
is
called
a
riparian
zone
given
the
status
of
the
ongoing
holocene
extinction
streams
play
an
important
corridor
role
in
connecting
fragmented
habitats
and
thus
in
conserving
biodiversity
the
study
of
streams
and
waterways
in
general
involves
many
branches
of
inter-disciplinary
natural
science
and
engineering
including
hydrology
fluvial
geomorphology
aquatic
ecology
fish
biology
riparian
ecology
and
others
ecosystems
are
composed
of
a
variety
of
abiotic
and
biotic
components
that
function
in
an
interrelated
way
the
structure
and
composition
is
determined
by
various
environmental
factors
that
are
interrelated
variations
of
these
factors
will
initiate
dynamic
modifications
to
the
ecosystem
some
of
the
more
important
components
are:
soil
atmosphere
radiation
from
the
sun
water
and
living
organisms
central
to
the
ecosystem
concept
is
the
idea
that
living
organisms
interact
with
every
other
element
in
their
local
environment
eugene
odum
a
founder
of
ecology
stated:
"any
unit
that
includes
all
of
the
organisms
(ie:
the
"community")
in
a
given
area
interacting
with
the
physical
environment
so
that
a
flow
of
energy
leads
to
clearly
defined
trophic
structure
biotic
diversity
and
material
cycles
(ie:
exchange
of
materials
between
living
and
nonliving
parts)
within
the
system
is
an
ecosystem"
within
the
ecosystem
species
are
connected
and
dependent
upon
one
another
in
the
food
chain
and
exchange
energy
and
matter
between
themselves
as
well
as
with
their
environment
the
human
ecosystem
concept
is
based
on
the
human/nature
dichotomy
and
the
idea
that
all
species
are
ecologically
dependent
on
each
other
as
well
as
with
the
abiotic
constituents
of
their
biotope
a
smaller
unit
of
size
is
called
a
microecosystem
for
example
a
microsystem
can
be
a
stone
and
all
the
life
under
it
a
"macroecosystem"
might
involve
a
whole
ecoregion
with
its
drainage
basin
wilderness
is
generally
defined
as
areas
that
have
not
been
significantly
modified
by
human
activity
wilderness
areas
can
be
found
in
preserves
estates
farms
conservation
preserves
ranches
national
parks
and
even
in
urban
areas
along
rivers
gulches
or
otherwise
undeveloped
areas
wilderness
areas
and
protected
parks
are
considered
important
for
the
survival
of
certain
species
ecological
studies
conservation
and
solitude
some
nature
writers
believe
wilderness
areas
are
vital
for
the
human
spirit
and
creativity
and
some
ecologists
consider
wilderness
areas
to
be
an
integral
part
of
the
earth's
self-sustaining
natural
ecosystem
(the
biosphere)
they
may
also
preserve
historic
genetic
traits
and
that
they
provide
habitat
for
wild
flora
and
fauna
that
may
be
difficult
or
impossible
to
recreate
in
zoos
arboretums
or
laboratories
although
there
is
no
universal
agreement
on
the
definition
of
life
scientists
generally
accept
that
the
biological
manifestation
of
life
is
characterized
by
organization
metabolism
growth
adaptation
response
to
stimuli
and
reproduction
life
may
also
be
said
to
be
simply
the
characteristic
state
of
organisms
properties
common
to
terrestrial
organisms
(plants
animals
fungi
protists
archaea
and
bacteria)
are
that
they
are
cellular
carbon-and-water-based
with
complex
organization
having
a
metabolism
a
capacity
to
grow
respond
to
stimuli
and
reproduce
an
entity
with
these
properties
is
generally
considered
life
however
not
every
definition
of
life
considers
all
of
these
properties
to
be
essential
human-made
analogs
of
life
may
also
be
considered
to
be
life
the
biosphere
is
the
part
of
earth's
outer
shell –
including
land
surface
rocks
water
air
and
the
atmosphere –
within
which
life
occurs
and
which
biotic
processes
in
turn
alter
or
transform
from
the
broadest
geophysiological
point
of
view
the
biosphere
is
the
global
ecological
system
integrating
all
living
beings
and
their
relationships
including
their
interaction
with
the
elements
of
the
lithosphere
(rocks)
hydrosphere
(water)
and
atmosphere
(air)
the
entire
earth
contains
over
75 billion
tons
(150
"trillion"
pounds
or
about
68×10 kilograms)
of
biomass
(life)
which
lives
within
various
environments
within
the
biosphere
over
nine-tenths
of
the
total
biomass
on
earth
is
plant
life
on
which
animal
life
depends
very
heavily
for
its
existence
more
than
2
million
species
of
plant
and
animal
life
have
been
identified
to
date
and
estimates
of
the
actual
number
of
existing
species
range
from
several
million
to
well
over
50 million
the
number
of
individual
species
of
life
is
constantly
in
some
degree
of
flux
with
new
species
appearing
and
others
ceasing
to
exist
on
a
continual
basis
the
total
number
of
species
is
in
rapid
decline
the
origin
of
life
on
earth
is
not
well
understood
but
it
is
known
to
have
occurred
at
least
35 billion
years
ago
during
the
hadean
or
archean
eons
on
a
primordial
earth
that
had
a
substantially
different
environment
than
is
found
at
present
these
life
forms
possessed
the
basic
traits
of
self-replication
and
inheritable
traits
once
life
had
appeared
the
process
of
evolution
by
natural
selection
resulted
in
the
development
of
ever-more
diverse
life
forms
species
that
were
unable
to
adapt
to
the
changing
environment
and
competition
from
other
life
forms
became
extinct
however
the
fossil
record
retains
evidence
of
many
of
these
older
species
current
fossil
and
dna
evidence
shows
that
all
existing
species
can
trace
a
continual
ancestry
back
to
the
first
primitive
life
forms
when
basic
forms
of
plant
life
developed
the
process
of
photosynthesis
the
sun's
energy
could
be
harvested
to
create
conditions
which
allowed
for
more
complex
life
forms
the
resultant
oxygen
accumulated
in
the
atmosphere
and
gave
rise
to
the
ozone
layer
the
incorporation
of
smaller
cells
within
larger
ones
resulted
in
the
development
of
yet
more
complex
cells
called
eukaryotes
cells
within
colonies
became
increasingly
specialized
resulting
in
true
multicellular
organisms
with
the
ozone
layer
absorbing
harmful
ultraviolet
radiation
life
colonized
the
surface
of
earth
the
first
form
of
life
to
develop
on
the
earth
were
microbes
and
they
remained
the
only
form
of
life
until
about
a
billion
years
ago
when
multi-cellular
organisms
began
to
appear
microorganisms
are
single-celled
organisms
that
are
generally
microscopic
and
smaller
than
the
human
eye
can
see
they
include
bacteria
fungi
archaea
and
protista
these
life
forms
are
found
in
almost
every
location
on
the
earth
where
there
is
liquid
water
including
in
the
earth's
interior
their
reproduction
is
both
rapid
and
profuse
the
combination
of
a
high
mutation
rate
and
a
horizontal
gene
transfer
ability
makes
them
highly
adaptable
and
able
to
survive
in
new
environments
including
outer
space
they
form
an
essential
part
of
the
planetary
ecosystem
however
some
microorganisms
are
pathogenic
and
can
post
health
risk
to
other
organisms
originally
aristotle
divided
all
living
things
between
plants
which
generally
do
not
move
fast
enough
for
humans
to
notice
and
animals
in
linnaeus'
system
these
became
the
kingdoms
vegetabilia
(later
plantae)
and
animalia
since
then
it
has
become
clear
that
the
plantae
as
originally
defined
included
several
unrelated
groups
and
the
fungi
and
several
groups
of
algae
were
removed
to
new
kingdoms
however
these
are
still
often
considered
plants
in
many
contexts
bacterial
life
is
sometimes
included
in
flora
and
some
classifications
use
the
term
"bacterial
flora"
separately
from
"plant
flora"
among
the
many
ways
of
classifying
plants
are
by
regional
floras
which
depending
on
the
purpose
of
study
can
also
include
"fossil
flora"
remnants
of
plant
life
from
a
previous
era
people
in
many
regions
and
countries
take
great
pride
in
their
individual
arrays
of
characteristic
flora
which
can
vary
widely
across
the
globe
due
to
differences
in
climate
and
terrain
regional
floras
commonly
are
divided
into
categories
such
as
"native
flora"
and
"agricultural
and
garden
flora"
the
lastly
mentioned
of
which
are
intentionally
grown
and
cultivated
some
types
of
"native
flora"
actually
have
been
introduced
centuries
ago
by
people
migrating
from
one
region
or
continent
to
another
and
become
an
integral
part
of
the
native
or
natural
flora
of
the
place
to
which
they
were
introduced
this
is
an
example
of
how
human
interaction
with
nature
can
blur
the
boundary
of
what
is
considered
nature
another
category
of
plant
has
historically
been
carved
out
for
"weeds"
though
the
term
has
fallen
into
disfavor
among
botanists
as
a
formal
way
to
categorize
"useless"
plants
the
informal
use
of
the
word
"weeds"
to
describe
those
plants
that
are
deemed
worthy
of
elimination
is
illustrative
of
the
general
tendency
of
people
and
societies
to
seek
to
alter
or
shape
the
course
of
nature
similarly
animals
are
often
categorized
in
ways
such
as
"domestic"
"farm
animals"
"wild
animals"
"pests"
etc
according
to
their
relationship
to
human
life
animals
as
a
category
have
several
characteristics
that
generally
set
them
apart
from
other
living
things
animals
are
eukaryotic
and
usually
multicellular
(although
see
myxozoa)
which
separates
them
from
bacteria
archaea
and
most
protists
they
are
heterotrophic
generally
digesting
food
in
an
internal
chamber
which
separates
them
from
plants
and
algae
they
are
also
distinguished
from
plants
algae
and
fungi
by
lacking
cell
walls
with
a
few
exceptions—most
notably
the
two
phyla
consisting
of
sponges
and
placozoans—animals
have
bodies
that
are
differentiated
into
tissues
these
include
muscles
which
are
able
to
contract
and
control
locomotion
and
a
nervous
system
which
sends
and
processes
signals
there
is
also
typically
an
internal
digestive
chamber
the
eukaryotic
cells
possessed
by
all
animals
are
surrounded
by
a
characteristic
extracellular
matrix
composed
of
collagen
and
elastic
glycoproteins
this
may
be
calcified
to
form
structures
like
shells
bones
and
spicules
a
framework
upon
which
cells
can
move
about
and
be
reorganized
during
development
and
maturation
and
which
supports
the
complex
anatomy
required
for
mobility
although
humans
comprise
only
a
minuscule
proportion
of
the
total
living
biomass
on
earth
the
human
effect
on
nature
is
disproportionately
large
because
of
the
extent
of
human
influence
the
boundaries
between
what
humans
regard
as
nature
and
"made
environments"
is
not
clear
cut
except
at
the
extremes
even
at
the
extremes
the
amount
of
natural
environment
that
is
free
of
discernible
human
influence
is
diminishing
at
an
increasingly
rapid
pace
the
development
of
technology
by
the
human
race
has
allowed
the
greater
exploitation
of
natural
resources
and
has
helped
to
alleviate
some
of
the
risk
from
natural
hazards
in
spite
of
this
progress
however
the
fate
of
human
civilization
remains
closely
linked
to
changes
in
the
environment
there
exists
a
highly
complex
feedback
loop
between
the
use
of
advanced
technology
and
changes
to
the
environment
that
are
only
slowly
becoming
understood
man-made
threats
to
the
earth's
natural
environment
include
pollution
deforestation
and
disasters
such
as
oil
spills
humans
have
contributed
to
the
extinction
of
many
plants
and
animals
humans
employ
nature
for
both
leisure
and
economic
activities
the
acquisition
of
natural
resources
for
industrial
use
remains
a
sizable
component
of
the
world's
economic
system
some
activities
such
as
hunting
and
fishing
are
used
for
both
sustenance
and
leisure
often
by
different
people
agriculture
was
first
adopted
around
the
9th
millennium
bce
ranging
from
food
production
to
energy
nature
influences
economic
wealth
although
early
humans
gathered
uncultivated
plant
materials
for
food
and
employed
the
medicinal
properties
of
vegetation
for
healing
most
modern
human
use
of
plants
is
through
agriculture
the
clearance
of
large
tracts
of
land
for
crop
growth
has
led
to
a
significant
reduction
in
the
amount
available
of
forestation
and
wetlands
resulting
in
the
loss
of
habitat
for
many
plant
and
animal
species
as
well
as
increased
erosion
beauty
in
nature
has
historically
been
a
prevalent
theme
in
art
and
books
filling
large
sections
of
libraries
and
bookstores
that
nature
has
been
depicted
and
celebrated
by
so
much
art
photography
poetry
and
other
literature
shows
the
strength
with
which
many
people
associate
nature
and
beauty
reasons
why
this
association
exists
and
what
the
association
consists
of
are
studied
by
the
branch
of
philosophy
called
aesthetics
beyond
certain
basic
characteristics
that
many
philosophers
agree
about
to
explain
what
is
seen
as
beautiful
the
opinions
are
virtually
endless
nature
and
wildness
have
been
important
subjects
in
various
eras
of
world
history
an
early
tradition
of
landscape
art
began
in
china
during
the
tang
dynasty
(618–907)
the
tradition
of
representing
nature
"as
it
is"
became
one
of
the
aims
of
chinese
painting
and
was
a
significant
influence
in
asian
art
although
natural
wonders
are
celebrated
in
the
psalms
and
the
book
of
job
wilderness
portrayals
in
art
became
more
prevalent
in
the
1800s
especially
in
the
works
of
the
romantic
movement
british
artists
john
constable
and
j
m
w
turner
turned
their
attention
to
capturing
the
beauty
of
the
natural
world
in
their
paintings
before
that
paintings
had
been
primarily
of
religious
scenes
or
of
human
beings
william
wordsworth's
poetry
described
the
wonder
of
the
natural
world
which
had
formerly
been
viewed
as
a
threatening
place
increasingly
the
valuing
of
nature
became
an
aspect
of
western
culture
this
artistic
movement
also
coincided
with
the
transcendentalist
movement
in
the
western
world
a
common
classical
idea
of
beautiful
art
involves
the
word
mimesis
the
imitation
of
nature
also
in
the
realm
of
ideas
about
beauty
in
nature
is
that
the
perfect
is
implied
through
perfect
mathematical
forms
and
more
generally
by
patterns
in
nature
as
david
rothenburg
writes
"the
beautiful
is
the
root
of
science
and
the
goal
of
art
the
highest
possibility
that
humanity
can
ever
hope
to
see"
some
fields
of
science
see
nature
as
matter
in
motion
obeying
certain
laws
of
nature
which
science
seeks
to
understand
for
this
reason
the
most
fundamental
science
is
generally
understood
to
be
"physics" –
the
name
for
which
is
still
recognizable
as
meaning
that
it
is
the
study
of
nature
matter
is
commonly
defined
as
the
substance
of
which
physical
objects
are
composed
it
constitutes
the
observable
universe
the
visible
components
of
the
universe
are
now
believed
to
compose
only
49
percent
of
the
total
mass
the
remainder
is
believed
to
consist
of
268
percent
cold
dark
matter
and
683
percent
dark
energy
the
exact
arrangement
of
these
components
is
still
unknown
and
is
under
intensive
investigation
by
physicists
the
behavior
of
matter
and
energy
throughout
the
observable
universe
appears
to
follow
well-defined
physical
laws
these
laws
have
been
employed
to
produce
cosmological
models
that
successfully
explain
the
structure
and
the
evolution
of
the
universe
we
can
observe
the
mathematical
expressions
of
the
laws
of
physics
employ
a
set
of
twenty
physical
constants
that
appear
to
be
static
across
the
observable
universe
the
values
of
these
constants
have
been
carefully
measured
but
the
reason
for
their
specific
values
remains
a
mystery
outer
space
also
simply
called
"space"
refers
to
the
relatively
empty
regions
of
the
universe
outside
the
atmospheres
of
celestial
bodies
"outer"
space
is
used
to
distinguish
it
from
airspace
(and
terrestrial
locations)
there
is
no
discrete
boundary
between
the
earth's
atmosphere
and
space
as
the
atmosphere
gradually
attenuates
with
increasing
altitude
outer
space
within
the
solar
system
is
called
interplanetary
space
which
passes
over
into
interstellar
space
at
what
is
known
as
the
heliopause
outer
space
is
sparsely
filled
with
several
dozen
types
of
organic
molecules
discovered
to
date
by
microwave
spectroscopy
blackbody
radiation
left
over
from
the
big
bang
and
the
origin
of
the
universe
and
cosmic
rays
which
include
ionized
atomic
nuclei
and
various
subatomic
particles
there
is
also
some
gas
plasma
and
dust
and
small
meteors
additionally
there
are
signs
of
human
life
in
outer
space
today
such
as
material
left
over
from
previous
manned
and
unmanned
launches
which
are
a
potential
hazard
to
spacecraft
some
of
this
debris
re-enters
the
atmosphere
periodically
although
the
earth
is
the
only
body
within
the
solar
system
known
to
support
life
evidence
suggests
that
in
the
distant
past
the
planet
mars
possessed
bodies
of
liquid
water
on
the
surface
for
a
brief
period
in
mars'
history
it
may
have
also
been
capable
of
forming
life
at
present
though
most
of
the
water
remaining
on
mars
is
frozen
if
life
exists
at
all
on
mars
it
is
most
likely
to
be
located
underground
where
liquid
water
can
still
exist
conditions
on
the
other
terrestrial
planets
mercury
and
venus
appear
to
be
too
harsh
to
support
life
as
we
know
it
but
it
has
been
conjectured
that
europa
the
fourth-largest
moon
of
jupiter
may
possess
a
sub-surface
ocean
of
liquid
water
and
could
potentially
host
life
astronomers
have
started
to
discover
extrasolar
earth
analogs
–
planets
that
lie
in
the
habitable
zone
of
space
surrounding
a
star
and
therefore
could
possibly
host
life
as
we
know
it
media:
organizations:
philosophy:
patterns
in
nature
patterns
in
nature
are
visible
regularities
of
form
found
in
the
natural
world
these
patterns
recur
in
different
contexts
and
can
sometimes
be
modelled
mathematically
natural
patterns
include
symmetries
trees
spirals
meanders
waves
foams
tessellations
cracks
and
stripes
early
greek
philosophers
studied
pattern
with
plato
pythagoras
and
empedocles
attempting
to
explain
order
in
nature
the
modern
understanding
of
visible
patterns
developed
gradually
over
time
in
the
19th
century
belgian
physicist
joseph
plateau
examined
soap
films
leading
him
to
formulate
the
concept
of
a
minimal
surface
german
biologist
and
artist
ernst
haeckel
painted
hundreds
of
marine
organisms
to
emphasise
their
symmetry
scottish
biologist
d'arcy
thompson
pioneered
the
study
of
growth
patterns
in
both
plants
and
animals
showing
that
simple
equations
could
explain
spiral
growth
in
the
20th
century
british
mathematician
alan
turing
predicted
mechanisms
of
morphogenesis
which
give
rise
to
patterns
of
spots
and
stripes
hungarian
biologist
aristid
lindenmayer
and
french
american
mathematician
benoît
mandelbrot
showed
how
the
mathematics
of
fractals
could
create
plant
growth
patterns
mathematics
physics
and
chemistry
can
explain
patterns
in
nature
at
different
levels
patterns
in
living
things
are
explained
by
the
biological
processes
of
natural
selection
and
sexual
selection
studies
of
pattern
formation
make
use
of
computer
models
to
simulate
a
wide
range
of
patterns
early
greek
philosophers
attempted
to
explain
order
in
nature
anticipating
modern
concepts
pythagoras
(c 570–c 495 bc)
explained
patterns
in
nature
like
the
harmonies
of
music
as
arising
from
number
which
he
took
to
be
the
basic
constituent
of
existence
empedocles
(c 494–c 434 bc)
to
an
extent
anticipated
darwin's
evolutionary
explanation
for
the
structures
of
organisms
plato
(c 427–c 347 bc)
argued
for
the
existence
of
natural
universals
he
considered
these
to
consist
of
ideal
forms
(
"eidos":
"form")
of
which
physical
objects
are
never
more
than
imperfect
copies
thus
a
flower
may
be
roughly
circular
but
it
is
never
a
perfect
circle
theophrastus
(c 372–c 287 bc)
noted
that
plants
"that
have
flat
leaves
have
them
in
a
regular
series";
pliny
the
elder
(23–79 ad)
noted
their
patterned
circular
arrangement
centuries
later
leonardo
da
vinci
(1452–1519)
noted
the
spiral
arrangement
of
leaf
patterns
johannes
kepler
(1571–1630)
pointed
out
the
presence
of
the
fibonacci
sequence
in
nature
using
it
to
explain
the
pentagonal
form
of
some
flowers
in
1754
charles
bonnet
observed
that
the
spiral
phyllotaxis
of
plants
were
frequently
expressed
in
both
clockwise
and
counter-clockwise
golden
ratio
series
mathematical
observations
of
phyllotaxis
followed
with
karl
friedric
schimper
and
his
friend
alexander
braun's
1830
and
1830
work
respectively;
auguste
bravais
and
his
brother
louis
connected
phyllotaxis
ratios
to
the
fibonacci
sequence
in
1837
also
noting
its
appearance
in
pinecones
and
pineapples
in
his
1854
book
german
psychologist
adolf
zeising
explored
the
golden
ratio
expressed
in
the
arrangement
of
plant
parts
the
skeletons
of
animals
and
the
branching
patterns
of
their
veins
and
nerves
as
well
as
in
crystals
a
h
church
studied
the
patterns
of
phyllotaxis
in
his
1904
book
in
1917
d'arcy
thompson
published
"on
growth
and
form";
his
description
of
phyllotaxis
and
the
fibonacci
sequence
the
mathematical
relationships
in
the
spiral
growth
patterns
of
plants
showed
that
simple
equations
could
describe
the
spiral
growth
patterns
of
animal
horns
and
mollusc
shells
in
1202
leonardo
fibonacci
introduced
the
fibonacci
sequence
to
the
western
world
with
his
book
"liber
abaci"
fibonacci
presented
a
thought
experiment
on
the
growth
of
an
idealized
rabbit
population
in
1658
the
english
physician
and
philosopher
sir
thomas
browne
discussed
"how
nature
geometrizeth"
in
"the
garden
of
cyrus"
citing
pythagorean
numerology
involving
the
number
5
and
the
platonic
form
of
the
quincunx
pattern
the
discourse's
central
chapter
features
examples
and
observations
of
the
quincunx
in
botany
the
belgian
physicist
joseph
plateau
(1801–1883)
formulated
the
mathematical
problem
of
the
existence
of
a
minimal
surface
with
a
given
boundary
which
is
now
named
after
him
he
studied
soap
films
intensively
formulating
plateau's
laws
which
describe
the
structures
formed
by
films
in
foams
ernst
haeckel
(1834–1919)
painted
beautiful
illustrations
of
marine
organisms
in
particular
radiolaria
emphasising
their
symmetry
to
support
his
faux-darwinian
theories
of
evolution
the
american
photographer
wilson
bentley
took
the
first
micrograph
of
a
snowflake
in
1885
in
1952
alan
turing
(1912–1954)
better
known
for
his
work
on
computing
and
codebreaking
wrote
"the
chemical
basis
of
morphogenesis"
an
analysis
of
the
mechanisms
that
would
be
needed
to
create
patterns
in
living
organisms
in
the
process
called
morphogenesis
he
predicted
oscillating
chemical
reactions
in
particular
the
belousov–zhabotinsky
reaction
these
activator-inhibitor
mechanisms
can
turing
suggested
generate
patterns
(dubbed
"turing
patterns")
of
stripes
and
spots
in
animals
and
contribute
to
the
spiral
patterns
seen
in
plant
phyllotaxis
in
1968
the
hungarian
theoretical
biologist
aristid
lindenmayer
(1925–1989)
developed
the
l-system
a
formal
grammar
which
can
be
used
to
model
plant
growth
patterns
in
the
style
of
fractals
l-systems
have
an
alphabet
of
symbols
that
can
be
combined
using
production
rules
to
build
larger
strings
of
symbols
and
a
mechanism
for
translating
the
generated
strings
into
geometric
structures
in
1975
after
centuries
of
slow
development
of
the
mathematics
of
patterns
by
gottfried
leibniz
georg
cantor
helge
von
koch
wacław
sierpiński
and
others
benoît
mandelbrot
wrote
a
famous
paper
"how
long
is
the
coast
of
britain?
statistical
self-similarity
and
fractional
dimension"
crystallising
mathematical
thought
into
the
concept
of
the
fractal
living
things
like
orchids
hummingbirds
and
the
peacock's
tail
have
abstract
designs
with
a
beauty
of
form
pattern
and
colour
that
artists
struggle
to
match
the
beauty
that
people
perceive
in
nature
has
causes
at
different
levels
notably
in
the
mathematics
that
governs
what
patterns
can
physically
form
and
among
living
things
in
the
effects
of
natural
selection
that
govern
how
patterns
evolve
mathematics
seeks
to
discover
and
explain
abstract
patterns
or
regularities
of
all
kinds
visual
patterns
in
nature
find
explanations
in
chaos
theory
fractals
logarithmic
spirals
topology
and
other
mathematical
patterns
for
example
l-systems
form
convincing
models
of
different
patterns
of
tree
growth
the
laws
of
physics
apply
the
abstractions
of
mathematics
to
the
real
world
often
as
if
it
were
perfect
for
example
a
crystal
is
perfect
when
it
has
no
structural
defects
such
as
dislocations
and
is
fully
symmetric
exact
mathematical
perfection
can
only
approximate
real
objects
visible
patterns
in
nature
are
governed
by
physical
laws;
for
example
meanders
can
be
explained
using
fluid
dynamics
in
biology
natural
selection
can
cause
the
development
of
patterns
in
living
things
for
several
reasons
including
camouflage
sexual
selection
and
different
kinds
of
signalling
including
mimicry
and
cleaning
symbiosis
in
plants
the
shapes
colours
and
patterns
of
insect-pollinated
flowers
like
the
lily
have
evolved
to
attract
insects
such
as
bees
radial
patterns
of
colours
and
stripes
some
visible
only
in
ultraviolet
light
serve
as
nectar
guides
that
can
be
seen
at
a
distance
symmetry
is
pervasive
in
living
things
animals
mainly
have
bilateral
or
mirror
symmetry
as
do
the
leaves
of
plants
and
some
flowers
such
as
orchids
plants
often
have
radial
or
rotational
symmetry
as
do
many
flowers
and
some
groups
of
animals
such
as
sea
anemones
fivefold
symmetry
is
found
in
the
echinoderms
the
group
that
includes
starfish
sea
urchins
and
sea
lilies
among
non-living
things
snowflakes
have
striking
sixfold
symmetry;
each
flake's
structure
forms
a
record
of
the
varying
conditions
during
its
crystallization
with
nearly
the
same
pattern
of
growth
on
each
of
its
six
arms
crystals
in
general
have
a
variety
of
symmetries
and
crystal
habits;
they
can
be
cubic
or
octahedral
but
true
crystals
cannot
have
fivefold
symmetry
(unlike
quasicrystals)
rotational
symmetry
is
found
at
different
scales
among
non-living
things
including
the
crown-shaped
splash
pattern
formed
when
a
drop
falls
into
a
pond
and
both
the
spheroidal
shape
and
rings
of
a
planet
like
saturn
symmetry
has
a
variety
of
causes
radial
symmetry
suits
organisms
like
sea
anemones
whose
adults
do
not
move:
food
and
threats
may
arrive
from
any
direction
but
animals
that
move
in
one
direction
necessarily
have
upper
and
lower
sides
head
and
tail
ends
and
therefore
a
left
and
a
right
the
head
becomes
specialised
with
a
mouth
and
sense
organs
(cephalisation)
and
the
body
becomes
bilaterally
symmetric
(though
internal
organs
need
not
be)
more
puzzling
is
the
reason
for
the
fivefold
(pentaradiate)
symmetry
of
the
echinoderms
early
echinoderms
were
bilaterally
symmetrical
as
their
larvae
still
are
sumrall
and
wray
argue
that
the
loss
of
the
old
symmetry
had
both
developmental
and
ecological
causes
fractals
are
infinitely
self-similar
iterated
mathematical
constructs
having
fractal
dimension
infinite
iteration
is
not
possible
in
nature
so
all
'fractal'
patterns
are
only
approximate
for
example
the
leaves
of
ferns
and
umbellifers
(apiaceae)
are
only
self-similar
(pinnate)
to
2
3
or
4
levels
fern-like
growth
patterns
occur
in
plants
and
in
animals
including
bryozoa
corals
hydrozoa
like
the
air
fern
"sertularia
argentea"
and
in
non-living
things
notably
electrical
discharges
lindenmayer
system
fractals
can
model
different
patterns
of
tree
growth
by
varying
a
small
number
of
parameters
including
branching
angle
distance
between
nodes
or
branch
points
(internode
length)
and
number
of
branches
per
branch
point
fractal-like
patterns
occur
widely
in
nature
in
phenomena
as
diverse
as
clouds
river
networks
geologic
fault
lines
mountains
coastlines
animal
coloration
snow
flakes
crystals
blood
vessel
branching
actin
cytoskeleton
and
ocean
waves
spirals
are
common
in
plants
and
in
some
animals
notably
molluscs
for
example
in
the
nautilus
a
cephalopod
mollusc
each
chamber
of
its
shell
is
an
approximate
copy
of
the
next
one
scaled
by
a
constant
factor
and
arranged
in
a
logarithmic
spiral
given
a
modern
understanding
of
fractals
a
growth
spiral
can
be
seen
as
a
special
case
of
self-similarity
plant
spirals
can
be
seen
in
phyllotaxis
the
arrangement
of
leaves
on
a
stem
and
in
the
arrangement
(parastichy)
of
other
parts
as
in
composite
flower
heads
and
seed
heads
like
the
sunflower
or
fruit
structures
like
the
pineapple
and
snake
fruit
as
well
as
in
the
pattern
of
scales
in
pine
cones
where
multiple
spirals
run
both
clockwise
and
anticlockwise
these
arrangements
have
explanations
at
different
levels
–
mathematics
physics
chemistry
biology
–
each
individually
correct
but
all
necessary
together
phyllotaxis
spirals
can
be
generated
mathematically
from
fibonacci
ratios:
the
fibonacci
sequence
runs
1
1
2
3
5
8
13
(each
subsequent
number
being
the
sum
of
the
two
preceding
ones)
for
example
when
leaves
alternate
up
a
stem
one
rotation
of
the
spiral
touches
two
leaves
so
the
pattern
or
ratio
is
1/2
in
hazel
the
ratio
is
1/3;
in
apricot
it
is
2/5;
in
pear
it
is
3/8;
in
almond
it
is
5/13
in
disc
phyllotaxis
as
in
the
sunflower
and
daisy
the
florets
are
arranged
in
fermat's
spiral
with
fibonacci
numbering
at
least
when
the
flowerhead
is
mature
so
all
the
elements
are
the
same
size
fibonacci
ratios
approximate
the
golden
angle
137508°
which
governs
the
curvature
of
fermat's
spiral
from
the
point
of
view
of
physics
spirals
are
lowest-energy
configurations
which
emerge
spontaneously
through
self-organizing
processes
in
dynamic
systems
from
the
point
of
view
of
chemistry
a
spiral
can
be
generated
by
a
reaction-diffusion
process
involving
both
activation
and
inhibition
phyllotaxis
is
controlled
by
proteins
that
manipulate
the
concentration
of
the
plant
hormone
auxin
which
activates
meristem
growth
alongside
other
mechanisms
to
control
the
relative
angle
of
buds
around
the
stem
from
a
biological
perspective
arranging
leaves
as
far
apart
as
possible
in
any
given
space
is
favoured
by
natural
selection
as
it
maximises
access
to
resources
especially
sunlight
for
photosynthesis
in
mathematics
a
dynamical
system
is
chaotic
if
it
is
(highly)
sensitive
to
initial
conditions
(the
so-called
"butterfly
effect")
which
requires
the
mathematical
properties
of
topological
mixing
and
dense
periodic
orbits
alongside
fractals
chaos
theory
ranks
as
an
essentially
universal
influence
on
patterns
in
nature
there
is
a
relationship
between
chaos
and
fractals—the
"strange
attractors"
in
chaotic
systems
have
a
fractal
dimension
some
cellular
automata
simple
sets
of
mathematical
rules
that
generate
patterns
have
chaotic
behaviour
notably
stephen
wolfram's
rule
30
vortex
streets
are
zigzagging
patterns
of
whirling
vortices
created
by
the
unsteady
separation
of
flow
of
a
fluid
most
often
air
or
water
over
obstructing
objects
smooth
(laminar)
flow
starts
to
break
up
when
the
size
of
the
obstruction
or
the
velocity
of
the
flow
become
large
enough
compared
to
the
viscosity
of
the
fluid
meanders
are
sinuous
bends
in
rivers
or
other
channels
which
form
as
a
fluid
most
often
water
flows
around
bends
as
soon
as
the
path
is
slightly
curved
the
size
and
curvature
of
each
loop
increases
as
helical
flow
drags
material
like
sand
and
gravel
across
the
river
to
the
inside
of
the
bend
the
outside
of
the
loop
is
left
clean
and
unprotected
so
erosion
accelerates
further
increasing
the
meandering
in
a
powerful
positive
feedback
loop
waves
are
disturbances
that
carry
energy
as
they
move
mechanical
waves
propagate
through
a
medium
–
air
or
water
making
it
oscillate
as
they
pass
by
wind
waves
are
sea
surface
waves
that
create
the
characteristic
chaotic
pattern
of
any
large
body
of
water
though
their
statistical
behaviour
can
be
predicted
with
wind
wave
models
as
waves
in
water
or
wind
pass
over
sand
they
create
patterns
of
ripples
when
winds
blow
over
large
bodies
of
sand
they
create
dunes
sometimes
in
extensive
dune
fields
as
in
the
taklamakan
desert
dunes
may
form
a
range
of
patterns
including
crescents
very
long
straight
lines
stars
domes
parabolas
and
longitudinal
or
seif
('sword')
shapes
barchans
or
crescent
dunes
are
produced
by
wind
acting
on
desert
sand;
the
two
horns
of
the
crescent
and
the
slip
face
point
downwind
sand
blows
over
the
upwind
face
which
stands
at
about
15
degrees
from
the
horizontal
and
falls
onto
the
slip
face
where
it
accumulates
up
to
the
angle
of
repose
of
the
sand
which
is
about
35
degrees
when
the
slip
face
exceeds
the
angle
of
repose
the
sand
avalanches
which
is
a
nonlinear
behaviour:
the
addition
of
many
small
amounts
of
sand
causes
nothing
much
to
happen
but
then
the
addition
of
a
further
small
amount
suddenly
causes
a
large
amount
to
avalanche
apart
from
this
nonlinearity
barchans
behave
rather
like
solitary
waves
a
soap
bubble
forms
a
sphere
a
surface
with
minimal
area
—
the
smallest
possible
surface
area
for
the
volume
enclosed
two
bubbles
together
form
a
more
complex
shape:
the
outer
surfaces
of
both
bubbles
are
spherical;
these
surfaces
are
joined
by
a
third
spherical
surface
as
the
smaller
bubble
bulges
slightly
into
the
larger
one
a
foam
is
a
mass
of
bubbles;
foams
of
different
materials
occur
in
nature
foams
composed
of
soap
films
obey
plateau's
laws
which
require
three
soap
films
to
meet
at
each
edge
at
120°
and
four
soap
edges
to
meet
at
each
vertex
at
the
tetrahedral
angle
of
about
1095°
plateau's
laws
further
require
films
to
be
smooth
and
continuous
and
to
have
a
constant
average
curvature
at
every
point
for
example
a
film
may
remain
nearly
flat
on
average
by
being
curved
up
in
one
direction
(say
left
to
right)
while
being
curved
downwards
in
another
direction
(say
front
to
back)
structures
with
minimal
surfaces
can
be
used
as
tents
lord
kelvin
identified
the
problem
of
the
most
efficient
way
to
pack
cells
of
equal
volume
as
a
foam
in
1887;
his
solution
uses
just
one
solid
the
bitruncated
cubic
honeycomb
with
very
slightly
curved
faces
to
meet
plateau's
laws
no
better
solution
was
found
until
1993
when
denis
weaire
and
robert
phelan
proposed
the
weaire–phelan
structure;
the
beijing
national
aquatics
center
adapted
the
structure
for
their
outer
wall
in
the
2008
summer
olympics
at
the
scale
of
living
cells
foam
patterns
are
common;
radiolarians
sponge
spicules
silicoflagellate
exoskeletons
and
the
calcite
skeleton
of
a
sea
urchin
"cidaris
rugosa"
all
resemble
mineral
casts
of
plateau
foam
boundaries
the
skeleton
of
the
radiolarian
"aulonia
hexagona"
a
beautiful
marine
form
drawn
by
ernst
haeckel
looks
as
if
it
is
a
sphere
composed
wholly
of
hexagons
but
this
is
mathematically
impossible
the
euler
characteristic
states
that
for
any
convex
polyhedron
the
number
of
faces
plus
the
number
of
vertices
(corners)
equals
the
number
of
edges
plus
two
a
result
of
this
formula
is
that
any
closed
polyhedron
of
hexagons
has
to
include
exactly
12
pentagons
like
a
soccer
ball
buckminster
fuller
geodesic
dome
or
fullerene
molecule
this
can
be
visualised
by
noting
that
a
mesh
of
hexagons
is
flat
like
a
sheet
of
chicken
wire
but
each
pentagon
that
is
added
forces
the
mesh
to
bend
(there
are
fewer
corners
so
the
mesh
is
pulled
in)
tessellations
are
patterns
formed
by
repeating
tiles
all
over
a
flat
surface
there
are
17
wallpaper
groups
of
tilings
while
common
in
art
and
design
exactly
repeating
tilings
are
less
easy
to
find
in
living
things
the
cells
in
the
paper
nests
of
social
wasps
and
the
wax
cells
in
honeycomb
built
by
honey
bees
are
well-known
examples
among
animals
bony
fish
reptiles
or
the
pangolin
or
fruits
like
the
salak
are
protected
by
overlapping
scales
or
osteoderms
these
form
more-or-less
exactly
repeating
units
though
often
the
scales
in
fact
vary
continuously
in
size
among
flowers
the
snake's
head
fritillary
"fritillaria
meleagris"
have
a
tessellated
chequerboard
pattern
on
their
petals
the
structures
of
minerals
provide
good
examples
of
regularly
repeating
three-dimensional
arrays
despite
the
hundreds
of
thousands
of
known
minerals
there
are
rather
few
possible
types
of
arrangement
of
atoms
in
a
crystal
defined
by
crystal
structure
crystal
system
and
point
group;
for
example
there
are
exactly
14
bravais
lattices
for
the
7
lattice
systems
in
three-dimensional
space
cracks
are
linear
openings
that
form
in
materials
to
relieve
stress
when
an
elastic
material
stretches
or
shrinks
uniformly
it
eventually
reaches
its
breaking
strength
and
then
fails
suddenly
in
all
directions
creating
cracks
with
120
degree
joints
so
three
cracks
meet
at
a
node
conversely
when
an
inelastic
material
fails
straight
cracks
form
to
relieve
the
stress
further
stress
in
the
same
direction
would
then
simply
open
the
existing
cracks;
stress
at
right
angles
can
create
new
cracks
at
90
degrees
to
the
old
ones
thus
the
pattern
of
cracks
indicates
whether
the
material
is
elastic
or
not
in
a
tough
fibrous
material
like
oak
tree
bark
cracks
form
to
relieve
stress
as
usual
but
they
do
not
grow
long
as
their
growth
is
interrupted
by
bundles
of
strong
elastic
fibres
since
each
species
of
tree
has
its
own
structure
at
the
levels
of
cell
and
of
molecules
each
has
its
own
pattern
of
splitting
in
its
bark
leopards
and
ladybirds
are
spotted;
angelfish
and
zebras
are
striped
these
patterns
have
an
evolutionary
explanation:
they
have
functions
which
increase
the
chances
that
the
offspring
of
the
patterned
animal
will
survive
to
reproduce
one
function
of
animal
patterns
is
camouflage;
for
instance
a
leopard
that
is
harder
to
see
catches
more
prey
another
function
is
signalling
—
for
instance
a
ladybird
is
less
likely
to
be
attacked
by
predatory
birds
that
hunt
by
sight
if
it
has
bold
warning
colours
and
is
also
distastefully
bitter
or
poisonous
or
mimics
other
distasteful
insects
a
young
bird
may
see
a
warning
patterned
insect
like
a
ladybird
and
try
to
eat
it
but
it
will
only
do
this
once;
very
soon
it
will
spit
out
the
bitter
insect;
the
other
ladybirds
in
the
area
will
remain
undisturbed
the
young
leopards
and
ladybirds
inheriting
genes
that
somehow
create
spottedness
survive
but
while
these
evolutionary
and
functional
arguments
explain
why
these
animals
need
their
patterns
they
do
not
explain
how
the
patterns
are
formed
alan
turing
and
later
the
mathematical
biologist
james
murray
described
a
mechanism
that
spontaneously
creates
spotted
or
striped
patterns:
a
reaction-diffusion
system
the
cells
of
a
young
organism
have
genes
that
can
be
switched
on
by
a
chemical
signal
a
morphogen
resulting
in
the
growth
of
a
certain
type
of
structure
say
a
darkly
pigmented
patch
of
skin
if
the
morphogen
is
present
everywhere
the
result
is
an
even
pigmentation
as
in
a
black
leopard
but
if
it
is
unevenly
distributed
spots
or
stripes
can
result
turing
suggested
that
there
could
be
feedback
control
of
the
production
of
the
morphogen
itself
this
could
cause
continuous
fluctuations
in
the
amount
of
morphogen
as
it
diffused
around
the
body
a
second
mechanism
is
needed
to
create
standing
wave
patterns
(to
result
in
spots
or
stripes):
an
inhibitor
chemical
that
switches
off
production
of
the
morphogen
and
that
itself
diffuses
through
the
body
more
quickly
than
the
morphogen
resulting
in
an
activator-inhibitor
scheme
the
belousov–zhabotinsky
reaction
is
a
non-biological
example
of
this
kind
of
scheme
a
chemical
oscillator
later
research
has
managed
to
create
convincing
models
of
patterns
as
diverse
as
zebra
stripes
giraffe
blotches
jaguar
spots
(medium-dark
patches
surrounded
by
dark
broken
rings)
and
ladybird
shell
patterns
(different
geometrical
layouts
of
spots
and
stripes
see
illustrations)
richard
prum's
activation-inhibition
models
developed
from
turing's
work
use
six
variables
to
account
for
the
observed
range
of
nine
basic
within-feather
pigmentation
patterns
from
the
simplest
a
central
pigment
patch
via
concentric
patches
bars
chevrons
eye
spot
pair
of
central
spots
rows
of
paired
spots
and
an
array
of
dots
more
elaborate
models
simulate
complex
feather
patterns
in
the
guineafowl
"numida
meleagris"
in
which
the
individual
feathers
feature
transitions
from
bars
at
the
base
to
an
array
of
dots
at
the
far
(distal)
end
these
require
an
oscillation
created
by
two
inhibiting
signals
with
interactions
in
both
space
and
time
patterns
can
form
for
other
reasons
in
the
vegetated
landscape
of
tiger
bush
and
fir
waves
tiger
bush
stripes
occur
on
arid
slopes
where
plant
growth
is
limited
by
rainfall
each
roughly
horizontal
stripe
of
vegetation
effectively
collects
the
rainwater
from
the
bare
zone
immediately
above
it
fir
waves
occur
in
forests
on
mountain
slopes
after
wind
disturbance
during
regeneration
when
trees
fall
the
trees
that
they
had
sheltered
become
exposed
and
are
in
turn
more
likely
to
be
damaged
so
gaps
tend
to
expand
downwind
meanwhile
on
the
windward
side
young
trees
grow
protected
by
the
wind
shadow
of
the
remaining
tall
trees
natural
patterns
are
sometimes
formed
by
animals
as
in
the
mima
mounds
of
the
northwestern
united
states
and
some
other
areas
which
appear
to
be
created
over
many
years
by
the
burrowing
activities
of
pocket
gophers
while
the
so-called
fairy
circles
of
namibia
appear
to
be
created
by
the
interaction
of
competing
groups
of
sand
termites
along
with
competition
for
water
among
the
desert
plants
in
permafrost
soils
with
an
active
upper
layer
subject
to
annual
freeze
and
thaw
patterned
ground
can
form
creating
circles
nets
ice
wedge
polygons
steps
and
stripes
thermal
contraction
causes
shrinkage
cracks
to
form;
in
a
thaw
water
fills
the
cracks
expanding
to
form
ice
when
next
frozen
and
widening
the
cracks
into
wedges
these
cracks
may
join
up
to
form
polygons
and
other
shapes
the
fissured
pattern
that
develops
on
vertebrate
brains
are
caused
by
a
physical
process
of
constrained
expansion
dependent
on
two
geometric
parameters:
relative
tangential
cortical
expansion
and
relative
thickness
of
the
cortex
similar
patterns
of
gyri
(peaks)
and
sulci
(troughs)
have
been
demonstrated
in
models
of
the
brain
starting
from
smooth
layered
gels
with
the
patterns
caused
by
compressive
mechanical
forces
resulting
from
the
expansion
of
the
outer
layer
(representing
the
cortex)
after
the
addition
of
a
solvent
numerical
models
in
computer
simulations
support
natural
and
experimental
observations
that
the
surface
folding
patterns
increase
in
larger
brains
footnotes
citations
pioneering
authors
general
books
patterns
from
nature
(as
art)
natural
landscape
a
natural
landscape
is
the
original
landscape
that
exists
before
it
is
acted
upon
by
human
culture
the
natural
landscape
and
the
cultural
landscape
are
separate
parts
of
the
landscape
however
in
the
twenty-first
century
landscapes
that
are
totally
untouched
by
human
activity
no
longer
exist
so
that
reference
is
sometimes
now
made
to
degrees
of
naturalness
within
a
landscape
in
"silent
spring"
(1962)
rachel
carson
describes
a
roadside
verge
as
it
used
to
look:
"along
the
roads
laurel
viburnum
and
alder
great
ferns
and
wildflowers
delighted
the
traveler’s
eye
through
much
of
the
year"
and
then
how
it
looks
now
following
the
use
of
herbicides:
"the
roadsides
once
so
attractive
were
now
lined
with
browned
and
withered
vegetation
as
though
swept
by
fire"
even
though
the
landscape
before
it
is
sprayed
is
biologically
degraded
and
may
well
contains
alien
species
the
concept
of
what
might
constitute
a
natural
landscape
can
still
be
deduced
from
the
context
the
phrase
"natural
landscape"
was
first
used
in
connection
with
landscape
painting
and
landscape
gardening
to
contrast
a
formal
style
with
a
more
natural
one
closer
to
nature
alexander
von
humboldt
(1769
–
1859)
was
to
further
conceptualize
this
into
the
idea
of
a
natural
landscape
"separate"
from
the
cultural
landscape
then
in
1908
geographer
otto
schlüter
developed
the
terms
original
landscape
("urlandschaft")
and
its
opposite
cultural
landscape
("kulturlandschaft")
in
an
attempt
to
give
the
science
of
geography
a
subject
matter
that
was
different
from
the
other
sciences
an
early
use
of
the
actual
phrase
"natural
landscape"
by
a
geographer
can
be
found
in
carl
o
sauer's
paper
"the
morphology
of
landscape"
(1925)
the
concept
of
a
natural
landscape
was
first
developed
in
connection
with
landscape
painting
though
the
actual
term
itself
was
first
used
in
relation
to
landscape
gardening
in
both
cases
it
was
used
to
contrast
a
formal
style
with
a
more
natural
one
that
is
closer
to
nature
chunglin
kwa
suggests
"that
a
seventeenth-century
or
early-eighteenth-century
person
could
experience
natural
scenery
‘just
like
on
a
painting’
and
so
with
or
without
the
use
of
the
word
itself
designate
it
as
a
landscape"
with
regard
to
landscape
gardening
john
aikin
commented
in
1794:
"whatever
therefore
there
be
of
"novelty"
in
the
singular
scenery
of
an
artificial
garden
it
is
soon
exhausted
whereas
the
infinite
diversity
of
a
natural
landscape
presents
an
inexhaustible
flore
of
new
forms"
writing
in
1844
the
prominent
american
landscape
gardener
andrew
jackson
downing
comments:
"straight
canals
round
or
oblong
pieces
of
water
and
all
the
regular
forms
of
the
geometric
mode
would
evidently
be
in
violent
opposition
to
the
whole
character
and
expression
of
natural
landscape"
in
his
extensive
travels
in
south
america
alexander
von
humboldt
became
the
first
to
conceptualize
a
natural
landscape
separate
from
the
cultural
landscape
though
he
does
not
actually
use
these
terms
andrew
jackson
downing
was
aware
of
and
sympathetic
to
humboldt's
ideas
which
therefore
influenced
american
landscape
gardening
subsequently
the
geographer
otto
schlüter
in
1908
argued
that
by
defining
geography
as
a
"landschaftskunde"
(landscape
science)
would
give
geography
a
logical
subject
matter
shared
by
no
other
discipline
he
defined
two
forms
of
landscape:
the
"urlandschaft"
(original
landscape)
or
landscape
that
existed
before
major
human
induced
changes
and
the
"kulturlandschaft"
(cultural
landscape)
a
landscape
created
by
human
culture
schlüter
argued
that
the
major
task
of
geography
was
to
trace
the
changes
in
these
two
landscapes
the
term
natural
landscape
is
sometimes
used
as
a
synonym
for
wilderness
but
for
geographers
natural
landscape
is
a
scientific
term
which
refers
to
the
biological
geological
climatological
and
other
aspects
of
a
landscape
not
the
cultural
values
that
are
implied
by
the
word
wilderness
matters
are
complicated
by
the
fact
that
the
words
nature
and
natural
have
more
than
one
meaning
on
the
one
hand
there
is
the
main
dictionary
meaning
for
nature:
"the
phenomena
of
the
physical
world
collectively
including
plants
animals
the
landscape
and
other
features
and
products
of
the
earth
as
opposed
to
humans
or
human
creations"
on
the
other
hand
there
is
the
growing
awareness
especially
since
charles
darwin
of
humanities
biological
affinity
with
nature
the
dualism
of
the
first
definition
has
its
roots
is
an
"ancient
concept"
because
early
people
viewed
"nature
or
the
nonhuman
world
[…]
as
a
divine
"other"
godlike
in
its
separation
from
humans"
in
the
west
christianity's
myth
of
the
fall
that
is
the
expulsion
of
humankind
from
the
garden
of
eden
where
all
creation
lived
in
harmony
into
an
imperfect
world
has
been
the
major
influence
cartesian
dualism
from
the
seventeenth
century
on
further
reinforced
this
dualistic
thinking
about
nature
with
this
dualism
goes
value
judgement
as
to
the
superiority
of
the
natural
over
the
artificial
modern
science
however
is
moving
towards
a
holistic
view
of
nature
what
is
meant
by
natural
within
the
american
conservation
movement
has
been
changing
over
the
last
century
and
a
half
in
the
mid-nineteenth
century
american
began
to
realize
that
the
land
was
becoming
more
and
more
domesticated
and
wildlife
was
disappearing
this
led
to
the
creation
of
american
national
parks
and
other
conservation
sites
initially
it
was
believed
that
all
that
was
needed
to
do
was
to
separate
what
was
seen
as
natural
landscape
and
"avoid
disturbances
such
as
logging
grazing
fire
and
insect
outbreaks"
this
and
subsequent
environmental
policy
until
recently
was
influenced
by
ideas
of
the
wilderness
however
this
policy
was
not
consistently
applied
and
in
yellowstone
park
to
take
one
example
the
existing
ecology
was
altered
firstly
by
the
exclusion
of
native
americans
and
later
with
the
virtual
extermination
of
the
wolf
population
a
century
later
in
the
mid-twentieth
century
it
began
to
be
believed
that
the
earlier
policy
of
"protection
from
disturbance
was
inadequate
to
preserve
park
values"
and
that
is
that
direct
human
intervention
was
necessary
to
restore
the
landscape
of
national
parks
to
its
‘’natural’’
condition
in
1963
the
leopold
report
argued
that
"a
national
park
should
represent
a
vignette
of
primitive
america"
this
policy
change
eventually
led
to
the
restoration
of
wolves
in
yellowstone
park
in
the
1990s
however
recent
research
in
various
disciplines
indicates
that
a
pristine
natural
or
"primitive"
landscape
is
a
myth
and
it
now
realised
that
people
have
been
changing
the
natural
into
a
cultural
landscape
for
a
long
while
and
that
there
are
few
places
untouched
in
some
way
from
human
influence
the
earlier
conservation
policies
were
now
seen
as
cultural
interventions
the
idea
of
what
is
natural
and
what
artificial
or
cultural
and
how
to
maintain
the
natural
elements
in
a
landscape
has
been
further
complicated
by
the
discovery
of
global
warming
and
how
it
is
changing
natural
landscapes
also
important
is
a
reaction
recently
amongst
scholars
against
dualistic
thinking
about
nature
and
culture
maria
kaika
comments:
"nowadays
we
are
beginning
to
see
nature
and
culture
as
intertwined
once
again
–
not
ontologically
separated
anymore
[…]what
i
used
to
perceive
as
a
compartmentalized
world
consisting
of
neatly
and
tightly
sealed
autonomous
‘space
envelopes’
(the
home
the
city
and
nature)
was
in
fact
a
messy
socio-spatial
continuum”
and
william
cronon
argues
against
the
idea
of
wilderness
because
it
"involves
a
dualistic
vision
in
which
the
human
is
entirely
outside
the
natural"
and
affirms
that
"wildness
(as
opposed
to
wilderness)
can
be
found
anywhere"
even
"in
the
cracks
of
a
manhattan
sidewalk"
according
to
cronon
we
have
to
"abandon
the
dualism
that
sees
the
tree
in
the
garden
as
artificial
[…]
and
the
tree
in
the
wilderness
as
natural
[…]
both
in
some
ultimate
sense
are
wild"
here
he
bends
somewhat
the
regular
dictionary
meaning
of
wild
to
emphasise
that
nothing
natural
even
in
a
garden
is
fully
under
human
control
the
landscape
of
europe
has
considerably
altered
by
people
and
even
in
an
area
like
the
cairngorm
mountains
of
scotland
with
a
low
population
density
only
"
the
high
summits
of
the
cairngorm
mountains
consist
entirely
of
natural
elements
these
"high
summits"
are
of
course
only
part
of
the
cairngorms
and
there
are
no
longer
wolves
bears
wild
boar
or
lynx
in
scotland's
wilderness
the
scots
pine
in
the
form
of
the
caledonian
forest
also
covered
much
more
of
the
scottish
landscape
than
today
the
swiss
national
park
however
represent
a
more
natural
landscape
it
was
founded
in
1914
and
is
one
of
the
earliest
national
parks
in
europe
visitors
are
not
allowed
to
leave
the
motor
road
or
paths
through
the
park
make
fire
or
camp
the
only
building
within
the
park
is
chamanna
cluozza
mountain
hut
it
is
also
forbidden
to
disturb
the
animals
or
the
plants
or
to
take
home
anything
found
in
the
park
dogs
are
not
allowed
due
to
these
strict
rules
the
swiss
national
park
is
the
only
park
in
the
alps
who
has
been
categorized
by
the
iucn
as
a
strict
nature
reserve
which
is
the
highest
protection
level
no
place
on
the
earth
is
unaffected
by
people
and
their
culture
people
are
part
of
biodiversity
but
human
activity
affects
biodiversity
and
this
alters
the
natural
landscape
mankind
have
altered
landscape
to
such
an
extent
that
few
places
on
earth
remain
pristine
but
once
free
of
human
influences
the
landscape
can
return
to
a
natural
or
near
natural
state
even
the
remote
yukon
and
alaskan
wilderness
the
bi-national
kluane-wrangell-st
elias-glacier
bay-tatshenshini-alsek
park
system
comprising
kluane
wrangell-st
elias
glacier
bay
and
tatshenshini-alsek
parks
a
unesco
world
heritage
site
is
not
free
from
human
influence
because
the
kluane
national
park
lies
within
the
traditional
territories
of
the
champagne
and
aishihik
first
nations
and
kluane
first
nation
who
have
a
long
history
of
living
in
this
region
through
their
respective
final
agreements
with
the
canadian
government
they
have
made
into
law
their
rights
to
harvest
in
this
region
cultural
forces
intentionally
or
unintentionally
have
an
influence
upon
the
landscape
cultural
landscapes
are
places
or
artifacts
created
and
maintained
by
people
examples
of
cultural
intrusions
into
a
landscape
are:
fences
roads
parking
lots
sand
pits
buildings
hiking
trails
management
of
plants
including
the
introduction
of
invasive
species
extraction
or
removal
of
plants
management
of
animals
mining
hunting
natural
landscaping
farming
and
forestry
pollution
areas
that
might
be
confused
with
a
natural
landscape
include
public
parks
farms
orchards
artificial
lakes
and
reservoirs
managed
forests
golf
courses
nature
center
trails
gardens
aesthetics
of
nature
aesthetics
of
nature
is
a
sub-field
of
philosophical
ethics
and
refers
to
the
study
of
natural
objects
from
their
aesthetical
perspective
aesthetics
of
nature
developed
as
a
sub-field
of
philosophical
ethics
in
the
18th
and
19th
century
the
aesthetics
of
nature
advanced
the
concepts
of
disinterestedness
the
pictures
and
the
introduction
of
the
idea
of
positive
aesthetics
the
first
major
developments
of
nature
occurred
in
the
18th
century
the
concept
of
disinterestedness
had
been
explained
by
many
thinkers
anthony
ashley-cooper
introduced
the
concept
as
a
way
of
characterizing
the
notion
of
the
aesthetic
later
magnified
by
francis
hutcheson
who
expanded
it
to
exclude
personal
and
utilitarianism
interests
and
associations
of
a
more
general
nature
from
aesthetic
experience
this
concept
was
further
developed
by
archibald
alison
who
referred
it
to
a
particular
state
of
mind
the
theory
of
disinterestedness
opened
doors
for
a
better
understanding
of
the
aesthetics
dimensions
of
nature
in
terms
of
three
conceptualizations:
objects
experienced
as
beautiful
tend
to
be
small
smooth
and
fair
in
color
in
contrast
objects
viewed
as
sublime
tend
to
be
powerful
intense
and
terrifying
picturesque
items
are
a
mixture
of
both
which
can
be
seen
as
varied
and
irregular
rich
and
forceful
and
even
vibrant
cognitive
and
non-cognitive
approaches
of
nature
have
directed
their
focus
from
natural
environments
to
the
consideration
of
human
and
human
influenced
environments
and
developed
aesthetic
investigations
of
everyday
life(carlson
and
lintott
2007;
parsons
2008a;
carlson
2010)
people
may
be
mistaken
by
the
art
object
analogy
for
instance
a
sandhill
crane
is
not
an
art
object;
an
art
object
is
not
a
sandhill
crane
in
fact
an
art
object
should
be
called
an
"artifact"
the
crane
is
wildlife
on
its
own
and
is
not
an
art
object
this
can
be
related
to
satio's
definition
of
the
cognitive
view
in
elaboration
the
crane
lives
through
various
ecosystems
such
as
yellowstone
nature
is
a
living
system
which
includes
animals
plants
and
eco-systems
in
contrast
an
art
object
has
no
regeneration
evolutionary
history
or
metabolism
an
individual
may
be
in
the
forest
and
perceive
it
as
beautiful
because
of
the
plethora
of
colors
such
as
red
green
and
yellow
this
is
a
result
of
the
chemicals
interacting
with
chlorophyll
an
individual's
aesthetic
experience
may
increase;
however
none
of
the
things
mentioned
have
anything
to
do
with
what
is
really
going
on
in
the
forest
the
chlorophyll
is
capturing
solar
energy
and
the
residual
chemicals
protect
the
trees
from
insect
grazing
any
color
perceived
by
human
visitors
for
a
few
hours
is
entirely
different
from
what
is
really
happening
according
to
leopold
the
three
features
of
ecosystems
that
generate
land
ethic
are
integrity
stability
and
beauty
none
of
the
mentioned
features
are
real
in
nature
ecosystems
are
not
stable:
they
are
dramatically
changing
and
they
have
little
integration;
ergo
beauty
is
in
the
eye
of
the
beholder
in
a
post-modern
approach
when
an
individual
engages
in
aesthetically
appreciating
a
natural
thing
we
give
meaning
to
the
thing
we
appreciate
and
in
that
meaning
we
express
and
develop
our
own
attitudes
values
and
beliefs
our
interest
in
natural
things
are
not
only
a
passive
reflection
of
our
inclinations
as
croce
describes
as
the
appreciation
of
nature
as
looking
in
a
mirror
or
what
we
might
call
our
inward
life;
but
may
instead
be
the
things
we
come
across
in
nature
that
engage
and
stimulate
our
imagination
as
a
result
we
are
challenged
to
think
differently
and
apply
thoughts
and
associations
to
in
new
situations
and
ways
as
a
characterization
of
the
appreciation
of
art
nature
aestheticists
argue
that
post
modernism
is
a
mistaken
view
because
we
do
not
have
a
case
of
anything
goesthe
aesthetics
appreciation
of
art
is
governed
by
some
normative
standards
in
the
world
of
art
criticism
may
take
place
when
people
come
together
and
discuss
books
and
films
or
critics
write
appraisals
for
publications
on
the
contrary
there
are
not
obvious
instances
of
debate
and
appraisals
where
different
judgments
about
the
aesthetics
of
character
of
nature
are
evaluated
physis
physis
(greek:
"phusis")
is
a
greek
theological
philosophical
and
scientific
term
usually
translated
into
english
as
"nature"
the
term
is
central
to
greek
philosophy
and
as
a
consequence
to
western
philosophy
as
a
whole
in
pre-socratic
usage
"phusis"
was
contrasted
with
"law
human
convention"
since
aristotle
however
the
"physical"
(the
subject
matter
of
"physics"
properly
"natural
things")
has
more
typically
been
juxtaposed
to
the
"metaphysical"
the
word
φύσις
is
a
verbal
noun
based
on
φύω
"to
grow
to
appear"
(cognate
with
english
"to
be")
in
homeric
greek
it
is
used
quite
literally
of
the
manner
of
growth
of
a
particular
species
of
plant
in
pre-socratic
philosophy
beginning
with
heraclitus
"phusis"
in
keeping
with
its
etymology
of
"growing
becoming"
is
always
used
in
the
sense
of
the
"natural"
"development"
although
the
focus
might
lie
either
with
the
origin
or
the
process
or
the
end
result
of
the
process
there
is
some
evidence
that
by
the
6th
century
bc
beginning
with
the
ionian
school
the
word
could
also
be
used
in
the
comprehensive
sense
as
referring
to
""all"
things"
as
it
were
"nature"
in
the
sense
of
"universe"
in
the
sophist
tradition
the
term
stood
in
opposition
to
"nomos"
()
"law"
or
"custom"
in
the
debate
on
which
parts
of
human
existence
are
natural
and
which
are
due
to
convention
the
contrast
of
"phisis"
vs
"nomos"
could
be
applied
to
any
subject
much
like
the
modern
contrast
of
"nature
vs
nurture"
in
book
10
of
"laws"
plato
criticizes
those
who
write
works
"peri
phuseōs"
the
criticism
is
that
such
authors
tend
to
focus
on
a
purely
"naturalistic"
explanation
of
the
world
ignoring
the
role
of
"intention"
or
"technē"
and
thus
becoming
prone
to
the
error
of
naive
atheism
plato
accuses
even
hesiod
of
this
for
the
reason
that
the
gods
in
hesiod
"grow"
out
of
primordial
entities
after
the
physical
universe
had
been
established
"because
those
who
use
the
term
mean
to
say
that
nature
is
the
first
creative
power;
but
if
the
soul
turns
out
to
be
the
primeval
element
and
not
fire
or
air
then
in
the
truest
sense
and
beyond
other
things
the
soul
may
be
said
to
exist
"by"
nature;
and
this
would
be
true
if
you
proved
that
the
soul
is
older
than
the
body
but
not
otherwise"
aristotle
sought
out
the
definition
of
"physis"
to
prove
that
there
was
more
than
one
definition
of
"physis"
and
more
than
one
way
to
interpret
nature
"though
aristotle
retains
the
ancient
sense
of
"physis"
as
growth
he
insists
that
an
adequate
definition
of
"physis"
requires
the
different
perspectives
of
the
four
causes
(aitia):
material
efficient
formal
and
final"
aristotle
believed
that
nature
itself
contained
its
own
source
of
matter
(material)
power/motion
(efficiency)
form
and
end
(final)
a
unique
feature
about
aristotle's
definition
of
"physis"
was
his
relationship
between
art
and
nature
aristotle
said
that
"physis"
(nature)
is
dependent
on
techne
(art)
"the
critical
distinction
between
art
and
nature
concerns
their
different
efficient
causes:
nature
is
its
own
source
of
motion
whereas
techne
always
requires
a
source
of
motion
outside
itself"
what
aristotle
was
trying
to
bring
to
light
was
that
art
does
not
contain
within
itself
its
form
or
source
of
motion
consider
the
process
of
an
acorn
becoming
an
oak
tree
this
is
a
natural
process
that
has
its
own
driving
force
behind
it
there
is
no
external
force
pushing
this
acorn
to
its
final
state
rather
it
is
progressively
developing
towards
one
specific
end
(telos)
though
φύσις
was
often
used
in
hellenistic
philosophy
it
is
used
only
14
times
in
the
new
testament
(10
of
those
in
the
writings
of
paul)
its
meaning
varies
throughout
paul's
writings
one
usage
refers
to
the
established
or
natural
order
of
things
as
in
"romans
2:14"
where
paul
writes
"for
when
gentiles
who
do
not
have
the
law
by
"nature"
do
what
the
law
requires
they
are
a
law
to
themselves
even
though
they
do
not
have
the
law"
another
use
of
φύσις
in
the
sense
of
"natural
order"
is
"romans
1:26"
where
he
writes
"the
men
likewise
gave
up
"natural"
relations
with
women
and
were
consumed
with
passion
for
one
another"
in
"1
corinthians
11:14"
paul
asks
"does
not
nature
itself
teach
you
that
if
a
man
wears
long
hair
it
is
a
disgrace
for
him?"
this
use
of
φύσις
as
referring
to
a
"natural
order"
in
"romans
1:26"
and
"1
corinthians
11:14"
may
have
been
influenced
by
stoicism
the
greek
philosophers
including
aristotle
and
the
stoics
are
credited
with
distinguishing
between
man-made
laws
and
a
natural
law
of
universal
validity
but
gerhard
kittel
states
that
the
stoic
philosophers
were
not
able
to
combine
the
concepts
of
νόμος
(law)
and
φύσις
(nature)
to
produce
the
concept
of
"natural
law"
in
the
sense
that
was
made
possible
by
judeo-christian
theology
as
part
of
the
pauline
theology
of
salvation
by
grace
paul
writes
in
"ephesians
2:3"
that
"we
all
once
lived
in
the
passions
of
our
flesh
carrying
out
the
desires
of
the
body
and
the
mind
and
were
by
"nature"
children
of
wrath
like
the
rest
of
mankind
in
the
next
verse
he
writes
"by
grace
you
have
been
saved"
theologians
of
the
early
christian
period
differed
in
the
usage
of
this
term
in
antiochene
circles
it
connoted
the
humanity
or
divinity
of
christ
conceived
as
a
concrete
set
of
characteristics
or
attributes
in
alexandrine
thinking
it
meant
a
concrete
individual
or
independent
existent
and
approximated
to
hypostasis
without
being
a
synonym
while
it
refers
to
much
the
same
thing
as
ousia
it
is
more
empirical
and
descriptive
focussing
on
function
while
ousia
is
metaphysical
and
focuses
more
on
reality
although
found
in
the
context
of
the
trinitarian
debate
it
is
chiefly
important
in
the
christology
of
cyril
of
alexandria
the
greek
adjective
"phusikos"
is
represented
in
various
forms
in
modern
english:
as
"physics"
"the
study
of
nature"
as
"physical"
(via
middle
latin
"physicalis")
referring
both
to
physics
(the
study
of
nature
the
material
universe)
and
to
the
human
body
the
term
physiology
("physiologia")
is
of
16th-century
coinage
(jean
fernel)
the
term
"physique"
for
"the
bodily
constitution
of
a
person"
is
a
19th-century
loan
from
french
in
medicine
the
suffix
"-physis"
occurs
in
such
compounds
as
"symphysis"
"epiphysis"
and
a
few
others
in
the
sense
of
"a
growth"
the
physis
also
refers
to
the
"growth
plate"
or
site
of
growth
at
the
end
of
long
bones
ecosystem
health
ecosystem
health
is
a
metaphor
used
to
describe
the
condition
of
an
ecosystem
ecosystem
condition
can
vary
as
a
result
of
fire
flooding
drought
extinctions
invasive
species
climate
change
mining
overexploitation
in
fishing
farming
or
logging
chemical
spills
and
a
host
of
other
reasons
there
is
no
universally
accepted
benchmark
for
a
healthy
ecosystem
rather
the
apparent
health
status
of
an
ecosystem
can
vary
depending
upon
which
health
metrics
are
employed
in
judging
it
and
which
societal
aspirations
are
driving
the
assessment
advocates
of
the
health
metaphor
argue
for
its
simplicity
as
a
communication
tool
"policy-makers
and
the
public
need
simple
understandable
concepts
like
health"
critics
worry
that
ecosystem
health
a
"value-laden
construct"
is
often
"passed
off
as
science
to
unsuspecting
policy
makers
and
the
public"
the
health
metaphor
applied
to
the
environment
has
been
in
use
at
least
since
the
early
1800s
and
the
great
american
conservationist
aldo
leopold
(1887–1948)
spoke
metaphorically
of
land
health
land
sickness
mutilation
and
violence
when
describing
land
use
practices
the
term
"ecosystem
management"
has
been
in
use
at
least
since
the
1950s
the
term
"ecosystem
health"
has
become
widespread
in
the
ecological
literature
as
a
general
metaphor
meaning
something
good
and
as
an
environmental
quality
goal
in
field
assessments
of
rivers
lakes
seas
and
forests
recently
however
this
metaphor
has
been
subject
of
quantitative
formulation
using
complex
systems
concepts
such
as
criticality
meaning
that
a
healthy
ecosystem
is
in
some
sort
of
balance
between
adaptability
(randomness)
and
robustness
(order)
nevertheless
the
universality
of
criticality
is
still
under
examination
and
is
known
as
the
criticality
hypothesis
which
states
that
systems
in
a
dynamic
regime
shifting
between
order
and
disorder
attain
the
highest
level
of
computational
capabilities
and
achieve
an
optimal
trade-off
between
robustness
and
flexibility
recent
results
in
cell
and
evolutionary
biology
neuroscience
and
computer
science
have
great
interest
in
the
criticality
hypothesis
emphasizing
its
role
as
a
viable
candidate
general
law
in
the
realm
of
adaptive
complex
systems
(see
and
references
therein)
the
term
ecosystem
health
has
been
employed
to
embrace
some
suite
of
environmental
goals
deemed
desirable
edward
grumbine's
highly
cited
paper
"what
is
ecosystem
management?"
surveyed
ecosystem
management
and
ecosystem
health
literature
and
summarized
frequently
encountered
goal
statements:
grumbine
describes
each
of
these
goals
as
a
"value
statement"
and
stresses
the
role
of
human
values
in
setting
ecosystem
management
goals
it
is
the
last
goal
mentioned
in
the
survey
accommodating
humans
that
is
most
contentious
"we
have
observed
that
when
groups
of
stakeholders
work
to
define
…
visions
this
leads
to
debate
over
whether
to
emphasize
ecosystem
health
or
human
well-being
…
whether
the
priority
is
ecosystems
or
people
greatly
influences
stakeholders'
assessment
of
desirable
ecological
and
social
states"
and
for
example
"for
some
wolves
are
critical
to
ecosystem
health
and
an
essential
part
of
nature
for
others
they
are
a
symbol
of
government
overreach
threatening
their
livelihoods
and
cultural
values"
measuring
ecosystem
health
requires
extensive
goal-driven
environmental
sampling
for
example
a
vision
for
ecosystem
health
of
lake
superior
was
developed
by
a
public
forum
and
a
series
of
objectives
were
prepared
for
protection
of
habitat
and
maintenance
of
populations
of
some
70
indigenous
fish
species
a
suite
of
80
lake
health
indicators
was
developed
for
the
great
lakes
basin
including
monitoring
native
fish
species
exotic
species
water
levels
phosphorus
levels
toxic
chemicals
phytoplankton
zooplankton
fish
tissue
contaminants
etc
some
authors
have
attempted
broad
definitions
of
ecosystem
health
such
as
benchmarking
as
healthy
the
historical
ecosystem
state
"prior
to
the
onset
of
anthropogenic
stress"
a
difficulty
is
that
the
historical
composition
of
many
human-altered
ecosystems
is
unknown
or
unknowable
also
fossil
and
pollen
records
indicate
that
the
species
that
occupy
an
ecosystem
reshuffle
through
time
so
it
is
difficult
to
identify
one
snapshot
in
time
as
optimum
or
"healthy"
a
commonly
cited
broad
definition
states
that
a
healthy
ecosystem
has
three
attributes:
while
this
captures
significant
ecosystem
properties
a
generalization
is
elusive
as
those
properties
do
not
necessarily
co-vary
in
nature
for
example
there
is
not
necessarily
a
clear
or
consistent
relationship
between
productivity
and
species
richness
similarly
the
relationship
between
resilience
and
diversity
is
complex
and
ecosystem
stability
may
depend
upon
one
or
a
few
species
rather
than
overall
diversity
and
some
undesirable
ecosystems
are
highly
productive
"resilience
is
not
desirable
per
se
there
can
be
highly
resilient
states
of
ecosystems
which
are
very
undesirable
from
some
human
perspectives
such
as
algal-dominated
coral
reefs"
ecological
resilience
is
a
"capacity"
that
varies
depending
upon
which
properties
of
the
ecosystem
are
to
be
studied
and
depending
upon
what
kinds
of
disturbances
are
considered
and
how
they
are
to
be
quantified
approaches
to
assessing
it
"face
high
uncertainties
and
still
require
a
considerable
amount
of
empirical
and
theoretical
research"
other
authors
have
sought
a
numerical
index
of
ecosystem
health
that
would
permit
quantitative
comparisons
among
ecosystems
and
within
ecosystems
over
time
one
such
system
employs
ratings
of
the
three
properties
mentioned
above:
health
=
system
vigor
x
system
organization
x
system
resilience
ecologist
glenn
suter
argues
that
such
indices
employ
"nonsense
units"
the
indices
have
"no
meaning;
they
cannot
be
predicted
so
they
are
not
applicable
to
most
regulatory
problems;
they
have
no
diagnostic
power;
effects
of
one
component
are
eclipsed
by
responses
of
other
components
and
the
reason
for
a
high
or
low
index
value
is
unknown"
health
metrics
are
determined
by
stakeholder
goals
which
drive
ecosystem
definition
an
ecosystem
is
an
abstraction
"ecosystems
cannot
be
identified
or
found
in
nature
instead
they
must
be
delimited
by
an
observer
this
can
be
done
in
many
different
ways
for
the
same
chunk
of
nature
depending
on
the
specific
perspectives
of
interest"
ecosystem
definition
determines
the
acceptable
range
of
variability
(reference
conditions)
and
determines
measurement
variables
the
latter
are
used
as
indicators
of
ecosystem
structure
and
function
and
can
be
used
as
indicators
of
"health"
an
indicator
is
a
variable
such
as
a
chemical
or
biological
property
that
when
measured
is
used
to
infer
trends
in
another
(unmeasured)
environmental
variable
or
cluster
of
unmeasured
variables
(the
indicandum)
for
example
rising
mortality
rate
of
canaries
in
a
coal
mine
is
an
indicator
of
rising
carbon
monoxide
levels
rising
chlorophyll-a
levels
in
a
lake
may
signal
eutrophication
ecosystem
assessments
employ
two
kinds
of
indicators
descriptive
indicators
and
normative
indicators
"indicators
can
be
used
descriptively
for
a
scientific
purpose
or
normatively
for
a
political
purpose"
used
descriptively
high
chlorophyll-a
is
an
indicator
of
eutrophication
but
it
may
also
be
used
as
an
ecosystem
health
indicator
when
used
as
a
normative
(health)
indicator
it
indicates
a
rank
on
a
health
scale
a
rank
that
can
vary
widely
depending
on
societal
preferences
as
to
what
is
desirable
a
high
chlorophyll-a
level
in
a
natural
successional
wetland
might
be
viewed
as
healthy
whereas
a
human-impacted
wetland
with
the
"same"
indicator
value
may
be
judged
unhealthy
estimation
of
ecosystem
health
has
been
criticized
for
intermingling
the
two
types
of
environmental
indicators
a
health
indicator
is
a
normative
indicator
and
if
conflated
with
descriptive
indicators
"implies
that
normative
values
can
be
measured
objectively
which
is
certainly
not
true
thus
implicit
values
are
insinuated
to
the
reader
a
situation
which
has
to
be
avoided"
it
can
be
argued
that
the
very
act
of
selecting
indicators
of
any
kind
is
biased
by
the
observer's
perspective
but
separation
of
goals
from
descriptions
has
been
advocated
as
a
step
toward
transparency:
"a
separation
of
descriptive
and
normative
indicators
is
essential
from
the
perspective
of
the
philosophy
of
science
…
goals
and
values
cannot
be
deduced
directly
from
descriptions
…
a
fact
that
is
emphasized
repeatedly
in
the
literature
of
environmental
ethics
…
hence
we
advise
always
specifying
the
definition
of
indicators
and
propose
clearly
distinguishing
ecological
indicators
in
science
from
policy
indicators
used
for
decision-making
processes"
and
integration
of
multiple
possibly
conflicting
normative
indicators
into
a
single
measure
of
"ecosystem
health"
is
problematic
using
56
indicators
"determining
environmental
status
and
assessing
marine
ecosystems
health
in
an
integrative
way
is
still
one
of
the
grand
challenges
in
marine
ecosystems
ecology
research
and
management"
another
issue
with
indicators
is
validity
good
indicators
must
have
an
independently
validated
high
predictive
value
that
is
high
sensitivity
(high
probability
of
indicating
a
significant
change
in
the
indicandum)
and
high
specificity
(low
probability
of
wrongly
indicating
a
change)
the
reliability
of
various
health
metrics
has
been
questioned
and
"what
combination
of
measurements
should
be
used
to
evaluate
ecosystems
is
a
matter
of
current
scientific
debate"
most
attempts
to
identify
ecological
indicators
have
been
correlative
rather
than
derived
from
prospective
testing
of
their
predictive
value
and
the
selection
process
for
many
indicators
has
been
based
upon
weak
evidence
or
has
been
lacking
in
evidence
in
some
cases
no
reliable
indicators
are
known:
"we
found
no
examples
of
invertebrates
successfully
used
in
[forest]
monitoring
programs
their
richness
and
abundance
ensure
that
they
play
significant
roles
in
ecosystem
function
but
thwart
focus
on
a
few
key
species"
and
"reviews
of
species-based
monitoring
approaches
reveal
that
no
single
species
nor
even
a
group
of
species
accurately
reflects
entire
communities
understanding
the
response
of
a
single
species
may
not
provide
reliable
predictions
about
a
group
of
species
even
when
the
group
is
a
few
very
similar
species"
a
trade-off
between
human
health
and
the
"health"
of
nature
has
been
termed
the
"health
paradox"
and
it
illuminates
how
human
values
drive
perceptions
of
ecosystem
health
human
health
has
benefited
by
sacrificing
the
"health"
of
wild
ecosystems
such
as
dismantling
and
damming
of
wild
valleys
destruction
of
mosquito-bearing
wetlands
diversion
of
water
for
irrigation
conversion
of
wilderness
to
farmland
timber
removal
and
extirpation
of
tigers
whales
ferrets
and
wolves
there
has
been
an
acrimonious
schism
among
conservationists
and
resource
managers
over
the
question
of
whether
to
"ratchet
back
human
domination
of
the
biosphere"
or
whether
to
embrace
it
these
two
perspectives
have
been
characterized
as
utilitarian
vs
protectionist
the
utilitarian
view
treats
human
health
and
well-being
as
criteria
of
ecosystem
health
for
example
destruction
of
wetlands
to
control
malaria
mosquitoes
"resulted
in
an
improvement
in
ecosystem
health"
the
protectionist
view
treats
humans
as
an
invasive
species:
"if
there
was
ever
a
species
that
qualified
as
an
invasive
pest
it
is
"homo
sapiens""
proponents
of
the
utilitarian
view
argue
that
"healthy
ecosystems
are
characterized
by
their
capability
to
sustain
healthy
human
populations"
and
"healthy
ecosystems
must
be
economically
viable"
as
it
is
"unhealthy"
ecosystems
that
are
likely
to
result
in
increases
in
contamination
infectious
diseases
fires
floods
crop
failures
and
fishery
collapse
protectionists
argue
that
privileging
of
human
health
is
a
conflict
of
interest
as
humans
have
demolished
massive
numbers
of
ecosystems
to
maintain
their
welfare
also
disease
and
parasitism
are
historically
normal
in
pre-industrial
nature
diseases
and
parasites
promote
ecosystem
functioning
driving
biodiversity
and
productivity
and
parasites
may
constitute
a
significant
fraction
of
ecosystem
biomass
the
very
choice
of
the
word
"health"
applied
to
ecology
has
been
questioned
as
lacking
in
neutrality
in
a
bioscience
article
on
responsible
use
of
scientific
language:
"some
conservationists
fear
that
these
terms
could
endorse
human
domination
of
the
planet
…
and
could
exacerbate
the
shifting
cognitive
baseline
whereby
humans
tend
to
become
accustomed
to
new
and
often
degraded
ecosystems
and
thus
forget
the
nature
of
the
past"
criticism
of
ecosystem
health
largely
targets
the
failure
of
proponents
to
explicitly
distinguish
the
normative
dimension
from
the
descriptive
dimension
and
has
included
the
following:
alternatives
have
been
proposed
for
the
term
ecosystem
health
including
more
neutral
language
such
as
ecosystem
status
ecosystem
prognosis
and
ecosystem
sustainability
another
alternative
to
the
use
of
a
health
metaphor
is
to
"express
exactly
and
clearly
the
public
policy
and
the
management
objective"
to
employ
habitat
descriptors
and
real
properties
of
ecosystems
an
example
of
a
policy
statement
is
"the
maintenance
of
viable
natural
populations
of
wildlife
and
ecological
functions
always
takes
precedence
over
any
human
use
of
wildlife"
an
example
of
a
goal
is
"maintain
viable
populations
of
all
native
species
in
situ"
an
example
of
a
management
objective
is
"maintain
self-sustaining
populations
of
lake
whitefish
within
the
range
of
abundance
observed
during
1990-99"
kurt
jax
presented
an
ecosystem
assessment
format
that
avoids
imposing
a
preconceived
notion
of
normality
that
avoids
the
muddling
of
normative
and
descriptive
and
that
gives
serious
attention
to
ecosystem
definition
(1)
societal
purposes
for
the
ecosystem
are
negotiated
by
stakeholders
(2)
a
functioning
ecosystem
is
defined
with
emphasis
on
phenomena
relevant
to
stakeholder
goals
(3)
benchmark
reference
conditions
and
permissible
variation
of
the
system
are
established
(4)
measurement
variables
are
chosen
for
use
as
indicators
and
(5)
the
time
scale
and
spatial
scale
of
assessment
are
decided
ecological
health
has
been
used
as
a
medical
term
in
reference
to
human
allergy
and
multiple
chemical
sensitivity
and
as
a
public
health
term
for
programs
to
modify
health
risks
(diabetes
obesity
smoking
etc)
human
health
itself
when
viewed
in
its
broadest
sense
is
viewed
as
having
ecological
foundations
it
is
also
an
urban
planning
term
in
reference
to
"green"
cities
(composting
recycling)
and
has
been
used
loosely
with
regard
to
various
environmental
issues
and
as
the
condition
of
human-disturbed
environmental
sites
ecosystem
integrity
implies
a
condition
of
an
ecosystem
exposed
to
a
minimum
of
human
influence
ecohealth
is
the
relationship
of
human
health
to
the
environment
including
the
effect
of
climate
change
wars
food
production
urbanization
and
ecosystem
structure
and
function
ecosystem
management
and
ecosystem-based
management
refer
to
the
sustainable
management
of
ecosystems
and
in
some
cases
may
employ
the
terms
ecosystem
health
or
ecosystem
integrity
as
a
goal
nature
religion
a
nature
religion
is
a
religious
movement
that
believes
nature
and
the
natural
world
is
an
embodiment
of
divinity
sacredness
or
spiritual
power
nature
religions
include
indigenous
religions
practiced
in
various
parts
of
the
world
by
cultures
who
consider
the
environment
to
be
imbued
with
spirits
and
other
sacred
entities
it
also
includes
contemporary
pagan
faiths
which
are
primarily
concentrated
in
europe
and
north
america
the
term
"nature
religion"
was
first
coined
by
the
american
religious
studies
scholar
catherine
albanese
who
used
it
in
her
work
"nature
religion
in
america:
from
the
algonkian
indians
to
the
new
age"
(1991)
and
later
went
on
to
use
it
in
other
studies
following
on
from
albanese's
development
of
the
term
it
has
since
been
used
by
other
academics
working
in
the
discipline
catherine
albanese
described
nature
religion
as
"a
symbolic
center
and
the
cluster
of
beliefs
behaviours
and
values
that
encircles
it"
deeming
it
to
be
useful
for
shining
a
light
on
aspects
of
history
that
are
rarely
viewed
as
religious
in
a
paper
of
his
on
the
subject
the
canadian
religious
studies
scholar
peter
beyer
described
"nature
religion"
as
a
"useful
analytical
abstraction"
to
refer
to
"any
religious
belief
or
practice
in
which
devotees
consider
nature
to
be
the
embodiment
of
divinity
sacredness
transcendence
spiritual
power
or
whatever
cognate
term
one
wishes
to
use"
he
went
on
to
note
that
in
this
way
nature
religion
was
not
an
"identifiable
religious
tradition"
such
as
buddhism
or
christianity
are
but
that
it
instead
covers
"a
range
of
religious
and
quasi-religious
movements
groups
and
social
networks
whose
participants
may
or
may
not
identify
with
one
of
the
many
constructed
religions
of
global
society
which
referred
to
many
other
nature
religion"
peter
beyer
noted
the
existence
of
a
series
of
common
characteristics
which
he
believed
were
shared
by
different
nature
religions
he
remarked
that
although
"one
must
be
careful
not
to
overgeneralise"
he
suspected
that
there
were
a
series
of
features
which
"occur
sufficiently
often"
in
those
nature
religions
known
to
recorded
scholarship
to
constitute
a
pattern
the
first
of
these
common
characteristics
was
nature
religion's
"comparative
resistance
to
institutionalisation
and
legitimisation
in
terms
of
identifiable
socio-religious
authorities
and
organisations"
meaning
that
nature
religionists
rarely
formed
their
religious
beliefs
into
large
visible
socio-political
structures
such
as
churches
furthermore
beyer
noted
nature
religionists
often
held
a
"concomitant
distrust
of
and
even
eschewing
of
politically
orientated
power"
instead
of
this
he
felt
that
among
nature
religious
communities
there
was
"a
valuing
of
community
as
non-hierarchical"
and
a
"conditional
optimism
with
regard
to
human
capacity
and
the
future"
in
the
sphere
of
the
environment
beyer
noted
that
nature
religionists
held
to
a
"holistic
conception
of
reality"
and
"a
valorisation
of
physical
place
as
vital
aspects
of
their
spiritualities"
similarly
beyer
noted
the
individualism
which
was
favoured
by
nature
religionists
he
remarked
that
those
adhering
to
such
beliefs
typically
had
respect
for
"charismatic
and
hence
purely
individual
authority"
and
place
a
"strong
emphasis
on
individual
paths"
which
led
them
to
believe
in
"the
equal
value
of
individuals
and
groups"
along
similar
lines
he
also
commented
on
the
"strong
experiential
basis"
to
nature
religionist
beliefs
"where
personal
experience
is
a
final
arbiter
of
truth
or
validity"
in
april
1996
the
university
of
lancaster
in
north
west
england
held
a
conference
on
contemporary
paganism
entitled
"nature
religion
today:
western
paganism
shamanism
and
esotericism
in
the
1990s"
and
ultimately
led
to
the
publication
of
an
academic
anthology
of
the
same
name
two
years
later
this
book
"nature
religion
today:
paganism
in
the
modern
world"
was
edited
by
members
of
the
university's
department
of
religious
studies
a
postgraduate
named
joanne
pearson
and
two
professors
richard
h
roberts
and
geoffrey
samuel
in
his
study
of
wicca
the
pagan
studies
scholar
ethan
doyle
white
expressed
the
view
that
the
category
of
"nature
religion"
was
problematic
from
a
"historical
perspective"
because
it
solely
emphasises
the
"commonalities
of
belief
and
attitude
to
the
natural
world"
that
are
found
between
different
religions
and
in
doing
so
divorces
these
different
belief
systems
from
their
distinctive
socio-cultural
and
historical
backgrounds
nature-based
solutions
nature-based
solutions
(nbs)
refers
to
the
sustainable
management
and
use
of
nature
for
tackling
socio-environmental
challenges
the
challenges
include
issues
such
as
climate
change
water
security
water
pollution
food
security
human
health
and
disaster
risk
management
a
definition
by
the
european
union
states
that
these
solutions
are
"inspired
and
supported
by
nature
which
are
cost-effective
simultaneously
provide
environmental
social
and
economic
benefits
and
help
build
resilience
the
nature-based
solutions
initiative
meanwhile
defines
them
as
"actions
that
work
with
and
enhance
nature
so
as
to
help
people
adapt
to
change
and
disasters"
such
solutions
bring
more
and
more
diverse
nature
and
natural
features
and
processes
into
cities
landscapes
and
seascapes
through
locally
adapted
resource-efficient
and
systemic
interventions"
with
nbs
healthy
resilient
and
diverse
ecosystems
(whether
natural
managed
or
newly
created)
can
provide
solutions
for
the
benefit
of
societies
and
overall
biodiversity
for
instance
the
restoration
or
protection
of
mangroves
along
coastlines
utilizes
a
nature-based
solution
to
accomplish
several
things
mangroves
moderate
the
impact
of
waves
and
wind
on
coastal
settlements
or
cities
and
sequester
co
they
also
provide
safe
nurseries
for
marine
life
that
can
be
the
basis
for
sustaining
populations
of
fish
that
local
populations
may
depend
on
additionally
the
mangrove
forests
can
help
control
coastal
erosion
resulting
from
sea
level
rise
similarly
in
cities
green
roofs
or
walls
are
nature-based
solutions
that
can
be
used
to
moderate
the
impact
of
high
temperatures
capture
storm
water
abate
pollution
and
act
as
carbon
sinks
while
enhancing
biodiversity
conservation
approaches
and
environment
management
initiatives
have
been
carried
out
for
decades
what
is
new
is
that
the
benefits
of
such
nature-based
solutions
to
human
well-being
have
been
articulated
well
more
recently
even
if
the
term
itself
is
still
being
framed
examples
of
nature-based
solutions
can
be
found
all
over
the
world
and
imitated
nature-based
solutions
are
on
their
way
to
being
mainstreamed
in
national
and
international
policies
and
programmes
(eg
climate
change
policy
law
infrastructure
investment
and
financing
mechanisms)
for
example
the
theme
for
world
water
day
2018
was
"nature
for
water"
and
by
un-water's
accompanying
un
world
water
development
report
had
the
title
"nature-based
solutions
for
water"
societies
increasingly
face
challenges
such
as
climate
change
urbanization
jeopardized
food
security
and
water
resource
provision
and
disaster
risk
one
approach
to
answer
these
challenges
is
to
singularly
rely
on
technological
strategies
an
alternative
approach
is
to
manage
the
(socio-)ecological
systems
in
a
comprehensive
way
in
order
to
sustain
and
potentially
increase
the
delivery
of
ecosystem
services
to
humans
in
this
context
nature-based
solutions
(nbs)
have
recently
been
put
forward
by
practitioners
and
quickly
thereafter
by
policymakers
these
solutions
stress
the
sustainable
use
of
nature
in
solving
coupled
environmental-social-economic
challenges
while
ecosystem
services
are
often
valued
in
terms
of
immediate
benefits
to
human
well-being
and
economy
nbs
focus
on
the
benefits
to
people
and
the
environment
itself
to
allow
for
sustainable
solutions
that
are
able
to
respond
to
environmental
change
and
hazards
in
the
long-term
nbs
go
beyond
the
traditional
biodiversity
conservation
and
management
principles
by
"re-focusing"
the
debate
on
humans
and
specifically
integrating
societal
factors
such
as
human
well-being
and
poverty
reduction
socio-economic
development
and
governance
principles
with
respect
to
water
issues
nbs
can
achieve
the
following
according
to
the
world
water
development
report
2018
by
un-water:
in
2015
the
european
network
biodiversa
highlighted
how
nbs
relate
to
concepts
like
ecosystem
approaches
and
ecological
engineering
nbs
are
strongly
connected
to
ideas
such
as
natural
systems
agriculture
natural
solutions
ecosystem-based
approaches
adaptation
services
natural
infrastructure
green
infrastructure
and
ecological
engineering
for
instance
ecosystem-based
approaches
are
increasingly
promoted
for
climate
change
adaptation
and
mitigation
by
organisations
like
united
nations
environment
programme
and
non-governmental
organisations
such
as
the
nature
conservancy
these
organisations
refer
to
"policies
and
measures
that
take
into
account
the
role
of
ecosystem
services
in
reducing
the
vulnerability
of
society
to
climate
change
in
a
multi-sectoral
and
multi-scale
approach"
likewise
natural
infrastructure
is
defined
as
a
"strategically
planned
and
managed
network
of
natural
lands
such
as
forests
and
wetlands
working
landscapes
and
other
open
spaces
that
conserves
or
enhances
ecosystem
values
and
functions
and
provides
associated
benefits
to
human
populations";
and
green
infrastructure
refers
to
an
"interconnected
network
of
green
spaces
that
conserves
natural
systems
and
provides
assorted
benefits
to
human
populations"
similarly
the
concept
of
ecological
engineering
generally
refers
to
"protecting
restoring
(ie
ecosystem
restoration)
or
modifying
ecological
systems
to
increase
the
quantity
quality
and
sustainability
of
particular
services
they
provide
or
to
build
new
ecological
systems
that
provide
services
that
would
otherwise
be
provided
through
more
conventional
engineering
based
on
non-renewable
resources"
the
international
union
for
the
conservation
of
nature
(iucn)
defines
nbs
as
actions
to
protect
sustainably
manage
and
restore
natural
or
modified
ecosystems
that
address
societal
challenges
effectively
and
adaptively
simultaneously
providing
human
well-being
and
biodiversity
benefits
with
climate
change
food
security
disaster
risks
water
security
social
and
economic
development
as
well
as
human
health
being
the
common
societal
challenges
iucn
proposes
to
consider
nbs
as
an
umbrella
concept
categories
and
examples
of
nbs
approaches
according
to
iucn
include:
the
general
objective
of
nbs
is
clear
namely
the
sustainable
management
and
use
of
nature
for
tackling
societal
challenges
however
different
stakeholders
view
nbs
from
other
perspectives
for
instance
iucn
defines
nbs
as
"actions
to
protect
sustainably
manage
and
restore
natural
or
modified
ecosystems
which
address
societal
challenges
effectively
and
adaptively
while
simultaneously
providing
human
well-being
and
biodiversity
benefits"
this
framing
puts
the
need
for
well-managed
and
restored
ecosystems
at
the
heart
of
nbs
with
the
overarching
goal
of
"supporting
the
achievement
of
society's
development
goals
and
safeguard
human
well-being
in
ways
that
reflect
cultural
and
societal
values
and
enhance
the
resilience
of
ecosystems
their
capacity
for
renewal
and
the
provision
of
services"
in
the
context
of
the
ongoing
political
debate
on
jobs
and
growth
(main
drivers
of
the
current
eu
policy
agenda)
the
european
commission
underlines
that
nbs
can
transform
environmental
and
societal
challenges
into
innovation
opportunities
by
turning
natural
capital
into
a
source
for
green
growth
and
sustainable
development
in
their
view
nbs
to
societal
challenges
are
"solutions
that
are
inspired
and
supported
by
nature
which
are
cost-effective
simultaneously
provide
environmental
social
and
economic
benefits
and
help
build
resilience
such
solutions
bring
more
and
more
diverse
nature
and
natural
features
and
processes
into
cities
landscapes
and
seascapes
through
locally
adapted
resource-efficient
and
systemic
interventions"
this
framing
is
somewhat
broader
and
puts
economy
and
social
assets
at
the
heart
of
nbs
as
importantly
as
sustaining
environmental
conditions
it
shares
similarities
with
the
definition
proposed
by
maes
and
jacobs
(2015)
defining
nbs
as
"any
transition
to
a
use
of
es
with
decreased
input
of
non-renewable
natural
capital
and
increased
investment
in
renewable
natural
processes"
in
their
view
development
and
evaluation
of
nbs
spans
three
basic
requirements:
(1)
decrease
of
fossil
fuel
input
per
produced
unit;
(2)
lowering
of
systemic
trade-offs
and
increasing
synergies
between
es;
and
(3)
increasing
labor
input
and
jobs
here
nature
is
seen
as
a
tool
to
inspire
more
systemic
solutions
to
societal
problems
whatever
definition
used
promoting
sustainability
and
the
increased
role
of
natural
self-sustained
processes
relying
on
biodiversity
are
inherent
to
nbs
they
constitute
actions
easily
seen
as
positive
for
a
wide
range
of
stakeholders
as
they
bring
about
benefits
at
environmental
economic
and
social
levels
as
a
consequence
the
concept
of
nbs
is
gaining
acceptance
outside
the
conservation
community
(eg
urban
planning)
and
is
now
on
its
way
to
be
mainstreamed
into
policies
and
programmes
(climate
change
policy
law
infrastructure
investment
and
financing
mechanisms)
in
2014-2015
the
european
network
biodiversa
mobilized
a
range
of
scientists
research
donors
and
stakeholders
and
proposed
a
typology
characterizing
nbs
along
two
gradients
1
"how
much
engineering
of
biodiversity
and
ecosystems
is
involved
in
nbs"
and
2
"how
many
ecosystem
services
and
stakeholder
groups
are
targeted
by
a
given
nbs"
the
typology
highlights
that
nbs
can
involve
very
different
actions
on
ecosystems
(from
protection
to
management
and
even
creation
of
new
ecosystems)
and
is
based
on
the
assumption
that
the
higher
the
number
of
services
and
stakeholder
groups
targeted
the
lower
the
capacity
to
maximize
the
delivery
of
each
service
and
simultaneously
fulfil
the
specific
needs
of
all
stakeholder
groups
as
such
three
types
of
nbs
are
distinguished
(figure
2):
type
1
nbs
consists
of
no
or
minimal
intervention
in
ecosystems
with
the
objectives
of
maintaining
or
improving
the
delivery
of
a
range
of
es
both
inside
and
outside
of
these
conserved
ecosystems
examples
include
the
protection
of
mangroves
in
coastal
areas
to
limit
risks
associated
to
extreme
weather
conditions
and
provide
benefits
and
opportunities
to
local
populations;
and
the
establishment
of
marine
protected
areas
to
conserve
biodiversity
within
these
areas
while
exporting
biomass
into
fishing
grounds
this
type
of
nbs
is
connected
to
for
example
the
concept
of
biosphere
reserves
which
incorporates
core
protected
areas
for
nature
conservation
and
buffer
zones
and
transition
areas
where
people
live
and
work
in
a
sustainable
way
type
2
nbs
corresponds
to
management
approaches
that
develop
sustainable
and
multifunctional
ecosystems
and
landscapes
(extensively
or
intensively
managed)
these
types
improve
the
delivery
of
selected
es
compared
to
what
would
be
obtained
with
a
more
conventional
intervention
examples
include
innovative
planning
of
agricultural
landscapes
to
increase
their
multi-functionality;
and
approaches
for
enhancing
tree
species
and
genetic
diversity
to
increase
forest
resilience
to
extreme
events
this
type
of
nbs
is
strongly
connected
to
concepts
like
natural
systems
agriculture
agro-ecology
and
evolutionary-orientated
forestry
type
3
nbs
consists
of
managing
ecosystems
in
very
extensive
ways
or
even
creating
new
ecosystems
(eg
artificial
ecosystems
with
new
assemblages
of
organisms
for
green
roofs
and
walls
to
mitigate
city
warming
and
clean
polluted
air)
type
3
is
linked
to
concepts
like
green
and
blue
infrastructures
and
objectives
like
restoration
of
heavily
degraded
or
polluted
areas
and
greening
cities
type
1
and
2
would
typically
fall
within
the
iucn
nbs
framework
whereas
type
2
and
moreover
type
3
are
often
exemplified
by
ec
for
turning
natural
capital
into
a
source
for
green
growth
and
sustainable
development
hybrid
solutions
exist
along
this
gradient
both
in
space
and
time
for
instance
at
landscape
scale
mixing
protected
and
managed
areas
could
be
needed
to
fulfil
multi-functionality
and
sustainability
goals
similarly
a
constructed
wetland
can
be
developed
as
a
type
3
but
when
well
established
may
subsequently
be
preserved
and
surveyed
as
a
type
1
demonstrating
the
benefits
of
nature
and
healthy
ecosystems
and
showcasing
the
return
on
investment
they
can
offer
is
necessary
in
order
to
increase
awareness
but
also
to
provide
support
and
guidance
on
how
to
implement
nbs
a
large
number
of
initiatives
around
the
world
already
highlight
the
effectiveness
of
nbs
approaches
to
address
a
wide
range
of
societal
challenges
the
following
table
shows
examples
from
around
the
world:
in
2018
the
hindu
reported
that
the
east
kolkata
wetlands
the
world's
largest
organic
sewage
treatment
facility
had
been
used
to
clean
the
sewage
of
kolkata
in
an
organic
manner
by
using
algae
for
several
decades
in
use
since
the
1930s
the
natural
system
was
discovered
by
dhrubajyoti
ghosh
an
ecologist
and
a
municipal
engineer
in
the
1970s
while
working
in
the
region
ghosh
worked
for
decades
to
protect
the
wetlands
it
had
been
a
practice
in
kolkata
one
of
the
five
largest
cities
in
india
for
the
municipal
authorities
to
pump
sewage
into
shallow
ponds
("bheris")
under
the
heat
of
the
tropical
sun
algae
proliferated
in
them
converting
the
sewage
into
clean
water
which
in
turn
was
used
by
villagers
to
grow
paddy
and
vegetables
this
system
has
been
in
use
in
the
region
since
the
1930s
and
treats
750
million
litres
of
wastewater
per
day
giving
livelihood
to
100000
people
in
the
vicinity
for
his
work
ghosh
was
included
in
the
un
global
500
roll
of
honour
in
1990
and
received
the
luc
hoffmann
award
in
2016
there
is
currently
no
accepted
basis
on
which
a
government
agency
municipality
or
private
company
can
systematically
assess
the
efficiency
effectiveness
and
sustainability
of
a
particular
nature-based
solution
however
a
series
of
principles
are
proposed
to
guide
effective
and
appropriate
implementation
and
thus
to
upscale
nbs
in
practice
for
example
nbs
embrace
and
are
not
meant
to
replace
nature
conservation
norms
also
nbs
are
determined
by
site-specific
natural
and
cultural
contexts
that
include
traditional
local
and
scientific
knowledge
nbs
are
an
integral
part
of
the
overall
design
of
policies
and
measure
or
actions
to
address
a
specific
challenges
finally
nbs
can
be
implemented
alone
or
in
an
integrated
manner
with
other
solutions
to
societal
challenges
(eg
technological
and
engineering
solutions)
and
they
are
applied
at
the
landscape
scale
implementing
nbs
requires
political
economic
and
scientific
challenges
to
be
tackled
first
and
foremost
private
sector
investment
is
needed
not
to
replace
but
to
supplement
traditional
sources
of
capital
such
as
public
funding
or
philanthropy
the
challenge
is
therefore
to
provide
a
robust
evidence
base
for
the
contribution
of
nature
to
economic
growth
and
jobs
and
to
demonstrate
the
economic
viability
of
these
solutions
–
compared
to
technological
ones
–
on
a
timescale
compatible
with
that
of
global
change
furthermore
it
requires
measures
like
adaptation
of
economic
subsidy
schemes
and
the
creation
of
opportunities
for
conservation
finance
to
name
a
few
indeed
such
measures
will
be
needed
to
scale
up
nbs
interventions
and
strengthen
their
impact
in
mitigating
the
world's
most
pressing
challenges
since
2016
the
eu
is
supporting
a
multi-stakeholder
dialogue
platform
(called
thinknature)
to
promote
the
co-design
testing
and
deployment
of
improved
and
innovative
nbs
in
an
integrated
way
creation
of
such
science-policy-business-society
interfaces
could
promote
the
market
uptake
of
nbs
the
project
is
part
of
the
eu’s
horizon
2020
–
research
and
innovation
programme
and
will
last
for
3
years
there
are
a
total
of
17
international
partners
involved
including
the
technical
university
of
crete
(project
leader)
the
university
of
helsinki
and
biodiversa
in
2017
as
part
of
the
presidency
of
the
estonian
republic
of
the
council
of
the
european
union
a
conference
called
“nature-based
solutions:
from
innovation
to
common-use”
was
organized
by
the
ministry
of
the
environment
of
estonia
and
the
university
of
tallinn
this
conference
aimed
to
strengthen
synergies
among
various
recent
initiatives
and
programs
related
to
nbs
launched
by
the
european
commission
and
by
the
eu
member
states
focusing
on
policy
and
governance
of
nbs
and
on
research
and
innovation
in
recognition
of
the
importance
of
natural
ecosystems
for
mitigation
and
adaptation
the
paris
agreement
calls
on
all
parties
to
acknowledge
“the
importance
of
the
conservation
and
enhancement
as
appropriate
of
sinks
and
reservoirs
of
the
greenhouse
gases”
and
to
“note
the
importance
of
ensuring
the
integrity
of
all
ecosystems
including
oceans
and
the
protection
of
biodiversity
recognized
by
some
cultures
as
mother
earth”
it
then
includes
in
its
articles
several
references
to
nature-based
solutions
for
example
article
52
encourages
parties
to
adopt
“…policy
approaches
and
positive
incentives
for
activities
relating
to
reducing
emissions
from
deforestation
and
forest
degradation
and
the
role
of
conservation
and
sustainable
management
of
forests
and
enhancement
of
forest
carbon
stocks
in
developing
countries;
and
alternative
policy
approaches
such
as
joint
mitigation
and
adaptation
approaches
for
the
integral
and
sustainable
management
of
forests
while
reaffirming
the
importance
of
incentivizing
as
appropriate
non-carbon
benefits
associated
with
such
approaches”
article
71
further
encourages
parties
to
build
the
resilience
of
socioeconomic
and
ecological
systems
including
through
economic
diversification
and
sustainable
management
of
natural
resources
in
total
the
agreement
refers
to
nature
(ecosystems
natural
resources
forests)
in
13
distinct
places
an
in-depth
analysis
of
all
nationally
determined
contributions
submitted
to
unfccc
revealed
that
around
130
ndcs
or
65%
of
signatories
commit
to
nature-based
solutions
in
their
climate
pledges
suggesting
broad
consensus
for
the
role
of
nature
in
helping
meet
climate
change
goals
however
high-level
commitments
rarely
translate
into
robust
measurable
actions
on-the-ground
the
term
nbs
was
put
forward
by
practitioners
in
the
late
2000s
(in
particular
the
international
union
for
the
conservation
of
nature
and
the
world
bank)
and
thereafter
by
policymakers
in
europe
(most
notably
the
european
commission)
the
term
"nature-based
solutions"
was
first
used
in
the
late
2000s
it
was
used
in
the
context
of
finding
new
solutions
to
mitigate
and
adapt
to
climate
change
effects
whilst
simultaneously
protecting
biodiversity
and
improving
sustainable
livelihoods
the
iucn
referred
to
nbs
in
a
position
paper
for
the
united
nations
framework
convention
on
climate
change
the
term
was
also
adopted
by
european
policymakers
in
particular
by
the
european
commission
in
a
report
stressing
that
nbs
can
offer
innovative
means
to
create
jobs
and
growth
as
part
of
a
green
economy
the
term
started
to
make
appearances
in
the
mainstream
media
around
the
time
of
the
global
climate
action
summit
in
california
in
september
2018
natural
environment
the
natural
environment
encompasses
all
living
and
non-living
things
occurring
naturally
meaning
in
this
case
not
artificial
the
term
is
most
often
applied
to
the
earth
or
some
parts
of
earth
this
environment
encompasses
the
interaction
of
all
living
species
climate
weather
and
natural
resources
that
affect
human
survival
and
economic
activity
the
concept
of
the
"natural
environment"
can
be
distinguished
as
components:
in
contrast
to
the
natural
environment
is
the
built
environment
in
such
areas
where
man
has
fundamentally
transformed
landscapes
such
as
urban
settings
and
agricultural
land
conversion
the
natural
environment
is
greatly
modified
into
a
simplified
human
environment
even
acts
which
seem
less
extreme
such
as
building
a
mud
hut
or
a
photovoltaic
system
in
the
desert
the
modified
environment
becomes
an
artificial
one
though
many
animals
build
things
to
provide
a
better
environment
for
themselves
they
are
not
human
hence
beaver
dams
and
the
works
of
mound-building
termites
are
thought
of
as
natural
people
seldom
find
"absolutely
natural"
environments
on
earth
and
naturalness
usually
varies
in
a
continuum
from
100%
natural
in
one
extreme
to
0%
natural
in
the
other
more
precisely
we
can
consider
the
different
aspects
or
components
of
an
environment
and
see
that
their
degree
of
naturalness
is
not
uniform
if
for
instance
in
an
agricultural
field
the
mineralogic
composition
and
the
structure
of
its
soil
are
similar
to
those
of
an
undisturbed
forest
soil
but
the
structure
is
quite
different
"natural
environment"
is
often
used
as
a
synonym
for
habitat
for
instance
when
we
say
that
the
natural
environment
of
giraffes
is
the
savanna
earth
science
generally
recognizes
4
spheres
the
lithosphere
the
hydrosphere
the
atmosphere
and
the
biosphere
as
correspondent
to
rocks
water
air
and
life
respectively
some
scientists
include
as
part
of
the
spheres
of
the
earth
the
cryosphere
(corresponding
to
ice)
as
a
distinct
portion
of
the
hydrosphere
as
well
as
the
pedosphere
(corresponding
to
soil)
as
an
active
and
intermixed
sphere
earth
science
(also
known
as
geoscience
the
geographical
sciences
or
the
earth
sciences)
is
an
all-embracing
term
for
the
sciences
related
to
the
planet
earth
there
are
four
major
disciplines
in
earth
sciences
namely
geography
geology
geophysics
and
geodesy
these
major
disciplines
use
physics
chemistry
biology
chronology
and
mathematics
to
build
a
qualitative
and
quantitative
understanding
of
the
principal
areas
or
"spheres"
of
earth
the
earth's
crust
or
lithosphere
is
the
outermost
solid
surface
of
the
planet
and
is
chemically
and
mechanically
different
from
underlying
mantle
it
has
been
generated
greatly
by
igneous
processes
in
which
magma
cools
and
solidifies
to
form
solid
rock
beneath
the
lithosphere
lies
the
mantle
which
is
heated
by
the
decay
of
radioactive
elements
the
mantle
though
solid
is
in
a
state
of
rheic
convection
this
convection
process
causes
the
lithospheric
plates
to
move
albeit
slowly
the
resulting
process
is
known
as
plate
tectonics
volcanoes
result
primarily
from
the
melting
of
subducted
crust
material
or
of
rising
mantle
at
mid-ocean
ridges
and
mantle
plumes
most
water
is
found
in
one
or
another
natural
kind
of
body
of
water
an
ocean
is
a
major
body
of
saline
water
and
a
component
of
the
hydrosphere
approximately
71%
of
the
earth's
surface
(an
area
of
some
362
million
square
kilometers)
is
covered
by
ocean
a
continuous
body
of
water
that
is
customarily
divided
into
several
principal
oceans
and
smaller
seas
more
than
half
of
this
area
is
over
3000
meters
(9800 ft)
deep
average
oceanic
salinity
is
around
35
parts
per
thousand
(ppt)
(35%)
and
nearly
all
seawater
has
a
salinity
in
the
range
of
30
to
38
ppt
though
generally
recognized
as
several
'separate'
oceans
these
waters
comprise
one
global
interconnected
body
of
salt
water
often
referred
to
as
the
world
ocean
or
global
ocean
the
deep
seabeds
are
more
than
half
the
earth's
surface
and
are
among
the
least-modified
natural
environments
the
major
oceanic
divisions
are
defined
in
part
by
the
continents
various
archipelagos
and
other
criteria:
these
divisions
are
(in
descending
order
of
size)
the
pacific
ocean
the
atlantic
ocean
the
indian
ocean
the
southern
ocean
and
the
arctic
ocean
a
river
is
a
natural
watercourse
usually
freshwater
flowing
toward
an
ocean
a
lake
a
sea
or
another
river
a
few
rivers
simply
flow
into
the
ground
and
dry
up
completely
before
reaching
another
body
of
water
the
water
in
a
river
is
usually
in
a
channel
made
up
of
a
stream
bed
between
banks
in
larger
rivers
there
is
also
a
wider
floodplain
shaped
by
waters
over-topping
the
channel
flood
plains
may
be
very
wide
in
relation
to
the
size
of
the
river
channel
rivers
are
a
part
of
the
hydrological
cycle
water
within
a
river
is
generally
collected
from
precipitation
through
surface
runoff
groundwater
recharge
springs
and
the
release
of
water
stored
in
glaciers
and
snowpacks
small
rivers
may
also
be
termed
by
several
other
names
including
stream
creek
and
brook
their
current
is
confined
within
a
bed
and
stream
banks
streams
play
an
important
corridor
role
in
connecting
fragmented
habitats
and
thus
in
conserving
biodiversity
the
study
of
streams
and
waterways
in
general
is
known
as
"surface
hydrology"
a
lake
(from
latin
"lacus")
is
a
terrain
feature
a
body
of
water
that
is
localized
to
the
bottom
of
basin
a
body
of
water
is
considered
a
lake
when
it
is
inland
is
not
part
of
an
ocean
and
is
larger
and
deeper
than
a
pond
natural
lakes
on
earth
are
generally
found
in
mountainous
areas
rift
zones
and
areas
with
ongoing
or
recent
glaciation
other
lakes
are
found
in
endorheic
basins
or
along
the
courses
of
mature
rivers
in
some
parts
of
the
world
there
are
many
lakes
because
of
chaotic
drainage
patterns
left
over
from
the
last
ice
age
all
lakes
are
temporary
over
geologic
time
scales
as
they
will
slowly
fill
in
with
sediments
or
spill
out
of
the
basin
containing
them
a
pond
is
a
body
of
standing
water
either
natural
or
man-made
that
is
usually
smaller
than
a
lake
a
wide
variety
of
man-made
bodies
of
water
are
classified
as
ponds
including
water
gardens
designed
for
aesthetic
ornamentation
fish
ponds
designed
for
commercial
fish
breeding
and
solar
ponds
designed
to
store
thermal
energy
ponds
and
lakes
are
distinguished
from
streams
by
their
current
speed
while
currents
in
streams
are
easily
observed
ponds
and
lakes
possess
thermally
driven
micro-currents
and
moderate
wind
driven
currents
these
features
distinguish
a
pond
from
many
other
aquatic
terrain
features
such
as
stream
pools
and
tide
pools
humans
impact
the
water
in
different
ways
such
as
modifying
rivers
(through
dams
and
stream
channelization)
urbanization
and
deforestation
these
impact
lake
levels
groundwater
conditions
water
pollution
thermal
pollution
and
marine
pollution
humans
modify
rivers
by
using
direct
channel
manipulation
they
are
building
dams
and
reservoirs
and
manipulating
the
direction
of
the
rivers
and
water
path
dams
are
good
for
humans
some
communities
need
the
reservoirs
to
survive
however
reservoirs
and
dams
may
negatively
impact
the
environment
and
wildlife
dams
stops
fish
migration
and
the
moving
of
organisms
down
stream
urbanization
effects
the
environment
because
of
deforestation
and
changing
lake
levels
groundwater
conditions
etc
deforestation
and
urbanization
go
hand
in
hand
deforestation
may
cause
flooding
declining
stream
flow
and
changes
in
riverside
vegetation
the
changing
vegetation
occurs
because
when
trees
cannot
get
adequate
water
they
start
to
deteriorate
leading
to
a
decreased
food
supply
for
the
wildlife
in
an
area
the
atmosphere
of
the
earth
serves
as
a
key
factor
in
sustaining
the
planetary
ecosystem
the
thin
layer
of
gases
that
envelops
the
earth
is
held
in
place
by
the
planet's
gravity
dry
air
consists
of
78%
nitrogen
21%
oxygen
1%
argon
and
other
inert
gases
such
as
carbon
dioxide
the
remaining
gases
are
often
referred
to
as
trace
gases
among
which
are
the
greenhouse
gases
such
as
water
vapor
carbon
dioxide
methane
nitrous
oxide
and
ozone
filtered
air
includes
trace
amounts
of
many
other
chemical
compounds
air
also
contains
a
variable
amount
of
water
vapor
and
suspensions
of
water
droplets
and
ice
crystals
seen
as
clouds
many
natural
substances
may
be
present
in
tiny
amounts
in
an
unfiltered
air
sample
including
dust
pollen
and
spores
sea
spray
volcanic
ash
and
meteoroids
various
industrial
pollutants
also
may
be
present
such
as
chlorine
(elementary
or
in
compounds)
fluorine
compounds
elemental
mercury
and
sulphur
compounds
such
as
sulphur
dioxide
[so]
the
ozone
layer
of
the
earth's
atmosphere
plays
an
important
role
in
depleting
the
amount
of
ultraviolet
(uv)
radiation
that
reaches
the
surface
as
dna
is
readily
damaged
by
uv
light
this
serves
to
protect
life
at
the
surface
the
atmosphere
also
retains
heat
during
the
night
thereby
reducing
the
daily
temperature
extremes
earth's
atmosphere
can
be
divided
into
five
main
layers
these
layers
are
mainly
determined
by
whether
temperature
increases
or
decreases
with
altitude
from
highest
to
lowest
these
layers
are:
within
the
five
principal
layers
determined
by
temperature
there
are
several
layers
determined
by
other
properties
the
dangers
of
global
warming
are
being
increasingly
studied
by
a
wide
global
consortium
of
scientists
these
scientists
are
increasingly
concerned
about
the
potential
long-term
effects
of
global
warming
on
our
natural
environment
and
on
the
planet
of
particular
concern
is
how
climate
change
and
global
warming
caused
by
anthropogenic
or
human-made
releases
of
greenhouse
gases
most
notably
carbon
dioxide
can
act
interactively
and
have
adverse
effects
upon
the
planet
its
natural
environment
and
humans'
existence
it
is
clear
the
planet
is
warming
and
warming
rapidly
this
is
due
to
the
greenhouse
effect
which
is
caused
by
greenhouse
gases
which
trap
heat
inside
the
earth's
atmosphere
because
of
their
more
complex
molecular
structure
which
allows
them
to
vibrate
and
in
turn
trap
heat
and
release
it
back
towards
the
earth
this
warming
is
also
responsible
for
the
extinction
of
natural
habitats
which
in
turn
leads
to
a
reduction
in
wildlife
populationthe
most
recent
report
from
the
intergovernmental
panel
on
climate
change
(the
group
of
the
leading
climate
scientists
in
the
world)
concluded
that
the
earth
will
warm
anywhere
from
27
to
almost
11
degrees
fahrenheit
(15
to
6
degrees
celsius)
between
1990
and
2100
efforts
have
been
increasingly
focused
on
the
mitigation
of
greenhouse
gases
that
are
causing
climatic
changes
on
developing
adaptative
strategies
to
global
warming
to
assist
humans
other
animal
and
plant
species
ecosystems
regions
and
nations
in
adjusting
to
the
effects
of
global
warming
some
examples
of
recent
collaboration
to
address
climate
change
and
global
warming
include:
a
significantly
profound
challenge
is
to
identify
the
natural
environmental
dynamics
in
contrast
to
environmental
changes
not
within
natural
variances
a
common
solution
is
to
adapt
a
static
view
neglecting
natural
variances
to
exist
methodologically
this
view
could
be
defended
when
looking
at
processes
which
change
slowly
and
short
time
series
while
the
problem
arrives
when
fast
processes
turns
essential
in
the
object
of
the
study
climate
looks
at
the
statistics
of
temperature
humidity
atmospheric
pressure
wind
rainfall
atmospheric
particle
count
and
other
meteorological
elements
in
a
given
region
over
long
periods
of
time
weather
on
the
other
hand
is
the
present
condition
of
these
same
elements
over
periods
up
to
two
weeks
climates
can
be
classified
according
to
the
average
and
typical
ranges
of
different
variables
most
commonly
temperature
and
precipitation
the
most
commonly
used
classification
scheme
is
the
one
originally
developed
by
wladimir
köppen
the
thornthwaite
system
in
use
since
1948
uses
evapotranspiration
as
well
as
temperature
and
precipitation
information
to
study
animal
species
diversity
and
the
potential
impacts
of
climate
changes
weather
is
a
set
of
all
the
phenomena
occurring
in
a
given
atmospheric
area
at
a
given
time
most
weather
phenomena
occur
in
the
troposphere
just
below
the
stratosphere
weather
refers
generally
to
day-to-day
temperature
and
precipitation
activity
whereas
climate
is
the
term
for
the
average
atmospheric
conditions
over
longer
periods
of
time
when
used
without
qualification
"weather"
is
understood
to
be
the
weather
of
earth
weather
occurs
due
to
density
(temperature
and
moisture)
differences
between
one
place
and
another
these
differences
can
occur
due
to
the
sun
angle
at
any
particular
spot
which
varies
by
latitude
from
the
tropics
the
strong
temperature
contrast
between
polar
and
tropical
air
gives
rise
to
the
jet
stream
weather
systems
in
the
mid-latitudes
such
as
extratropical
cyclones
are
caused
by
instabilities
of
the
jet
stream
flow
because
the
earth's
axis
is
tilted
relative
to
its
orbital
plane
sunlight
is
incident
at
different
angles
at
different
times
of
the
year
on
the
earth's
surface
temperatures
usually
range
±40 °c
(100 °f
to
−40 °f)
annually
over
thousands
of
years
changes
in
the
earth's
orbit
have
affected
the
amount
and
distribution
of
solar
energy
received
by
the
earth
and
influence
long-term
climate
surface
temperature
differences
in
turn
cause
pressure
differences
higher
altitudes
are
cooler
than
lower
altitudes
due
to
differences
in
compressional
heating
weather
forecasting
is
the
application
of
science
and
technology
to
predict
the
state
of
the
atmosphere
for
a
future
time
and
a
given
location
the
atmosphere
is
a
chaotic
system
and
small
changes
to
one
part
of
the
system
can
grow
to
have
large
effects
on
the
system
as
a
whole
human
attempts
to
control
the
weather
have
occurred
throughout
human
history
and
there
is
evidence
that
civilized
human
activity
such
as
agriculture
and
industry
has
inadvertently
modified
weather
patterns
evidence
suggests
that
life
on
earth
has
existed
for
about
37
billion
years
all
known
life
forms
share
fundamental
molecular
mechanisms
and
based
on
these
observations
theories
on
the
origin
of
life
attempt
to
find
a
mechanism
explaining
the
formation
of
a
primordial
single
cell
organism
from
which
all
life
originates
there
are
many
different
hypotheses
regarding
the
path
that
might
have
been
taken
from
simple
organic
molecules
via
pre-cellular
life
to
protocells
and
metabolism
although
there
is
no
universal
agreement
on
the
definition
of
life
scientists
generally
accept
that
the
biological
manifestation
of
life
is
characterized
by
organization
metabolism
growth
adaptation
response
to
stimuli
and
reproduction
life
may
also
be
said
to
be
simply
the
characteristic
state
of
organisms
in
biology
the
science
of
living
organisms
"life"
is
the
condition
which
distinguishes
active
organisms
from
inorganic
matter
including
the
capacity
for
growth
functional
activity
and
the
continual
change
preceding
death
a
diverse
variety
of
living
organisms
(life
forms)
can
be
found
in
the
biosphere
on
earth
and
properties
common
to
these
organisms—plants
animals
fungi
protists
archaea
and
bacteria—are
a
carbon-
and
water-based
cellular
form
with
complex
organization
and
heritable
genetic
information
living
organisms
undergo
metabolism
maintain
homeostasis
possess
a
capacity
to
grow
respond
to
stimuli
reproduce
and
through
natural
selection
adapt
to
their
environment
in
successive
generations
more
complex
living
organisms
can
communicate
through
various
means
an
ecosystem
(also
called
as
environment)
is
a
natural
unit
consisting
of
all
plants
animals
and
micro-organisms
(biotic
factors)
in
an
area
functioning
together
with
all
of
the
non-living
physical
(abiotic)
factors
of
the
environment
central
to
the
ecosystem
concept
is
the
idea
that
living
organisms
are
continually
engaged
in
a
highly
interrelated
set
of
relationships
with
every
other
element
constituting
the
environment
in
which
they
exist
eugene
odum
one
of
the
founders
of
the
science
of
ecology
stated:
"any
unit
that
includes
all
of
the
organisms
(ie:
the
"community")
in
a
given
area
interacting
with
the
physical
environment
so
that
a
flow
of
energy
leads
to
clearly
defined
trophic
structure
biotic
diversity
and
material
cycles
(ie:
exchange
of
materials
between
living
and
nonliving
parts)
within
the
system
is
an
ecosystem"
a
greater
number
or
variety
of
species
or
biological
diversity
of
an
ecosystem
may
contribute
to
greater
resilience
of
an
ecosystem
because
there
are
more
species
present
at
a
location
to
respond
to
change
and
thus
"absorb"
or
reduce
its
effects
this
reduces
the
effect
before
the
ecosystem's
structure
is
fundamentally
changed
to
a
different
state
this
is
not
universally
the
case
and
there
is
no
proven
relationship
between
the
species
diversity
of
an
ecosystem
and
its
ability
to
provide
goods
and
services
on
a
sustainable
level
the
term
ecosystem
can
also
pertain
to
human-made
environments
such
as
human
ecosystems
and
human-influenced
ecosystems
and
can
describe
any
situation
where
there
is
relationship
between
living
organisms
and
their
environment
fewer
areas
on
the
surface
of
the
earth
today
exist
free
from
human
contact
although
some
genuine
wilderness
areas
continue
to
exist
without
any
forms
of
human
intervention
biomes
are
terminologically
similar
to
the
concept
of
ecosystems
and
are
climatically
and
geographically
defined
areas
of
ecologically
similar
climatic
conditions
on
the
earth
such
as
communities
of
plants
animals
and
soil
organisms
often
referred
to
"as"
ecosystems
biomes
are
defined
on
the
basis
of
factors
such
as
plant
structures
(such
as
trees
shrubs
and
grasses)
leaf
types
(such
as
broadleaf
and
needleleaf)
plant
spacing
(forest
woodland
savanna)
and
climate
unlike
ecozones
biomes
are
not
defined
by
genetic
taxonomic
or
historical
similarities
biomes
are
often
identified
with
particular
patterns
of
ecological
succession
and
climax
vegetation
global
biogeochemical
cycles
are
critical
to
life
most
notably
those
of
water
oxygen
carbon
nitrogen
and
phosphorus
wilderness
is
generally
defined
as
a
natural
environment
on
earth
that
has
not
been
significantly
modified
by
human
activity
the
wild
foundation
goes
into
more
detail
defining
wilderness
as:
"the
most
intact
undisturbed
wild
natural
areas
left
on
our
planet
-
those
last
truly
wild
places
that
humans
do
not
control
and
have
not
developed
with
roads
pipelines
or
other
industrial
infrastructure"
wilderness
areas
and
protected
parks
are
considered
important
for
the
survival
of
certain
species
ecological
studies
conservation
solitude
and
recreation
wilderness
is
deeply
valued
for
cultural
spiritual
moral
and
aesthetic
reasons
some
nature
writers
believe
wilderness
areas
are
vital
for
the
human
spirit
and
creativity
the
word
"wilderness"
derives
from
the
notion
of
wildness;
in
other
words
that
which
is
not
controllable
by
humans
the
word's
etymology
is
from
the
old
english
"wildeornes"
which
in
turn
derives
from
"wildeor"
meaning
"wild
beast"
(wild
+
deor
=
beast
deer)
from
this
point
of
view
it
is
the
wildness
of
a
place
that
makes
it
a
wilderness
the
mere
presence
or
activity
of
people
does
not
disqualify
an
area
from
being
"wilderness"
many
ecosystems
that
are
or
have
been
inhabited
or
influenced
by
activities
of
people
may
still
be
considered
"wild"
this
way
of
looking
at
wilderness
includes
areas
within
which
natural
processes
operate
without
very
noticeable
human
interference
wildlife
includes
all
non-domesticated
plants
animals
and
other
organisms
domesticating
wild
plant
and
animal
species
for
human
benefit
has
occurred
many
times
all
over
the
planet
and
has
a
major
impact
on
the
environment
both
positive
and
negative
wildlife
can
be
found
in
all
ecosystems
deserts
rain
forests
plains
and
other
areas—including
the
most
developed
urban
sites—all
have
distinct
forms
of
wildlife
while
the
term
in
popular
culture
usually
refers
to
animals
that
are
untouched
by
civilized
human
factors
most
scientists
agree
that
wildlife
around
the
world
is
(now)
impacted
by
human
activities
it
is
the
common
understanding
of
"natural
environment"
that
underlies
environmentalism
—
a
broad
political
social
and
philosophical
movement
that
advocates
various
actions
and
policies
in
the
interest
of
protecting
what
nature
remains
in
the
natural
environment
or
restoring
or
expanding
the
role
of
nature
in
this
environment
while
true
wilderness
is
increasingly
rare
"wild"
nature
(eg
unmanaged
forests
uncultivated
grasslands
wildlife
wildflowers)
can
be
found
in
many
locations
previously
inhabited
by
humans
goals
for
the
benefit
of
people
and
natural
systems
commonly
expressed
by
environmental
scientists
and
environmentalists
include:
in
some
cultures
the
term
environment
is
meaningless
because
there
is
no
separation
between
people
and
what
they
view
as
the
natural
world
or
their
surroundings
specifically
in
the
united
states
many
native
cultures
do
not
recognize
the
"environment"
or
see
themselves
as
environmentalists
final
straw:
food
earth
happiness
final
straw:
food
earth
happiness
is
a
documentary/art
film
released
in
june
2015
that
takes
audiences
through
farms
and
urban
landscapes
in
japan
south
korea
and
the
united
states
interviewing
leading
practitioners
in
the
natural
farming
movement
the
film
began
when
an
environmental
artist
(patrick
m
lydon)
and
an
environmental
book
editor
(suhee
kang)
had
a
chance
meeting
in
seoul
south
korea
and
began
conducting
short
interviews
together
with
leaders
in
the
ecology
and
social
justice
movements
upon
meeting
korean
farmer
seong
hyun
choi
however
the
two
were
so
impressed
by
his
ecological
mindset
and
way
of
working
that
they
set
out
to
produce
a
feature
film
about
the
movement
lydon
and
kang
ended
up
quitting
their
jobs
giving
away
most
of
their
possessions
and
becoming
voluntarily
homeless
for
four
years
in
order
to
afford
producing
the
film
the
film
is
split
into
three
sections
1)
modern
life
2)
foundations
and
mindset
of
natural
farming
and
3)
natural
farming
in
practice
and
life
according
to
the
filmmakers
as
they
began
to
understand
more
about
how
natural
farming
itself
was
not
rooted
in
methods
but
in
a
way
of
thinking
they
chose
to
explore
the
life
philosophies
and
ways
of
thinking
of
natural
farming
practitioners
in
a
more
free-flowing
and
artistic
way
rather
than
an
instructive
one;
the
result
is
an
unconventional
documentary
that
features
slow
paced
musical
interludes
alongside
interviews
reviewers
have
called
both
"meditative
and
mindful"
and
"an
inspiring
call
to
action"
author
and
musician
alicia
bay
laurel
called
the
film
"both
art
and
documentary"
lydon
and
kang
spent
what
they
call
a
"meager"
life
savings
to
make
the
film
along
with
the
volunteer
efforts
of
farmers
translators
writers
musicians
they
had
met
during
their
journey
although
the
film
was
filmed
written
and
edited
entirely
by
the
two
directors
they
readily
admit
that
the
process
of
making
the
film
was
co-operative
effort
with
more
than
200
volunteers
directly
involved
in
the
process
in
some
way
the
soundtrack
was
recorded
with
professional
musicians
from
each
of
the
three
countries
where
filming
took
place
all
of
whom
donated
their
time
to
contribute
to
the
film
project
with
the
continued
help
of
international
volunteers
the
film
is
available
in
four
languages
(english
korean
japanese
vietnamese)
and
three
more
(chinese
portuguese
french)
are
in
progress
frustrated
by
the
lack
of
distribution
and
film
festival
options
for
low-
and
no-budget
films
the
filmmakers
made
the
decision
to
manage
distribution
and
touring
in
the
same
way
they
went
about
filming
through
co-operative
effort
with
the
help
of
volunteers
independent
theater
owners
and
community
organizers
they
launched
an
extensive
tour
throughout
japan
and
south
korea
from
2015-2016
eventually
screening
the
film
at
over
130
venues
rather
than
simply
screening
the
film
the
filmmakers
decided
to
transition
their
existing
media
production
organization
"sociecity"
into
a
vehicle
for
art
and
community
engagement
they
made
a
point
of
hosting
interactive
events
along
with
their
screenings
and
in
several
cases
stayed
in
communities
for
up
to
three
months
at
a
time
to
build
natural
gardens
and
host
a
project
they
call
realtimefood
a
grown-to-order
restaurant
which
connects
the
ideas
from
the
film
with
real-world
practices
in
farming
food
and
crafts
in
most
cases
these
efforts
were
funded
by
grants
from
local
philanthropic
organizations
and/or
supported
by
the
communities
themselves
interested
in
the
unconventional
way
the
film
was
being
made
and
toured
multiple
magazines
and
newspapers
in
japan
and
korea
followed
the
directors
during
several
parts
of
their
journey
notably
essen
bar
and
dining
and
road
magazines
and
shikoku
shinbun
and
huffington
post
newspapers
during
the
tour
the
film
was
eventually
picked
up
by
festivals
including
tassie
eco
film
festival
and
belleville
doc
fest
balance
of
nature
the
balance
of
nature
is
a
theory
that
proposes
that
ecological
systems
are
usually
in
a
stable
equilibrium
or
homeostasis
which
is
to
say
that
a
small
change
in
some
particular
parameter
(the
size
of
a
particular
population
for
example)
will
be
corrected
by
some
negative
feedback
that
will
bring
the
parameter
back
to
its
original
"point
of
balance"
with
the
rest
of
the
system
it
may
apply
where
populations
depend
on
each
other
for
example
in
predator/prey
systems
or
relationships
between
herbivores
and
their
food
source
it
is
also
sometimes
applied
to
the
relationship
between
the
earth's
ecosystem
the
composition
of
the
atmosphere
and
the
world's
weather
the
gaia
hypothesis
is
a
balance
of
nature-based
theory
that
suggests
that
the
earth
and
its
ecology
may
act
as
co-ordinated
systems
in
order
to
maintain
the
balance
of
nature
the
theory
that
nature
is
permanently
in
balance
has
been
largely
discredited
by
scientists
working
in
ecology
as
it
has
been
found
that
chaotic
changes
in
population
levels
are
common
but
nevertheless
the
idea
continues
to
be
popular
in
the
general
public
during
the
later
half
of
the
twentieth
century
the
theory
was
superseded
by
catastrophe
theory
and
chaos
theory
the
concept
that
nature
maintains
its
condition
is
of
ancient
provenance;
herodotus
commented
on
the
wonderful
relationship
between
predator
and
prey
species
which
remained
in
a
steady
proportion
to
one
another
with
predators
never
excessively
consuming
their
prey
populations
the
"balance
of
nature"
concept
once
ruled
ecological
research
as
well
as
once
governing
the
management
of
natural
resources
this
led
to
a
doctrine
popular
among
some
conservationists
that
nature
was
best
left
to
its
own
devices
and
that
human
intervention
into
it
was
by
definition
unacceptable
the
validity
of
a
"balance
of
nature"
was
already
questioned
in
the
early
1900s
but
the
general
abandonment
of
the
theory
by
scientists
working
in
ecology
only
happened
in
the
last
quarter
of
that
century
when
studies
showed
that
it
did
not
match
what
could
be
observed
among
plant
and
animal
populations
predator-prey
populations
tend
to
show
chaotic
behavior
within
limits
where
the
sizes
of
populations
change
in
a
way
that
may
appear
random
but
is
in
fact
obeying
deterministic
laws
based
only
on
the
relationship
between
a
population
and
its
food
source
illustrated
by
the
lotka–volterra
equation
an
experimental
example
of
this
was
shown
in
an
eight-year
study
on
small
baltic
sea
creatures
such
as
plankton
which
were
isolated
from
the
rest
of
the
ocean
each
member
of
the
food
web
was
shown
to
take
turns
multiplying
and
declining
even
though
the
scientists
kept
the
outside
conditions
constant
an
article
in
the
journal
"nature"
stated;
"advanced
mathematical
techniques
proved
the
indisputable
presence
of
chaos
in
this
food
web
short-term
prediction
is
possible
but
long-term
prediction
is
not"
although
some
conservationist
organizations
argue
that
human
activity
is
incompatible
with
a
balanced
ecosystem
there
are
numerous
examples
in
history
showing
that
several
modern
day
habitats
originate
from
human
activity:
some
of
latin
america's
rain
forests
owe
their
existence
to
humans
planting
and
transplanting
them
while
the
abundance
of
grazing
animals
in
the
serengeti
plain
of
africa
is
thought
by
some
ecologists
to
be
partly
due
to
human-set
fires
that
created
savanna
habitats
possibly
one
of
the
best
examples
of
an
ecosystem
fundamentally
modified
by
human
activity
can
be
observed
as
a
consequence
of
the
australian
aboriginal
practice
of
"fire-stick
farming"
the
legacy
of
this
practice
over
long
periods
has
resulted
in
forests
being
converted
to
grasslands
capable
of
sustaining
larger
populations
of
faunal
prey
particularly
in
the
northern
and
western
regions
of
the
continent
so
thorough
has
been
the
effect
of
these
deliberate
regular
burnings
that
many
plant
and
tree
species
from
affected
regions
have
now
completely
adapted
to
the
annual
fire
regime
in
that
they
require
the
passage
of
a
fire
before
their
seeds
will
even
germinate
one
school
in
los
angeles
states
"
“we
have
let
our
kids
go
to
the
forest
area
of
the
playground
however
five
years
later
we
found
that
none
of
the
flowers
were
growing
the
natural
damp
soil
had
been
hardened
and
all
of
the
beautiful
grass
had
been
plucked”
despite
being
discredited
among
ecologists
the
theory
is
widely
held
to
be
true
by
the
general
public
with
one
authority
calling
it
an
"enduring
myth"
at
least
in
midwestern
america
the
"balance
of
nature"
idea
was
shown
to
be
widely
held
by
both
science
majors
and
the
general
student
population
in
a
study
at
the
university
of
patras
educational
sciences
students
were
asked
to
reason
about
the
future
of
ecosystems
which
suffered
human-driven
disturbances
subjects
agreed
that
it
was
very
likely
for
the
ecosystems
to
fully
recover
their
initial
state
referring
to
either
a
'recovery
process'
which
restores
the
initial
'balance'
or
specific
'recovery
mechanisms'
as
an
ecosystem's
inherent
characteristic
in
a
2017
study
ampatzidis
and
ergazaki
discuss
the
learning
objectives
and
design
criteria
that
a
learning
environment
for
non-biology
major
students
should
meet
to
support
them
challenge
the
"balance
of
nature"
idea
earth
earth
is
the
third
planet
from
the
sun
and
the
only
astronomical
object
known
to
harbor
life
according
to
radiometric
dating
and
other
sources
of
evidence
earth
formed
over
45
billion
years
ago
earth's
gravity
interacts
with
other
objects
in
space
especially
the
sun
and
the
moon
earth's
only
natural
satellite
earth
revolves
around
the
sun
in
36526
days
a
period
known
as
an
earth
year
during
this
time
earth
rotates
about
its
axis
about
36626
times
earth's
axis
of
rotation
is
tilted
with
respect
to
its
orbital
plane
producing
seasons
on
earth
the
gravitational
interaction
between
earth
and
the
moon
causes
ocean
tides
stabilizes
earth's
orientation
on
its
axis
and
gradually
slows
its
rotation
earth
is
the
densest
planet
in
the
solar
system
and
the
largest
of
the
four
terrestrial
planets
earth's
lithosphere
is
divided
into
several
rigid
tectonic
plates
that
migrate
across
the
surface
over
periods
of
many
millions
of
years
about
71%
of
earth's
surface
is
covered
with
water
mostly
by
oceans
the
remaining
29%
is
land
consisting
of
continents
and
islands
that
together
have
many
lakes
rivers
and
other
sources
of
water
that
contribute
to
the
hydrosphere
the
majority
of
earth's
polar
regions
are
covered
in
ice
including
the
antarctic
ice
sheet
and
the
sea
ice
of
the
arctic
ice
pack
earth's
interior
remains
active
with
a
solid
iron
inner
core
a
liquid
outer
core
that
generates
the
earth's
magnetic
field
and
a
convecting
mantle
that
drives
plate
tectonics
within
the
first
billion
years
of
earth's
history
life
appeared
in
the
oceans
and
began
to
affect
the
earth's
atmosphere
and
surface
leading
to
the
proliferation
of
aerobic
and
anaerobic
organisms
some
geological
evidence
indicates
that
life
may
have
arisen
as
much
as
41 billion
years
ago
since
then
the
combination
of
earth's
distance
from
the
sun
physical
properties
and
geological
history
have
allowed
life
to
evolve
and
thrive
in
the
history
of
the
earth
biodiversity
has
gone
through
long
periods
of
expansion
occasionally
punctuated
by
mass
extinction
events
over
99%
of
all
species
that
ever
lived
on
earth
are
extinct
estimates
of
the
number
of
species
on
earth
today
vary
widely;
most
species
have
not
been
described
over
76 billion
humans
live
on
earth
and
depend
on
its
biosphere
and
natural
resources
for
their
survival
humans
have
developed
diverse
societies
and
cultures;
politically
the
world
has
about
200
sovereign
states
the
modern
english
word
"earth"
developed
from
a
wide
variety
of
middle
english
forms
which
derived
from
an
old
english
noun
most
often
spelled
'
it
has
cognates
in
every
germanic
language
and
their
proto-germanic
root
has
been
reconstructed
as
*"erþō"
in
its
earliest
appearances
"eorðe"
was
already
being
used
to
translate
the
many
senses
of
latin
'
and
greek
("gē"):
the
ground
its
soil
dry
land
the
human
world
the
surface
of
the
world
(including
the
sea)
and
the
globe
itself
as
with
terra
and
gaia
earth
was
a
personified
goddess
in
germanic
paganism:
the
angles
were
listed
by
tacitus
as
among
the
devotees
of
nerthus
and
later
norse
mythology
included
jörð
a
giantess
often
given
as
the
mother
of
thor
originally
"earth"
was
written
in
lowercase
and
from
early
middle
english
its
definite
sense
as
"the
globe"
was
expressed
as
"the
earth"
by
early
modern
english
many
nouns
were
capitalized
and
"the
earth"
became
(and
often
remained)
"the
earth"
particularly
when
referenced
along
with
other
heavenly
bodies
more
recently
the
name
is
sometimes
simply
given
as
"earth"
by
analogy
with
the
names
of
the
other
planets
house
styles
now
vary:
oxford
spelling
recognizes
the
lowercase
form
as
the
most
common
with
the
capitalized
form
an
acceptable
variant
another
convention
capitalizes
"earth"
when
appearing
as
a
name
(eg
"earth's
atmosphere")
but
writes
it
in
lowercase
when
preceded
by
"the"
(eg
"the
atmosphere
of
the
earth")
it
almost
always
appears
in
lowercase
in
colloquial
expressions
such
as
"what
on
earth
are
you
doing?"
the
oldest
material
found
in
the
solar
system
is
dated
to
(bya)
by
the
primordial
earth
had
formed
the
bodies
in
the
solar
system
formed
and
evolved
with
the
sun
in
theory
a
solar
nebula
partitions
a
volume
out
of
a
molecular
cloud
by
gravitational
collapse
which
begins
to
spin
and
flatten
into
a
circumstellar
disk
and
then
the
planets
grow
out
of
that
disk
with
the
sun
a
nebula
contains
gas
ice
grains
and
dust
(including
primordial
nuclides)
according
to
nebular
theory
planetesimals
formed
by
accretion
with
the
primordial
earth
taking
10–
(mys)
to
form
a
subject
of
research
is
the
formation
of
the
moon
some
453
bya
a
leading
hypothesis
is
that
it
was
formed
by
accretion
from
material
loosed
from
earth
after
a
mars-sized
object
named
theia
hit
earth
in
this
view
the
mass
of
theia
was
approximately
10
percent
of
earth
it
hit
earth
with
a
glancing
blow
and
some
of
its
mass
merged
with
earth
between
approximately
41
and
numerous
asteroid
impacts
during
the
late
heavy
bombardment
caused
significant
changes
to
the
greater
surface
environment
of
the
moon
and
by
inference
to
that
of
earth
earth's
atmosphere
and
oceans
were
formed
by
volcanic
activity
and
outgassing
water
vapor
from
these
sources
condensed
into
the
oceans
augmented
by
water
and
ice
from
asteroids
protoplanets
and
comets
in
this
model
atmospheric
"greenhouse
gases"
kept
the
oceans
from
freezing
when
the
newly
forming
sun
had
only
70%
of
its
current
luminosity
by
earth's
magnetic
field
was
established
which
helped
prevent
the
atmosphere
from
being
stripped
away
by
the
solar
wind
a
crust
formed
when
the
molten
outer
layer
of
earth
cooled
to
form
a
solid
the
two
models
that
explain
land
mass
propose
either
a
steady
growth
to
the
present-day
forms
or
more
likely
a
rapid
growth
early
in
earth
history
followed
by
a
long-term
steady
continental
area
continents
formed
by
plate
tectonics
a
process
ultimately
driven
by
the
continuous
loss
of
heat
from
earth's
interior
over
the
period
of
hundreds
of
millions
of
years
the
supercontinents
have
assembled
and
broken
apart
roughly
(mya)
one
of
the
earliest
known
supercontinents
rodinia
began
to
break
apart
the
continents
later
recombined
to
form
pannotia
then
finally
pangaea
which
also
broke
apart
the
present
pattern
of
ice
ages
began
about
and
then
intensified
during
the
pleistocene
about
high-latitude
regions
have
since
undergone
repeated
cycles
of
glaciation
and
thaw
repeating
about
every
the
last
continental
glaciation
ended
ago
chemical
reactions
led
to
the
first
self-replicating
molecules
about
four
billion
years
ago
a
half
billion
years
later
the
last
common
ancestor
of
all
current
life
arose
the
evolution
of
photosynthesis
allowed
the
sun's
energy
to
be
harvested
directly
by
life
forms
the
resultant
molecular
oxygen
()
accumulated
in
the
atmosphere
and
due
to
interaction
with
ultraviolet
solar
radiation
formed
a
protective
ozone
layer
()
in
the
upper
atmosphere
the
incorporation
of
smaller
cells
within
larger
ones
resulted
in
the
development
of
complex
cells
called
eukaryotes
true
multicellular
organisms
formed
as
cells
within
colonies
became
increasingly
specialized
aided
by
the
absorption
of
harmful
ultraviolet
radiation
by
the
ozone
layer
life
colonized
earth's
surface
among
the
earliest
fossil
evidence
for
life
is
microbial
mat
fossils
found
in
348 billion-year-old
sandstone
in
western
australia
biogenic
graphite
found
in
37 billion-year-old
metasedimentary
rocks
in
western
greenland
and
remains
of
biotic
material
found
in
41 billion-year-old
rocks
in
western
australia
the
earliest
direct
evidence
of
life
on
earth
is
contained
in
345
billion-year-old
australian
rocks
showing
fossils
of
microorganisms
during
the
neoproterozoic
much
of
earth
might
have
been
covered
in
ice
this
hypothesis
has
been
termed
"snowball
earth"
and
it
is
of
particular
interest
because
it
preceded
the
cambrian
explosion
when
multicellular
life
forms
significantly
increased
in
complexity
following
the
cambrian
explosion
there
have
been
five
mass
extinctions
the
most
recent
such
event
was
when
an
asteroid
impact
triggered
the
extinction
of
the
non-avian
dinosaurs
and
other
large
reptiles
but
spared
some
small
animals
such
as
mammals
which
at
the
time
resembled
shrews
mammalian
life
has
diversified
over
the
past
and
several
million
years
ago
an
african
ape-like
animal
such
as
"orrorin
tugenensis"
gained
the
ability
to
stand
upright
this
facilitated
tool
use
and
encouraged
communication
that
provided
the
nutrition
and
stimulation
needed
for
a
larger
brain
which
led
to
the
evolution
of
humans
the
development
of
agriculture
and
then
civilization
led
to
humans
having
an
influence
on
earth
and
the
nature
and
quantity
of
other
life
forms
that
continues
to
this
day
earth's
expected
long-term
future
is
tied
to
that
of
the
sun
over
the
next
solar
luminosity
will
increase
by
10%
and
over
the
next
by
40%
the
earth's
increasing
surface
temperature
will
accelerate
the
inorganic
carbon
cycle
reducing
concentration
to
levels
lethally
low
for
plants
(
for
c4
photosynthesis)
in
approximately
the
lack
of
vegetation
will
result
in
the
loss
of
oxygen
in
the
atmosphere
making
animal
life
impossible
after
another
billion
years
all
surface
water
will
have
disappeared
and
the
mean
global
temperature
will
reach
from
that
point
the
earth
is
expected
to
be
habitable
for
another
possibly
up
to
if
nitrogen
is
removed
from
the
atmosphere
even
if
the
sun
were
eternal
and
stable
27%
of
the
water
in
the
modern
oceans
will
descend
to
the
mantle
in
one
billion
years
due
to
reduced
steam
venting
from
mid-ocean
ridges
the
sun
will
evolve
to
become
a
red
giant
in
about
models
predict
that
the
sun
will
expand
to
roughly
about
250
times
its
present
radius
earth's
fate
is
less
clear
as
a
red
giant
the
sun
will
lose
roughly
30%
of
its
mass
so
without
tidal
effects
earth
will
move
to
an
orbit
from
the
sun
when
the
star
reaches
its
maximum
radius
most
if
not
all
remaining
life
will
be
destroyed
by
the
sun's
increased
luminosity
(peaking
at
about
5000
times
its
present
level)
a
2008
simulation
indicates
that
earth's
orbit
will
eventually
decay
due
to
tidal
effects
and
drag
causing
it
to
enter
the
sun's
atmosphere
and
be
vaporized
the
shape
of
earth
is
approximately
oblate
spheroidal
due
to
rotation
the
earth
is
flattened
at
the
poles
and
bulging
around
the
equator
the
diameter
of
the
earth
at
the
equator
is
larger
than
the
pole-to-pole
diameter
thus
the
point
on
the
surface
farthest
from
earth's
center
of
mass
is
the
summit
of
the
equatorial
chimborazo
volcano
in
ecuador
()
the
average
diameter
of
the
reference
spheroid
is
local
topography
deviates
from
this
idealized
spheroid
although
on
a
global
scale
these
deviations
are
small
compared
to
earth's
radius:
the
maximum
deviation
of
only
017%
is
at
the
mariana
trench
(
below
local
sea
level)
whereas
mount
everest
(
above
local
sea
level)
represents
a
deviation
of
014%
in
geodesy
the
exact
shape
that
earth's
oceans
would
adopt
in
the
absence
of
land
and
perturbations
such
as
tides
and
winds
is
called
the
geoid
more
precisely
the
geoid
is
the
surface
of
gravitational
equipotential
at
mean
sea
level
earth's
mass
is
approximately
(5970
yg)
it
is
composed
mostly
of
iron
(321%)
oxygen
(301%)
silicon
(151%)
magnesium
(139%)
sulfur
(29%)
nickel
(18%)
calcium
(15%)
and
aluminium
(14%)
with
the
remaining
12%
consisting
of
trace
amounts
of
other
elements
due
to
mass
segregation
the
core
region
is
estimated
to
be
primarily
composed
of
iron
(888%)
with
smaller
amounts
of
nickel
(58%)
sulfur
(45%)
and
less
than
1%
trace
elements
the
most
common
rock
constituents
of
the
crust
are
nearly
all
oxides:
chlorine
sulfur
and
fluorine
are
the
important
exceptions
to
this
and
their
total
amount
in
any
rock
is
usually
much
less
than
1%
over
99%
of
the
crust
is
composed
of
11
oxides
principally
silica
alumina
iron
oxides
lime
magnesia
potash
and
soda
earth's
interior
like
that
of
the
other
terrestrial
planets
is
divided
into
layers
by
their
chemical
or
physical
(rheological)
properties
the
outer
layer
is
a
chemically
distinct
silicate
solid
crust
which
is
underlain
by
a
highly
viscous
solid
mantle
the
crust
is
separated
from
the
mantle
by
the
mohorovičić
discontinuity
the
thickness
of
the
crust
varies
from
about
under
the
oceans
to
for
the
continents
the
crust
and
the
cold
rigid
top
of
the
upper
mantle
are
collectively
known
as
the
lithosphere
and
it
is
of
the
lithosphere
that
the
tectonic
plates
are
composed
beneath
the
lithosphere
is
the
asthenosphere
a
relatively
low-viscosity
layer
on
which
the
lithosphere
rides
important
changes
in
crystal
structure
within
the
mantle
occur
at
below
the
surface
spanning
a
transition
zone
that
separates
the
upper
and
lower
mantle
beneath
the
mantle
an
extremely
low
viscosity
liquid
outer
core
lies
above
a
solid
inner
core
the
earth's
inner
core
might
rotate
at
a
slightly
higher
angular
velocity
than
the
remainder
of
the
planet
advancing
by
01–05°
per
year
the
radius
of
the
inner
core
is
about
one
fifth
of
that
of
earth
earth's
internal
heat
comes
from
a
combination
of
residual
heat
from
planetary
accretion
(about
20%)
and
heat
produced
through
radioactive
decay
(80%)
the
major
heat-producing
isotopes
within
earth
are
potassium-40
uranium-238
and
thorium-232
at
the
center
the
temperature
may
be
up
to
and
the
pressure
could
reach
because
much
of
the
heat
is
provided
by
radioactive
decay
scientists
postulate
that
early
in
earth's
history
before
isotopes
with
short
half-lives
were
depleted
earth's
heat
production
was
much
higher
at
approximately
twice
the
present-day
heat
would
have
been
produced
increasing
the
rates
of
mantle
convection
and
plate
tectonics
and
allowing
the
production
of
uncommon
igneous
rocks
such
as
komatiites
that
are
rarely
formed
today
the
mean
heat
loss
from
earth
is
for
a
global
heat
loss
of
a
portion
of
the
core's
thermal
energy
is
transported
toward
the
crust
by
mantle
plumes
a
form
of
convection
consisting
of
upwellings
of
higher-temperature
rock
these
plumes
can
produce
hotspots
and
flood
basalts
more
of
the
heat
in
earth
is
lost
through
plate
tectonics
by
mantle
upwelling
associated
with
mid-ocean
ridges
the
final
major
mode
of
heat
loss
is
through
conduction
through
the
lithosphere
the
majority
of
which
occurs
under
the
oceans
because
the
crust
there
is
much
thinner
than
that
of
the
continents
earth's
mechanically
rigid
outer
layer
the
lithosphere
is
divided
into
tectonic
plates
these
plates
are
rigid
segments
that
move
relative
to
each
other
at
one
of
three
boundaries
types:
at
convergent
boundaries
two
plates
come
together;
at
divergent
boundaries
two
plates
are
pulled
apart;
and
at
transform
boundaries
two
plates
slide
past
one
another
laterally
along
these
plate
boundaries
earthquakes
volcanic
activity
mountain-building
and
oceanic
trench
formation
can
occur
the
tectonic
plates
ride
on
top
of
the
asthenosphere
the
solid
but
less-viscous
part
of
the
upper
mantle
that
can
flow
and
move
along
with
the
plates
as
the
tectonic
plates
migrate
oceanic
crust
is
subducted
under
the
leading
edges
of
the
plates
at
convergent
boundaries
at
the
same
time
the
upwelling
of
mantle
material
at
divergent
boundaries
creates
mid-ocean
ridges
the
combination
of
these
processes
recycles
the
oceanic
crust
back
into
the
mantle
due
to
this
recycling
most
of
the
ocean
floor
is
less
than
old
the
oldest
oceanic
crust
is
located
in
the
western
pacific
and
is
estimated
to
be
old
by
comparison
the
oldest
dated
continental
crust
is
the
seven
major
plates
are
the
pacific
north
american
eurasian
african
antarctic
indo-australian
and
south
american
other
notable
plates
include
the
arabian
plate
the
caribbean
plate
the
nazca
plate
off
the
west
coast
of
south
america
and
the
scotia
plate
in
the
southern
atlantic
ocean
the
australian
plate
fused
with
the
indian
plate
between
the
fastest-moving
plates
are
the
oceanic
plates
with
the
cocos
plate
advancing
at
a
rate
of
and
the
pacific
plate
moving
at
the
other
extreme
the
slowest-moving
plate
is
the
eurasian
plate
progressing
at
a
typical
rate
of
the
total
surface
area
of
earth
is
about
of
this
708%
or
is
below
sea
level
and
covered
by
ocean
water
below
the
ocean's
surface
are
much
of
the
continental
shelf
mountains
volcanoes
oceanic
trenches
submarine
canyons
oceanic
plateaus
abyssal
plains
and
a
globe-spanning
mid-ocean
ridge
system
the
remaining
292%
or
not
covered
by
water
has
terrain
that
varies
greatly
from
place
to
place
and
consists
of
mountains
deserts
plains
plateaus
and
other
landforms
tectonics
and
erosion
volcanic
eruptions
flooding
weathering
glaciation
the
growth
of
coral
reefs
and
meteorite
impacts
are
among
the
processes
that
constantly
reshape
the
earth's
surface
over
geological
time
the
continental
crust
consists
of
lower
density
material
such
as
the
igneous
rocks
granite
and
andesite
less
common
is
basalt
a
denser
volcanic
rock
that
is
the
primary
constituent
of
the
ocean
floors
sedimentary
rock
is
formed
from
the
accumulation
of
sediment
that
becomes
buried
and
compacted
together
nearly
75%
of
the
continental
surfaces
are
covered
by
sedimentary
rocks
although
they
form
about
5%
of
the
crust
the
third
form
of
rock
material
found
on
earth
is
metamorphic
rock
which
is
created
from
the
transformation
of
pre-existing
rock
types
through
high
pressures
high
temperatures
or
both
the
most
abundant
silicate
minerals
on
earth's
surface
include
quartz
feldspars
amphibole
mica
pyroxene
and
olivine
common
carbonate
minerals
include
calcite
(found
in
limestone)
and
dolomite
the
elevation
of
the
land
surface
varies
from
the
low
point
of
at
the
dead
sea
to
a
maximum
altitude
of
at
the
top
of
mount
everest
the
mean
height
of
land
above
sea
level
is
about
the
pedosphere
is
the
outermost
layer
of
earth's
continental
surface
and
is
composed
of
soil
and
subject
to
soil
formation
processes
the
total
arable
land
is
109%
of
the
land
surface
with
13%
being
permanent
cropland
close
to
40%
of
earth's
land
surface
is
used
for
agriculture
or
an
estimated
of
cropland
and
of
pastureland
the
abundance
of
water
on
earth's
surface
is
a
unique
feature
that
distinguishes
the
"blue
planet"
from
other
planets
in
the
solar
system
earth's
hydrosphere
consists
chiefly
of
the
oceans
but
technically
includes
all
water
surfaces
in
the
world
including
inland
seas
lakes
rivers
and
underground
waters
down
to
a
depth
of
the
deepest
underwater
location
is
challenger
deep
of
the
mariana
trench
in
the
pacific
ocean
with
a
depth
of
the
mass
of
the
oceans
is
approximately
135 metric
tons
or
about
1/4400
of
earth's
total
mass
the
oceans
cover
an
area
of
with
a
mean
depth
of
resulting
in
an
estimated
volume
of
if
all
of
earth's
crustal
surface
were
at
the
same
elevation
as
a
smooth
sphere
the
depth
of
the
resulting
world
ocean
would
be
about
975%
of
the
water
is
saline;
the
remaining
25%
is
fresh
water
most
fresh
water
about
687%
is
present
as
ice
in
ice
caps
and
glaciers
the
average
salinity
of
earth's
oceans
is
about
35 grams
of
salt
per
kilogram
of
sea
water
(35%
salt)
most
of
this
salt
was
released
from
volcanic
activity
or
extracted
from
cool
igneous
rocks
the
oceans
are
also
a
reservoir
of
dissolved
atmospheric
gases
which
are
essential
for
the
survival
of
many
aquatic
life
forms
sea
water
has
an
important
influence
on
the
world's
climate
with
the
oceans
acting
as
a
large
heat
reservoir
shifts
in
the
oceanic
temperature
distribution
can
cause
significant
weather
shifts
such
as
the
el
niño–southern
oscillation
the
atmospheric
pressure
at
earth's
sea
level
averages
with
a
scale
height
of
about
a
dry
atmosphere
is
composed
of
78084%
nitrogen
20946%
oxygen
0934%
argon
and
trace
amounts
of
carbon
dioxide
and
other
gaseous
molecules
water
vapor
content
varies
between
001%
and
4%
but
averages
about
1%
the
height
of
the
troposphere
varies
with
latitude
ranging
between
at
the
poles
to
at
the
equator
with
some
variation
resulting
from
weather
and
seasonal
factors
earth's
biosphere
has
significantly
altered
its
atmosphere
oxygenic
photosynthesis
evolved
forming
the
primarily
nitrogen–oxygen
atmosphere
of
today
this
change
enabled
the
proliferation
of
aerobic
organisms
and
indirectly
the
formation
of
the
ozone
layer
due
to
the
subsequent
conversion
of
atmospheric
into
the
ozone
layer
blocks
ultraviolet
solar
radiation
permitting
life
on
land
other
atmospheric
functions
important
to
life
include
transporting
water
vapor
providing
useful
gases
causing
small
meteors
to
burn
up
before
they
strike
the
surface
and
moderating
temperature
this
last
phenomenon
is
known
as
the
greenhouse
effect:
trace
molecules
within
the
atmosphere
serve
to
capture
thermal
energy
emitted
from
the
ground
thereby
raising
the
average
temperature
water
vapor
carbon
dioxide
methane
nitrous
oxide
and
ozone
are
the
primary
greenhouse
gases
in
the
atmosphere
without
this
heat-retention
effect
the
average
surface
temperature
would
be
in
contrast
to
the
current
and
life
on
earth
probably
would
not
exist
in
its
current
form
in
may
2017
glints
of
light
seen
as
twinkling
from
an
orbiting
satellite
a
million
miles
away
were
found
to
be
reflected
light
from
ice
crystals
in
the
atmosphere
earth's
atmosphere
has
no
definite
boundary
slowly
becoming
thinner
and
fading
into
outer
space
three-quarters
of
the
atmosphere's
mass
is
contained
within
the
first
of
the
surface
this
lowest
layer
is
called
the
troposphere
energy
from
the
sun
heats
this
layer
and
the
surface
below
causing
expansion
of
the
air
this
lower-density
air
then
rises
and
is
replaced
by
cooler
higher-density
air
the
result
is
atmospheric
circulation
that
drives
the
weather
and
climate
through
redistribution
of
thermal
energy
the
primary
atmospheric
circulation
bands
consist
of
the
trade
winds
in
the
equatorial
region
below
30°
latitude
and
the
westerlies
in
the
mid-latitudes
between
30°
and
60°
ocean
currents
are
also
important
factors
in
determining
climate
particularly
the
thermohaline
circulation
that
distributes
thermal
energy
from
the
equatorial
oceans
to
the
polar
regions
water
vapor
generated
through
surface
evaporation
is
transported
by
circulatory
patterns
in
the
atmosphere
when
atmospheric
conditions
permit
an
uplift
of
warm
humid
air
this
water
condenses
and
falls
to
the
surface
as
precipitation
most
of
the
water
is
then
transported
to
lower
elevations
by
river
systems
and
usually
returned
to
the
oceans
or
deposited
into
lakes
this
water
cycle
is
a
vital
mechanism
for
supporting
life
on
land
and
is
a
primary
factor
in
the
erosion
of
surface
features
over
geological
periods
precipitation
patterns
vary
widely
ranging
from
several
meters
of
water
per
year
to
less
than
a
millimeter
atmospheric
circulation
topographic
features
and
temperature
differences
determine
the
average
precipitation
that
falls
in
each
region
the
amount
of
solar
energy
reaching
earth's
surface
decreases
with
increasing
latitude
at
higher
latitudes
the
sunlight
reaches
the
surface
at
lower
angles
and
it
must
pass
through
thicker
columns
of
the
atmosphere
as
a
result
the
mean
annual
air
temperature
at
sea
level
decreases
by
about
per
degree
of
latitude
from
the
equator
earth's
surface
can
be
subdivided
into
specific
latitudinal
belts
of
approximately
homogeneous
climate
ranging
from
the
equator
to
the
polar
regions
these
are
the
tropical
(or
equatorial)
subtropical
temperate
and
polar
climates
this
latitudinal
rule
has
several
anomalies:
the
commonly
used
köppen
climate
classification
system
has
five
broad
groups
(humid
tropics
arid
humid
middle
latitudes
continental
and
cold
polar)
which
are
further
divided
into
more
specific
subtypes
the
köppen
system
rates
regions
of
terrain
based
on
observed
temperature
and
precipitation
the
highest
air
temperature
ever
measured
on
earth
was
in
furnace
creek
california
in
death
valley
in
1913
the
lowest
air
temperature
ever
directly
measured
on
earth
was
at
vostok
station
in
1983
but
satellites
have
used
remote
sensing
to
measure
temperatures
as
low
as
in
east
antarctica
these
temperature
records
are
only
measurements
made
with
modern
instruments
from
the
20th
century
onwards
and
likely
do
not
reflect
the
full
range
of
temperature
on
earth
above
the
troposphere
the
atmosphere
is
usually
divided
into
the
stratosphere
mesosphere
and
thermosphere
each
layer
has
a
different
lapse
rate
defining
the
rate
of
change
in
temperature
with
height
beyond
these
the
exosphere
thins
out
into
the
magnetosphere
where
the
geomagnetic
fields
interact
with
the
solar
wind
within
the
stratosphere
is
the
ozone
layer
a
component
that
partially
shields
the
surface
from
ultraviolet
light
and
thus
is
important
for
life
on
earth
the
kármán
line
defined
as
100 km
above
earth's
surface
is
a
working
definition
for
the
boundary
between
the
atmosphere
and
outer
space
thermal
energy
causes
some
of
the
molecules
at
the
outer
edge
of
the
atmosphere
to
increase
their
velocity
to
the
point
where
they
can
escape
from
earth's
gravity
this
causes
a
slow
but
steady
loss
of
the
atmosphere
into
space
because
unfixed
hydrogen
has
a
low
molecular
mass
it
can
achieve
escape
velocity
more
readily
and
it
leaks
into
outer
space
at
a
greater
rate
than
other
gases
the
leakage
of
hydrogen
into
space
contributes
to
the
shifting
of
earth's
atmosphere
and
surface
from
an
initially
reducing
state
to
its
current
oxidizing
one
photosynthesis
provided
a
source
of
free
oxygen
but
the
loss
of
reducing
agents
such
as
hydrogen
is
thought
to
have
been
a
necessary
precondition
for
the
widespread
accumulation
of
oxygen
in
the
atmosphere
hence
the
ability
of
hydrogen
to
escape
from
the
atmosphere
may
have
influenced
the
nature
of
life
that
developed
on
earth
in
the
current
oxygen-rich
atmosphere
most
hydrogen
is
converted
into
water
before
it
has
an
opportunity
to
escape
instead
most
of
the
hydrogen
loss
comes
from
the
destruction
of
methane
in
the
upper
atmosphere
the
gravity
of
earth
is
the
acceleration
that
is
imparted
to
objects
due
to
the
distribution
of
mass
within
the
earth
near
the
earth's
surface
gravitational
acceleration
is
approximately
local
differences
in
topography
geology
and
deeper
tectonic
structure
cause
local
and
broad
regional
differences
in
the
earth's
gravitational
field
known
as
gravity
anomalies
the
main
part
of
earth's
magnetic
field
is
generated
in
the
core
the
site
of
a
dynamo
process
that
converts
the
kinetic
energy
of
thermally
and
compositionally
driven
convection
into
electrical
and
magnetic
field
energy
the
field
extends
outwards
from
the
core
through
the
mantle
and
up
to
earth's
surface
where
it
is
approximately
a
dipole
the
poles
of
the
dipole
are
located
close
to
earth's
geographic
poles
at
the
equator
of
the
magnetic
field
the
magnetic-field
strength
at
the
surface
is
with
global
magnetic
dipole
moment
of
the
convection
movements
in
the
core
are
chaotic;
the
magnetic
poles
drift
and
periodically
change
alignment
this
causes
secular
variation
of
the
main
field
and
field
reversals
at
irregular
intervals
averaging
a
few
times
every
million
years
the
most
recent
reversal
occurred
approximately
700000
years
ago
the
extent
of
earth's
magnetic
field
in
space
defines
the
magnetosphere
ions
and
electrons
of
the
solar
wind
are
deflected
by
the
magnetosphere;
solar
wind
pressure
compresses
the
dayside
of
the
magnetosphere
to
about
10
earth
radii
and
extends
the
nightside
magnetosphere
into
a
long
tail
because
the
velocity
of
the
solar
wind
is
greater
than
the
speed
at
which
waves
propagate
through
the
solar
wind
a
supersonic
bowshock
precedes
the
dayside
magnetosphere
within
the
solar
wind
charged
particles
are
contained
within
the
magnetosphere;
the
plasmasphere
is
defined
by
low-energy
particles
that
essentially
follow
magnetic
field
lines
as
earth
rotates;
the
ring
current
is
defined
by
medium-energy
particles
that
drift
relative
to
the
geomagnetic
field
but
with
paths
that
are
still
dominated
by
the
magnetic
field
and
the
van
allen
radiation
belt
are
formed
by
high-energy
particles
whose
motion
is
essentially
random
but
otherwise
contained
by
the
magnetosphere
during
magnetic
storms
and
substorms
charged
particles
can
be
deflected
from
the
outer
magnetosphere
and
especially
the
magnetotail
directed
along
field
lines
into
earth's
ionosphere
where
atmospheric
atoms
can
be
excited
and
ionized
causing
the
aurora
earth's
rotation
period
relative
to
the
sun—its
mean
solar
day—is
of
mean
solar
time
()
because
earth's
solar
day
is
now
slightly
longer
than
it
was
during
the
19th
century
due
to
tidal
deceleration
each
day
varies
between
longer
earth's
rotation
period
relative
to
the
fixed
stars
called
its
"stellar
day"
by
the
international
earth
rotation
and
reference
systems
service
(iers)
is
of
mean
solar
time
(ut1)
or
earth's
rotation
period
relative
to
the
precessing
or
moving
mean
vernal
equinox
misnamed
its
"sidereal
day"
is
of
mean
solar
time
(ut1)
thus
the
sidereal
day
is
shorter
than
the
stellar
day
by
about
84 ms
the
length
of
the
mean
solar
day
in
si
seconds
is
available
from
the
iers
for
the
periods
1623–2005
and
1962–2005
apart
from
meteors
within
the
atmosphere
and
low-orbiting
satellites
the
main
apparent
motion
of
celestial
bodies
in
earth's
sky
is
to
the
west
at
a
rate
of
15°/h
=
15'/min
for
bodies
near
the
celestial
equator
this
is
equivalent
to
an
apparent
diameter
of
the
sun
or
the
moon
every
two
minutes;
from
earth's
surface
the
apparent
sizes
of
the
sun
and
the
moon
are
approximately
the
same
earth
orbits
the
sun
at
an
average
distance
of
about
every
3652564
mean
solar
days
or
one
sidereal
year
this
gives
an
apparent
movement
of
the
sun
eastward
with
respect
to
the
stars
at
a
rate
of
about
1°/day
which
is
one
apparent
sun
or
moon
diameter
every
12 hours
due
to
this
motion
on
average
it
takes
24 hours—a
solar
day—for
earth
to
complete
a
full
rotation
about
its
axis
so
that
the
sun
returns
to
the
meridian
the
orbital
speed
of
earth
averages
about
which
is
fast
enough
to
travel
a
distance
equal
to
earth's
diameter
about
in
seven
minutes
and
the
distance
to
the
moon
in
about
35
hours
the
moon
and
earth
orbit
a
common
barycenter
every
2732 days
relative
to
the
background
stars
when
combined
with
the
earth–moon
system's
common
orbit
around
the
sun
the
period
of
the
synodic
month
from
new
moon
to
new
moon
is
2953 days
viewed
from
the
celestial
north
pole
the
motion
of
earth
the
moon
and
their
axial
rotations
are
all
counterclockwise
viewed
from
a
vantage
point
above
the
north
poles
of
both
the
sun
and
earth
earth
orbits
in
a
counterclockwise
direction
about
the
sun
the
orbital
and
axial
planes
are
not
precisely
aligned:
earth's
axis
is
tilted
some
2344 degrees
from
the
perpendicular
to
the
earth–sun
plane
(the
ecliptic)
and
the
earth–moon
plane
is
tilted
up
to
±51 degrees
against
the
earth–sun
plane
without
this
tilt
there
would
be
an
eclipse
every
two
weeks
alternating
between
lunar
eclipses
and
solar
eclipses
the
hill
sphere
or
the
sphere
of
gravitational
influence
of
the
earth
is
about
in
radius
this
is
the
maximum
distance
at
which
the
earth's
gravitational
influence
is
stronger
than
the
more
distant
sun
and
planets
objects
must
orbit
the
earth
within
this
radius
or
they
can
become
unbound
by
the
gravitational
perturbation
of
the
sun
earth
along
with
the
solar
system
is
situated
in
the
milky
way
and
orbits
about
28000 light-years
from
its
center
it
is
about
20 light-years
above
the
galactic
plane
in
the
orion
arm
the
axial
tilt
of
the
earth
is
approximately
23439281°
with
the
axis
of
its
orbit
plane
always
pointing
towards
the
celestial
poles
due
to
earth's
axial
tilt
the
amount
of
sunlight
reaching
any
given
point
on
the
surface
varies
over
the
course
of
the
year
this
causes
the
seasonal
change
in
climate
with
summer
in
the
northern
hemisphere
occurring
when
the
tropic
of
cancer
is
facing
the
sun
and
winter
taking
place
when
the
tropic
of
capricorn
in
the
southern
hemisphere
faces
the
sun
during
the
summer
the
day
lasts
longer
and
the
sun
climbs
higher
in
the
sky
in
winter
the
climate
becomes
cooler
and
the
days
shorter
in
northern
temperate
latitudes
the
sun
rises
north
of
true
east
during
the
summer
solstice
and
sets
north
of
true
west
reversing
in
the
winter
the
sun
rises
south
of
true
east
in
the
summer
for
the
southern
temperate
zone
and
sets
south
of
true
west
above
the
arctic
circle
an
extreme
case
is
reached
where
there
is
no
daylight
at
all
for
part
of
the
year
up
to
six
months
at
the
north
pole
itself
a
polar
night
in
the
southern
hemisphere
the
situation
is
exactly
reversed
with
the
south
pole
oriented
opposite
the
direction
of
the
north
pole
six
months
later
this
pole
will
experience
a
midnight
sun
a
day
of
24
hours
again
reversing
with
the
south
pole
by
astronomical
convention
the
four
seasons
can
be
determined
by
the
solstices—the
points
in
the
orbit
of
maximum
axial
tilt
toward
or
away
from
the
sun—and
the
equinoxes
when
the
direction
of
the
tilt
and
the
direction
to
the
sun
are
perpendicular
in
the
northern
hemisphere
winter
solstice
currently
occurs
around
21
december;
summer
solstice
is
near
21
june
spring
equinox
is
around
20
march
and
autumnal
equinox
is
about
22
or
23
september
in
the
southern
hemisphere
the
situation
is
reversed
with
the
summer
and
winter
solstices
exchanged
and
the
spring
and
autumnal
equinox
dates
swapped
the
angle
of
earth's
axial
tilt
is
relatively
stable
over
long
periods
of
time
its
axial
tilt
does
undergo
nutation;
a
slight
irregular
motion
with
a
main
period
of
186 years
the
orientation
(rather
than
the
angle)
of
earth's
axis
also
changes
over
time
precessing
around
in
a
complete
circle
over
each
25800 year
cycle;
this
precession
is
the
reason
for
the
difference
between
a
sidereal
year
and
a
tropical
year
both
of
these
motions
are
caused
by
the
varying
attraction
of
the
sun
and
the
moon
on
earth's
equatorial
bulge
the
poles
also
migrate
a
few
meters
across
earth's
surface
this
polar
motion
has
multiple
cyclical
components
which
collectively
are
termed
quasiperiodic
motion
in
addition
to
an
annual
component
to
this
motion
there
is
a
14-month
cycle
called
the
chandler
wobble
earth's
rotational
velocity
also
varies
in
a
phenomenon
known
as
length-of-day
variation
in
modern
times
earth's
perihelion
occurs
around
3
january
and
its
aphelion
around
4
july
these
dates
change
over
time
due
to
precession
and
other
orbital
factors
which
follow
cyclical
patterns
known
as
milankovitch
cycles
the
changing
earth–sun
distance
causes
an
increase
of
about
69%
in
solar
energy
reaching
earth
at
perihelion
relative
to
aphelion
because
the
southern
hemisphere
is
tilted
toward
the
sun
at
about
the
same
time
that
earth
reaches
the
closest
approach
to
the
sun
the
southern
hemisphere
receives
slightly
more
energy
from
the
sun
than
does
the
northern
over
the
course
of
a
year
this
effect
is
much
less
significant
than
the
total
energy
change
due
to
the
axial
tilt
and
most
of
the
excess
energy
is
absorbed
by
the
higher
proportion
of
water
in
the
southern
hemisphere
a
study
from
2016
suggested
that
planet
nine
tilted
all
solar
system
planets
including
earth's
by
about
six
degrees
a
planet
that
can
sustain
life
is
termed
habitable
even
if
life
did
not
originate
there
earth
provides
liquid
water—an
environment
where
complex
organic
molecules
can
assemble
and
interact
and
sufficient
energy
to
sustain
metabolism
the
distance
of
earth
from
the
sun
as
well
as
its
orbital
eccentricity
rate
of
rotation
axial
tilt
geological
history
sustaining
atmosphere
and
magnetic
field
all
contribute
to
the
current
climatic
conditions
at
the
surface
a
planet's
life
forms
inhabit
ecosystems
whose
total
is
sometimes
said
to
form
a
"biosphere"
earth's
biosphere
is
thought
to
have
begun
evolving
about
the
biosphere
is
divided
into
a
number
of
biomes
inhabited
by
broadly
similar
plants
and
animals
on
land
biomes
are
separated
primarily
by
differences
in
latitude
height
above
sea
level
and
humidity
terrestrial
biomes
lying
within
the
arctic
or
antarctic
circles
at
high
altitudes
or
in
extremely
arid
areas
are
relatively
barren
of
plant
and
animal
life;
species
diversity
reaches
a
peak
in
humid
lowlands
at
equatorial
latitudes
in
july
2016
scientists
reported
identifying
a
set
of
355
genes
from
the
last
universal
common
ancestor
(luca)
of
all
organisms
living
on
earth
earth
has
resources
that
have
been
exploited
by
humans
those
termed
non-renewable
resources
such
as
fossil
fuels
only
renew
over
geological
timescales
large
deposits
of
fossil
fuels
are
obtained
from
earth's
crust
consisting
of
coal
petroleum
and
natural
gas
these
deposits
are
used
by
humans
both
for
energy
production
and
as
feedstock
for
chemical
production
mineral
ore
bodies
have
also
been
formed
within
the
crust
through
a
process
of
ore
genesis
resulting
from
actions
of
magmatism
erosion
and
plate
tectonics
these
bodies
form
concentrated
sources
for
many
metals
and
other
useful
elements
earth's
biosphere
produces
many
useful
biological
products
for
humans
including
food
wood
pharmaceuticals
oxygen
and
the
recycling
of
many
organic
wastes
the
land-based
ecosystem
depends
upon
topsoil
and
fresh
water
and
the
oceanic
ecosystem
depends
upon
dissolved
nutrients
washed
down
from
the
land
in
1980
of
earth's
land
surface
consisted
of
forest
and
woodlands
was
grasslands
and
pasture
and
was
cultivated
as
croplands
the
estimated
amount
of
irrigated
land
in
1993
was
humans
also
live
on
the
land
by
using
building
materials
to
construct
shelters
large
areas
of
earth's
surface
are
subject
to
extreme
weather
such
as
tropical
cyclones
hurricanes
or
typhoons
that
dominate
life
in
those
areas
from
1980
to
2000
these
events
caused
an
average
of
11800
human
deaths
per
year
many
places
are
subject
to
earthquakes
landslides
tsunamis
volcanic
eruptions
tornadoes
sinkholes
blizzards
floods
droughts
wildfires
and
other
calamities
and
disasters
many
localized
areas
are
subject
to
human-made
pollution
of
the
air
and
water
acid
rain
and
toxic
substances
loss
of
vegetation
(overgrazing
deforestation
desertification)
loss
of
wildlife
species
extinction
soil
degradation
soil
depletion
and
erosion
there
is
a
scientific
consensus
linking
human
activities
to
global
warming
due
to
industrial
carbon
dioxide
emissions
this
is
predicted
to
produce
changes
such
as
the
melting
of
glaciers
and
ice
sheets
more
extreme
temperature
ranges
significant
changes
in
weather
and
a
global
rise
in
average
sea
levels
cartography
the
study
and
practice
of
map-making
and
geography
the
study
of
the
lands
features
inhabitants
and
phenomena
on
earth
have
historically
been
the
disciplines
devoted
to
depicting
earth
surveying
the
determination
of
locations
and
distances
and
to
a
lesser
extent
navigation
the
determination
of
position
and
direction
have
developed
alongside
cartography
and
geography
providing
and
suitably
quantifying
the
requisite
information
earth's
human
population
reached
approximately
seven
billion
on
31
october
2011
projections
indicate
that
the
world's
human
population
will
reach
92 billion
in
2050
most
of
the
growth
is
expected
to
take
place
in
developing
nations
human
population
density
varies
widely
around
the
world
but
a
majority
live
in
asia
by
2020
60%
of
the
world's
population
is
expected
to
be
living
in
urban
rather
than
rural
areas
68%
of
the
land
mass
of
the
world
is
in
the
northern
hemisphere
partly
due
to
the
predominance
of
land
mass
90%
of
humans
live
in
the
northern
hemisphere
it
is
estimated
that
one-eighth
of
earth's
surface
is
suitable
for
humans
to
live
on –
three-quarters
of
earth's
surface
is
covered
by
oceans
leaving
one-quarter
as
land
half
of
that
land
area
is
desert
(14%)
high
mountains
(27%)
or
other
unsuitable
terrains
the
northernmost
permanent
settlement
in
the
world
is
alert
on
ellesmere
island
in
nunavut
canada
(82°28′n)
the
southernmost
is
the
amundsen–scott
south
pole
station
in
antarctica
almost
exactly
at
the
south
pole
(90°s)
independent
sovereign
nations
claim
the
planet's
entire
land
surface
except
for
some
parts
of
antarctica
a
few
land
parcels
along
the
danube
river's
western
bank
and
the
unclaimed
area
of
bir
tawil
between
egypt
and
sudan
there
are
193
sovereign
states
that
are
member
states
of
the
united
nations
plus
two
observer
states
and
72
dependent
territories
and
states
with
limited
recognition
earth
has
never
had
a
sovereign
government
with
authority
over
the
entire
globe
although
some
nation-states
have
striven
for
world
domination
and
failed
the
united
nations
is
a
worldwide
intergovernmental
organization
that
was
created
with
the
goal
of
intervening
in
the
disputes
between
nations
thereby
avoiding
armed
conflict
the
un
serves
primarily
as
a
forum
for
international
diplomacy
and
international
law
when
the
consensus
of
the
membership
permits
it
provides
a
mechanism
for
armed
intervention
the
first
human
to
orbit
earth
was
yuri
gagarin
on
12
april
1961
in
total
about
487
people
have
visited
outer
space
and
reached
orbit
and
of
these
twelve
have
walked
on
the
moon
normally
the
only
humans
in
space
are
those
on
the
international
space
station
the
station's
crew
made
up
of
six
people
is
usually
replaced
every
six
months
the
farthest
that
humans
have
traveled
from
earth
is
achieved
during
the
apollo
13
mission
in
1970
the
moon
is
a
relatively
large
terrestrial
planet-like
natural
satellite
with
a
diameter
about
one-quarter
of
earth's
it
is
the
largest
moon
in
the
solar
system
relative
to
the
size
of
its
planet
although
charon
is
larger
relative
to
the
dwarf
planet
pluto
the
natural
satellites
of
other
planets
are
also
referred
to
as
"moons"
after
earth's
the
gravitational
attraction
between
earth
and
the
moon
causes
tides
on
earth
the
same
effect
on
the
moon
has
led
to
its
tidal
locking:
its
rotation
period
is
the
same
as
the
time
it
takes
to
orbit
earth
as
a
result
it
always
presents
the
same
face
to
the
planet
as
the
moon
orbits
earth
different
parts
of
its
face
are
illuminated
by
the
sun
leading
to
the
lunar
phases;
the
dark
part
of
the
face
is
separated
from
the
light
part
by
the
solar
terminator
due
to
their
tidal
interaction
the
moon
recedes
from
earth
at
the
rate
of
approximately
over
millions
of
years
these
tiny
modifications—and
the
lengthening
of
earth's
day
by
about
23 µs/yr—add
up
to
significant
changes
during
the
devonian
period
for
example
(approximately
)
there
were
400
days
in
a
year
with
each
day
lasting
218
hours
the
moon
may
have
dramatically
affected
the
development
of
life
by
moderating
the
planet's
climate
paleontological
evidence
and
computer
simulations
show
that
earth's
axial
tilt
is
stabilized
by
tidal
interactions
with
the
moon
some
theorists
think
that
without
this
stabilization
against
the
torques
applied
by
the
sun
and
planets
to
earth's
equatorial
bulge
the
rotational
axis
might
be
chaotically
unstable
exhibiting
chaotic
changes
over
millions
of
years
as
appears
to
be
the
case
for
mars
viewed
from
earth
the
moon
is
just
far
enough
away
to
have
almost
the
same
apparent-sized
disk
as
the
sun
the
angular
size
(or
solid
angle)
of
these
two
bodies
match
because
although
the
sun's
diameter
is
about
400
times
as
large
as
the
moon's
it
is
also
400
times
more
distant
this
allows
total
and
annular
solar
eclipses
to
occur
on
earth
the
most
widely
accepted
theory
of
the
moon's
origin
the
giant-impact
hypothesis
states
that
it
formed
from
the
collision
of
a
mars-size
protoplanet
called
theia
with
the
early
earth
this
hypothesis
explains
(among
other
things)
the
moon's
relative
lack
of
iron
and
volatile
elements
and
the
fact
that
its
composition
is
nearly
identical
to
that
of
earth's
crust
earth
has
at
least
five
co-orbital
asteroids
including
3753
cruithne
and
a
trojan
asteroid
companion
is
librating
around
the
leading
lagrange
triangular
point
l4
in
the
earth's
orbit
around
the
sun
the
tiny
near-earth
asteroid
makes
close
approaches
to
the
earth–moon
system
roughly
every
twenty
years
during
these
approaches
it
can
orbit
earth
for
brief
periods
of
time
there
are
1886
operational
human-made
satellites
orbiting
earth
there
are
also
inoperative
satellites
including
vanguard
1
the
oldest
satellite
currently
in
orbit
and
over
16000
pieces
of
tracked
space
debris
earth's
largest
artificial
satellite
is
the
international
space
station
the
standard
astronomical
symbol
of
earth
consists
of
a
cross
circumscribed
by
a
circle
representing
the
four
corners
of
the
world
human
cultures
have
developed
many
views
of
the
planet
earth
is
sometimes
personified
as
a
deity
in
many
cultures
it
is
a
mother
goddess
that
is
also
the
primary
fertility
deity
and
by
the
mid-20th
century
the
gaia
principle
compared
earth's
environments
and
life
as
a
single
self-regulating
organism
leading
to
broad
stabilization
of
the
conditions
of
habitability
creation
myths
in
many
religions
involve
the
creation
of
earth
by
a
supernatural
deity
or
deities
scientific
investigation
has
resulted
in
several
culturally
transformative
shifts
in
people's
view
of
the
planet
initial
belief
in
a
flat
earth
was
gradually
displaced
in
the
greek
colonies
of
southern
italy
during
the
late
6th
century
bc
by
the
idea
of
spherical
earth
which
was
attributed
to
both
the
philosophers
pythagoras
and
parmenides
by
the
end
of
the
5th
century
bc
the
sphericity
of
earth
was
universally
accepted
among
greek
intellectuals
earth
was
generally
believed
to
be
the
center
of
the
universe
until
the
16th
century
when
scientists
first
conclusively
demonstrated
that
it
was
a
moving
object
comparable
to
the
other
planets
in
the
solar
system
due
to
the
efforts
of
influential
christian
scholars
and
clerics
such
as
james
ussher
who
sought
to
determine
the
age
of
earth
through
analysis
of
genealogies
in
scripture
westerners
before
the
19th
century
generally
believed
earth
to
be
a
few
thousand
years
old
at
most
it
was
only
during
the
19th
century
that
geologists
realized
earth's
age
was
at
least
many
millions
of
years
lord
kelvin
used
thermodynamics
to
estimate
the
age
of
earth
to
be
between
20
million
and
400 million
years
in
1864
sparking
a
vigorous
debate
on
the
subject;
it
was
only
when
radioactivity
and
radioactive
dating
were
discovered
in
the
late
19th
and
early
20th
centuries
that
a
reliable
mechanism
for
determining
earth's
age
was
established
proving
the
planet
to
be
billions
of
years
old
the
perception
of
earth
shifted
again
in
the
20th
century
when
humans
first
viewed
it
from
orbit
and
especially
with
photographs
of
earth
returned
by
the
apollo
program
selvaggio
blu
(sardinia)
the
selvaggio
blu
(wild
blue)
is
a
trekking
route
in
the
territory
of
the
district
of
baunei
(sardinia)
it
was
conceived
in
1987
by
mario
verin
(photographer
and
alpinist)
and
peppino
cicalò
(architect)
president
of
the
nuoro
section
of
the
italian
alpine
club
the
itinerary
extends
for
over
40
kilometers
(approximately
25
miles)
from
the
touristic
port
of
santa
maria
navarrese
(baunei)
to
the
beach
of
cala
sisine
(baunei)
it
takes
on
average
4
days
to
complete
the
selvaggio
blu
is
considered
one
of
the
last
wild
trekking
routes
of
the
mediterranean
because
for
the
major
part
of
the
itinerary
it
can
only
be
accessed
by
boat
or
by
following
the
path
along
the
coast
of
the
gulf
of
orosei
verin
and
cicalò
used
the
name
'selvaggio
blu'
to
reflect
the
main
characteristics
of
the
journey:
"selvaggio"
to
reflect
the
wildness
and
pureness
of
the
experience
and
"blu"
because
the
trek
goes
along
the
coast
where
the
color
of
the
sea
and
the
sky
is
predominant
the
selvaggio
blu
is
located
entirely
in
the
territory
of
the
district
of
baunei
which
extends
for
2119
km2
on
the
east
coast
of
sardinia
in
the
province
of
ogliastra
the
baunei
area
is
considered
one
of
the
wildest
in
sardinia
going
from
the
coastal
town
of
santa
maria
navarrese
and
traversing
limestone
plateaux
and
coastal
scenery
to
the
beach
of
cala
luna
it
contains
all
the
main
centers
on
the
route
including:
santa
maria
navarrese
pedra
longa
portu
pedrosu
cala
goloritzè
su
feilau
cala
sisine
and
also
all
the
centers
included
in
selvaggio
blu
variations:
cala
mariolu
cala
biriala;
s'istrada
longa
grotta
del
fico
and
the
plateau
of
golgo
selvaggio
blu
has
a
strategic
location
to
see
the
geological
history
of
sardinia
as
it
is
located
40 km
along
the
coast
there
are
several
important
geology
observations
on
selvaggio
blu's
hiking
route
which
can
be
reached
both
by
land
and
sea
in
the
northern
part
of
santa
maria
navarrese
hikers
have
the
possibility
to
move
along
a
section
with
fractured
granite
from
the
palazoic
age
where
rocks
are
several
meters
thick
this
area
contains
some
of
sardinia's
oldest
rocks
covered
with
layers
of
cambrian-ordovician
metasandstones
phyllites
and
quartzites
in
pedra
longa
the
section
with
granite
ends
and
there
is
a
transition
to
the
limestones
which
are
a
common
feature
of
the
gulf
of
orosei
coast
because
of
climate
changes
the
limestone
has
been
affected
by
rising
and
falling
of
sea
level
changes
caused
by
the
mixing
of
fresh
water
from
the
limestone
and
salt
sea
water
include
larger
limestones
cavities
and
fluctuations
in
different
colours
grotta
del
fico
is
a
karst
cave
located
on
the
selvaggio
blu
between
santa
maria
navarrese
and
cala
luna
grotta
del
fico
is
accessible
by
boat
or
walking
it
was
discovered
by
fishermen
in
the
early
20th
century and
opened
to
the
public
with
guided
tours
in
2003
inside
grotta
del
fico
there
is
a
lake
with
clear
water
which
reflects
inside
the
cave
the
lake
is
created
by
karst
water
that
flows
in
the
main
part
of
the
cave
and
fills
up
the
lake
selvaggio
blu
is
a
40
kilometers
trek
that
has
an
estimated
travel
time
of
4
days
but
many
people
take
about
6
–
7
days
depending
on
experience
and
fitness
the
selvaggio
blu
starts
at
pedra
longa
(40°1'38"n
9°42'25"e)
and
goes
to
portu
pedrosu
(40°4'5"n
9°44'2"e)
it
has
an
estimated
travel
time
of
9
hours
to
cover
its
12 km
of
length
in
this
stage
it
is
reached
the
maximum
height
of
all
the
selvaggio
blu
which
is
770
m
this
section
gets
a
climbing
grade
of
eea
on
the
uaii
scale
which
makes
this
stage
the
third
hardest
in
technical
difficulty
of
the
entire
journey
the
path
crosses
a
small
valley
into
the
woods
until
it
reaches
a
point
where
it
is
possible
to
see
the
sea
the
path
continues
into
another
small
valley
with
holm
oaks
walking
on
very
sharp
limestone
flakes
the
path
then
crosses
two
gates
after
which
it
descends
toward
the
sea
following
the
cliffs
then
passing
on
the
top
of
the
grotta
dei
colombi
the
path
once
again
entersa
small
valley
descending
this
valley
on
the
left
there
is
a
section
equipped
with
juniper
trunks
that
allow
climbers
to
descend
into
bacu
tenadili
the
path
at
this
point
challenges
the
orientation
skills
of
the
climbers
because
of
the
lack
of
a
gps
signal
after
this
section
there
is
a
cracked
limestone
area
where
the
shepherds
are
used
to
collecting
water;
after
this
the
path
becomes
less
clear
following
a
steep
zigzag
the
path
leads
to
the
mooring
of
portu
pedrosu
were
the
first
stage
ends
walking
for
10
minutes
more
to
portu
cuau(40°5'11"n
9°43'55"e)
there
is
a
large
space
for
camping
the
itinerary
of
the
second
day
starts
at
portu
pedrosu
and
ends
in
cala
goloritzè
(40°6'29"n
9°41'23"e)
the
maximum
height
reached
during
this
day
is
about
495
m
this
stage
is
95 km
and
the
estimated
travel
time
is
of
6
hours
it
is
not
as
difficult
as
the
other
paths;
its
difficulty
has
been
ranked
as
ee
on
the
uaii
scale
which
makes
this
the
easiest
technical
section
of
the
selvaggio
blu
from
portu
pedrosu
the
path
is
well
defined
for
a
short
section
due
to
a
good
muletrack
surrounded
by
vegetation
passing
a
small
valley
the
path
climbs
up
to
a
rocky
plateau
where
the
track
becomes
less
well
defined
making
a
large
curve
towards
the
north
which
is
partially
covered
by
the
vegetation
the
path
follows
the
edge
of
a
small
valley
continuing
until
it
reaches
a
balcony
overlooking
the
sea
(2 km
from
portu
cuau)
keeping
the
sea
behind
the
path
proceeds
through
small
rocks
and
holm
oak
sections
until
it
reaches
the
ovile
of
kenos
trainos
one
of
the
most
important
along
the
selvaggio
blu
from
this
point
the
path
becomes
less
clear
because
of
thick
vegetation
and
a
small
rocky
step
reaching
su
runcu'e
su
press
it
is
possible
to
escape
from
the
selvaggio
blu
and
in
15
minutes
reach
a
clearing
which
is
accessible
by
a
suv
the
itinerary
of
the
third
day
is
74 km
long
with
a
maximum
height
of
485
m
it
is
the
most
difficult
stage
of
the
whole
route;
its
difficulty
has
been
ranked
iv+
of
the
uiaa
scale
some
people
decide
to
stop
during
this
stage
because
of
all
the
obstacles
on
the
path
to
bacu
su
feilau
this
section
requires
the
capability
to
travel
on
all
types
of
terrain
with
difficulties
including
the
exposure
difficult
vegetation
and
the
lack
of
gps
signal
during
this
stage
which
starts
from
cala
golorizè
and
ends
in
bacu
su
feilau
(40°3'59"n
9°34'52"e)
the
itinerary
climbs
two
rock
walls
the
first
one
of
20
meters
and
the
second
one
of
4
meters
these
represent
the
hardest
technical
climbing
difficulties
on
the
selvaggio
blu
this
section
includes
several
caves
and
woods
and
2
abseils
of
20
meters
each
bacu
su
feliau
which
is
a
big
hole
through
the
rocky
spur
that
overlooks
bacu
padente
was
originally
a
bivouac
shelter
on
selvaggio
blu
but
for
large
groups
it
is
recommended
to
descend
it
and
climb
up
towards
ololbizzi
a
charcoal
burners'
circle
in
the
upper
part
of
the
bacu
the
last
day
starts
from
bacu
su
feliau
and
ends
7 km
later
in
cala
sisine(40°10'45"n
9°38'1"e)
it
offers
a
large
variety
of
scenery
the
highest
altitude
reached
during
this
stage
is
480
m
the
estimated
travel
time
is
65
hours
and
the
technical
difficulty
is
ranked
iv
on
the
uiaa
scale
this
makes
this
stage
the
second
most
difficult
of
the
selvaggio
blu
at
the
start
of
this
stage
it
is
necessary
to
climb
a
juniper
trunk
with
giant
moss-covered
oaks
the
route
then
traverses
a
gully
and
is
then
marked
with
blue
waymarks
painted
on
rocks
these
mark
the
way
to
exit
the
woods
utilizing
ancient
mule
tracks
and
gives
a
panoramic
view
of
the
sea
before
descending
there
is
the
possibility
to
climb
a
cliff
named
"rottura
delle
altezze"
(which
means
"breaking
the
heights")
from
which
walkers
can
look
at
the
sea
200
m
below
one
of
the
paths
leads
to
the
ovule
piddi
meaning
mandragora
or
mandrake
in
sards;
this
poisonous
plant
grows
all
over
the
supramonte
helped
by
the
locals
verin
and
cicalò
built
a
network
of
mule
tracks
that
with
a
series
of
bends
pass
through
the
most
impervious
gullies
and
lead
to
the
sea
which
can
be
seen
only
in
cala
sisine;
here
there
are
docks
built
to
transport
charcoal
and
woods
from
cala
sisine
it
is
possible
to
return
to
the
starting
point
by
the
sea
with
an
inflatable
boat
or
by
land
with
a
suv
there
are
several
different
versions
of
the
selvaggio
blu:
there
are
other
variations
of
the
selvaggio
blu
as
daily
excursions:
the
guides
that
conceived
the
trek
are:
the
official
guides
of
the
trek
are:
in
the
first
stage
of
selvaggio
blu
oleanders
find
their
ideal
habitat
and
a
limestone
plateau
characterise
all
the
area
true
natural
monuments
as
holm
oaks
can
be
easily
found
the
path
is
coloured
by
the
bushy
euphorbias
(euphorbia
dendroides)
snakes
are
integral
parts
of
the
fauna
as
for
example
biacco
(coluber
viridiflavus)
in
the
second
stage
the
cistus
(rockrose)
can
be
found
in
numerous
pink
and
white
varieties
ferula
is
a
plant
belonging
to
the
apiaceae
family
that
are
in
the
third
stage
of
selvaggio
blu
during
the
fourth
part
of
the
path
the
possibility
to
find
shepherds
is
very
high
because
of
the
great
number
of
ovili
goats
are
basic
animals
of
the
fauna
sardinian
mouflon
can
also
be
seen
many
books
and
guides
about
selvaggio
blu
these
include:
selvaggio
blu
is
not
the
only
trekking
route
that
you
can
find
in
sardinia
there
are
4
more
trekking
routes
called:
