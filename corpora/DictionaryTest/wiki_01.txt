mixlink
mixlink
(mixlink
ii)
is
a
computer
used
with
agfa
scales
it
was
developed
to
facilitate
calculation
of
color
mixes
mixlink
contains
the
intel
80386
processor
with
the
clock
rate
which
may
be
set
to
either
92
or
33 mhz
the
ram
size
is
640
kb
the
hdd
function
is
served
by
the
built-in
flash
memory
that
has
the
size
of
approximately
800
kb
mixlink
has
the
monochrome
display
mixlink
is
also
staffed
with
cd
drive
and
floppy
drive
mixlink
was
intended
to
be
used
with
the
supplied
floppy
and
cd
disk
which
provided
the
system
environment
("operating
system")
and
the
application
to
be
used
for
calculating
color
mixes
however
it
is
possible
to
install
and
run
ms-dos
on
mixlink
currently
mixlink
computers
are
not
used;
their
functions
may
solely
be
performed
by
personal
computers
(pcs)
outline
of
computers
the
following
outline
is
provided
as
an
overview
of
and
topical
guide
to
computers:
computers
–
programmable
machines
designed
to
automatically
carry
out
sequences
of
arithmetic
or
logical
operations
the
sequences
of
operations
can
be
changed
readily
allowing
computers
to
solve
more
than
one
kind
of
problem
computers
can
be
described
as
all
of
the
following:
computer
architecture
–
history
of
computing
hardware
hp
toshiba
dell
apple
acer
asus
software
development
–
computer
magazines
–
"see
list
of
computer
magazines"
online
–
electronic
media
and
sleep
the
use
of
computers
(including
devices
such
as
smartphones
tablet
computers
and
laptops)
by
children
and
adolescents
before
bed
has
been
associated
with
a
reduction
in
the
hours
of
sleep
experienced
by
frequent
users
along
with
a
decreased
quality
of
sleep
in
most
cases
the
results
of
computer
use
at
night
have
been
linked
with
tiredness
a
2010
review
concluded
that
"the
use
of
electronic
media
by
children
and
adolescents
does
have
a
negative
impact
on
their
sleep
although
the
precise
effects
and
mechanisms
remain
unclear"
with
the
most
consistent
results
associating
excessive
media
use
with
shorter
sleep
duration
and
delayed
bed
times
a
2016
meta-analysis
found
that
"bedtime
access
and
use
of
media
devices
was
significantly
associated
with
inadequate
sleep
quantity;
poor
sleep
quality;
and
excessive
daytime
sleepiness"
the
american
academy
of
pediatrics
recommends
screen
time
for
children
be
limited
for
multiple
reasons
among
them
that
"too
much
screen
time
can
also
harm
the
amount
and
quality
of
sleep"
many
apps
promise
to
improve
sleep
by
filtering
out
blue
light
produced
by
media
devices;
there
have
been
no
large
studies
to
assess
whether
such
apps
work
some
users
express
dissatisfaction
with
the
resultant
orange
tint
of
screens
some
people
use
blue-blocking
glasses
for
the
purpose
of
attempting
to
block
out
blue
light
both
from
electronic
media
and
from
other
artificial
light
sources
computer
a
computer
is
a
device
that
can
be
instructed
to
carry
out
sequences
of
arithmetic
or
logical
operations
automatically
via
computer
programming
modern
computers
have
the
ability
to
follow
generalized
sets
of
operations
called
"programs"
these
programs
enable
computers
to
perform
an
extremely
wide
range
of
tasks
a
"complete"
computer
including
the
hardware
the
operating
system
(main
software)
and
peripheral
equipment
required
and
used
for
"full"
operation
can
be
referred
to
as
a
computer
system
this
term
may
as
well
be
used
for
a
group
of
computers
that
are
connected
and
work
together
in
particular
a
computer
network
or
computer
cluster
computers
are
used
as
control
systems
for
a
wide
variety
of
industrial
and
consumer
devices
this
includes
simple
special
purpose
devices
like
microwave
ovens
and
remote
controls
factory
devices
such
as
industrial
robots
and
computer-aided
design
and
also
general
purpose
devices
like
personal
computers
and
mobile
devices
such
as
smartphones
the
internet
is
run
on
computers
and
it
connects
hundreds
of
millions
of
other
computers
and
their
users
early
computers
were
only
conceived
as
calculating
devices
since
ancient
times
simple
manual
devices
like
the
abacus
aided
people
in
doing
calculations
early
in
the
industrial
revolution
some
mechanical
devices
were
built
to
automate
long
tedious
tasks
such
as
guiding
patterns
for
looms
more
sophisticated
electrical
machines
did
specialized
analog
calculations
in
the
early
20th
century
the
first
digital
electronic
calculating
machines
were
developed
during
world
war
ii
the
speed
power
and
versatility
of
computers
have
been
increasing
dramatically
ever
since
then
conventionally
a
modern
computer
consists
of
at
least
one
processing
element
typically
a
central
processing
unit
(cpu)
and
some
form
of
memory
the
processing
element
carries
out
arithmetic
and
logical
operations
and
a
sequencing
and
control
unit
can
change
the
order
of
operations
in
response
to
stored
information
peripheral
devices
include
input
devices
(keyboards
mice
joystick
etc)
output
devices
(monitor
screens
printers
etc)
and
input/output
devices
that
perform
both
functions
(eg
the
2000s-era
touchscreen)
peripheral
devices
allow
information
to
be
retrieved
from
an
external
source
and
they
enable
the
result
of
operations
to
be
saved
and
retrieved
according
to
the
"oxford
english
dictionary"
the
first
known
use
of
the
word
"computer"
was
in
1613
in
a
book
called
"the
yong
mans
gleanings"
by
english
writer
richard
braithwait:
"i
haue
[sic]
read
the
truest
computer
of
times
and
the
best
arithmetician
that
euer
[sic]
breathed
and
he
reduceth
thy
dayes
into
a
short
number"
this
usage
of
the
term
referred
to
a
human
computer
a
person
who
carried
out
calculations
or
computations
the
word
continued
with
the
same
meaning
until
the
middle
of
the
20th
century
during
the
latter
part
of
this
period
women
were
often
hired
as
computers
because
they
could
be
paid
less
than
their
male
counterparts
by
1943
most
human
computers
were
women
from
the
end
of
the
19th
century
the
word
slowly
began
to
take
on
its
more
familiar
meaning
a
machine
that
carries
out
computations
the
"online
etymology
dictionary"
gives
the
first
attested
use
of
"computer"
in
the
"1640s
[meaning]
"one
who
calculates";
this
is
an
" agent
noun
from
compute
(v)"
the
"online
etymology
dictionary"
states
that
the
use
of
the
term
to
mean
"calculating
machine"
(of
any
type)
is
from
1897"
the
"online
etymology
dictionary"
indicates
that
the
"modern
use"
of
the
term
to
mean
"programmable
digital
electronic
computer"
dates
from
" 1945
under
this
name;
[in
a]
theoretical
[sense]
from
1937
as
turing
machine"
devices
have
been
used
to
aid
computation
for
thousands
of
years
mostly
using
one-to-one
correspondence
with
fingers
the
earliest
counting
device
was
probably
a
form
of
tally
stick
later
record
keeping
aids
throughout
the
fertile
crescent
included
calculi
(clay
spheres
cones
etc)
which
represented
counts
of
items
probably
livestock
or
grains
sealed
in
hollow
unbaked
clay
containers
the
use
of
counting
rods
is
one
example
the
abacus
was
initially
used
for
arithmetic
tasks
the
roman
abacus
was
developed
from
devices
used
in
babylonia
as
early
as
2400
bc
since
then
many
other
forms
of
reckoning
boards
or
tables
have
been
invented
in
a
medieval
european
counting
house
a
checkered
cloth
would
be
placed
on
a
table
and
markers
moved
around
on
it
according
to
certain
rules
as
an
aid
to
calculating
sums
of
money
the
antikythera
mechanism
is
believed
to
be
the
earliest
mechanical
analog
"computer"
according
to
derek
j
de
solla
price
it
was
designed
to
calculate
astronomical
positions
it
was
discovered
in
1901
in
the
antikythera
wreck
off
the
greek
island
of
antikythera
between
kythera
and
crete
and
has
been
dated
to
devices
of
a
level
of
complexity
comparable
to
that
of
the
antikythera
mechanism
would
not
reappear
until
a
thousand
years
later
many
mechanical
aids
to
calculation
and
measurement
were
constructed
for
astronomical
and
navigation
use
the
planisphere
was
a
star
chart
invented
by
abū
rayhān
al-bīrūnī
in
the
early
11th
century
the
astrolabe
was
invented
in
the
hellenistic
world
in
either
the
1st
or
2nd
centuries
bc
and
is
often
attributed
to
hipparchus
a
combination
of
the
planisphere
and
dioptra
the
astrolabe
was
effectively
an
analog
computer
capable
of
working
out
several
different
kinds
of
problems
in
spherical
astronomy
an
astrolabe
incorporating
a
mechanical
calendar
computer
and
gear-wheels
was
invented
by
abi
bakr
of
isfahan
persia
in
1235
abū
rayhān
al-bīrūnī
invented
the
first
mechanical
geared
lunisolar
calendar
astrolabe
an
early
fixed-wired
knowledge
processing
machine
with
a
gear
train
and
gear-wheels
the
sector
a
calculating
instrument
used
for
solving
problems
in
proportion
trigonometry
multiplication
and
division
and
for
various
functions
such
as
squares
and
cube
roots
was
developed
in
the
late
16th
century
and
found
application
in
gunnery
surveying
and
navigation
the
planimeter
was
a
manual
instrument
to
calculate
the
area
of
a
closed
figure
by
tracing
over
it
with
a
mechanical
linkage
the
slide
rule
was
invented
around
1620–1630
shortly
after
the
publication
of
the
concept
of
the
logarithm
it
is
a
hand-operated
analog
computer
for
doing
multiplication
and
division
as
slide
rule
development
progressed
added
scales
provided
reciprocals
squares
and
square
roots
cubes
and
cube
roots
as
well
as
transcendental
functions
such
as
logarithms
and
exponentials
circular
and
hyperbolic
trigonometry
and
other
functions
slide
rules
with
special
scales
are
still
used
for
quick
performance
of
routine
calculations
such
as
the
e6b
circular
slide
rule
used
for
time
and
distance
calculations
on
light
aircraft
in
the
1770s
pierre
jaquet-droz
a
swiss
watchmaker
built
a
mechanical
doll
(automaton)
that
could
write
holding
a
quill
pen
by
switching
the
number
and
order
of
its
internal
wheels
different
letters
and
hence
different
messages
could
be
produced
in
effect
it
could
be
mechanically
"programmed"
to
read
instructions
along
with
two
other
complex
machines
the
doll
is
at
the
musée
d'art
et
d'histoire
of
neuchâtel
switzerland
and
still
operates
the
tide-predicting
machine
invented
by
sir
william
thomson
in
1872
was
of
great
utility
to
navigation
in
shallow
waters
it
used
a
system
of
pulleys
and
wires
to
automatically
calculate
predicted
tide
levels
for
a
set
period
at
a
particular
location
the
differential
analyser
a
mechanical
analog
computer
designed
to
solve
differential
equations
by
integration
used
wheel-and-disc
mechanisms
to
perform
the
integration
in
1876
lord
kelvin
had
already
discussed
the
possible
construction
of
such
calculators
but
he
had
been
stymied
by
the
limited
output
torque
of
the
ball-and-disk
integrators
in
a
differential
analyzer
the
output
of
one
integrator
drove
the
input
of
the
next
integrator
or
a
graphing
output
the
torque
amplifier
was
the
advance
that
allowed
these
machines
to
work
starting
in
the
1920s
vannevar
bush
and
others
developed
mechanical
differential
analyzers
charles
babbage
an
english
mechanical
engineer
and
polymath
originated
the
concept
of
a
programmable
computer
considered
the
"father
of
the
computer"
he
conceptualized
and
invented
the
first
mechanical
computer
in
the
early
19th
century
after
working
on
his
revolutionary
difference
engine
designed
to
aid
in
navigational
calculations
in
1833
he
realized
that
a
much
more
general
design
an
analytical
engine
was
possible
the
input
of
programs
and
data
was
to
be
provided
to
the
machine
via
punched
cards
a
method
being
used
at
the
time
to
direct
mechanical
looms
such
as
the
jacquard
loom
for
output
the
machine
would
have
a
printer
a
curve
plotter
and
a
bell
the
machine
would
also
be
able
to
punch
numbers
onto
cards
to
be
read
in
later
the
engine
incorporated
an
arithmetic
logic
unit
control
flow
in
the
form
of
conditional
branching
and
loops
and
integrated
memory
making
it
the
first
design
for
a
general-purpose
computer
that
could
be
described
in
modern
terms
as
turing-complete
the
machine
was
about
a
century
ahead
of
its
time
all
the
parts
for
his
machine
had
to
be
made
by
hand –
this
was
a
major
problem
for
a
device
with
thousands
of
parts
eventually
the
project
was
dissolved
with
the
decision
of
the
british
government
to
cease
funding
babbage's
failure
to
complete
the
analytical
engine
can
be
chiefly
attributed
to
difficulties
not
only
of
politics
and
financing
but
also
to
his
desire
to
develop
an
increasingly
sophisticated
computer
and
to
move
ahead
faster
than
anyone
else
could
follow
nevertheless
his
son
henry
babbage
completed
a
simplified
version
of
the
analytical
engine's
computing
unit
(the
"mill")
in
1888
he
gave
a
successful
demonstration
of
its
use
in
computing
tables
in
1906
during
the
first
half
of
the
20th
century
many
scientific
computing
needs
were
met
by
increasingly
sophisticated
analog
computers
which
used
a
direct
mechanical
or
electrical
model
of
the
problem
as
a
basis
for
computation
however
these
were
not
programmable
and
generally
lacked
the
versatility
and
accuracy
of
modern
digital
computers
the
first
modern
analog
computer
was
a
tide-predicting
machine
invented
by
sir
william
thomson
in
1872
the
differential
analyser
a
mechanical
analog
computer
designed
to
solve
differential
equations
by
integration
using
wheel-and-disc
mechanisms
was
conceptualized
in
1876
by
james
thomson
the
brother
of
the
more
famous
lord
kelvin
the
art
of
mechanical
analog
computing
reached
its
zenith
with
the
differential
analyzer
built
by
h
l
hazen
and
vannevar
bush
at
mit
starting
in
1927
this
built
on
the
mechanical
integrators
of
james
thomson
and
the
torque
amplifiers
invented
by
h
w
nieman
a
dozen
of
these
devices
were
built
before
their
obsolescence
became
obvious
by
the
1950s
the
success
of
digital
electronic
computers
had
spelled
the
end
for
most
analog
computing
machines
but
analog
computers
remained
in
use
during
the
1950s
in
some
specialized
applications
such
as
education
(control
systems)
and
aircraft
(slide
rule)
by
1938
the
united
states
navy
had
developed
an
electromechanical
analog
computer
small
enough
to
use
aboard
a
submarine
this
was
the
torpedo
data
computer
which
used
trigonometry
to
solve
the
problem
of
firing
a
torpedo
at
a
moving
target
during
world
war
ii
similar
devices
were
developed
in
other
countries
as
well
early
digital
computers
were
electromechanical;
electric
switches
drove
mechanical
relays
to
perform
the
calculation
these
devices
had
a
low
operating
speed
and
were
eventually
superseded
by
much
faster
all-electric
computers
originally
using
vacuum
tubes
the
z2
created
by
german
engineer
konrad
zuse
in
1939
was
one
of
the
earliest
examples
of
an
electromechanical
relay
computer
in
1941
zuse
followed
his
earlier
machine
up
with
the
z3
the
world's
first
working
electromechanical
programmable
fully
automatic
digital
computer
the
z3
was
built
with
2000
relays
implementing
a
22 bit
word
length
that
operated
at
a
clock
frequency
of
about
5–10 hz
program
code
was
supplied
on
punched
film
while
data
could
be
stored
in
64
words
of
memory
or
supplied
from
the
keyboard
it
was
quite
similar
to
modern
machines
in
some
respects
pioneering
numerous
advances
such
as
floating
point
numbers
rather
than
the
harder-to-implement
decimal
system
(used
in
charles
babbage's
earlier
design)
using
a
binary
system
meant
that
zuse's
machines
were
easier
to
build
and
potentially
more
reliable
given
the
technologies
available
at
that
time
the
z3
was
turing
complete
purely
electronic
circuit
elements
soon
replaced
their
mechanical
and
electromechanical
equivalents
at
the
same
time
that
digital
calculation
replaced
analog
the
engineer
tommy
flowers
working
at
the
post
office
research
station
in
london
in
the
1930s
began
to
explore
the
possible
use
of
electronics
for
the
telephone
exchange
experimental
equipment
that
he
built
in
1934
went
into
operation
five
years
later
converting
a
portion
of
the
telephone
exchange
network
into
an
electronic
data
processing
system
using
thousands
of
vacuum
tubes
in
the
us
john
vincent
atanasoff
and
clifford
e
berry
of
iowa
state
university
developed
and
tested
the
atanasoff–berry
computer
(abc)
in
1942
the
first
"automatic
electronic
digital
computer"
this
design
was
also
all-electronic
and
used
about
300
vacuum
tubes
with
capacitors
fixed
in
a
mechanically
rotating
drum
for
memory
during
world
war
ii
the
british
at
bletchley
park
achieved
a
number
of
successes
at
breaking
encrypted
german
military
communications
the
german
encryption
machine
enigma
was
first
attacked
with
the
help
of
the
electro-mechanical
bombes
which
were
often
run
by
women
to
crack
the
more
sophisticated
german
lorenz
sz
40/42
machine
used
for
high-level
army
communications
max
newman
and
his
colleagues
commissioned
flowers
to
build
the
colossus
he
spent
eleven
months
from
early
february
1943
designing
and
building
the
first
colossus
after
a
functional
test
in
december
1943
colossus
was
shipped
to
bletchley
park
where
it
was
delivered
on
18
january
1944
and
attacked
its
first
message
on
5
february
colossus
was
the
world's
first
electronic
digital
programmable
computer
it
used
a
large
number
of
valves
(vacuum
tubes)
it
had
paper-tape
input
and
was
capable
of
being
configured
to
perform
a
variety
of
boolean
logical
operations
on
its
data
but
it
was
not
turing-complete
nine
mk
ii
colossi
were
built
(the
mk
i
was
converted
to
a
mk
ii
making
ten
machines
in
total)
colossus
mark
i
contained
1500
thermionic
valves
(tubes)
but
mark
ii
with
2400
valves
was
both
5
times
faster
and
simpler
to
operate
than
mark
i
greatly
speeding
the
decoding
process
the
us-built
eniac
(electronic
numerical
integrator
and
computer)
was
the
first
electronic
programmable
computer
built
in
the
us
although
the
eniac
was
similar
to
the
colossus
it
was
much
faster
more
flexible
and
it
was
turing-complete
like
the
colossus
a
"program"
on
the
eniac
was
defined
by
the
states
of
its
patch
cables
and
switches
a
far
cry
from
the
stored
program
electronic
machines
that
came
later
once
a
program
was
written
it
had
to
be
mechanically
set
into
the
machine
with
manual
resetting
of
plugs
and
switches
the
programmers
of
the
eniac
were
six
women
often
known
collectively
as
the
"eniac
girls"
it
combined
the
high
speed
of
electronics
with
the
ability
to
be
programmed
for
many
complex
problems
it
could
add
or
subtract
5000
times
a
second
a
thousand
times
faster
than
any
other
machine
it
also
had
modules
to
multiply
divide
and
square
root
high
speed
memory
was
limited
to
20
words
(about
80
bytes)
built
under
the
direction
of
john
mauchly
and
j
presper
eckert
at
the
university
of
pennsylvania
eniac's
development
and
construction
lasted
from
1943
to
full
operation
at
the
end
of
1945
the
machine
was
huge
weighing
30
tons
using
200
kilowatts
of
electric
power
and
contained
over
18000
vacuum
tubes
1500
relays
and
hundreds
of
thousands
of
resistors
capacitors
and
inductors
the
principle
of
the
modern
computer
was
proposed
by
alan
turing
in
his
seminal
1936
paper
"on
computable
numbers"
turing
proposed
a
simple
device
that
he
called
"universal
computing
machine"
and
that
is
now
known
as
a
universal
turing
machine
he
proved
that
such
a
machine
is
capable
of
computing
anything
that
is
computable
by
executing
instructions
(program)
stored
on
tape
allowing
the
machine
to
be
programmable
the
fundamental
concept
of
turing's
design
is
the
stored
program
where
all
the
instructions
for
computing
are
stored
in
memory
von
neumann
acknowledged
that
the
central
concept
of
the
modern
computer
was
due
to
this
paper
turing
machines
are
to
this
day
a
central
object
of
study
in
theory
of
computation
except
for
the
limitations
imposed
by
their
finite
memory
stores
modern
computers
are
said
to
be
turing-complete
which
is
to
say
they
have
algorithm
execution
capability
equivalent
to
a
universal
turing
machine
early
computing
machines
had
fixed
programs
changing
its
function
required
the
re-wiring
and
re-structuring
of
the
machine
with
the
proposal
of
the
stored-program
computer
this
changed
a
stored-program
computer
includes
by
design
an
instruction
set
and
can
store
in
memory
a
set
of
instructions
(a
program)
that
details
the
computation
the
theoretical
basis
for
the
stored-program
computer
was
laid
by
alan
turing
in
his
1936
paper
in
1945
turing
joined
the
national
physical
laboratory
and
began
work
on
developing
an
electronic
stored-program
digital
computer
his
1945
report
"proposed
electronic
calculator"
was
the
first
specification
for
such
a
device
john
von
neumann
at
the
university
of
pennsylvania
also
circulated
his
"first
draft
of
a
report
on
the
edvac"
in
1945
the
manchester
baby
was
the
world's
first
stored-program
computer
it
was
built
at
the
victoria
university
of
manchester
by
frederic
c
williams
tom
kilburn
and
geoff
tootill
and
ran
its
first
program
on
21
june
1948
it
was
designed
as
a
testbed
for
the
williams
tube
the
first
random-access
digital
storage
device
although
the
computer
was
considered
"small
and
primitive"
by
the
standards
of
its
time
it
was
the
first
working
machine
to
contain
all
of
the
elements
essential
to
a
modern
electronic
computer
as
soon
as
the
baby
had
demonstrated
the
feasibility
of
its
design
a
project
was
initiated
at
the
university
to
develop
it
into
a
more
usable
computer
the
manchester
mark
1
grace
hopper
was
the
first
person
to
develop
a
compiler
for
programming
language
the
mark
1
in
turn
quickly
became
the
prototype
for
the
ferranti
mark
1
the
world's
first
commercially
available
general-purpose
computer
built
by
ferranti
it
was
delivered
to
the
university
of
manchester
in
february
1951
at
least
seven
of
these
later
machines
were
delivered
between
1953
and
1957
one
of
them
to
shell
labs
in
amsterdam
in
october
1947
the
directors
of
british
catering
company
j
lyons
company
decided
to
take
an
active
role
in
promoting
the
commercial
development
of
computers
the
leo
i
computer
became
operational
in
april
1951
and
ran
the
world's
first
regular
routine
office
computer
job
the
bipolar
transistor
was
invented
in
1947
from
1955
onwards
transistors
replaced
vacuum
tubes
in
computer
designs
giving
rise
to
the
"second
generation"
of
computers
compared
to
vacuum
tubes
transistors
have
many
advantages:
they
are
smaller
and
require
less
power
than
vacuum
tubes
so
give
off
less
heat
silicon
junction
transistors
were
much
more
reliable
than
vacuum
tubes
and
had
longer
indefinite
service
life
transistorized
computers
could
contain
tens
of
thousands
of
binary
logic
circuits
in
a
relatively
compact
space
at
the
university
of
manchester
a
team
under
the
leadership
of
tom
kilburn
designed
and
built
a
machine
using
the
newly
developed
transistors
instead
of
valves
their
first
transistorised
computer
and
the
first
in
the
world
was
operational
by
1953
and
a
second
version
was
completed
there
in
april
1955
however
the
machine
did
make
use
of
valves
to
generate
its
125 khz
clock
waveforms
and
in
the
circuitry
to
read
and
write
on
its
magnetic
drum
memory
so
it
was
not
the
first
completely
transistorized
computer
that
distinction
goes
to
the
harwell
cadet
of
1955
built
by
the
electronics
division
of
the
atomic
energy
research
establishment
at
harwell
the
next
great
advance
in
computing
power
came
with
the
advent
of
the
integrated
circuit
the
idea
of
the
integrated
circuit
was
first
conceived
by
a
radar
scientist
working
for
the
royal
radar
establishment
of
the
ministry
of
defence
geoffrey
wa
dummer
dummer
presented
the
first
public
description
of
an
integrated
circuit
at
the
symposium
on
progress
in
quality
electronic
components
in
washington dc
on
7
may
1952
the
first
practical
ics
were
invented
by
jack
kilby
at
texas
instruments
and
robert
noyce
at
fairchild
semiconductor
kilby
recorded
his
initial
ideas
concerning
the
integrated
circuit
in
july
1958
successfully
demonstrating
the
first
working
integrated
example
on
12
september
1958
in
his
patent
application
of
6
february
1959
kilby
described
his
new
device
as
"a
body
of
semiconductor
material 
wherein
all
the
components
of
the
electronic
circuit
are
completely
integrated"
noyce
also
came
up
with
his
own
idea
of
an
integrated
circuit
half
a
year
later
than
kilby
his
chip
solved
many
practical
problems
that
kilby's
had
not
produced
at
fairchild
semiconductor
it
was
made
of
silicon
whereas
kilby's
chip
was
made
of
germanium
this
new
development
heralded
an
explosion
in
the
commercial
and
personal
use
of
computers
and
led
to
the
invention
of
the
microprocessor
while
the
subject
of
exactly
which
device
was
the
first
microprocessor
is
contentious
partly
due
to
lack
of
agreement
on
the
exact
definition
of
the
term
"microprocessor"
it
is
largely
undisputed
that
the
first
single-chip
microprocessor
was
the
intel
4004
designed
and
realized
by
ted
hoff
federico
faggin
and
stanley
mazor
at
intel
the
first
mobile
computers
were
heavy
and
ran
from
mains
power
the
50lb
ibm
5100
was
an
early
example
later
portables
such
as
the
osborne
1
and
compaq
portable
were
considerably
lighter
but
still
needed
to
be
plugged
in
the
first
laptops
such
as
the
grid
compass
removed
this
requirement
by
incorporating
batteries
–
and
with
the
continued
miniaturization
of
computing
resources
and
advancements
in
portable
battery
life
portable
computers
grew
in
popularity
in
the
2000s
the
same
developments
allowed
manufacturers
to
integrate
computing
resources
into
cellular
phones
these
smartphones
and
tablets
run
on
a
variety
of
operating
systems
and
soon
became
the
dominant
computing
device
on
the
market
with
manufacturers
reporting
having
shipped
an
estimated
237
million
devices
in
2q
2013
computers
are
typically
classified
based
on
their
uses:
the
term
"hardware"
covers
all
of
those
parts
of
a
computer
that
are
tangible
physical
objects
circuits
computer
chips
graphic
cards
sound
cards
memory
(ram)
motherboard
displays
power
supplies
cables
keyboards
printers
and
"mice"
input
devices
are
all
hardware
a
general
purpose
computer
has
four
main
components:
the
arithmetic
logic
unit
(alu)
the
control
unit
the
memory
and
the
input
and
output
devices
(collectively
termed
i/o)
these
parts
are
interconnected
by
buses
often
made
of
groups
of
wires
inside
each
of
these
parts
are
thousands
to
trillions
of
small
electrical
circuits
which
can
be
turned
off
or
on
by
means
of
an
electronic
switch
each
circuit
represents
a
bit
(binary
digit)
of
information
so
that
when
the
circuit
is
on
it
represents
a
"1"
and
when
off
it
represents
a
"0"
(in
positive
logic
representation)
the
circuits
are
arranged
in
logic
gates
so
that
one
or
more
of
the
circuits
may
control
the
state
of
one
or
more
of
the
other
circuits
when
unprocessed
data
is
sent
to
the
computer
with
the
help
of
input
devices
the
data
is
processed
and
sent
to
output
devices
the
input
devices
may
be
hand-operated
or
automated
the
act
of
processing
is
mainly
regulated
by
the
cpu
some
examples
of
input
devices
are:
the
means
through
which
computer
gives
output
are
known
as
output
devices
some
examples
of
output
devices
are:
the
control
unit
(often
called
a
control
system
or
central
controller)
manages
the
computer's
various
components;
it
reads
and
interprets
(decodes)
the
program
instructions
transforming
them
into
control
signals
that
activate
other
parts
of
the
computer
control
systems
in
advanced
computers
may
change
the
order
of
execution
of
some
instructions
to
improve
performance
a
key
component
common
to
all
cpus
is
the
program
counter
a
special
memory
cell
(a
register)
that
keeps
track
of
which
location
in
memory
the
next
instruction
is
to
be
read
from
the
control
system's
function
is
as
follows—note
that
this
is
a
simplified
description
and
some
of
these
steps
may
be
performed
concurrently
or
in
a
different
order
depending
on
the
type
of
cpu:
since
the
program
counter
is
(conceptually)
just
another
set
of
memory
cells
it
can
be
changed
by
calculations
done
in
the
alu
adding
100
to
the
program
counter
would
cause
the
next
instruction
to
be
read
from
a
place
100
locations
further
down
the
program
instructions
that
modify
the
program
counter
are
often
known
as
"jumps"
and
allow
for
loops
(instructions
that
are
repeated
by
the
computer)
and
often
conditional
instruction
execution
(both
examples
of
control
flow)
the
sequence
of
operations
that
the
control
unit
goes
through
to
process
an
instruction
is
in
itself
like
a
short
computer
program
and
indeed
in
some
more
complex
cpu
designs
there
is
another
yet
smaller
computer
called
a
microsequencer
which
runs
a
microcode
program
that
causes
all
of
these
events
to
happen
the
control
unit
alu
and
registers
are
collectively
known
as
a
central
processing
unit
(cpu)
early
cpus
were
composed
of
many
separate
components
but
since
the
mid-1970s
cpus
have
typically
been
constructed
on
a
single
integrated
circuit
called
a
"microprocessor"
the
alu
is
capable
of
performing
two
classes
of
operations:
arithmetic
and
logic
the
set
of
arithmetic
operations
that
a
particular
alu
supports
may
be
limited
to
addition
and
subtraction
or
might
include
multiplication
division
trigonometry
functions
such
as
sine
cosine
etc
and
square
roots
some
can
only
operate
on
whole
numbers
(integers)
while
others
use
floating
point
to
represent
real
numbers
albeit
with
limited
precision
however
any
computer
that
is
capable
of
performing
just
the
simplest
operations
can
be
programmed
to
break
down
the
more
complex
operations
into
simple
steps
that
it
can
perform
therefore
any
computer
can
be
programmed
to
perform
any
arithmetic
operation—although
it
will
take
more
time
to
do
so
if
its
alu
does
not
directly
support
the
operation
an
alu
may
also
compare
numbers
and
return
boolean
truth
values
(true
or
false)
depending
on
whether
one
is
equal
to
greater
than
or
less
than
the
other
("is
64
greater
than
65?")
logic
operations
involve
boolean
logic:
and
or
xor
and
not
these
can
be
useful
for
creating
complicated
conditional
statements
and
processing
boolean
logic
superscalar
computers
may
contain
multiple
alus
allowing
them
to
process
several
instructions
simultaneously
graphics
processors
and
computers
with
simd
and
mimd
features
often
contain
alus
that
can
perform
arithmetic
on
vectors
and
matrices
a
computer's
memory
can
be
viewed
as
a
list
of
cells
into
which
numbers
can
be
placed
or
read
each
cell
has
a
numbered
"address"
and
can
store
a
single
number
the
computer
can
be
instructed
to
"put
the
number
123
into
the
cell
numbered
1357"
or
to
"add
the
number
that
is
in
cell
1357
to
the
number
that
is
in
cell
2468
and
put
the
answer
into
cell
1595"
the
information
stored
in
memory
may
represent
practically
anything
letters
numbers
even
computer
instructions
can
be
placed
into
memory
with
equal
ease
since
the
cpu
does
not
differentiate
between
different
types
of
information
it
is
the
software's
responsibility
to
give
significance
to
what
the
memory
sees
as
nothing
but
a
series
of
numbers
in
almost
all
modern
computers
each
memory
cell
is
set
up
to
store
binary
numbers
in
groups
of
eight
bits
(called
a
byte)
each
byte
is
able
to
represent
256
different
numbers
(2
=
256);
either
from
0
to
255
or
−128
to
+127
to
store
larger
numbers
several
consecutive
bytes
may
be
used
(typically
two
four
or
eight)
when
negative
numbers
are
required
they
are
usually
stored
in
two's
complement
notation
other
arrangements
are
possible
but
are
usually
not
seen
outside
of
specialized
applications
or
historical
contexts
a
computer
can
store
any
kind
of
information
in
memory
if
it
can
be
represented
numerically
modern
computers
have
billions
or
even
trillions
of
bytes
of
memory
the
cpu
contains
a
special
set
of
memory
cells
called
registers
that
can
be
read
and
written
to
much
more
rapidly
than
the
main
memory
area
there
are
typically
between
two
and
one
hundred
registers
depending
on
the
type
of
cpu
registers
are
used
for
the
most
frequently
needed
data
items
to
avoid
having
to
access
main
memory
every
time
data
is
needed
as
data
is
constantly
being
worked
on
reducing
the
need
to
access
main
memory
(which
is
often
slow
compared
to
the
alu
and
control
units)
greatly
increases
the
computer's
speed
computer
main
memory
comes
in
two
principal
varieties:
ram
can
be
read
and
written
to
anytime
the
cpu
commands
it
but
rom
is
preloaded
with
data
and
software
that
never
changes
therefore
the
cpu
can
only
read
from
it
rom
is
typically
used
to
store
the
computer's
initial
start-up
instructions
in
general
the
contents
of
ram
are
erased
when
the
power
to
the
computer
is
turned
off
but
rom
retains
its
data
indefinitely
in
a
pc
the
rom
contains
a
specialized
program
called
the
bios
that
orchestrates
loading
the
computer's
operating
system
from
the
hard
disk
drive
into
ram
whenever
the
computer
is
turned
on
or
reset
in
embedded
computers
which
frequently
do
not
have
disk
drives
all
of
the
required
software
may
be
stored
in
rom
software
stored
in
rom
is
often
called
firmware
because
it
is
notionally
more
like
hardware
than
software
flash
memory
blurs
the
distinction
between
rom
and
ram
as
it
retains
its
data
when
turned
off
but
is
also
rewritable
it
is
typically
much
slower
than
conventional
rom
and
ram
however
so
its
use
is
restricted
to
applications
where
high
speed
is
unnecessary
in
more
sophisticated
computers
there
may
be
one
or
more
ram
cache
memories
which
are
slower
than
registers
but
faster
than
main
memory
generally
computers
with
this
sort
of
cache
are
designed
to
move
frequently
needed
data
into
the
cache
automatically
often
without
the
need
for
any
intervention
on
the
programmer's
part
i/o
is
the
means
by
which
a
computer
exchanges
information
with
the
outside
world
devices
that
provide
input
or
output
to
the
computer
are
called
peripherals
on
a
typical
personal
computer
peripherals
include
input
devices
like
the
keyboard
and
mouse
and
output
devices
such
as
the
display
and
printer
hard
disk
drives
floppy
disk
drives
and
optical
disc
drives
serve
as
both
input
and
output
devices
computer
networking
is
another
form
of
i/o
i/o
devices
are
often
complex
computers
in
their
own
right
with
their
own
cpu
and
memory
a
graphics
processing
unit
might
contain
fifty
or
more
tiny
computers
that
perform
the
calculations
necessary
to
display
3d
graphics
modern
desktop
computers
contain
many
smaller
computers
that
assist
the
main
cpu
in
performing
i/o
a
2016-era
flat
screen
display
contains
its
own
computer
circuitry
while
a
computer
may
be
viewed
as
running
one
gigantic
program
stored
in
its
main
memory
in
some
systems
it
is
necessary
to
give
the
appearance
of
running
several
programs
simultaneously
this
is
achieved
by
multitasking
ie
having
the
computer
switch
rapidly
between
running
each
program
in
turn
one
means
by
which
this
is
done
is
with
a
special
signal
called
an
interrupt
which
can
periodically
cause
the
computer
to
stop
executing
instructions
where
it
was
and
do
something
else
instead
by
remembering
where
it
was
executing
prior
to
the
interrupt
the
computer
can
return
to
that
task
later
if
several
programs
are
running
"at
the
same
time"
then
the
interrupt
generator
might
be
causing
several
hundred
interrupts
per
second
causing
a
program
switch
each
time
since
modern
computers
typically
execute
instructions
several
orders
of
magnitude
faster
than
human
perception
it
may
appear
that
many
programs
are
running
at
the
same
time
even
though
only
one
is
ever
executing
in
any
given
instant
this
method
of
multitasking
is
sometimes
termed
"time-sharing"
since
each
program
is
allocated
a
"slice"
of
time
in
turn
before
the
era
of
inexpensive
computers
the
principal
use
for
multitasking
was
to
allow
many
people
to
share
the
same
computer
seemingly
multitasking
would
cause
a
computer
that
is
switching
between
several
programs
to
run
more
slowly
in
direct
proportion
to
the
number
of
programs
it
is
running
but
most
programs
spend
much
of
their
time
waiting
for
slow
input/output
devices
to
complete
their
tasks
if
a
program
is
waiting
for
the
user
to
click
on
the
mouse
or
press
a
key
on
the
keyboard
then
it
will
not
take
a
"time
slice"
until
the
event
it
is
waiting
for
has
occurred
this
frees
up
time
for
other
programs
to
execute
so
that
many
programs
may
be
run
simultaneously
without
unacceptable
speed
loss
some
computers
are
designed
to
distribute
their
work
across
several
cpus
in
a
multiprocessing
configuration
a
technique
once
employed
only
in
large
and
powerful
machines
such
as
supercomputers
mainframe
computers
and
servers
multiprocessor
and
multi-core
(multiple
cpus
on
a
single
integrated
circuit)
personal
and
laptop
computers
are
now
widely
available
and
are
being
increasingly
used
in
lower-end
markets
as
a
result
supercomputers
in
particular
often
have
highly
unique
architectures
that
differ
significantly
from
the
basic
stored-program
architecture
and
from
general
purpose
computers
they
often
feature
thousands
of
cpus
customized
high-speed
interconnects
and
specialized
computing
hardware
such
designs
tend
to
be
useful
only
for
specialized
tasks
due
to
the
large
scale
of
program
organization
required
to
successfully
utilize
most
of
the
available
resources
at
once
supercomputers
usually
see
usage
in
large-scale
simulation
graphics
rendering
and
cryptography
applications
as
well
as
with
other
so-called
"embarrassingly
parallel"
tasks
"software"
refers
to
parts
of
the
computer
which
do
not
have
a
material
form
such
as
programs
data
protocols
etc
software
is
that
part
of
a
computer
system
that
consists
of
encoded
information
or
computer
instructions
in
contrast
to
the
physical
hardware
from
which
the
system
is
built
computer
software
includes
computer
programs
libraries
and
related
non-executable
data
such
as
online
documentation
or
digital
media
it
is
often
divided
into
system
software
and
application
software]]
computer
hardware
and
software
require
each
other
and
neither
can
be
realistically
used
on
its
own
when
software
is
stored
in
hardware
that
cannot
easily
be
modified
such
as
with
bios
rom
in
an
ibm
pc
compatible
computer
it
is
sometimes
called
"firmware"
there
are
thousands
of
different
programming
languages—some
intended
to
be
general
purpose
others
useful
only
for
highly
specialized
applications
the
defining
feature
of
modern
computers
which
distinguishes
them
from
all
other
machines
is
that
they
can
be
programmed
that
is
to
say
that
some
type
of
instructions
(the
program)
can
be
given
to
the
computer
and
it
will
process
them
modern
computers
based
on
the
von
neumann
architecture
often
have
machine
code
in
the
form
of
an
imperative
programming
language
in
practical
terms
a
computer
program
may
be
just
a
few
instructions
or
extend
to
many
millions
of
instructions
as
do
the
programs
for
word
processors
and
web
browsers
for
example
a
typical
modern
computer
can
execute
billions
of
instructions
per
second
(gigaflops)
and
rarely
makes
a
mistake
over
many
years
of
operation
large
computer
programs
consisting
of
several
million
instructions
may
take
teams
of
programmers
years
to
write
and
due
to
the
complexity
of
the
task
almost
certainly
contain
errors
this
section
applies
to
most
common
ram
machine–based
computers
in
most
cases
computer
instructions
are
simple:
add
one
number
to
another
move
some
data
from
one
location
to
another
send
a
message
to
some
external
device
etc
these
instructions
are
read
from
the
computer's
memory
and
are
generally
carried
out
(executed)
in
the
order
they
were
given
however
there
are
usually
specialized
instructions
to
tell
the
computer
to
jump
ahead
or
backwards
to
some
other
place
in
the
program
and
to
carry
on
executing
from
there
these
are
called
"jump"
instructions
(or
branches)
furthermore
jump
instructions
may
be
made
to
happen
conditionally
so
that
different
sequences
of
instructions
may
be
used
depending
on
the
result
of
some
previous
calculation
or
some
external
event
many
computers
directly
support
subroutines
by
providing
a
type
of
jump
that
"remembers"
the
location
it
jumped
from
and
another
instruction
to
return
to
the
instruction
following
that
jump
instruction
program
execution
might
be
likened
to
reading
a
book
while
a
person
will
normally
read
each
word
and
line
in
sequence
they
may
at
times
jump
back
to
an
earlier
place
in
the
text
or
skip
sections
that
are
not
of
interest
similarly
a
computer
may
sometimes
go
back
and
repeat
the
instructions
in
some
section
of
the
program
over
and
over
again
until
some
internal
condition
is
met
this
is
called
the
flow
of
control
within
the
program
and
it
is
what
allows
the
computer
to
perform
tasks
repeatedly
without
human
intervention
comparatively
a
person
using
a
pocket
calculator
can
perform
a
basic
arithmetic
operation
such
as
adding
two
numbers
with
just
a
few
button
presses
but
to
add
together
all
of
the
numbers
from
1
to
1000
would
take
thousands
of
button
presses
and
a
lot
of
time
with
a
near
certainty
of
making
a
mistake
on
the
other
hand
a
computer
may
be
programmed
to
do
this
with
just
a
few
simple
instructions
the
following
example
is
written
in
the
mips
assembly
language:
once
told
to
run
this
program
the
computer
will
perform
the
repetitive
addition
task
without
further
human
intervention
it
will
almost
never
make
a
mistake
and
a
modern
pc
can
complete
the
task
in
a
fraction
of
a
second
in
most
computers
individual
instructions
are
stored
as
machine
code
with
each
instruction
being
given
a
unique
number
(its
operation
code
or
opcode
for
short)
the
command
to
add
two
numbers
together
would
have
one
opcode;
the
command
to
multiply
them
would
have
a
different
opcode
and
so
on
the
simplest
computers
are
able
to
perform
any
of
a
handful
of
different
instructions;
the
more
complex
computers
have
several
hundred
to
choose
from
each
with
a
unique
numerical
code
since
the
computer's
memory
is
able
to
store
numbers
it
can
also
store
the
instruction
codes
this
leads
to
the
important
fact
that
entire
programs
(which
are
just
lists
of
these
instructions)
can
be
represented
as
lists
of
numbers
and
can
themselves
be
manipulated
inside
the
computer
in
the
same
way
as
numeric
data
the
fundamental
concept
of
storing
programs
in
the
computer's
memory
alongside
the
data
they
operate
on
is
the
crux
of
the
von
neumann
or
stored
program
architecture
in
some
cases
a
computer
might
store
some
or
all
of
its
program
in
memory
that
is
kept
separate
from
the
data
it
operates
on
this
is
called
the
harvard
architecture
after
the
harvard
mark
i
computer
modern
von
neumann
computers
display
some
traits
of
the
harvard
architecture
in
their
designs
such
as
in
cpu
caches
while
it
is
possible
to
write
computer
programs
as
long
lists
of
numbers
(machine
language)
and
while
this
technique
was
used
with
many
early
computers
it
is
extremely
tedious
and
potentially
error-prone
to
do
so
in
practice
especially
for
complicated
programs
instead
each
basic
instruction
can
be
given
a
short
name
that
is
indicative
of
its
function
and
easy
to
remember –
a
mnemonic
such
as
add
sub
mult
or
jump
these
mnemonics
are
collectively
known
as
a
computer's
assembly
language
converting
programs
written
in
assembly
language
into
something
the
computer
can
actually
understand
(machine
language)
is
usually
done
by
a
computer
program
called
an
assembler
programming
languages
provide
various
ways
of
specifying
programs
for
computers
to
run
unlike
natural
languages
programming
languages
are
designed
to
permit
no
ambiguity
and
to
be
concise
they
are
purely
written
languages
and
are
often
difficult
to
read
aloud
they
are
generally
either
translated
into
machine
code
by
a
compiler
or
an
assembler
before
being
run
or
translated
directly
at
run
time
by
an
interpreter
sometimes
programs
are
executed
by
a
hybrid
method
of
the
two
techniques
machine
languages
and
the
assembly
languages
that
represent
them
(collectively
termed
"low-level
programming
languages")
tend
to
be
unique
to
a
particular
type
of
computer
for
instance
an
arm
architecture
computer
(such
as
may
be
found
in
a
smartphone
or
a
hand-held
videogame)
cannot
understand
the
machine
language
of
an
x86
cpu
that
might
be
in
a
pc
although
considerably
easier
than
in
machine
language
writing
long
programs
in
assembly
language
is
often
difficult
and
is
also
error
prone
therefore
most
practical
programs
are
written
in
more
abstract
high-level
programming
languages
that
are
able
to
express
the
needs
of
the
programmer
more
conveniently
(and
thereby
help
reduce
programmer
error)
high
level
languages
are
usually
"compiled"
into
machine
language
(or
sometimes
into
assembly
language
and
then
into
machine
language)
using
another
computer
program
called
a
compiler
high
level
languages
are
less
related
to
the
workings
of
the
target
computer
than
assembly
language
and
more
related
to
the
language
and
structure
of
the
problem(s)
to
be
solved
by
the
final
program
it
is
therefore
often
possible
to
use
different
compilers
to
translate
the
same
high
level
language
program
into
the
machine
language
of
many
different
types
of
computer
this
is
part
of
the
means
by
which
software
like
video
games
may
be
made
available
for
different
computer
architectures
such
as
personal
computers
and
various
video
game
consoles
program
design
of
small
programs
is
relatively
simple
and
involves
the
analysis
of
the
problem
collection
of
inputs
using
the
programming
constructs
within
languages
devising
or
using
established
procedures
and
algorithms
providing
data
for
output
devices
and
solutions
to
the
problem
as
applicable
as
problems
become
larger
and
more
complex
features
such
as
subprograms
modules
formal
documentation
and
new
paradigms
such
as
object-oriented
programming
are
encountered
large
programs
involving
thousands
of
line
of
code
and
more
require
formal
software
methodologies
the
task
of
developing
large
software
systems
presents
a
significant
intellectual
challenge
producing
software
with
an
acceptably
high
reliability
within
a
predictable
schedule
and
budget
has
historically
been
difficult;
the
academic
and
professional
discipline
of
software
engineering
concentrates
specifically
on
this
challenge
errors
in
computer
programs
are
called
"bugs"
they
may
be
benign
and
not
affect
the
usefulness
of
the
program
or
have
only
subtle
effects
but
in
some
cases
they
may
cause
the
program
or
the
entire
system
to
"hang"
becoming
unresponsive
to
input
such
as
mouse
clicks
or
keystrokes
to
completely
fail
or
to
crash
otherwise
benign
bugs
may
sometimes
be
harnessed
for
malicious
intent
by
an
unscrupulous
user
writing
an
exploit
code
designed
to
take
advantage
of
a
bug
and
disrupt
a
computer's
proper
execution
bugs
are
usually
not
the
fault
of
the
computer
since
computers
merely
execute
the
instructions
they
are
given
bugs
are
nearly
always
the
result
of
programmer
error
or
an
oversight
made
in
the
program's
design
admiral
grace
hopper
an
american
computer
scientist
and
developer
of
the
first
compiler
is
credited
for
having
first
used
the
term
"bugs"
in
computing
after
a
dead
moth
was
found
shorting
a
relay
in
the
harvard
mark
ii
computer
in
september
1947
computers
have
been
used
to
coordinate
information
between
multiple
locations
since
the
1950s
the
us
military's
sage
system
was
the
first
large-scale
example
of
such
a
system
which
led
to
a
number
of
special-purpose
commercial
systems
such
as
sabre
in
the
1970s
computer
engineers
at
research
institutions
throughout
the
united
states
began
to
link
their
computers
together
using
telecommunications
technology
the
effort
was
funded
by
arpa
(now
darpa)
and
the
computer
network
that
resulted
was
called
the
arpanet
the
technologies
that
made
the
arpanet
possible
spread
and
evolved
in
time
the
network
spread
beyond
academic
and
military
institutions
and
became
known
as
the
internet
the
emergence
of
networking
involved
a
redefinition
of
the
nature
and
boundaries
of
the
computer
computer
operating
systems
and
applications
were
modified
to
include
the
ability
to
define
and
access
the
resources
of
other
computers
on
the
network
such
as
peripheral
devices
stored
information
and
the
like
as
extensions
of
the
resources
of
an
individual
computer
initially
these
facilities
were
available
primarily
to
people
working
in
high-tech
environments
but
in
the
1990s
the
spread
of
applications
like
e-mail
and
the
world
wide
web
combined
with
the
development
of
cheap
fast
networking
technologies
like
ethernet
and
adsl
saw
computer
networking
become
almost
ubiquitous
in
fact
the
number
of
computers
that
are
networked
is
growing
phenomenally
a
very
large
proportion
of
personal
computers
regularly
connect
to
the
internet
to
communicate
and
receive
information
"wireless"
networking
often
utilizing
mobile
phone
networks
has
meant
networking
is
becoming
increasingly
ubiquitous
even
in
mobile
computing
environments
a
computer
does
not
need
to
be
electronic
nor
even
have
a
processor
nor
ram
nor
even
a
hard
disk
while
popular
usage
of
the
word
"computer"
is
synonymous
with
a
personal
electronic
computer
the
modern
definition
of
a
computer
is
literally:
""a
device
that
computes"
especially
a
programmable
[usually]
electronic
machine
that
performs
high-speed
mathematical
or
logical
operations
or
that
assembles
stores
correlates
or
otherwise
processes
information"
any
device
which
"processes
information"
qualifies
as
a
computer
especially
if
the
processing
is
purposeful
there
is
active
research
to
make
computers
out
of
many
promising
new
types
of
technology
such
as
optical
computers
dna
computers
neural
computers
and
quantum
computers
most
computers
are
universal
and
are
able
to
calculate
any
computable
function
and
are
limited
only
by
their
memory
capacity
and
operating
speed
however
different
designs
of
computers
can
give
very
different
performance
for
particular
problems;
for
example
quantum
computers
can
potentially
break
some
modern
encryption
algorithms
(by
quantum
factoring)
very
quickly
there
are
many
types
of
computer
architectures:
of
all
these
abstract
machines
a
quantum
computer
holds
the
most
promise
for
revolutionizing
computing
logic
gates
are
a
common
abstraction
which
can
apply
to
most
of
the
above
digital
or
analog
paradigms
the
ability
to
store
and
execute
lists
of
instructions
called
programs
makes
computers
extremely
versatile
distinguishing
them
from
calculators
the
church–turing
thesis
is
a
mathematical
statement
of
this
versatility:
any
computer
with
a
minimum
capability
(being
turing-complete)
is
in
principle
capable
of
performing
the
same
tasks
that
any
other
computer
can
perform
therefore
any
type
of
computer
(netbook
supercomputer
cellular
automaton
etc)
is
able
to
perform
the
same
computational
tasks
given
enough
time
and
storage
capacity
a
computer
will
solve
problems
in
exactly
the
way
it
is
programmed
to
without
regard
to
efficiency
alternative
solutions
possible
shortcuts
or
possible
errors
in
the
code
computer
programs
that
learn
and
adapt
are
part
of
the
emerging
field
of
artificial
intelligence
and
machine
learning
artificial
intelligence
based
products
generally
fall
into
two
major
categories:
rule
based
systems
and
pattern
recognition
systems
rule
based
systems
attempt
to
represent
the
rules
used
by
human
experts
and
tend
to
be
expensive
to
develop
pattern
based
systems
use
data
about
a
problem
to
generate
conclusions
examples
of
pattern
based
systems
include
voice
recognition
font
recognition
translation
and
the
emerging
field
of
on-line
marketing
as
the
use
of
computers
has
spread
throughout
society
there
are
an
increasing
number
of
careers
involving
computers
the
need
for
computers
to
work
well
together
and
to
be
able
to
exchange
information
has
spawned
the
need
for
many
standards
organizations
clubs
and
societies
of
both
a
formal
and
informal
nature
cognitive
computer
a
cognitive
computer
combines
artificial
intelligence
and
machine-learning
algorithms
in
an
approach
which
attempts
to
reproduce
the
behaviour
of
the
human
brain
it
generally
adopts
a
neuromorphic
engineering
approach
an
example
of
neural
network
implementations
of
cognitive
convolution
and
deep
learning
is
provided
by
the
ibm
company's
watson
machine
a
subsequent
development
by
ibm
is
the
truenorth
microchip
architecture
which
is
designed
to
be
closer
in
structure
to
the
human
brain
than
the
von
neumann
architecture
used
in
conventional
computers
in
2017
intel
announced
its
own
version
of
a
cognitive
chip
in
"loihi"
which
will
be
available
to
university
and
research
labs
in
2018
intel's
self-learning
neuromorphic
chip
named
loihi
perhaps
named
after
the
hawaiian
seamount
loihi
offers
substantial
power
efficiency
designed
after
the
human
brain
intel
claims
loihi
is
about
1000
times
more
energy
efficient
than
the
general-purpose
computing
power
needed
to
train
the
neural
networks
that
rival
loihi's
performance
in
theory
this
would
support
both
machine
learning
training
and
inference
on
the
same
silicon
independently
of
a
cloud
connection
and
more
efficient
than
using
convolutional
neural
networks
(cnns)
or
deep
learning
neural
networks
intel
points
to
a
system
for
monitoring
a
person's
heartbeat
taking
readings
after
events
such
as
exercise
or
eating
and
uses
the
cognitive
computing
chip
to
normalize
the
data
and
work
out
the
‘normal’
heartbeat
it
can
then
spot
abnormalities
but
also
deal
with
any
new
events
or
conditions
the
first
iteration
of
the
loihi
chip
was
made
using
intel's
14 nm
fabrication
process
and
houses
128
clusters
of
1024
artificial
neurons
each
for
a
total
of
131072
simulated
neurons
this
offers
around
130
million
synapses
which
is
still
a
rather
long
way
from
the
human
brain's
800
trillion
synapses
and
behind
ibm's
truenorth
which
has
around
16
billion
by
using
64
by
4096
cores
loihi
is
now
available
for
research
purposes
among
more
than
40
academic
research
groups
as
a
usb
from
factor
the
ibm
cognitive
computers
implement
learning
using
hebbian
theory
instead
of
being
programmable
in
a
traditional
sense
within
machine
language
or
a
higher
level
programming
language
such
a
device
learns
by
inputting
instances
through
an
input
device
that
are
aggregated
within
a
computational
convolution
or
neural
network
architecture
consisting
of
weights
within
a
parallel
memory
system
an
early
instantiation
of
such
a
device
has
been
developed
in
2012
under
the
darpa
synapse
program
at
ibm
directed
by
dharmendra
modha
in
2017
this
ibm
64-chip
array
will
contain
the
processing
equivalent
of
64
million
neurons
and
16
billion
synapses
yet
each
processor
consumes
just
10
watts
of
electricity
like
other
neural
networks
this
system
will
be
put
to
use
in
pattern
recognition
and
sensory
processing
roles
the
air
force
wants
to
combine
the
truenorth
ability
to
convert
multiple
data
feeds
— whether
it's
audio
video
or
text —
into
machine
readable
symbols
with
a
conventional
supercomputer's
ability
to
crunch
data
this
isn't
the
first
time
that
ibm's
neural
chip
system
has
been
integrated
into
cutting-edge
technology
in
august
2017
samsung
installed
the
chips
in
its
dynamic
vision
sensors
enabling
cameras
to
capture
images
at
up
to
2000
fps
while
using
just
300
milliwatts
of
power
google
has
created
three
generations
of
a
similar
device
tensor
processing
unit
using
low
resolution
8
bit
computing
rather
than
a
spiking
neural
network
there
are
many
approaches
and
definitions
for
a
cognitive
computer
and
other
approaches
may
be
more
fruitful
than
the
others
specifically
there
are
critics
who
argue
that
a
room-sized
computer
-
like
the
case
of
watson
-
is
not
a
viable
alternative
to
a
three-pound
human
brain
some
also
cite
the
difficulty
for
a
single
system
to
bring
so
many
elements
together
such
as
the
disparate
sources
of
information
as
well
as
computing
resources
during
the
2018
world
economic
forum
there
are
experts
who
claim
that
cognitive
systems
could
adopt
the
biases
of
their
developers
and
this
was
demonstrated
in
the
case
of
the
google
image-recognition
or
computer
vision
algorithm
which
identified
african
americans
unfavorably
http://wwwfoxnewscom/tech/2018/01/09/ces-2018-intel-gives-glimpse-into-mind-blowing-future-computinghtml
microcomputer
a
microcomputer
is
a
small
relatively
inexpensive
computer
with
a
microprocessor
as
its
central
processing
unit
(cpu)
it
includes
a
microprocessor
memory
and
minimal
input/output
(i/o)
circuitry
mounted
on
a
single
printed
circuit
board
microcomputers
became
popular
in
the
1970s
and
1980s
with
the
advent
of
increasingly
powerful
microprocessors
the
predecessors
to
these
computers
mainframes
and
minicomputers
were
comparatively
much
larger
and
more
expensive
(though
indeed
present-day
mainframes
such
as
the
ibm
system
z
machines
use
one
or
more
custom
microprocessors
as
their
cpus)
many
microcomputers
(when
equipped
with
a
keyboard
and
screen
for
input
and
output)
are
also
personal
computers
(in
the
generic
sense)
the
abbreviation
"micro"
was
common
during
the
1970s
and
1980s
but
has
now
fallen
out
of
common
usage
the
term
"microcomputer"
came
into
popular
use
after
the
introduction
of
the
minicomputer
although
isaac
asimov
used
the
term
in
his
short
story
"the
dying
night"
as
early
as
1956
(published
in
"the
magazine
of
fantasy
and
science
fiction"
in
july
that
year)
most
notably
the
microcomputer
replaced
the
many
separate
components
that
made
up
the
minicomputer's
cpu
with
one
integrated
microprocessor
chip
the
french
developers
of
the
micral
n
(1973)
filed
their
patents
with
the
term
"micro-ordinateur"
a
literal
equivalent
of
"microcomputer"
to
designate
a
solid
state
machine
designed
with
a
microprocessor
in
the
usa
the
earliest
models
such
as
the
altair
8800
were
often
sold
as
kits
to
be
assembled
by
the
user
and
came
with
as
little
as
256
bytes
of
ram
and
no
input/output
devices
other
than
indicator
lights
and
switches
useful
as
a
proof
of
concept
to
demonstrate
what
such
a
simple
device
could
do
however
as
microprocessors
and
semiconductor
memory
became
less
expensive
microcomputers
in
turn
grew
cheaper
and
easier
to
use:
all
these
improvements
in
cost
and
usability
resulted
in
an
explosion
in
their
popularity
during
the
late
1970s
and
early
1980s
a
large
number
of
computer
makers
packaged
microcomputers
for
use
in
small
business
applications
by
1979
many
companies
such
as
cromemco
processor
technology
imsai
north
star
computers
southwest
technical
products
corporation
ohio
scientific
altos
computer
systems
morrow
designs
and
others
produced
systems
designed
either
for
a
resourceful
end
user
or
consulting
firm
to
deliver
business
systems
such
as
accounting
database
management
and
word
processing
to
small
businesses
this
allowed
businesses
unable
to
afford
leasing
of
a
minicomputer
or
time-sharing
service
the
opportunity
to
automate
business
functions
without
(usually)
hiring
a
full-time
staff
to
operate
the
computers
a
representative
system
of
this
era
would
have
used
an
s100
bus
an
8-bit
processor
such
as
an
intel
8080
or
zilog
z80
and
either
cp/m
or
mp/m
operating
system
the
increasing
availability
and
power
of
desktop
computers
for
personal
use
attracted
the
attention
of
more
software
developers
in
time
and
as
the
industry
matured
the
market
for
personal
computers
standardized
around
ibm
pc
compatibles
running
dos
and
later
windows
modern
desktop
computers
video
game
consoles
laptops
tablet
pcs
and
many
types
of
handheld
devices
including
mobile
phones
pocket
calculators
and
industrial
embedded
systems
may
all
be
considered
examples
of
microcomputers
according
to
the
definition
given
above
everyday
use
of
the
expression
"microcomputer"
(and
in
particular
the
"micro"
abbreviation)
has
declined
significantly
from
the
mid-1980s
and
has
declined
in
commonplace
usage
since
2000
the
term
is
most
commonly
associated
with
the
first
wave
of
all-in-one
8-bit
home
computers
and
small
business
microcomputers
(such
as
the
apple
ii
commodore
64
bbc
micro
and
trs
80)
although
or
perhaps
because
an
increasingly
diverse
range
of
modern
microprocessor-based
devices
fit
the
definition
of
"microcomputer"
they
are
no
longer
referred
to
as
such
in
everyday
speech
in
common
usage
"microcomputer"
has
been
largely
supplanted
by
the
term
"personal
computer"
or
"pc"
which
specifies
a
computer
that
has
been
designed
to
be
used
by
one
individual
at
a
time
a
term
first
coined
in
1959
ibm
first
promoted
the
term
"personal
computer"
to
differentiate
themselves
from
other
microcomputers
often
called
"home
computers"
and
also
ibm's
own
mainframes
and
minicomputers
however
following
its
release
the
ibm
pc
itself
was
widely
imitated
as
well
as
the
term
the
component
parts
were
commonly
available
to
producers
and
the
bios
was
reverse
engineered
through
cleanroom
design
techniques
ibm
pc
compatible
"clones"
became
commonplace
and
the
terms
"personal
computer"
and
especially
"pc"
stuck
with
the
general
public
often
specifically
for
a
dos
or
(nowadays)
windows-compatible
computer
since
the
advent
of
microcontrollers
(monolithic
integrated
circuits
containing
ram
rom
and
cpu
all
onboard)
the
term
"micro"
is
more
commonly
used
to
refer
to
that
meaning
monitors
keyboards
and
other
devices
for
input
and
output
may
be
integrated
or
separate
computer
memory
in
the
form
of
ram
and
at
least
one
other
less
volatile
memory
storage
device
are
usually
combined
with
the
cpu
on
a
system
bus
in
one
unit
other
devices
that
make
up
a
complete
microcomputer
system
include
batteries
a
power
supply
unit
a
keyboard
and
various
input/output
devices
used
to
convey
information
to
and
from
a
human
operator
(printers
monitors
human
interface
devices)
microcomputers
are
designed
to
serve
only
one
user
at
a
time
although
they
can
often
be
modified
with
software
or
hardware
to
concurrently
serve
more
than
one
user
microcomputers
fit
well
on
or
under
desks
or
tables
so
that
they
are
within
easy
access
of
users
bigger
computers
like
minicomputers
mainframes
and
supercomputers
take
up
large
cabinets
or
even
dedicated
rooms
a
microcomputer
comes
equipped
with
at
least
one
type
of
data
storage
usually
ram
although
some
microcomputers
(particularly
early
8-bit
home
micros)
perform
tasks
using
ram
alone
some
form
of
secondary
storage
is
normally
desirable
in
the
early
days
of
home
micros
this
was
often
a
data
cassette
deck
(in
many
cases
as
an
external
unit)
later
secondary
storage
(particularly
in
the
form
of
floppy
disk
and
hard
disk
drives)
were
built
into
the
microcomputer
case
although
they
did
not
contain
any
microprocessors
but
were
built
around
transistor-transistor
logic
(ttl)
hewlett-packard
calculators
as
far
back
as
1968
had
various
levels
of
programmability
comparable
to
microcomputers
the
hp
9100b
(1968)
had
rudimentary
conditional
(if)
statements
statement
line
numbers
jump
statements
(go
to)
registers
that
could
be
used
as
variables
and
primitive
subroutines
the
programming
language
resembled
assembly
language
in
many
ways
later
models
incrementally
added
more
features
including
the
basic
programming
language
(hp
9830a
in
1971)
some
models
had
tape
storage
and
small
printers
however
displays
were
limited
to
one
line
at
a
time
the
hp
9100a
was
referred
to
as
a
personal
computer
in
an
advertisement
in
a
1968
science
magazine
but
that
advertisement
was
quickly
dropped
hp
was
reluctant
to
sell
them
as
"computers"
because
the
perception
at
that
time
was
that
a
computer
had
to
be
big
in
size
to
be
powerful
and
thus
decided
to
market
them
as
calculators
additionally
at
that
time
people
were
more
likely
to
buy
calculators
than
computers
and
purchasing
agents
also
preferred
the
term
"calculator"
because
purchasing
a
"computer"
required
additional
layers
of
purchasing
authority
approvals
hp
virtual
museum
the
datapoint
2200
made
by
ctc
in
1970
was
also
comparable
to
microcomputers
while
it
contains
no
microprocessor
the
instruction
set
of
its
custom
ttl
processor
was
the
basis
of
the
instruction
set
for
the
intel
8008
and
for
practical
purposes
the
system
behaves
approximately
as
if
it
contains
an
8008
this
is
because
intel
was
the
contractor
in
charge
of
developing
the
datapoint's
cpu
but
ultimately
ctc
rejected
the
8008
design
because
it
needed
20
support
chips
another
early
system
the
kenbak-1
was
released
in
1971
like
the
datapoint
2200
it
used
discrete
transistor–transistor
logic
instead
of
a
microprocessor
but
it
functioned
like
a
microcomputer
in
some
ways
it
was
marketed
as
an
educational
and
hobbyist
tool
but
it
was
not
a
commercial
success;
production
ceased
shortly
after
introduction
in
late
1972
a
french
team
headed
by
françois
gernelle
within
a
small
company
réalisations
etudes
electroniqes
(r2e)
developed
and
patented
a
computer
based
on
a
microprocessor
–
the
intel
8008
8-bit
microprocessor
this
micral-n
was
marketed
in
early
1973
as
a
"micro-ordinateur"
or
"microcomputer"
mainly
for
scientific
and
process-control
applications
about
a
hundred
micral-n
were
installed
in
the
next
two
years
followed
by
a
new
version
based
on
the
intel
8080
meanwhile
another
french
team
developed
the
alvan
a
small
computer
for
office
automation
which
found
clients
in
banks
and
other
sectors
the
first
version
was
based
on
lsi
chips
with
an
intel
8008
as
peripheral
controller
(keyboard
monitor
and
printer)
before
adopting
the
zilog
z80
as
main
processor
in
late
1972
a
sacramento
state
university
team
led
by
bill
pentz
built
the
sac
state
8008
computer
able
to
handle
thousands
of
patients'
medical
records
the
sac
state
8008
was
designed
with
the
intel
8008
it
had
a
full
set
of
hardware
and
software
components:
a
disk
operating
system
included
in
a
series
of
programmable
read-only
memory
chips
(proms);
8
kilobytes
of
ram;
ibm's
basic
assembly
language
(bal);
a
hard
drive;
a
color
display;
a
printer
output;
a
150
bit/s
serial
interface
for
connecting
to
a
mainframe;
and
even
the
world's
first
microcomputer
front
panel
in
early
1973
sord
computer
corporation
(now
toshiba
personal
computer
system
corporation)
completed
the
smp80/08
which
used
the
intel
8008
microprocessor
the
smp80/08
however
did
not
have
a
commercial
release
after
the
first
general-purpose
microprocessor
the
intel
8080
was
announced
in
april
1974
sord
announced
the
smp80/x
the
first
microcomputer
to
use
the
8080
in
may
1974
virtually
all
early
microcomputers
were
essentially
boxes
with
lights
and
switches;
one
had
to
read
and
understand
binary
numbers
and
machine
language
to
program
and
use
them
(the
datapoint
2200
was
a
striking
exception
bearing
a
modern
design
based
on
a
monitor
keyboard
and
tape
and
disk
drives)
of
the
early
"box
of
switches"-type
microcomputers
the
mits
altair
8800
(1975)
was
arguably
the
most
famous
most
of
these
simple
early
microcomputers
were
sold
as
electronic
kits—bags
full
of
loose
components
which
the
buyer
had
to
solder
together
before
the
system
could
be
used
the
period
from
about
1971
to
1976
is
sometimes
called
the
of
microcomputers
many
companies
such
as
dec
national
semiconductor
texas
instruments
offered
their
microcomputers
for
use
in
terminal
control
peripheral
device
interface
control
and
industrial
machine
control
there
were
also
machines
for
engineering
development
and
hobbyist
personal
use
in
1975
the
processor
technology
sol-20
was
designed
which
consisted
of
one
board
which
included
all
the
parts
of
the
computer
system
the
sol-20
had
built-in
eprom
software
which
eliminated
the
need
for
rows
of
switches
and
lights
the
mits
altair
just
mentioned
played
an
instrumental
role
in
sparking
significant
hobbyist
interest
which
itself
eventually
led
to
the
founding
and
success
of
many
well-known
personal
computer
hardware
and
software
companies
such
as
microsoft
and
apple
computer
although
the
altair
itself
was
only
a
mild
commercial
success
it
helped
spark
a
huge
industry
by
1977
the
introduction
of
the
second
generation
known
as
home
computers
made
microcomputers
considerably
easier
to
use
than
their
predecessors
because
their
predecessors'
operation
often
demanded
thorough
familiarity
with
practical
electronics
the
ability
to
connect
to
a
monitor
(screen)
or
tv
set
allowed
visual
manipulation
of
text
and
numbers
the
basic
language
which
was
easier
to
learn
and
use
than
raw
machine
language
became
a
standard
feature
these
features
were
already
common
in
minicomputers
with
which
many
hobbyists
and
early
produces
were
familiar
in
1979
the
launch
of
the
visicalc
spreadsheet
(initially
for
the
apple
ii)
first
turned
the
microcomputer
from
a
hobby
for
computer
enthusiasts
into
a
business
tool
after
the
1981
release
by
ibm
of
its
ibm
pc
the
term
personal
computer
became
generally
used
for
microcomputers
compatible
with
the
ibm
pc
architecture
(pc
compatible)
data
(word)
the
word
data
has
generated
considerable
controversy
on
whether
it
is
an
uncountable
noun
used
with
verbs
conjugated
in
the
singular
or
should
be
treated
as
the
plural
of
the
now-rarely-used
"datum"
in
one
sense
"data"
is
the
plural
form
of
"datum"
"datum"
actually
can
also
be
a
count
noun
with
the
plural
"datums"
(see
usage
in
datum
article)
that
can
be
used
with
cardinal
numbers
(eg
"80
datums");
"data"
(originally
a
latin
plural)
is
not
used
like
a
normal
count
noun
with
cardinal
numbers
and
can
be
plural
with
such
plural
determiners
as
"these"
and
"many"
or
as
an
uncountable
noun
with
a
verb
in
the
singular
form
even
when
a
very
small
quantity
of
data
is
referenced
(one
number
for
example)
the
phrase
"piece
of
data"
is
often
used
as
opposed
to
"datum"
the
debate
over
appropriate
usage
continues
but
"data"
as
a
singular
form
is
far
more
common
in
english
the
word
"datum"
is
still
used
in
the
general
sense
of
"an
item
given"
in
cartography
geography
nuclear
magnetic
resonance
and
technical
drawing
it
is
often
used
to
refer
to
a
single
specific
reference
datum
from
which
distances
to
all
other
data
are
measured
any
measurement
or
result
is
a
"datum"
though
"data
point"
is
now
far
more
common
"data"
is
most
often
used
as
a
singular
mass
noun
in
everyday
usage
some
major
newspapers
such
as
"the
new
york
times"
use
it
either
in
the
singular
or
plural
in
the
"new
york
times"
the
phrases
"the
survey
data
are
still
being
analyzed"
and
"the
first
year
for
which
data
is
available"
have
appeared
within
one
day
the
"wall
street
journal"
explicitly
allows
this
usage
in
its
style
guide
the
associated
press
style
guide
classifies
"data"
as
a
collective
noun
that
takes
the
singular
when
treated
as
a
unit
but
the
plural
when
referring
to
individual
items
(eg
"the
data
is
sound"
and
"the
data
have
been
carefully
collected")
in
scientific
writing
"data"
is
often
treated
as
a
plural
as
in
"these
data
do
not
support
the
conclusions"
but
the
word
is
also
used
as
a
singular
mass
entity
like
"information"
for
instance
in
computing
and
related
disciplines
british
usage
now
widely
accepts
treating
"data"
as
singular
in
standard
english
including
everyday
newspaper
usage
at
least
in
non-scientific
use
uk
scientific
publishing
still
prefers
treating
it
as
a
plural
some
uk
university
style
guides
recommend
using
"data"
for
both
singular
and
plural
use
and
others
recommend
treating
it
only
as
a
singular
in
connection
with
computers
the
ieee
computer
society
allows
usage
of
"data"
as
either
a
mass
noun
or
plural
based
on
author
preference
while
ieee
in
the
editorial
style
manual
indicates
to
always
use
the
plural
form
some
professional
organizations
and
style
guides
require
that
authors
treat
"data"
as
a
plural
noun
for
example
the
air
force
flight
test
center
specifically
states
that
the
word
"data"
is
always
plural
never
singular
motion
history
images
the
motion
history
image
(mhi)
is
a
static
image
template
helps
in
understanding
the
motion
location
and
path
as
it
progresses
in
mhi
the
temporal
motion
information
is
collapsed
into
a
single
image
template
where
intensity
is
a
function
of
recency
of
motion
thus
the
mhi
pixel
intensity
is
a
function
of
the
motion
history
at
that
location
where
brighter
values
correspond
to
a
more
recent
motion
using
mhi
moving
parts
of
a
video
sequence
can
be
engraved
with
a
single
image
from
where
one
can
predict
the
motion
flow
as
well
as
the
moving
parts
of
the
video
action
some
important
features
of
the
mhi
representation
are:
for
each
time
"t"
list
of
countries
by
computer
exports
the
following
is
a
list
of
countries
by
computer
exports
data
is
for
2014
in
millions
of
united
states
dollars
as
reported
by
the
observatory
of
economic
complexity
currently
the
top
fifteen
countries
are
listed
atlasmediamitedu
-
observatory
of
economic
complexity
-
countries
that
export
computers
(2012)
computer
says
no
"computer
says
no"
or
the
"computer
says
no
attitude"
is
the
popular
name
given
to
an
attitude
seen
in
some
public-facing
organisations
where
the
default
response
to
a
customer’s
request
is
to
check
with
information
stored
on
or
generated
by
a
computer
and
then
make
decisions
based
on
that
often
in
the
face
of
common
sense
there
may
also
be
an
element
of
deliberate
unhelpfulness
towards
customers
and
service-users
whereby
more
"could"
be
done
to
reach
a
mutually
satisfactory
outcome
but
is
not
the
name
gained
popularity
through
the
british
sketch
comedy
"little
britain"
in
"little
britain"
"computer
says
no"
is
the
catchphrase
of
the
character
carol
beer
(played
by
david
walliams)
a
bank
worker
and
later
holiday
rep
and
hospital
receptionist
who
always
responds
to
a
customer's
enquiry
by
typing
it
into
her
computer
and
responding
with
"computer
says
no"
to
even
the
most
reasonable
of
requests
when
asked
to
do
something
aside
from
asking
the
computer
she
would
shrug
and
remain
obstinate
in
her
unhelpfulness
and
ultimately
cough
in
the
customer's
face
the
phrase
was
also
used
in
the
australian
soap
opera
"neighbours"
in
2006
as
a
reference
to
"little
britain"
the
"computer
says
no"
attitude
often
comes
from
larger
companies
that
rely
on
information
stored
electronically
when
this
information
is
not
updated
it
can
often
lead
to
refusals
of
financial
products
or
incorrect
information
being
sent
out
to
customers
these
situations
can
often
be
resolved
by
an
employee
updating
the
information;
however
when
this
cannot
be
done
easily
the
"computer
says
no"
attitude
can
be
viewed
as
becoming
prevalent
when
there
is
unhelpfulness
as
a
result
this
attitude
can
also
occur
when
an
employee
fails
to
read
human
emotion
in
the
customer
and
reacts
according
to
his
or
her
professional
training
or
relies
upon
a
script
this
attitude
also
crops
up
when
larger
companies
rely
on
computer
credit
scores
and
do
not
meet
with
a
customer
to
discuss
his
or
her
individual
needs
instead
basing
a
decision
upon
information
stored
in
computers
some
organisations
attempt
to
offset
this
attitude
by
moving
away
from
reliance
on
electronic
information
and
using
a
human
approach
towards
requests
"computer
says
no"
happens
in
a
more
literal
sense
when
computer
systems
employ
filters
that
prevent
messages
being
passed
along
as
when
these
messages
are
perceived
to
include
obscenities
when
information
is
not
passed
through
to
the
person
operating
the
computer
decisions
may
be
made
without
seeing
the
whole
picture
computer
rage
computer
rage
refers
to
negative
psychological
responses
towards
a
computer
due
to
heightened
anger
or
frustration
examples
of
computer
rage
include
cursing
or
yelling
at
a
computer
slamming
or
throwing
keyboards
and
mice
and
assaulting
the
computer
or
monitor
with
an
object
or
weapon
in
april
2015
a
colorado
man
was
cited
for
firing
a
gun
within
a
residential
area
when
he
took
his
computer
into
a
back
alley
and
shot
it
eight
times
with
a
9mm
pistol
when
questioned
he
told
police
that
he
had
become
so
frustrated
with
his
computer
that
he
had
"reached
critical
mass"
and
stated
that
after
he
had
shot
his
computer
"the
angels
sung
on
high"
in
2007
a
german
man
threw
his
computer
out
the
window
in
the
middle
of
the
night
startling
his
neighbors
german
police
were
sympathetic
and
did
not
press
charges
stating
"who
hasn't
felt
like
doing
that?"
in
2006
the
staged
surveillance
video
"bad
day"
showing
a
man
assaulting
his
computer
at
work
became
a
viral
hit
on
the
internet
reaching
over
two
million
views
other
instances
of
reported
computer
rage
have
ranged
from
a
restaurant
owner
who
threw
his
laptop
into
a
deep
fryer
to
an
individual
who
threw
his
computer
out
the
window
but
forgot
that
the
window
was
closed
in
1999
it
was
speculated
that
computer
rage
had
become
more
common
than
road
rage
in
traffic
but
in
a
2015
study
it
was
found
that
reported
rates
of
anger
when
using
a
computer
were
lower
than
reported
rates
of
anger
while
driving
however
reports
of
anger
while
driving
or
using
computers
were
found
to
be
far
more
common
than
anger
in
other
situations
in
a
2013
survey
of
american
adults
36%
of
respondents
who
reported
experiencing
computer
issues
also
reported
that
they
had
screamed
yelled
cursed
or
physically
assaulted
their
computers
within
the
last
six
months
in
2009
a
survey
was
conducted
with
british
computer
users
about
their
experiences
with
computers
this
survey
found
that
54%
of
respondents
reported
verbally
abusing
their
computers
and
40%
reported
that
they
had
become
physically
violent
toward
their
computers
the
survey
also
found
that
most
users
experienced
computer
rage
three
to
four
times
a
month
differences
in
types
of
computer
rage
have
also
been
found
between
different
geographical
regions
for
example
one
survey
found
that
individuals
from
london
have
been
found
to
be
five
times
more
likely
to
physically
assault
their
computers
while
those
from
yorkshire
and
humberside
were
found
to
be
more
likely
to
yell
at
their
computers
differences
have
also
been
observed
for
age
groups
as
younger
adults
(18–24
years
old)
have
reported
more
abusive
behaviors
in
the
face
of
computer
frustration
when
compared
to
older
adults
(over
35
years
old)
individuals
with
less
computer
experience
in
particular
have
also
been
reported
to
experience
increased
feelings
of
anger
and
helplessness
when
it
comes
to
computers
but
other
research
has
argued
that
it
is
the
self-efficacy
beliefs
about
computers
that
are
predictive
of
computer
frustration
not
the
amount
of
computer
experience
or
use
in
1999
professor
robert
j
edelmann
a
chartered
clinical
forensic
and
health
psychologist
and
a
fellow
of
the
british
psychological
society
was
offering
a
special
helpline
in
the
uk
for
those
suffering
from
technology
related
anger
users
can
experience
computer
anger
and
frustration
for
a
number
of
reasons
american
adults
surveyed
in
2013
reported
that
almost
half
(46%)
of
their
computer
problems
were
due
to
malware
or
computer
viruses
followed
by
software
issues
(10%)
and
not
enough
memory
(8%)
in
another
survey
users
reported
email
word
processors
web
browsing
operating
system
crashes
inability
to
locate
features
and
program
crashes
as
frequent
initiators
of
computer
frustration
these
technical
issues
paired
with
tight
timelines
poor
work
progress
and
failure
to
complete
a
computer
task
can
create
heightened
computer
anger
and
frustration
when
this
anger
and
frustration
exceeds
a
person's
control
it
can
turn
into
rage
research
on
emotion
has
shown
that
anger
is
often
caused
by
interruptions
of
plans
and
expectations
especially
through
the
violation
of
social
norms
this
sense
of
anger
can
be
magnified
when
the
individual
does
not
understand
why
they
are
unable
to
meet
their
goal
or
task
at
hand
or
why
there
was
a
violation
of
social
norms
psychologists
have
argued
that
this
is
particularly
relevant
to
computer
rage
as
computer
users
interact
with
computers
in
a
similar
manner
that
they
interact
with
other
people
(for
more
information
see
the
media
equation)
thus
when
computers
fail
to
function
in
the
face
of
incoming
deadlines
or
an
important
task
to
accomplish
users
can
feel
betrayed
by
the
computer
in
the
same
way
they
can
feel
betrayed
by
other
people
specifically
when
users
fail
to
understand
why
their
computer
will
not
work
properly
often
in
the
times
they
need
it
to
the
most
it
can
invoke
a
sense
of
hostility
as
it
is
interpreted
as
a
breach
of
social
norms
or
a
personal
attack
consistent
with
this
finding
perceived
betrayal
by
the
computer
can
also
elicit
other
negative
emotions
one
survey
of
us
adults
reported
that
10%
of
users
who
experience
computer
issues
experienced
feeling
helplessness
and
4%
reported
feeling
victimized
in
the
same
survey
7%
adults
ages
18–34
reported
that
they
had
cried
over
their
computer
problems
within
the
previous
six
months
computer
rage
can
result
in
damaged
property
or
physical
injuries
as
well
as
psychological
harm
some
experts
have
suggested
that
venting
frustrations
on
the
computer
may
have
some
benefits
but
other
experts
disagree
for
example
yelling
at
the
computer
has
been
suggested
as
a
way
to
moderate
one's
anger
to
avoid
the
ill
effects
of
anger
suppression
but
new
research
has
suggested
that
yelling
can
negatively
affect
health
in
itself
alternatively
releasing
anger
on
a
computer
has
been
viewed
as
advantageous
as
it
directs
this
rage
at
an
object
as
opposed
to
another
person
and
can
make
individuals
feel
better
afterwards
in
response
to
computer
issues
that
invoke
frustration
some
experts
have
suggested
walking
away
from
the
computer
for
15
minutes
to
"cool
off"
other
methods
to
prevent
computer
rage
can
be
backing
up
computer
data
often
increasing
memory
of
the
computer
and
even
imagining
pleasant
images
such
as
petting
an
animal
adopting
a
goal
of
improving
computer
knowledge
may
also
be
beneficial
as
users
are
less
likely
to
report
computer
rage
when
they
view
the
issue
as
a
challenge
and
not
as
a
setback
if
computer
rage
cannot
be
avoided
guidelines
on
how
to
rage
with
minimal
consequences
such
as
wearing
safety
goggles
and
taking
frustration
out
on
older
equipment
can
be
followed
to
reduce
the
likelihood
of
injury
and
significant
property
loss
employers
of
staff
who
work
with
computers
often
in
situations
where
time
is
crucial
can
take
steps
to
prevent
computer
rage
such
as
making
sure
there
is
adequate
software
and
providing
employees
with
anger
management
strategies
some
computer
technician
companies
have
reported
that
to
reduce
computer
rage
their
technicians
are
trained
on
how
to
work
with
customers
in
sensitive
psychological
states
just
as
much
as
how
to
diagnose
and
fix
technical
issues
designing
computer
interfaces
to
display
more
emotional
support
when
errors
occur
or
provide
therapy
strategies
has
also
been
suggested
as
a
way
to
mitigate
computer
anger
and
rage
the
application
of
affective
computing
has
been
shown
to
effectively
mitigate
negative
emotions
connected
to
computer
use
one
study
found
that
an
interface
that
sought
the
user's
feelings
provided
empathy
and
validated
reported
emotional
states
significantly
reduced
negative
emotions
associated
with
computer
frustration
for
users
another
study
found
that
when
error
messages
contain
positive
wording
("great
that
the
computer
will
soon
work
again")
compared
to
negative
wording
("this
is
frustrating")
or
a
neutral
error
message
users
exhibited
more
signs
of
happiness
pcaas
pcaas
or
"personal
computer
as
a
service"
is
a
personal
computer
hardware
and
optionally
software
leasing
licensing
and
delivery
model
in
which
personal
computer
and
optionally
software
(particularly
installed
on
the
pc)
are
leased
and
licensed
on
a
subscription
basis
the
subscription
often
includes
services
such
as
staging
imaging
maintenance
fix
logistics
services
and
may
also
be
bundled
with
helpdesk
services
data
backup
and
recovery
there
are
several
vendors
that
have
pcaas
offerings
including
bizbang
dell
hp
(they
call
theirs
device
as
a
service)
and
lenovo
(in
australia
only
for
now)
eniac
day
eniac
day
or
the
world’s
first
computer
day
is
celebrated
on
15
february
on
february
10
2011
the
city
of
philadelphia
officially
declared
that
february
15
2011
-
the
65th
anniversary
of
the
unveiling
of
the
electronic
numerical
integrator
and
computer
(eniac)
the
world's
first
general-purpose
electronic
computer
developed
at
the
university
of
pennsylvania's
moore
school
of
electrical
engineering
-
would
that
year
and
henceforth
be
known
as
eniac
day
computer
addiction
computer
addiction
can
be
described
as
the
excessive
or
compulsive
use
of
the
computer
which
persists
despite
serious
negative
consequences
for
personal
social
or
occupational
function
another
clear
conceptualization
is
made
by
block
who
stated
that
"conceptually
the
diagnosis
is
a
compulsive-impulsive
spectrum
disorder
that
involves
online
and/or
offline
computer
usage
and
consists
of
at
least
three
subtypes:
excessive
gaming
sexual
preoccupations
and
e-mail/text
messaging"
while
it
was
expected
that
this
new
type
of
addiction
would
find
a
place
under
the
compulsive
disorders
in
the
dsm-5
the
current
edition
of
the
"diagnostic
and
statistical
manual
of
mental
disorders"
it
is
still
counted
as
an
unofficial
disorder
the
concept
of
computer
addiction
is
broadly
divided
into
two
types
namely
offline
computer
addiction
and
online
computer
addiction
the
term
offline
computer
addiction
is
normally
used
when
speaking
about
excessive
gaming
behavior
which
can
be
practiced
both
offline
and
online
online
computer
addiction
also
known
as
internet
addiction
gets
more
attention
in
general
from
scientific
research
than
offline
computer
addiction
mainly
because
most
cases
of
computer
addiction
are
related
to
the
excessive
use
of
the
internet
although
addiction
is
usually
used
to
describe
dependence
on
substances
addiction
can
also
be
used
to
describe
pathological
internet
use
experts
on
internet
addiction
have
described
this
syndrome
as
an
individual
being
intensely
working
on
the
internet
prolonged
use
of
the
internet
uncontrollable
use
of
the
internet
unable
to
use
the
internet
with
efficient
time
not
being
interested
in
the
outside
world
not
spending
time
with
people
from
the
outside
world
and
an
increase
in
their
loneliness
and
dejection
however
simply
working
long
hours
on
the
computer
does
not
necessarily
mean
someone
is
addicted
excessive
computer
use
may
result
in
or
occur
with:
kimberly
young
indicates
that
previous
research
links
internet/computer
addiction
with
existing
mental
health
issues
most
notably
depression
she
states
that
computer
addiction
has
significant
effects
socially
such
as
low
self-esteem
psychologically
and
occupationally
which
led
many
subjects
to
academic
failure
according
to
a
korean
study
on
internet/computer
addiction
pathological
use
of
the
internet
results
in
negative
life
impacts
such
as
job
loss
marriage
breakdown
financial
debt
and
academic
failure
70%
of
internet
users
in
korea
are
reported
to
play
online
games
18%
of
which
are
diagnosed
as
game
addicts
which
relates
to
internet/computer
addiction
the
authors
of
the
article
conducted
a
study
using
kimberly
young's
questionnaire
the
study
showed
that
the
majority
of
those
who
met
the
requirements
of
internet/computer
addiction
suffered
from
interpersonal
difficulties
and
stress
and
that
those
addicted
to
online
games
specifically
responded
that
they
hoped
to
avoid
reality
computers
nowadays
rely
almost
entirely
on
the
internet
and
thus
relevant
research
articles
relating
to
internet
addiction
may
also
be
relevant
to
computer
addiction
many
studies
and
surveys
are
being
conducted
to
measure
the
extent
of
this
type
of
addiction
kimberly
young
has
created
a
questionnaire
based
on
other
disorders
to
assess
the
level
of
addiction
it
is
called
the
internet
addict
diagnostic
questionnaire
or
iadq
the
questionnaire
asks
users
about
their
online
usage
habits
as
well
as
their
feelings
about
their
internet
usage
according
to
the
iadq
sample
internet
addiction
resembles
that
of
a
gambling
disorder
answering
positively
to
five
out
of
the
eight
questions
on
the
iadq
may
be
indicative
of
an
online
addiction
observations
about
the
addictiveness
of
computers
and
more
specifically
computer
games
date
back
at
least
to
the
mid
1970s
addiction
and
addictive
behavior
was
common
among
the
users
of
the
plato
system
at
the
university
of
illinois
british
e-learning
academic
nicholas
rushby
suggested
in
his
1979
book
"an
introduction
to
educational
computing"
that
people
can
be
addicted
to
computers
and
suffer
withdrawal
symptoms
the
term
was
also
used
by
m
shotton
in
1989
in
her
book
"computer
addiction"
however
shotton
concludes
that
the
'addicts'
are
not
truly
addicted
dependency
on
computers
she
argues
is
better
understood
as
a
challenging
and
exiting
pastime
that
can
also
lead
to
a
professional
career
in
the
field
computers
do
not
turn
gregarious
extroverted
people
into
recluses;
instead
they
offer
introverts
a
source
of
inspiration
excitement
and
intellectual
stimulation
shotton's
work
seriously
questions
the
legitimacy
of
the
claim
that
computers
cause
addiction
the
term
became
more
widespread
with
the
explosive
growth
of
the
internet
as
well
the
availability
of
the
personal
computer
computers
and
the
internet
both
started
to
take
shape
as
a
personal
and
comfortable
medium
which
could
be
used
by
anyone
who
wanted
to
make
use
of
it
with
that
explosive
growth
of
individuals
making
use
of
pcs
and
the
internet
the
question
started
to
arise
whether
or
not
misuse
or
excessive
use
of
these
new
technologies
could
be
possible
as
well
it
was
hypothesized
that
like
any
technology
aimed
specifically
at
human
consumption
and
use
that
abuse
could
have
severe
consequences
for
the
individual
in
the
short
term
and
for
the
society
in
the
long
term
in
the
late
nineties
people
who
made
use
of
pcs
and
the
internet
where
already
referred
to
the
term
webaholics
or
cyberholics
pratarelli
et
al
suggested
at
that
point
already
to
label
"a
cluster
of
behaviors
potentially
causing
problems"
as
computer
or
internet
addiction
there
are
other
examples
of
computer
overuse
that
date
back
to
the
earliest
computer
games
press
reports
have
furthermore
noted
that
some
finnish
defence
forces
conscripts
were
not
mature
enough
to
meet
the
demands
of
military
life
and
were
required
to
interrupt
or
postpone
military
service
for
a
year
one
reported
source
of
the
lack
of
needed
social
skills
is
overuse
of
computer
games
or
the
internet
forbes
termed
this
overuse
"web
fixations"
and
stated
that
they
were
responsible
for
12
such
interruptions
or
deferrals
over
the
5
years
from
2000–2005
works
cited
laser
50
the
laser
50
is
an
educational
portable
computer
that
ran
the
basic
programming
language
released
in
1984
the
laser
50
used
a
zilog
z80
central
processing
unit
running
at
35
mhz
2
kb
to
18
kb
of
ram
a
12
kb
rom
and
a
80x7
dots
lcd
screen
computer
maintenance
computer
maintenance
is
the
practice
of
keeping
computers
in
a
good
state
of
repair
a
computer
containing
accumulated
dust
and
debris
may
not
run
properly
dust
and
debris
will
accumulate
as
a
result
of
air
cooling
any
filters
used
to
mitigate
this
need
regular
service
and
changes
if
the
cooling
system
is
not
filtered
then
regular
computer
cleaning
may
prevent
short
circuits
and
overheating
the
crumbs
dust
and
other
particulate
that
fall
between
the
keys
and
build
up
underneath
are
loosened
by
spraying
pressurized
air
into
the
keyboard
then
removed
with
a
low-pressure
vacuum
cleaner
a
plastic-cleaning
agent
applied
to
the
surface
of
the
keys
with
a
cloth
is
used
to
remove
the
accumulation
of
oil
and
dirt
from
repeated
contact
with
a
user's
fingertips
if
this
is
not
sufficient
for
a
more
severely
dirty
keyboard
keys
are
physically
removed
for
more
focused
individual
cleaning
or
for
better
access
to
the
area
beneath
finally
the
surface
is
wiped
with
a
disinfectant
a
monitor
displays
information
in
visual
form
using
text
and
graphics
the
portion
of
the
monitor
that
displays
the
information
is
called
the
screen
like
a
television
screen
a
computer
screen
can
show
still
or
moving
pictures
and
it’s
a
part
of
output
devices
fingerprints
water
spots
and
dust
are
removed
from
the
screen
with
a
cleaning
wipe
specialized
for
the
screen
type
(crt
lcd
etc)
a
general
plastic-cleaning
agent
is
used
on
the
outer
casing
which
requires
a
less
gentle
cleanser
but
may
need
more
focused
attention
to
unusual
buildups
of
dust
grime
pen
marks
etc
idiosyncratic
to
the
user
and
environment
the
top
surface
of
the
mouse
is
wiped
with
a
plastic
cleanser
to
remove
the
dirt
that
accumulates
from
contact
with
the
hand
as
on
the
keyboard
the
bottom
surface
is
also
cleaned
to
ensure
that
it
can
slide
freely
if
it
is
a
mechanical
mouse
the
trackball
is
taken
out
not
only
to
clean
the
ball
itself
but
to
scrape
dirt
from
the
runners
that
sense
the
ball's
movement
and
can
become
jittery
or
stuck
if
impeded
by
grime
internal
components
accumulate
dust
brought
in
by
the
airflow
maintained
by
fans
to
keep
the
pc
from
overheating
a
soft
brush
may
remove
loose
dirt;
the
remainder
is
dislodged
with
compressed
air
and
removed
with
a
low-pressure
vacuum
the
case
is
wiped
down
with
a
cleaning
agent
a
pressurized
blower
or
gas
duster
can
remove
dust
that
cannot
be
reached
with
a
brush
important
data
stored
on
computers
may
be
copied
and
archived
securely
so
that
in
the
event
of
failure
the
data
and
systems
may
be
reconstructed
when
major
maintenance
such
as
patching
is
performed
a
backup
is
recommended
as
the
first
step
in
case
the
update
fails
and
reversion
is
required
disk
cleanup
may
be
performed
as
regular
maintenance
to
remove
these
files
may
become
fragmented
and
so
slow
the
performance
of
the
computer
"disk
defragmentation"
may
be
performed
to
combine
these
fragments
and
so
improve
performance
in
the
usa
the
digital
millennium
copyright
act
specifically
exempts
computer-maintenance
activities
so
copies
of
copyright
files
may
be
made
in
the
course
of
maintenance
provided
that
they
are
destroyed
afterwards
operating-system
files
such
as
the
windows
registry
may
require
maintenance
a
utility
such
as
a
registry
cleaner
may
be
used
for
this
also
inbuilt
disk
defragmenter
will
also
help
software
packages
and
operating
systems
may
require
regular
updates
to
correct
software
bugs
and
to
address
security
weaknesses
maintaining
security
involves
vulnerability
management
and
installation
and
proper
operation
of
antivirus
softwares
like
kaspersky
avast
antivirus
mcafee
and
many
are
available
digital
electronic
computer
in
computer
science
a
digital
electronic
computer
is
a
computer
machine
which
is
both
an
electronic
computer
and
a
digital
computer
examples
of
a
digital
electronic
computers
include
the
ibm
pc
the
apple
macintosh
as
well
as
modern
smartphones
when
computers
that
were
both
digital
and
electronic
appeared
they
displaced
almost
all
other
kinds
of
computers
but
computation
has
historically
been
performed
in
various
non-digital
and
non-electronic
ways:
the
lehmer
sieve
is
an
example
of
a
digital
non-electronic
computer
while
analog
computers
are
examples
of
non-digital
computers
which
can
be
electronic
(with
analog
electronics)
and
mechanical
computers
are
examples
of
non-electronic
computers
(which
may
be
digital
or
not)
an
example
of
a
computer
which
is
both
non-digital
and
non-electronic
is
the
ancient
antikythera
mechanism
found
in
greece
all
kinds
of
computers
whether
they
are
digital
or
analog
and
electronic
or
non-electronic
can
be
turing
complete
if
they
have
sufficient
memory
a
digital
electronic
computer
is
not
necessarily
a
programmable
computer
a
stored
program
computer
or
a
general
purpose
computer
since
in
essence
a
digital
electronic
computer
can
be
built
for
one
specific
application
and
be
non-reprogrammable
as
of
2014
most
personal
computers
and
smartphones
in
people's
homes
that
use
multicore
central
processing
units
(such
as
amd
fx
intel
core
i7
or
the
multicore
varieties
of
arm-based
chips)
are
also
parallel
computers
using
the
mimd
(multiple
instructions
-
multiple
data)
paradigm
a
technology
previously
only
used
in
digital
electronic
supercomputers
as
of
2014
most
digital
electronic
supercomputers
are
also
cluster
computers
a
technology
that
can
be
used
at
home
in
the
form
of
small
beowulf
clusters
parallel
computation
is
also
possible
with
non-digital
or
non-electronic
computers
an
example
of
a
parallel
computation
system
using
the
abacus
would
be
a
group
of
human
computers
using
a
number
of
abacus
machines
for
computation
and
communicating
using
natural
language
a
digital
computer
can
perform
its
operations
in
the
decimal
system
in
binary
in
ternary
or
in
other
numeral
systems
as
of
2014
all
digital
electronic
computers
commonly
used
whether
personal
computers
or
supercomputers
are
working
in
the
binary
number
system
and
also
use
binary
logic
a
few
ternary
computers
using
ternary
logic
were
built
mainly
in
the
soviet
union
as
research
projects
a
digital
electronic
computer
is
not
necessarily
a
transistorized
computer:
before
the
advent
of
the
transistor
computers
used
vacuum
tubes
the
transistor
enabled
electronic
computers
to
become
much
more
powerful
and
recent
and
future
developments
in
digital
electronics
may
enable
humanity
to
build
even
more
powerful
electronic
computers
one
such
possible
development
is
the
memristor
people
living
in
the
beginning
of
the
21st
century
use
digital
electronic
computers
for
storing
data
such
as
photos
music
documents
and
for
performing
complex
mathematical
computations
or
for
communication
commonly
over
a
worldwide
computer
network
called
the
internet
which
connects
many
of
the
world's
computers
all
these
activities
made
possible
by
digital
electronic
computers
could
in
essence
be
performed
with
non-digital
or
non-electronic
computers
if
they
were
sufficiently
powerful
but
it
was
only
the
combination
of
electronics
technology
with
digital
computation
in
binary
that
enabled
humanity
to
reach
the
computation
power
necessary
for
today's
computing
advances
in
quantum
computing
dna
computing
optical
computing
or
other
technologies
could
lead
to
the
development
of
more
powerful
computers
in
the
future
digital
computers
are
inherently
best
described
by
discrete
mathematics
while
analog
computers
are
most
commonly
associated
with
continuous
mathematics
the
philosophy
of
digital
physics
views
the
universe
as
being
digital
konrad
zuse
wrote
a
book
known
as
"rechnender
raum"
in
which
he
described
the
whole
universe
as
one
all-encompassing
computer
octagon
systems
octagon
systems
corporation
is
an
industrial
computer
design
and
manufacturing
company
based
in
westminster
colorado
octagon
systems
designs
manufactures
sells
repairs
and
supports
its
line
of
industrial
mobile
and
rugged
computer
systems
for
industries
including
mining
military
transportation
and
others
the
company
has
international
representatives
in
africa
asia
europe
north
america
and
south
america
octagon
systems
was
founded
in
1981
and
introduced
an
embedded
computer
with
a
high
level
language
and
software
development
system
and
operating
systems
on
a
solid
state
disk
octagon’s
services
and
systems
grew
with
new
industrial
computer
system
solutions
including
use
in
the
std
bus
market
and
development
of
single-board
computers
octagon
systems
co-authored
the
epic
(form
factor)
epic™
embedded
computing
specification
that
became
a
world
standard
growing
applicational
use
of
octagon’s
products
led
to
use
in
areas
such
as
public
transportation
systems
rugged
computing
systems
for
mining
operations
as
well
as
others
octagon
systems
products
expanded
into
new
markets
as
the
use
of
industrial
transportation
and
rugged
computer
systems
became
increasingly
common
for
a
wide
array
of
applications
the
us
navy
chose
octagon’
products
for
a
major
long
term
contract
to
support
amphibious
warfare
computing
needs
and
octagon
products
have
been
deployed
in
more
than
100
mines
worldwide
octagon
systems’
xmb
mobile
servers
won
the
flagship
product
award
from
cots
journal
-
the
journal
of
military
electronics
computing
in
2006
octagon
systems
has
been
iso
certified
since
1993
octagon
systems
was
a
founding
member
of
the
small
form
factor
special
interest
group
in
2007
octagon
systems
co-authored
the
epic
(form
factor)
or
epic™
embedded
computing
specification
wdr
paper
computer
the
wdr
paper
computer
or
know-how
computer
was
a
"computer"
that
could
be
easily
assembled
from
a
sheet
of
paper
and
individual
matches
this
would
allow
anyone
interested
to
learn
how
to
program
without
having
an
electronic
computer
at
their
disposal
thus
this
"computer"
served
as
an
educational
aid
in
the
field
of
computer
science
the
know-how
computer
was
developed
by
wolfgang
back
and
ulrich
rohde
and
first
presented
in
the
television
program
wdr
computerclub
in
1983
he
was
also
published
in
german
magazines
mc
and
the
"computer"
worked
on
paper;
matches
were
used
as
information
units
only
five
commands
were
enough
to
represent
all
mathematical
functions
this
exercise
computer
on
paper
was
sent
in
over
400000
copies
and
belonged
at
that
time
to
the
computers
with
the
widest
circulation
an
implementation
as
a
computer
program
is
available
on
wolfgang
back's
website
the
method
of
operation
is
based
on
register
machine
but
is
more
to
the
approach
of
shepherdson
and
sturgis
a
derived
version
of
the
"paper
computer"
is
used
as
a
"know
how
computer"
in
namibian
school
education
wevolver
wevolver
provides
access
to
high
quality
engineering
projects
to
help
people
develop
better
technology
their
stated
mission
is
to
"enable
anyone
anywhere
to
develop
hardware
that
improves
life"
wevolver
10
was
a
project
repository
for
strictly
open
source
projects
but
in
20
it
has
been
expanded
to
include
a
version
control
system
and
is
available
for
use
by
private
teams
in
addition
to
open
source
projects
among
the
projects
using
their
version
control
system
are
openrov
plen
exiii
hackberry
and
inmoov
in
interviews
wevolver
team
members
have
said
the
strongest
part
of
their
platform
is
their
community
with
over
300k
followers
on
instagram
three
news
blogs
and
myriad
hosted
projects
they've
said
the
most
important
part
of
any
open-source
platform
is
the
people
using
it
watiac
watiac
was
a
virtual
computer
developed
for
teaching
the
principles
of
assembly
language
programming
to
undergraduates
watiac
and
the
watmap
assembly
language
that
ran
on
it
were
developed
in
1973
by
the
newly
founded
computer
systems
group
at
the
university
of
waterloo
under
the
direction
of
wes
graham
in
the
1970s
most
programming
was
conducted
through
batch
stream
processing
where
the
operating
systems
of
the
day
like
ibm`s
os-360
would
allow
a
single
program
to
use
all
the
resources
of
a
large
computer
for
a
limited
period
of
time
since
student
programs
were
only
run
a
few
times
possibly
only
once
after
they
had
been
successfully
written
and
debugged
efficient
running
of
those
programs
was
of
relatively
little
importance
compared
with
quick
compilation
and
relatively
good
error
messages
waterloo
had
been
a
leader
in
writing
single
pass
compile-and-go
teaching
compilers
with
first
its
watfor
fortran
compiler
and
its
watbol
cobol
compiler
watmap
was
developed
to
be
a
similar
compile-and-go
teaching
compiler
stanford
dash
stanford
dash
was
a
cache
coherent
multiprocessor
developed
in
the
late
1980s
by
a
group
led
by
anoop
gupta
john
l
hennessy
mark
horowitz
and
monica
s
lam
at
stanford
university
it
was
based
on
adding
a
pair
of
directory
boards
designed
at
stanford
to
up
to
16
sgi
iris
4d
power
series
machines
and
then
cabling
the
systems
in
a
mesh
topology
using
a
stanford-modified
version
of
the
torus
routing
chip
the
boards
designed
at
stanford
implemented
a
directory-based
cache
coherence
protocol
allowing
stanford
dash
to
support
distributed
shared
memory
for
up
to
64
processors
stanford
dash
was
also
notable
for
both
supporting
and
helping
to
formalize
weak
memory
consistency
models
including
release
consistency
because
stanford
dash
was
the
first
operational
machine
to
include
scalable
cache
coherence
it
influenced
subsequent
computer
science
research
as
well
as
the
commercially
available
sgi
origin
2000
stanford
dash
is
included
in
the
25th
anniversary
retrospective
of
selected
papers
from
the
international
symposium
on
computer
architecture
and
several
computer
science
books
has
been
simulated
by
the
university
of
edinburgh
and
is
used
as
a
case
study
in
contemporary
computer
science
classes
computer
programming
computer
programming
is
the
process
of
designing
and
building
an
executable
computer
program
for
accomplishing
a
specific
computing
task
programming
involves
tasks
such
as
analysis
generating
algorithms
profiling
algorithms'
accuracy
and
resource
consumption
and
the
implementation
of
algorithms
in
a
chosen
programming
language
(commonly
referred
to
as
coding)
the
source
code
of
a
program
is
written
in
one
or
more
programming
languages
the
purpose
of
programming
is
to
find
a
sequence
of
instructions
that
will
automate
the
performance
of
a
task
for
solving
a
given
problem
the
process
of
programming
thus
often
requires
expertise
in
several
different
subjects
including
knowledge
of
the
application
domain
specialized
algorithms
and
formal
logic
related
programming
tasks
include
testing
debugging
maintaining
a
program's
source
code
implementation
of
build
systems
and
management
of
derived
artifacts
such
as
machine
code
of
computer
programs
these
might
be
considered
part
of
the
programming
process
but
often
the
term
"software
development"
is
used
for
this
larger
process
with
the
term
"programming"
"implementation"
or
"coding"
reserved
for
the
actual
writing
of
source
code
software
engineering
combines
engineering
techniques
with
software
development
practices
programmable
devices
have
existed
at
least
as
far
back
as
1206
ad
when
the
automata
of
al-jazari
were
programmable
via
pegs
and
cams
to
play
various
rhythms
and
drum
patterns;
and
the
1801
jacquard
loom
could
produce
entirely
different
weaves
by
changing
the
"program"
-
a
series
of
pasteboard
cards
with
holes
punched
in
them
however
the
first
computer
program
is
generally
dated
to
1843
when
mathematician
ada
lovelace
published
an
algorithm
to
calculate
a
sequence
of
bernoulli
numbers
intended
to
be
carried
out
by
charles
babbage's
analytical
engine
women
would
continue
to
dominate
the
field
of
computer
programming
until
the
mid
1960s
in
the
1880s
herman
hollerith
invented
the
concept
of
storing
"data"
in
machine-readable
form
later
a
control
panel
(plugboard)
added
to
his
1906
type
i
tabulator
allowed
it
to
be
programmed
for
different
jobs
and
by
the
late
1940s
unit
record
equipment
such
as
the
ibm
602
and
ibm
604
were
programmed
by
control
panels
in
a
similar
way;
as
were
the
first
electronic
computers
however
with
the
concept
of
the
stored-program
computers
introduced
in
1949
both
programs
and
data
were
stored
and
manipulated
in
the
same
way
in
computer
memory
machine
code
was
the
language
of
early
programs
written
in
the
instruction
set
of
the
particular
machine
often
in
binary
notation
assembly
languages
were
soon
developed
that
let
the
programmer
specify
instruction
in
a
text
format
(eg
add
x
total)
with
abbreviations
for
each
operation
code
and
meaningful
names
for
specifying
addresses
however
because
an
assembly
language
is
little
more
than
a
different
notation
for
a
machine
language
any
two
machines
with
different
instruction
sets
also
have
different
assembly
languages
kathleen
booth
created
one
of
the
first
assembly
languages
in
1950
for
various
computers
at
birkbeck
college
high-level
languages
allow
the
programmer
to
write
programs
in
terms
that
are
syntactically
richer
and
more
capable
of
abstracting
the
code
making
it
targetable
to
varying
machine
instruction
sets
via
compilation
declarations
and
heuristics
the
first
compiler
for
a
programming
language
was
developed
by
grace
hopper
when
hopper
went
to
work
on
univac
in
1949
she
brought
the
idea
of
using
compilers
with
her
compilers
harness
the
power
of
computers
to
make
programming
easier
by
allowing
programmers
to
specify
calculations
by
entering
a
formula
using
infix
notation
(eg
)
for
example
fortran
the
first
widely
used
high-level
language
to
have
a
functional
implementation
which
permitted
the
abstraction
of
reusable
blocks
of
code
came
out
in
1957
in
1951
frances
e
holberton
developed
the
first
sort-merge
generator
which
ran
on
the
univac
i
another
woman
working
at
univac
adele
mildred
koss
developed
a
program
that
was
a
precursor
to
report
generators
in
russia
kateryna
yushchenko
developed
the
address
programming
language
for
the
mesm
in
1955
the
idea
for
the
creation
of
cobol
started
in
1959
when
mary
k
hawes
who
worked
for
burroughs
corporation
set
up
a
meeting
to
discuss
creating
a
common
business
language
she
invited
six
people
including
grace
hopper
hopper
was
involved
in
developing
cobol
as
a
business
language
and
creating
"self-documenting"
programming
hopper's
contribution
to
cobol
was
based
on
her
programming
language
called
flow-matic
in
1961
jean
e
sammet
developed
formac
and
also
published
"programming
languages:
history
and
fundamentals"
which
went
on
to
be
a
standard
work
on
programming
languages
programs
were
mostly
still
entered
using
punched
cards
or
paper
tape
see
computer
programming
in
the
punch
card
era
by
the
late
1960s
data
storage
devices
and
computer
terminals
became
inexpensive
enough
that
programs
could
be
created
by
typing
directly
into
the
computers
frances
holberton
created
a
code
to
allow
keyboard
inputs
while
she
worked
at
univac
text
editors
were
developed
that
allowed
changes
and
corrections
to
be
made
much
more
easily
than
with
punched
cards
sister
mary
kenneth
keller
worked
on
developing
the
programming
language
basic
which
she
was
a
graduate
student
at
dartmouth
in
the
1960s
one
of
the
first
object-oriented
programming
languages
smalltalk
was
developed
by
seven
programmers
including
adele
goldberg
in
the
1970s
in
1985
radia
perlman
developed
the
spinning
tree
protocol
in
order
to
route
packets
of
network
information
efficiently
whatever
the
approach
to
development
may
be
the
final
program
must
satisfy
some
fundamental
properties
the
following
properties
are
among
the
most
important:
in
computer
programming
readability
refers
to
the
ease
with
which
a
human
reader
can
comprehend
the
purpose
control
flow
and
operation
of
source
code
it
affects
the
aspects
of
quality
above
including
portability
usability
and
most
importantly
maintainability
readability
is
important
because
programmers
spend
the
majority
of
their
time
reading
trying
to
understand
and
modifying
existing
source
code
rather
than
writing
new
source
code
unreadable
code
often
leads
to
bugs
inefficiencies
and
duplicated
code
a
study
found
that
a
few
simple
readability
transformations
made
code
shorter
and
drastically
reduced
the
time
to
understand
it
following
a
consistent
programming
style
often
helps
readability
however
readability
is
more
than
just
programming
style
many
factors
having
little
or
nothing
to
do
with
the
ability
of
the
computer
to
efficiently
compile
and
execute
the
code
contribute
to
readability
some
of
these
factors
include:
the
presentation
aspects
of
this
(such
as
indents
line
breaks
color
highlighting
and
so
on)
are
often
handled
by
the
source
code
editor
but
the
content
aspects
reflect
the
programmer's
talent
and
skills
various
visual
programming
languages
have
also
been
developed
with
the
intent
to
resolve
readability
concerns
by
adopting
non-traditional
approaches
to
code
structure
and
display
integrated
development
environments
(ides)
aim
to
integrate
all
such
help
techniques
like
code
refactoring
can
enhance
readability
the
academic
field
and
the
engineering
practice
of
computer
programming
are
both
largely
concerned
with
discovering
and
implementing
the
most
efficient
algorithms
for
a
given
class
of
problem
for
this
purpose
algorithms
are
classified
into
"orders"
using
so-called
big
o
notation
which
expresses
resource
use
such
as
execution
time
or
memory
consumption
in
terms
of
the
size
of
an
input
expert
programmers
are
familiar
with
a
variety
of
well-established
algorithms
and
their
respective
complexities
and
use
this
knowledge
to
choose
algorithms
that
are
best
suited
to
the
circumstances
"programming
a
computer
for
playing
chess"
was
a
1950
paper
that
evaluated
a
"minimax"
algorithm
that
is
part
of
the
history
of
algorithmic
complexity;
a
course
on
ibm's
deep
blue
(chess
computer)
is
part
of
the
computer
science
curriculum
at
stanford
university
the
first
step
in
most
formal
software
development
processes
is
requirements
analysis
followed
by
testing
to
determine
value
modeling
implementation
and
failure
elimination
(debugging)
there
exist
a
lot
of
differing
approaches
for
each
of
those
tasks
one
approach
popular
for
requirements
analysis
is
use
case
analysis
many
programmers
use
forms
of
agile
software
development
where
the
various
stages
of
formal
software
development
are
more
integrated
together
into
short
cycles
that
take
a
few
weeks
rather
than
years
there
are
many
approaches
to
the
software
development
process
popular
modeling
techniques
include
object-oriented
analysis
and
design
(ooad)
and
model-driven
architecture
(mda)
the
unified
modeling
language
(uml)
is
a
notation
used
for
both
the
ooad
and
mda
a
similar
technique
used
for
database
design
is
entity-relationship
modeling
(er
modeling)
implementation
techniques
include
imperative
languages
(object-oriented
or
procedural)
functional
languages
and
logic
languages
it
is
very
difficult
to
determine
what
are
the
most
popular
of
modern
programming
languages
methods
of
measuring
programming
language
popularity
include:
counting
the
number
of
job
advertisements
that
mention
the
language
the
number
of
books
sold
and
courses
teaching
the
language
(this
overestimates
the
importance
of
newer
languages)
and
estimates
of
the
number
of
existing
lines
of
code
written
in
the
language
(this
underestimates
the
number
of
users
of
business
languages
such
as
cobol)
some
languages
are
very
popular
for
particular
kinds
of
applications
while
some
languages
are
regularly
used
to
write
many
different
kinds
of
applications
for
example
cobol
is
still
strong
in
corporate
data
centers
often
on
large
mainframe
computers
fortran
in
engineering
applications
scripting
languages
in
web
development
and
c
in
embedded
software
many
applications
use
a
mix
of
several
languages
in
their
construction
and
use
new
languages
are
generally
designed
around
the
syntax
of
a
prior
language
with
new
functionality
added
(for
example
c++
adds
object-orientation
to
c
and
java
adds
memory
management
and
bytecode
to
c++
but
as
a
result
loses
efficiency
and
the
ability
for
low-level
manipulation)
debugging
is
a
very
important
task
in
the
software
development
process
since
having
defects
in
a
program
can
have
significant
consequences
for
its
users
some
languages
are
more
prone
to
some
kinds
of
faults
because
their
specification
does
not
require
compilers
to
perform
as
much
checking
as
other
languages
use
of
a
static
code
analysis
tool
can
help
detect
some
possible
problems
normally
the
first
step
in
debugging
is
to
attempt
to
reproduce
the
problem
this
can
be
a
non-trivial
task
for
example
as
with
parallel
processes
or
some
unusual
software
bugs
also
specific
user
environment
and
usage
history
can
make
it
difficult
to
reproduce
the
problem
after
the
bug
is
reproduced
the
input
of
the
program
may
need
to
be
simplified
to
make
it
easier
to
debug
for
example
a
bug
in
a
compiler
can
make
it
crash
when
parsing
some
large
source
file
however
after
simplification
of
the
test
case
only
few
lines
from
the
original
source
file
can
be
sufficient
to
reproduce
the
same
crash
such
simplification
can
be
done
manually
using
a
divide-and-conquer
approach
the
programmer
will
try
to
remove
some
parts
of
original
test
case
and
check
if
the
problem
still
exists
when
debugging
the
problem
in
a
gui
the
programmer
can
try
to
skip
some
user
interaction
from
the
original
problem
description
and
check
if
remaining
actions
are
sufficient
for
bugs
to
appear
debugging
is
often
done
with
ides
like
eclipse
visual
studio
xcode
kdevelop
netbeans
and
standalone
debuggers
like
gdb
are
also
used
and
these
often
provide
less
of
a
visual
environment
usually
using
a
command
line
some
text
editors
such
as
emacs
allow
gdb
to
be
invoked
through
them
to
provide
a
visual
environment
different
programming
languages
support
different
styles
of
programming
(called
"programming
paradigms")
the
choice
of
language
used
is
subject
to
many
considerations
such
as
company
policy
suitability
to
task
availability
of
third-party
packages
or
individual
preference
ideally
the
programming
language
best
suited
for
the
task
at
hand
will
be
selected
trade-offs
from
this
ideal
involve
finding
enough
programmers
who
know
the
language
to
build
a
team
the
availability
of
compilers
for
that
language
and
the
efficiency
with
which
programs
written
in
a
given
language
execute
languages
form
an
approximate
spectrum
from
"low-level"
to
"high-level";
"low-level"
languages
are
typically
more
machine-oriented
and
faster
to
execute
whereas
"high-level"
languages
are
more
abstract
and
easier
to
use
but
execute
less
quickly
it
is
usually
easier
to
code
in
"high-level"
languages
than
in
"low-level"
ones
allen
downey
in
his
book
"how
to
think
like
a
computer
scientist"
writes:
many
computer
languages
provide
a
mechanism
to
call
functions
provided
by
shared
libraries
provided
the
functions
in
a
library
follow
the
appropriate
run-time
conventions
(eg
method
of
passing
arguments)
then
these
functions
may
be
written
in
any
other
language
computer
programmers
are
those
who
write
computer
software
their
jobs
usually
involve:
information
technology
information
technology
(it)
is
the
use
of
computers
to
store
retrieve
transmit
and
manipulate
data
or
information
often
in
the
context
of
a
business
or
other
enterprise
it
is
considered
to
be
a
subset
of
information
and
communications
technology
(ict)
an
information
technology
system
(it
system)
is
generally
an
information
system
a
communications
system
or
more
specifically
speaking
a
computer
system
–
including
all
hardware
software
and
peripheral
equipment
–
operated
by
a
limited
group
of
users
humans
have
been
storing
retrieving
manipulating
and
communicating
information
since
the
sumerians
in
mesopotamia
developed
writing
in
about
3000 bc
but
the
term
"information
technology"
in
its
modern
sense
first
appeared
in
a
1958
article
published
in
the
"harvard
business
review";
authors
harold
j
leavitt
and
thomas
l
whisler
commented
that
"the
new
technology
does
not
yet
have
a
single
established
name
we
shall
call
it
information
technology
(it)"
their
definition
consists
of
three
categories:
techniques
for
processing
the
application
of
statistical
and
mathematical
methods
to
decision-making
and
the
simulation
of
higher-order
thinking
through
computer
programs
the
term
is
commonly
used
as
a
synonym
for
computers
and
computer
networks
but
it
also
encompasses
other
information
distribution
technologies
such
as
television
and
telephones
several
products
or
services
within
an
economy
are
associated
with
information
technology
including
computer
hardware
software
electronics
semiconductors
internet
telecom
equipment
and
e-commerce
based
on
the
storage
and
processing
technologies
employed
it
is
possible
to
distinguish
four
distinct
phases
of
it
development:
pre-mechanical
(3000 bc –
1450 ad)
mechanical
(1450–1840)
electromechanical
(1840–1940)
and
electronic
(1940–present)
this
article
focuses
on
the
most
recent
period
(electronic)
which
began
in
about
1940
devices
have
been
used
to
aid
computation
for
thousands
of
years
probably
initially
in
the
form
of
a
tally
stick
the
antikythera
mechanism
dating
from
about
the
beginning
of
the
first
century
bc
is
generally
considered
to
be
the
earliest
known
mechanical
analog
computer
and
the
earliest
known
geared
mechanism
comparable
geared
devices
did
not
emerge
in
europe
until
the
16th
century
and
it
was
not
until
1645
that
the
first
mechanical
calculator
capable
of
performing
the
four
basic
arithmetical
operations
was
developed
electronic
computers
using
either
relays
or
valves
began
to
appear
in
the
early
1940s
the
electromechanical
zuse
z3
completed
in
1941
was
the
world's
first
programmable
computer
and
by
modern
standards
one
of
the
first
machines
that
could
be
considered
a
complete
computing
machine
colossus
developed
during
the
second
world
war
to
decrypt
german
messages
was
the
first
electronic
digital
computer
although
it
was
programmable
it
was
not
general-purpose
being
designed
to
perform
only
a
single
task
it
also
lacked
the
ability
to
store
its
program
in
memory;
programming
was
carried
out
using
plugs
and
switches
to
alter
the
internal
wiring
the
first
recognisably
modern
electronic
digital
stored-program
computer
was
the
manchester
baby
which
ran
its
first
program
on
21
june
1948
the
development
of
transistors
in
the
late
1940s
at
bell
laboratories
allowed
a
new
generation
of
computers
to
be
designed
with
greatly
reduced
power
consumption
the
first
commercially
available
stored-program
computer
the
ferranti
mark
i
contained
4050
valves
and
had
a
power
consumption
of
25
kilowatts
by
comparison
the
first
transistorised
computer
developed
at
the
university
of
manchester
and
operational
by
november
1953
consumed
only
150
watts
in
its
final
version
early
electronic
computers
such
as
colossus
made
use
of
punched
tape
a
long
strip
of
paper
on
which
data
was
represented
by
a
series
of
holes
a
technology
now
obsolete
electronic
data
storage
which
is
used
in
modern
computers
dates
from
world
war
ii
when
a
form
of
delay
line
memory
was
developed
to
remove
the
clutter
from
radar
signals
the
first
practical
application
of
which
was
the
mercury
delay
line
the
first
random-access
digital
storage
device
was
the
williams
tube
based
on
a
standard
cathode
ray
tube
but
the
information
stored
in
it
and
delay
line
memory
was
volatile
in
that
it
had
to
be
continuously
refreshed
and
thus
was
lost
once
power
was
removed
the
earliest
form
of
non-volatile
computer
storage
was
the
magnetic
drum
invented
in
1932
and
used
in
the
ferranti
mark
1
the
world's
first
commercially
available
general-purpose
electronic
computer
ibm
introduced
the
first
hard
disk
drive
in
1956
as
a
component
of
their
305
ramac
computer
system
most
digital
data
today
is
still
stored
magnetically
on
hard
disks
or
optically
on
media
such
as
cd-roms
until
2002
most
information
was
stored
on
analog
devices
but
that
year
digital
storage
capacity
exceeded
analog
for
the
first
time
as
of
2007
almost
94%
of
the
data
stored
worldwide
was
held
digitally:
52%
on
hard
disks
28%
on
optical
devices
and
11%
on
digital
magnetic
tape
it
has
been
estimated
that
the
worldwide
capacity
to
store
information
on
electronic
devices
grew
from
less
than
3
exabytes
in
1986
to
295
exabytes
in
2007
doubling
roughly
every
3
years
database
management
systems
emerged
in
the
1960s
to
address
the
problem
of
storing
and
retrieving
large
amounts
of
data
accurately
and
quickly
one
of
the
earliest
such
systems
was
ibm's
information
management
system
(ims)
which
is
still
widely
deployed
more
than
50
years
later
ims
stores
data
hierarchically
but
in
the
1970s
ted
codd
proposed
an
alternative
relational
storage
model
based
on
set
theory
and
predicate
logic
and
the
familiar
concepts
of
tables
rows
and
columns
the
first
commercially
available
relational
database
management
system
(rdbms)
was
available
from
oracle
in
1981
all
database
management
systems
consist
of
a
number
of
components
that
together
allow
the
data
they
store
to
be
accessed
simultaneously
by
many
users
while
maintaining
its
integrity
a
characteristic
of
all
databases
is
that
the
structure
of
the
data
they
contain
is
defined
and
stored
separately
from
the
data
itself
in
a
database
schema
the
extensible
markup
language
(xml)
has
become
a
popular
format
for
data
representation
in
recent
years
although
xml
data
can
be
stored
in
normal
file
systems
it
is
commonly
held
in
relational
databases
to
take
advantage
of
their
"robust
implementation
verified
by
years
of
both
theoretical
and
practical
effort"
as
an
evolution
of
the
standard
generalized
markup
language
(sgml)
xml's
text-based
structure
offers
the
advantage
of
being
both
machine
and
human-readable
the
relational
database
model
introduced
a
programming-language
independent
structured
query
language
(sql)
based
on
relational
algebra
the
terms
"data"
and
"information"
are
not
synonymous
anything
stored
is
data
but
it
only
becomes
information
when
it
is
organized
and
presented
meaningfully
most
of
the
world's
digital
data
is
unstructured
and
stored
in
a
variety
of
different
physical
formats
even
within
a
single
organization
data
warehouses
began
to
be
developed
in
the
1980s
to
integrate
these
disparate
stores
they
typically
contain
data
extracted
from
various
sources
including
external
sources
such
as
the
internet
organized
in
such
a
way
as
to
facilitate
decision
support
systems
(dss)
data
transmission
has
three
aspects:
transmission
propagation
and
reception
it
can
be
broadly
categorized
as
broadcasting
in
which
information
is
transmitted
unidirectionally
downstream
or
telecommunications
with
bidirectional
upstream
and
downstream
channels
xml
has
been
increasingly
employed
as
a
means
of
data
interchange
since
the
early
2000s
particularly
for
machine-oriented
interactions
such
as
those
involved
in
web-oriented
protocols
such
as
soap
describing
"data-in-transit
rather
than 
data-at-rest"
one
of
the
challenges
of
such
usage
is
converting
data
from
relational
databases
into
xml
document
object
model
(dom)
structures
hilbert
and
lopez
identify
the
exponential
pace
of
technological
change
(a
kind
of
moore's
law):
machines'
application-specific
capacity
to
compute
information
per
capita
roughly
doubled
every
14
months
between
1986
and
2007;
the
per
capita
capacity
of
the
world's
general-purpose
computers
doubled
every
18
months
during
the
same
two
decades;
the
global
telecommunication
capacity
per
capita
doubled
every
34
months;
the
world's
storage
capacity
per
capita
required
roughly
40
months
to
double
(every
3
years);
and
per
capita
broadcast
information
has
doubled
every
123
years
massive
amounts
of
data
are
stored
worldwide
every
day
but
unless
it
can
be
analysed
and
presented
effectively
it
essentially
resides
in
what
have
been
called
data
tombs:
"data
archives
that
are
seldom
visited"
to
address
that
issue
the
field
of
data
mining –
"the
process
of
discovering
interesting
patterns
and
knowledge
from
large
amounts
of
data" –
emerged
in
the
late
1980s
in
an
academic
context
the
association
for
computing
machinery
defines
it
as
"undergraduate
degree
programs
that
prepare
students
to
meet
the
computer
technology
needs
of
business
government
healthcare
schools
and
other
kinds
of
organizations 
it
specialists
assume
responsibility
for
selecting
hardware
and
software
products
appropriate
for
an
organization
integrating
those
products
with
organizational
needs
and
infrastructure
and
installing
customizing
and
maintaining
those
applications
for
the
organization’s
computer
users"
companies
in
the
information
technology
field
are
often
discussed
as
a
group
as
the
"tech
sector"
or
the
"tech
industry"
in
a
business
context
the
information
technology
association
of
america
has
defined
information
technology
as
"the
study
design
development
application
implementation
support
or
management
of
computer-based
information
systems"
the
responsibilities
of
those
working
in
the
field
include
network
administration
software
development
and
installation
and
the
planning
and
management
of
an
organization's
technology
life
cycle
by
which
hardware
and
software
are
maintained
upgraded
and
replaced
the
business
value
of
information
technology
lies
in
the
automation
of
business
processes
provision
of
information
for
decision
making
connecting
businesses
with
their
customers
and
the
provision
of
productivity
tools
to
increase
efficiency
the
field
of
information
ethics
was
established
by
mathematician
norbert
wiener
in
the
1940s
some
of
the
ethical
issues
associated
with
the
use
of
information
technology
include:
lithotope
a
lithotope
is
either
an
environment
in
which
a
sediment
was
deposited
or
an
area
of
uniform
sedimentation
1
surface
or
area
of
uniform
precipitation
2
sediment
having
a
relatively
homogeneous
sedimentation
environment
atmosphere
of
earth
the
atmosphere
of
earth
is
the
layer
of
gases
commonly
known
as
air
that
surrounds
the
planet
earth
and
is
retained
by
earth's
gravity
the
atmosphere
of
earth
protects
life
on
earth
by
creating
pressure
allowing
for
liquid
water
to
exist
on
the
earth's
surface
absorbing
ultraviolet
solar
radiation
warming
the
surface
through
heat
retention
(greenhouse
effect)
and
reducing
temperature
extremes
between
day
and
night
(the
diurnal
temperature
variation)
by
volume
dry
air
contains
7809%
nitrogen
2095%
oxygen
093%
argon
004%
carbon
dioxide
and
small
amounts
of
other
gases
air
also
contains
a
variable
amount
of
water
vapor
on
average
around
1%
at
sea
level
and
04%
over
the
entire
atmosphere
air
content
and
atmospheric
pressure
vary
at
different
layers
and
air
suitable
for
use
in
photosynthesis
by
terrestrial
plants
and
breathing
of
terrestrial
animals
is
found
only
in
earth's
troposphere
and
in
artificial
atmospheres
the
atmosphere
has
a
mass
of
about
515 kg
three
quarters
of
which
is
within
about
of
the
surface
the
atmosphere
becomes
thinner
and
thinner
with
increasing
altitude
with
no
definite
boundary
between
the
atmosphere
and
outer
space
the
kármán
line
at
or
157%
of
earth's
radius
is
often
used
as
the
border
between
the
atmosphere
and
outer
space
atmospheric
effects
become
noticeable
during
atmospheric
reentry
of
spacecraft
at
an
altitude
of
around
several
layers
can
be
distinguished
in
the
atmosphere
based
on
characteristics
such
as
temperature
and
composition
the
study
of
earth's
atmosphere
and
its
processes
is
called
atmospheric
science
(aerology)
early
pioneers
in
the
field
include
léon
teisserenc
de
bort
and
richard
assmann
the
three
major
constituents
of
earth's
atmosphere
are
nitrogen
oxygen
and
argon
water
vapor
accounts
for
roughly
025%
of
the
atmosphere
by
mass
the
concentration
of
water
vapor
(a
greenhouse
gas)
varies
significantly
from
around
10
ppm
by
volume
in
the
coldest
portions
of
the
atmosphere
to
as
much
as
5%
by
volume
in
hot
humid
air
masses
and
concentrations
of
other
atmospheric
gases
are
typically
quoted
in
terms
of
dry
air
(without
water
vapor)
the
remaining
gases
are
often
referred
to
as
trace
gases
among
which
are
the
greenhouse
gases
principally
carbon
dioxide
methane
nitrous
oxide
and
ozone
filtered
air
includes
trace
amounts
of
many
other
chemical
compounds
many
substances
of
natural
origin
may
be
present
in
locally
and
seasonally
variable
small
amounts
as
aerosols
in
an
unfiltered
air
sample
including
dust
of
mineral
and
organic
composition
pollen
and
spores
sea
spray
and
volcanic
ash
various
industrial
pollutants
also
may
be
present
as
gases
or
aerosols
such
as
chlorine
(elemental
or
in
compounds)
fluorine
compounds
and
elemental
mercury
vapor
sulfur
compounds
such
as
hydrogen
sulfide
and
sulfur
dioxide
(so)
may
be
derived
from
natural
sources
or
from
industrial
air
pollution
the
relative
concentration
of
gases
remains
constant
until
about
in
general
air
pressure
and
density
decrease
with
altitude
in
the
atmosphere
however
temperature
has
a
more
complicated
profile
with
altitude
and
may
remain
relatively
constant
or
even
increase
with
altitude
in
some
regions
(see
the
temperature
section
below)
because
the
general
pattern
of
the
temperature/altitude
profile
is
constant
and
measurable
by
means
of
instrumented
balloon
soundings
the
temperature
behavior
provides
a
useful
metric
to
distinguish
atmospheric
layers
in
this
way
earth's
atmosphere
can
be
divided
(called
atmospheric
stratification)
into
five
main
layers
excluding
the
exosphere
the
atmosphere
has
four
primary
layers
which
are
the
troposphere
stratosphere
mesosphere
and
thermosphere
from
highest
to
lowest
the
five
main
layers
are:
the
exosphere
is
the
outermost
layer
of
earth's
atmosphere
(ie
the
upper
limit
of
the
atmosphere)
it
extends
from
the
exobase
which
is
located
at
the
top
of
the
thermosphere
at
an
altitude
of
about
700 km
above
sea
level
to
about
10000 km
(6200 mi;
33000000 ft)
where
it
merges
into
the
solar
wind
this
layer
is
mainly
composed
of
extremely
low
densities
of
hydrogen
helium
and
several
heavier
molecules
including
nitrogen
oxygen
and
carbon
dioxide
closer
to
the
exobase
the
atoms
and
molecules
are
so
far
apart
that
they
can
travel
hundreds
of
kilometers
without
colliding
with
one
another
thus
the
exosphere
no
longer
behaves
like
a
gas
and
the
particles
constantly
escape
into
space
these
free-moving
particles
follow
ballistic
trajectories
and
may
migrate
in
and
out
of
the
magnetosphere
or
the
solar
wind
the
exosphere
is
located
too
far
above
earth
for
any
meteorological
phenomena
to
be
possible
however
the
aurora
borealis
and
aurora
australis
sometimes
occur
in
the
lower
part
of
the
exosphere
where
they
overlap
into
the
thermosphere
the
exosphere
contains
most
of
the
satellites
orbiting
earth
the
thermosphere
is
the
second-highest
layer
of
earth's
atmosphere
it
extends
from
the
mesopause
(which
separates
it
from
the
mesosphere)
at
an
altitude
of
about
up
to
the
thermopause
at
an
altitude
range
of
the
height
of
the
thermopause
varies
considerably
due
to
changes
in
solar
activity
because
the
thermopause
lies
at
the
lower
boundary
of
the
exosphere
it
is
also
referred
to
as
the
exobase
the
lower
part
of
the
thermosphere
from
above
earth's
surface
contains
the
ionosphere
the
temperature
of
the
thermosphere
gradually
increases
with
height
unlike
the
stratosphere
beneath
it
wherein
a
temperature
inversion
is
due
to
the
absorption
of
radiation
by
ozone
the
inversion
in
the
thermosphere
occurs
due
to
the
extremely
low
density
of
its
molecules
the
temperature
of
this
layer
can
rise
as
high
as
though
the
gas
molecules
are
so
far
apart
that
its
temperature
in
the
usual
sense
is
not
very
meaningful
the
air
is
so
rarefied
that
an
individual
molecule
(of
oxygen
for
example)
travels
an
average
of
between
collisions
with
other
molecules
although
the
thermosphere
has
a
high
proportion
of
molecules
with
high
energy
it
would
not
feel
hot
to
a
human
in
direct
contact
because
its
density
is
too
low
to
conduct
a
significant
amount
of
energy
to
or
from
the
skin
this
layer
is
completely
cloudless
and
free
of
water
vapor
however
non-hydrometeorological
phenomena
such
as
the
aurora
borealis
and
aurora
australis
are
occasionally
seen
in
the
thermosphere
the
international
space
station
orbits
in
this
layer
between
the
mesosphere
is
the
third
highest
layer
of
earth's
atmosphere
occupying
the
region
above
the
stratosphere
and
below
the
thermosphere
it
extends
from
the
stratopause
at
an
altitude
of
about
to
the
mesopause
at
above
sea
level
temperatures
drop
with
increasing
altitude
to
the
mesopause
that
marks
the
top
of
this
middle
layer
of
the
atmosphere
it
is
the
coldest
place
on
earth
and
has
an
average
temperature
around
just
below
the
mesopause
the
air
is
so
cold
that
even
the
very
scarce
water
vapor
at
this
altitude
can
be
sublimated
into
polar-mesospheric
noctilucent
clouds
these
are
the
highest
clouds
in
the
atmosphere
and
may
be
visible
to
the
naked
eye
if
sunlight
reflects
off
them
about
an
hour
or
two
after
sunset
or
a
similar
length
of
time
before
sunrise
they
are
most
readily
visible
when
the
sun
is
around
4
to
16
degrees
below
the
horizon
lightning-induced
discharges
known
as
transient
luminous
events
(tles)
occasionally
form
in
the
mesosphere
above
tropospheric
thunderclouds
the
mesosphere
is
also
the
layer
where
most
meteors
burn
up
upon
atmospheric
entrance
it
is
too
high
above
earth
to
be
accessible
to
jet-powered
aircraft
and
balloons
and
too
low
to
permit
orbital
spacecraft
the
mesosphere
is
mainly
accessed
by
sounding
rockets
and
rocket-powered
aircraft
the
stratosphere
is
the
second-lowest
layer
of
earth's
atmosphere
it
lies
above
the
troposphere
and
is
separated
from
it
by
the
tropopause
this
layer
extends
from
the
top
of
the
troposphere
at
roughly
above
earth's
surface
to
the
stratopause
at
an
altitude
of
about
the
atmospheric
pressure
at
the
top
of
the
stratosphere
is
roughly
1/1000
the
pressure
at
sea
level
it
contains
the
ozone
layer
which
is
the
part
of
earth's
atmosphere
that
contains
relatively
high
concentrations
of
that
gas
the
stratosphere
defines
a
layer
in
which
temperatures
rise
with
increasing
altitude
this
rise
in
temperature
is
caused
by
the
absorption
of
ultraviolet
radiation
(uv)
radiation
from
the
sun
by
the
ozone
layer
which
restricts
turbulence
and
mixing
although
the
temperature
may
be
at
the
tropopause
the
top
of
the
stratosphere
is
much
warmer
and
may
be
near
0 °c
the
stratospheric
temperature
profile
creates
very
stable
atmospheric
conditions
so
the
stratosphere
lacks
the
weather-producing
air
turbulence
that
is
so
prevalent
in
the
troposphere
consequently
the
stratosphere
is
almost
completely
free
of
clouds
and
other
forms
of
weather
however
polar
stratospheric
or
nacreous
clouds
are
occasionally
seen
in
the
lower
part
of
this
layer
of
the
atmosphere
where
the
air
is
coldest
the
stratosphere
is
the
highest
layer
that
can
be
accessed
by
jet-powered
aircraft
the
troposphere
is
the
lowest
layer
of
earth's
atmosphere
it
extends
from
earth's
surface
to
an
average
height
of
about
although
this
altitude
varies
from
about
at
the
geographic
poles
to
at
the
equator
with
some
variation
due
to
weather
the
troposphere
is
bounded
above
by
the
tropopause
a
boundary
marked
in
most
places
by
a
temperature
inversion
(ie
a
layer
of
relatively
warm
air
above
a
colder
one)
and
in
others
by
a
zone
which
is
isothermal
with
height
although
variations
do
occur
the
temperature
usually
declines
with
increasing
altitude
in
the
troposphere
because
the
troposphere
is
mostly
heated
through
energy
transfer
from
the
surface
thus
the
lowest
part
of
the
troposphere
(ie
earth's
surface)
is
typically
the
warmest
section
of
the
troposphere
this
promotes
vertical
mixing
(hence
the
origin
of
its
name
in
the
greek
word
τρόπος
"tropos"
meaning
"turn")
the
troposphere
contains
roughly
80%
of
the
mass
of
earth's
atmosphere
the
troposphere
is
denser
than
all
its
overlying
atmospheric
layers
because
a
larger
atmospheric
weight
sits
on
top
of
the
troposphere
and
causes
it
to
be
most
severely
compressed
fifty
percent
of
the
total
mass
of
the
atmosphere
is
located
in
the
lower
of
the
troposphere
nearly
all
atmospheric
water
vapor
or
moisture
is
found
in
the
troposphere
so
it
is
the
layer
where
most
of
earth's
weather
takes
place
it
has
basically
all
the
weather-associated
cloud
genus
types
generated
by
active
wind
circulation
although
very
tall
cumulonimbus
thunder
clouds
can
penetrate
the
tropopause
from
below
and
rise
into
the
lower
part
of
the
stratosphere
most
conventional
aviation
activity
takes
place
in
the
troposphere
and
it
is
the
only
layer
that
can
be
accessed
by
propeller-driven
aircraft
within
the
five
principal
layers
above
that
are
largely
determined
by
temperature
several
secondary
layers
may
be
distinguished
by
other
properties:
the
average
temperature
of
the
atmosphere
at
earth's
surface
is
or
depending
on
the
reference
the
average
atmospheric
pressure
at
sea
level
is
defined
by
the
international
standard
atmosphere
as
this
is
sometimes
referred
to
as
a
unit
of
standard
atmospheres
(atm)
total
atmospheric
mass
is
51480×10
kg
(1135×10
lb)
about
25%
less
than
would
be
inferred
from
the
average
sea
level
pressure
and
earth's
area
of
510072
megahectares
this
portion
being
displaced
by
earth's
mountainous
terrain
atmospheric
pressure
is
the
total
weight
of
the
air
above
unit
area
at
the
point
where
the
pressure
is
measured
thus
air
pressure
varies
with
location
and
weather
if
the
entire
mass
of
the
atmosphere
had
a
uniform
density
equal
to
sea
level
density
(about
12
kg
per
m)
from
sea
level
upwards
it
would
terminate
abruptly
at
an
altitude
of
it
actually
decreases
exponentially
with
altitude
dropping
by
half
every
or
by
a
factor
of
1/e
every
the
average
scale
height
of
the
atmosphere
below
however
the
atmosphere
is
more
accurately
modeled
with
a
customized
equation
for
each
layer
that
takes
gradients
of
temperature
molecular
composition
solar
radiation
and
gravity
into
account
in
summary
the
mass
of
earth's
atmosphere
is
distributed
approximately
as
follows:
by
comparison
the
summit
of
mt
everest
is
at
;
commercial
airliners
typically
cruise
between
where
the
thinner
air
improves
fuel
economy;
weather
balloons
reach
and
above;
and
the
highest
x-15
flight
in
1963
reached
even
above
the
kármán
line
significant
atmospheric
effects
such
as
auroras
still
occur
meteors
begin
to
glow
in
this
region
though
the
larger
ones
may
not
burn
up
until
they
penetrate
more
deeply
the
various
layers
of
earth's
ionosphere
important
to
hf
radio
propagation
begin
below
100 km
and
extend
beyond
500 km
by
comparison
the
international
space
station
and
space
shuttle
typically
orbit
at
350–400 km
within
the
f-layer
of
the
ionosphere
where
they
encounter
enough
atmospheric
drag
to
require
reboosts
every
few
months
depending
on
solar
activity
satellites
can
experience
noticeable
atmospheric
drag
at
altitudes
as
high
as
700–800 km
the
division
of
the
atmosphere
into
layers
mostly
by
reference
to
temperature
is
discussed
above
temperature
decreases
with
altitude
starting
at
sea
level
but
variations
in
this
trend
begin
above
11 km
where
the
temperature
stabilizes
through
a
large
vertical
distance
through
the
rest
of
the
troposphere
in
the
stratosphere
starting
above
about
20 km
the
temperature
increases
with
height
due
to
heating
within
the
ozone
layer
caused
by
capture
of
significant
ultraviolet
radiation
from
the
sun
by
the
dioxygen
and
ozone
gas
in
this
region
still
another
region
of
increasing
temperature
with
altitude
occurs
at
very
high
altitudes
in
the
aptly-named
thermosphere
above
90 km
because
in
an
ideal
gas
of
constant
composition
the
speed
of
sound
depends
only
on
temperature
and
not
on
the
gas
pressure
or
density
the
speed
of
sound
in
the
atmosphere
with
altitude
takes
on
the
form
of
the
complicated
temperature
profile
(see
illustration
to
the
right)
and
does
not
mirror
altitudinal
changes
in
density
or
pressure
the
density
of
air
at
sea
level
is
about
12 kg/m
(12 g/l
00012
g/cm)
density
is
not
measured
directly
but
is
calculated
from
measurements
of
temperature
pressure
and
humidity
using
the
equation
of
state
for
air
(a
form
of
the
ideal
gas
law)
atmospheric
density
decreases
as
the
altitude
increases
this
variation
can
be
approximately
modeled
using
the
barometric
formula
more
sophisticated
models
are
used
to
predict
orbital
decay
of
satellites
the
average
mass
of
the
atmosphere
is
about
5
quadrillion
(5)
tonnes
or
1/1200000
the
mass
of
earth
according
to
the
american
national
center
for
atmospheric
research
"the
total
mean
mass
of
the
atmosphere
is
51480 kg
with
an
annual
range
due
to
water
vapor
of
12
or
15 kg
depending
on
whether
surface
pressure
or
water
vapor
data
are
used;
somewhat
smaller
than
the
previous
estimate
the
mean
mass
of
water
vapor
is
estimated
as
127 kg
and
the
dry
air
mass
as
51352
±00003 kg"
solar
radiation
(or
sunlight)
is
the
energy
earth
receives
from
the
sun
earth
also
emits
radiation
back
into
space
but
at
longer
wavelengths
that
we
cannot
see
part
of
the
incoming
and
emitted
radiation
is
absorbed
or
reflected
by
the
atmosphere
in
may
2017
glints
of
light
seen
as
twinkling
from
an
orbiting
satellite
a
million
miles
away
were
found
to
be
reflected
light
from
ice
crystals
in
the
atmosphere
when
light
passes
through
earth's
atmosphere
photons
interact
with
it
through
"scattering"
if
the
light
does
not
interact
with
the
atmosphere
it
is
called
"direct
radiation"
and
is
what
you
see
if
you
were
to
look
directly
at
the
sun
"indirect
radiation"
is
light
that
has
been
scattered
in
the
atmosphere
for
example
on
an
overcast
day
when
you
cannot
see
your
shadow
there
is
no
direct
radiation
reaching
you
it
has
all
been
scattered
as
another
example
due
to
a
phenomenon
called
rayleigh
scattering
shorter
(blue)
wavelengths
scatter
more
easily
than
longer
(red)
wavelengths
this
is
why
the
sky
looks
blue;
you
are
seeing
scattered
blue
light
this
is
also
why
sunsets
are
red
because
the
sun
is
close
to
the
horizon
the
sun's
rays
pass
through
more
atmosphere
than
normal
to
reach
your
eye
much
of
the
blue
light
has
been
scattered
out
leaving
the
red
light
in
a
sunset
different
molecules
absorb
different
wavelengths
of
radiation
for
example
o
and
o
absorb
almost
all
wavelengths
shorter
than
300
nanometers
water
(ho)
absorbs
many
wavelengths
above
700 nm
when
a
molecule
absorbs
a
photon
it
increases
the
energy
of
the
molecule
this
heats
the
atmosphere
but
the
atmosphere
also
cools
by
emitting
radiation
as
discussed
below
the
combined
absorption
spectra
of
the
gases
in
the
atmosphere
leave
"windows"
of
low
opacity
allowing
the
transmission
of
only
certain
bands
of
light
the
optical
window
runs
from
around
300 nm
(ultraviolet-c)
up
into
the
range
humans
can
see
the
visible
spectrum
(commonly
called
light)
at
roughly
400–700 nm
and
continues
to
the
infrared
to
around
1100 nm
there
are
also
infrared
and
radio
windows
that
transmit
some
infrared
and
radio
waves
at
longer
wavelengths
for
example
the
radio
window
runs
from
about
one
centimeter
to
about
eleven-meter
waves
"emission"
is
the
opposite
of
absorption
it
is
when
an
object
emits
radiation
objects
tend
to
emit
amounts
and
wavelengths
of
radiation
depending
on
their
"black
body"
emission
curves
therefore
hotter
objects
tend
to
emit
more
radiation
with
shorter
wavelengths
colder
objects
emit
less
radiation
with
longer
wavelengths
for
example
the
sun
is
approximately
its
radiation
peaks
near
500 nm
and
is
visible
to
the
human
eye
earth
is
approximately
so
its
radiation
peaks
near
10000 nm
and
is
much
too
long
to
be
visible
to
humans
because
of
its
temperature
the
atmosphere
emits
infrared
radiation
for
example
on
clear
nights
earth's
surface
cools
down
faster
than
on
cloudy
nights
this
is
because
clouds
(ho)
are
strong
absorbers
and
emitters
of
infrared
radiation
this
is
also
why
it
becomes
colder
at
night
at
higher
elevations
the
greenhouse
effect
is
directly
related
to
this
absorption
and
emission
effect
some
gases
in
the
atmosphere
absorb
and
emit
infrared
radiation
but
do
not
interact
with
sunlight
in
the
visible
spectrum
common
examples
of
these
are
and
ho
the
refractive
index
of
air
is
close
to
but
just
greater
than
1
systematic
variations
in
refractive
index
can
lead
to
the
bending
of
light
rays
over
long
optical
paths
one
example
is
that
under
some
circumstances
observers
onboard
ships
can
see
other
vessels
just
over
the
horizon
because
light
is
refracted
in
the
same
direction
as
the
curvature
of
earth's
surface
the
refractive
index
of
air
depends
on
temperature
giving
rise
to
refraction
effects
when
the
temperature
gradient
is
large
an
example
of
such
effects
is
the
mirage
"atmospheric
circulation"
is
the
large-scale
movement
of
air
through
the
troposphere
and
the
means
(with
ocean
circulation)
by
which
heat
is
distributed
around
earth
the
large-scale
structure
of
the
atmospheric
circulation
varies
from
year
to
year
but
the
basic
structure
remains
fairly
constant
because
it
is
determined
by
earth's
rotation
rate
and
the
difference
in
solar
radiation
between
the
equator
and
poles
the
first
atmosphere
consisted
of
gases
in
the
solar
nebula
primarily
hydrogen
there
were
probably
simple
hydrides
such
as
those
now
found
in
the
gas
giants
(jupiter
and
saturn)
notably
water
vapor
methane
and
ammonia
outgassing
from
volcanism
supplemented
by
gases
produced
during
the
late
heavy
bombardment
of
earth
by
huge
asteroids
produced
the
next
atmosphere
consisting
largely
of
nitrogen
plus
carbon
dioxide
and
inert
gases
a
major
part
of
carbon-dioxide
emissions
dissolved
in
water
and
reacted
with
metals
such
as
calcium
and
magnesium
during
weathering
of
crustal
rocks
to
form
carbonates
that
were
deposited
as
sediments
water-related
sediments
have
been
found
that
date
from
as
early
as
38
billion
years
ago
about
34
billion
years
ago
nitrogen
formed
the
major
part
of
the
then
stable
"second
atmosphere"
the
influence
of
life
has
to
be
taken
into
account
rather
soon
in
the
history
of
the
atmosphere
because
hints
of
early
life-forms
appear
as
early
as
35
billion
years
ago
how
earth
at
that
time
maintained
a
climate
warm
enough
for
liquid
water
and
life
if
the
early
sun
put
out
30%
lower
solar
radiance
than
today
is
a
puzzle
known
as
the
"faint
young
sun
paradox"
the
geological
record
however
shows
a
continuous
relatively
warm
surface
during
the
complete
early
temperature
record
of
earth
–
with
the
exception
of
one
cold
glacial
phase
about
24
billion
years
ago
in
the
late
archean
eon
an
oxygen-containing
atmosphere
began
to
develop
apparently
produced
by
photosynthesizing
cyanobacteria
(see
great
oxygenation
event)
which
have
been
found
as
stromatolite
fossils
from
27
billion
years
ago
the
early
basic
carbon
isotopy
(isotope
ratio
proportions)
strongly
suggests
conditions
similar
to
the
current
and
that
the
fundamental
features
of
the
carbon
cycle
became
established
as
early
as
4
billion
years
ago
ancient
sediments
in
the
gabon
dating
from
between
about
2150
and
2080
million
years
ago
provide
a
record
of
earth's
dynamic
oxygenation
evolution
these
fluctuations
in
oxygenation
were
likely
driven
by
the
lomagundi
carbon
isotope
excursion
the
constant
re-arrangement
of
continents
by
plate
tectonics
influences
the
long-term
evolution
of
the
atmosphere
by
transferring
carbon
dioxide
to
and
from
large
continental
carbonate
stores
free
oxygen
did
not
exist
in
the
atmosphere
until
about
24
billion
years
ago
during
the
great
oxygenation
event
and
its
appearance
is
indicated
by
the
end
of
the
banded
iron
formations
before
this
time
any
oxygen
produced
by
photosynthesis
was
consumed
by
oxidation
of
reduced
materials
notably
iron
molecules
of
free
oxygen
did
not
start
to
accumulate
in
the
atmosphere
until
the
rate
of
production
of
oxygen
began
to
exceed
the
availability
of
reducing
materials
that
removed
oxygen
this
point
signifies
a
shift
from
a
reducing
atmosphere
to
an
oxidizing
atmosphere
o
showed
major
variations
until
reaching
a
steady
state
of
more
than
15%
by
the
end
of
the
precambrian
the
following
time
span
from
541
million
years
ago
to
the
present
day
is
the
phanerozoic
eon
during
the
earliest
period
of
which
the
cambrian
oxygen-requiring
metazoan
life
forms
began
to
appear
the
amount
of
oxygen
in
the
atmosphere
has
fluctuated
over
the
last
600
million
years
reaching
a
peak
of
about
30%
around
280
million
years
ago
significantly
higher
than
today's
21%
two
main
processes
govern
changes
in
the
atmosphere:
plants
use
carbon
dioxide
from
the
atmosphere
releasing
oxygen
breakdown
of
pyrite
and
volcanic
eruptions
release
sulfur
into
the
atmosphere
which
oxidizes
and
hence
reduces
the
amount
of
oxygen
in
the
atmosphere
however
volcanic
eruptions
also
release
carbon
dioxide
which
plants
can
convert
to
oxygen
the
exact
cause
of
the
variation
of
the
amount
of
oxygen
in
the
atmosphere
is
not
known
periods
with
much
oxygen
in
the
atmosphere
are
associated
with
rapid
development
of
animals
today's
atmosphere
contains
21%
oxygen
which
is
great
enough
for
this
rapid
development
of
animals
"air
pollution"
is
the
introduction
into
the
atmosphere
of
chemicals
particulate
matter
or
biological
materials
that
cause
harm
or
discomfort
to
organisms
stratospheric
ozone
depletion
is
caused
by
air
pollution
chiefly
from
chlorofluorocarbons
and
other
ozone-depleting
substances
the
scientific
consensus
is
that
the
anthropogenic
greenhouse
gases
currently
accumulating
in
the
atmosphere
are
the
main
cause
of
global
warming
on
october
19
2015
nasa
started
a
website
containing
daily
images
of
the
full
sunlit
side
of
earth
on
http://epicgsfcnasagov/
the
images
are
taken
from
the
deep
space
climate
observatory
(dscovr)
and
show
earth
as
it
rotates
during
a
day
underwater
underwater
refers
to
the
region
below
the
surface
of
water
where
the
water
exists
in
a
swimming
pool
or
a
natural
feature
(called
a
body
of
water)
such
as
an
ocean
sea
lake
pond
or
river
three
quarters
of
the
planet
earth
is
covered
by
water
a
majority
of
the
planet's
solid
surface
is
abyssal
plain
at
depths
between
below
the
surface
of
the
oceans
the
solid
surface
location
on
the
planet
closest
to
the
centre
of
the
orb
is
the
challenger
deep
located
in
the
mariana
trench
at
a
depth
of
although
a
number
of
human
activities
are
conducted
underwater—such
as
research
scuba
diving
for
work
or
recreation
or
even
underwater
warfare
with
submarines
this
very
extensive
environment
on
planet
earth
is
hostile
to
humans
in
many
ways
and
therefore
little
explored
but
it
can
be
explored
by
sonar
or
more
directly
via
manned
or
autonomous
submersibles
the
ocean
floors
have
been
surveyed
via
sonar
to
at
least
a
coarse
resolution;
particularly-strategic
areas
have
been
mapped
in
detail
in
the
name
of
detecting
enemy
submarines
or
aiding
friendly
ones
though
the
resulting
maps
may
still
be
classified
an
immediate
obstacle
to
human
activity
under
water
is
the
fact
that
human
lungs
cannot
naturally
function
in
this
environment
unlike
the
gills
of
fish
human
lungs
are
adapted
to
the
exchange
of
gases
at
atmospheric
pressure
not
liquids
aside
from
simply
having
insufficient
musculature
to
rapidly
move
water
in
and
out
of
the
lungs
a
more
significant
problem
for
all
air-breathing
animals
such
as
mammals
and
birds
is
that
water
contains
so
little
dissolved
oxygen
compared
with
atmospheric
air
air
is
around
21%
o;
water
typically
is
less
than
0001%
dissolved
oxygen
the
density
of
water
also
causes
problems
that
increase
dramatically
with
depth
the
atmospheric
pressure
at
the
surface
is
147
pounds
per
square
inch
or
around
100 kpa
a
comparable
water
pressure
occurs
at
a
depth
of
only
(
for
sea
water)
thus
at
about
10
m
below
the
surface
the
water
exerts
twice
the
pressure
(2
atmospheres
or
200 kpa)
on
the
body
as
air
at
surface
level
for
solids
and
liquids
like
bone
muscle
and
blood
this
added
pressure
is
not
much
of
a
problem;
but
it
is
a
problem
for
any
air-filled
spaces
like
the
mouth
ears
paranasal
sinuses
and
lungs
this
is
because
the
air
in
those
spaces
reduces
in
volume
when
under
pressure
and
so
does
not
provide
those
spaces
with
support
against
the
higher
outside
pressure
even
at
a
depth
of
underwater
an
inability
to
equalize
air
pressure
in
the
middle
ear
with
outside
water
pressure
can
cause
pain
and
the
tympanic
membrane
(eardrum)
can
rupture
at
depths
under
10 ft
(3 m)
the
danger
of
pressure
damage
is
greatest
in
shallow
water
because
the
ratio
of
pressure
change
is
greatest
near
the
surface
of
the
water
for
example
the
pressure
increase
between
the
surface
and
10 m
(33 ft)
is
100%
(100 kpa
to
200 kpa)
but
the
pressure
increase
from
30 m
(100 ft)
to
40 m
(130 ft)
is
only
25%
(400 kpa
to
500 kpa)
any
object
immersed
in
water
is
provided
with
a
buoyant
force
that
counters
the
force
of
gravity
appearing
to
make
the
object
less
heavy
if
the
overall
density
of
the
object
exceeds
the
density
of
water
the
object
sinks
if
the
overall
density
is
less
than
the
density
of
water
the
object
rises
until
it
floats
on
the
surface
with
increasing
depth
underwater
sunlight
is
absorbed
and
the
amount
of
visible
light
diminishes
because
absorption
is
greater
for
long
wavelengths
(red
end
of
the
visible
spectrum)
than
for
short
wavelengths
(blue
end
of
the
visible
spectrum)
the
colour
spectrum
is
rapidly
altered
with
increasing
depth
white
objects
at
the
surface
appear
bluish
underwater
and
red
objects
appear
dark
even
black
although
light
penetration
will
be
less
if
water
is
turbid
in
the
very
clear
water
of
the
open
ocean
less
than
25%
of
the
surface
light
reaches
a
depth
of
10 m
(33 feet)
at
100 m
(330 ft)
the
light
present
from
the
sun
is
normally
about
05%
of
that
at
the
surface
the
euphotic
depth
is
the
depth
at
which
light
intensity
falls
to
1%
of
the
value
at
the
surface
this
depth
is
dependent
upon
water
clarity
being
only
a
few
metres
underwater
in
a
turbid
estuary
but
may
reach
up
to
200
metres
in
the
open
ocean
at
the
euphotic
depth
plants
(such
as
phytoplankton)
have
no
net
energy
gain
from
photosynthesis
and
thus
cannot
grow
there
are
three
layers
of
ocean
temperature:
the
surface
layer
the
thermocline
and
the
deep
ocean
the
average
temperature
of
surface
layer
is
about
17 °c
about
90%
of
ocean's
water
is
below
the
thermocline
in
the
deep
ocean
where
most
of
the
water
is
below
4 °c
water
conducts
heat
around
25
times
more
efficiently
than
air
hypothermia
a
potentially
fatal
condition
occurs
when
the
human
body's
core
temperature
falls
below
35 °c
insulating
the
body's
warmth
from
water
is
the
main
purpose
of
diving
suits
and
exposure
suits
when
used
in
water
temperatures
below
25 °c
sound
is
transmitted
about
43
times
faster
in
water
(1484 m/s
in
fresh
water)
as
it
is
in
air
(343 m/s)
the
human
brain
can
determine
the
direction
of
sound
in
air
by
detecting
small
differences
in
the
time
it
takes
for
sound
waves
in
air
to
reach
each
of
the
two
ears
for
these
reasons
divers
find
it
difficult
to
determine
the
direction
of
sound
underwater
however
some
animals
have
adapted
to
this
difference
and
many
use
sound
to
navigate
underwater
universe
the
universe
is
all
of
space
and
time
and
their
contents
including
planets
stars
galaxies
and
all
other
forms
of
matter
and
energy
while
the
spatial
size
of
the
entire
universe
is
unknown
it
is
possible
to
measure
the
observable
universe
the
earliest
scientific
models
of
the
universe
were
developed
by
ancient
greek
and
indian
philosophers
and
were
geocentric
placing
earth
at
the
center
of
the
universe
over
the
centuries
more
precise
astronomical
observations
led
nicolaus
copernicus
to
develop
the
heliocentric
model
with
the
sun
at
the
center
of
the
solar
system
in
developing
the
law
of
universal
gravitation
isaac
newton
built
upon
copernicus'
work
as
well
as
observations
by
tycho
brahe
and
johannes
kepler's
laws
of
planetary
motion
further
observational
improvements
led
to
the
realization
that
the
sun
is
one
of
hundreds
of
billions
of
stars
in
the
milky
way
which
is
one
of
at
least
hundreds
of
billions
of
galaxies
in
the
universe
many
of
the
stars
in
our
galaxy
have
planets
at
the
largest
scale
galaxies
are
distributed
uniformly
and
the
same
in
all
directions
meaning
that
the
universe
has
neither
an
edge
nor
a
center
at
smaller
scales
galaxies
are
distributed
in
clusters
and
superclusters
which
form
immense
filaments
and
voids
in
space
creating
a
vast
foam-like
structure
discoveries
in
the
early
20th
century
have
suggested
that
the
universe
had
a
beginning
and
that
space
has
been
expanding
since
then
and
is
currently
still
expanding
at
an
increasing
rate
the
big
bang
theory
is
the
prevailing
cosmological
description
of
the
development
of
the
universe
under
this
theory
space
and
time
emerged
together
ago
with
a
fixed
amount
of
energy
and
matter
that
has
become
less
dense
as
the
universe
has
expanded
after
an
initial
accelerated
expansion
at
around
10
seconds
and
the
separation
of
the
four
known
fundamental
forces
the
universe
gradually
cooled
and
continued
to
expand
allowing
the
first
subatomic
particles
and
simple
atoms
to
form
dark
matter
gradually
gathered
forming
a
foam-like
structure
of
filaments
and
voids
under
the
influence
of
gravity
giant
clouds
of
hydrogen
and
helium
were
gradually
drawn
to
the
places
where
dark
matter
was
most
dense
forming
the
first
galaxies
stars
and
everything
else
seen
today
it
is
possible
to
see
objects
that
are
now
further
away
than
13799
billion
light-years
because
space
itself
has
expanded
and
it
is
still
expanding
today
this
means
that
objects
which
are
now
up
to
465
billion
light-years
away
can
still
be
seen
in
their
distant
past
because
in
the
past
when
their
light
was
emitted
they
were
much
closer
to
the
earth
from
studying
the
movement
of
galaxies
it
has
been
discovered
that
the
universe
contains
much
more
matter
than
is
accounted
for
by
visible
objects;
stars
galaxies
nebulas
and
interstellar
gas
this
unseen
matter
is
known
as
dark
matter
("dark"
means
that
there
is
a
wide
range
of
strong
indirect
evidence
that
it
exists
but
we
have
not
yet
detected
it
directly)
the
λcdm
model
is
the
most
widely
accepted
model
of
our
universe
it
suggests
that
about
[2015]
of
the
mass
and
energy
in
the
universe
is
a
cosmological
constant
(or
in
extensions
to
λcdm
other
forms
of
dark
energy
such
as
a
scalar
field)
which
is
responsible
for
the
current
expansion
of
space
and
about
[2015]
is
dark
matter
ordinary
("baryonic")
matter
is
therefore
only
49%
[2015]
of
the
physical
universe
stars
planets
and
visible
gas
clouds
only
form
about
6%
of
ordinary
matter
or
about
03%
of
the
entire
universe
there
are
many
competing
hypotheses
about
the
ultimate
fate
of
the
universe
and
about
what
if
anything
preceded
the
big
bang
while
other
physicists
and
philosophers
refuse
to
speculate
doubting
that
information
about
prior
states
will
ever
be
accessible
some
physicists
have
suggested
various
multiverse
hypotheses
in
which
the
universe
might
be
one
among
many
universes
that
likewise
exist
the
physical
universe
is
defined
as
all
of
space
and
time
(collectively
referred
to
as
spacetime)
and
their
contents
such
contents
comprise
all
of
energy
in
its
various
forms
including
electromagnetic
radiation
and
matter
and
therefore
planets
moons
stars
galaxies
and
the
contents
of
intergalactic
space
the
universe
also
includes
the
physical
laws
that
influence
energy
and
matter
such
as
conservation
laws
classical
mechanics
and
relativity
the
universe
is
often
defined
as
"the
totality
of
existence"
or
everything
that
exists
everything
that
has
existed
and
everything
that
will
exist
in
fact
some
philosophers
and
scientists
support
the
inclusion
of
ideas
and
abstract
concepts
–
such
as
mathematics
and
logic
–
in
the
definition
of
the
universe
the
word
"universe"
may
also
refer
to
concepts
such
as
"the
cosmos"
"the
world"
and
"nature"
the
word
"universe"
derives
from
the
old
french
word
"univers"
which
in
turn
derives
from
the
latin
word
"universum"
the
latin
word
was
used
by
cicero
and
later
latin
authors
in
many
of
the
same
senses
as
the
modern
english
word
is
used
a
term
for
"universe"
among
the
ancient
greek
philosophers
from
pythagoras
onwards
was
"tò
pân"
("the
all")
defined
as
all
matter
and
all
space
and
"tò
hólon"
("all
things")
which
did
not
necessarily
include
the
void
another
synonym
was
"ho
kósmos"
(meaning
the
world
the
cosmos)
synonyms
are
also
found
in
latin
authors
("totum"
"mundus"
"natura")
and
survive
in
modern
languages
eg
the
german
words
"das
all"
"weltall"
and
"natur"
for
"universe"
the
same
synonyms
are
found
in
english
such
as
everything
(as
in
the
theory
of
everything)
the
cosmos
(as
in
cosmology)
the
world
(as
in
the
many-worlds
interpretation)
and
nature
(as
in
natural
laws
or
natural
philosophy)
the
prevailing
model
for
the
evolution
of
the
universe
is
the
big
bang
theory
the
big
bang
model
states
that
the
earliest
state
of
the
universe
was
an
extremely
hot
and
dense
one
and
that
the
universe
subsequently
expanded
and
cooled
the
model
is
based
on
general
relativity
and
on
simplifying
assumptions
such
as
homogeneity
and
isotropy
of
space
a
version
of
the
model
with
a
cosmological
constant
(lambda)
and
cold
dark
matter
known
as
the
lambda-cdm
model
is
the
simplest
model
that
provides
a
reasonably
good
account
of
various
observations
about
the
universe
the
big
bang
model
accounts
for
observations
such
as
the
correlation
of
distance
and
redshift
of
galaxies
the
ratio
of
the
number
of
hydrogen
to
helium
atoms
and
the
microwave
radiation
background
the
initial
hot
dense
state
is
called
the
planck
epoch
a
brief
period
extending
from
time
zero
to
one
planck
time
unit
of
approximately
10
seconds
during
the
planck
epoch
all
types
of
matter
and
all
types
of
energy
were
concentrated
into
a
dense
state
and
gravity
-
currently
the
weakest
by
far
of
the
four
known
forces
-
is
believed
to
have
been
as
strong
as
the
other
fundamental
forces
and
all
the
forces
may
have
been
unified
since
the
planck
epoch
space
has
been
expanding
to
its
present
scale
with
a
very
short
but
intense
period
of
cosmic
inflation
believed
to
have
occurred
within
the
first
10
seconds
this
was
a
kind
of
expansion
different
from
those
we
can
see
around
us
today
objects
in
space
did
not
physically
move;
instead
the
metric
that
defines
space
itself
changed
although
objects
in
spacetime
cannot
move
faster
than
the
speed
of
light
this
limitation
does
not
apply
to
the
metric
governing
spacetime
itself
this
initial
period
of
inflation
is
believed
to
explain
why
space
appears
to
be
very
flat
and
much
larger
than
light
could
travel
since
the
start
of
the
universe
within
the
first
fraction
of
a
second
of
the
universe's
existence
the
four
fundamental
forces
had
separated
as
the
universe
continued
to
cool
down
from
its
inconceivably
hot
state
various
types
of
subatomic
particles
were
able
to
form
in
short
periods
of
time
known
as
the
quark
epoch
the
hadron
epoch
and
the
lepton
epoch
together
these
epochs
encompassed
less
than
10
seconds
of
time
following
the
big
bang
these
elementary
particles
associated
stably
into
ever
larger
combinations
including
stable
protons
and
neutrons
which
then
formed
more
complex
atomic
nuclei
through
nuclear
fusion
this
process
known
as
big
bang
nucleosynthesis
only
lasted
for
about
17
minutes
and
ended
about
20
minutes
after
the
big
bang
so
only
the
fastest
and
simplest
reactions
occurred
about
25%
of
the
protons
and
all
the
neutrons
in
the
universe
by
mass
were
converted
to
helium
with
small
amounts
of
deuterium
(a
form
of
hydrogen)
and
traces
of
lithium
any
other
element
was
only
formed
in
very
tiny
quantities
the
other
75%
of
the
protons
remained
unaffected
as
hydrogen
nuclei
after
nucleosynthesis
ended
the
universe
entered
a
period
known
as
the
photon
epoch
during
this
period
the
universe
was
still
far
too
hot
for
matter
to
form
neutral
atoms
so
it
contained
a
hot
dense
foggy
plasma
of
negatively
charged
electrons
neutral
neutrinos
and
positive
nuclei
after
about
377000
years
the
universe
had
cooled
enough
that
electrons
and
nuclei
could
form
the
first
stable
atoms
this
is
known
as
recombination
for
historical
reasons;
in
fact
electrons
and
nuclei
were
combining
for
the
first
time
unlike
plasma
neutral
atoms
are
transparent
to
many
wavelengths
of
light
so
for
the
first
time
the
universe
also
became
transparent
the
photons
released
("decoupled")
when
these
atoms
formed
can
still
be
seen
today;
they
form
the
cosmic
microwave
background
(cmb)
as
the
universe
expands
the
energy
density
of
electromagnetic
radiation
decreases
more
quickly
than
does
that
of
matter
because
the
energy
of
a
photon
decreases
with
its
wavelength
at
around
47000
years
the
energy
density
of
matter
became
larger
than
that
of
photons
and
neutrinos
and
began
to
dominate
the
large
scale
behavior
of
the
universe
this
marked
the
end
of
the
radiation-dominated
era
and
the
start
of
the
matter-dominated
era
in
the
earliest
stages
of
the
universe
tiny
fluctuations
within
the
universe's
density
led
to
concentrations
of
dark
matter
gradually
forming
ordinary
matter
attracted
to
these
by
gravity
formed
large
gas
clouds
and
eventually
stars
and
galaxies
where
the
dark
matter
was
most
dense
and
voids
where
it
was
least
dense
after
around
100
-
300
million
years
the
first
stars
formed
known
as
population
iii
stars
these
were
probably
very
massive
luminous
non
metallic
and
short-lived
they
were
responsible
for
the
gradual
reionization
of
the
universe
between
about
200-500
million
years
and
1
billion
years
and
also
for
seeding
the
universe
with
elements
heavier
than
helium
through
stellar
nucleosynthesis
the
universe
also
contains
a
mysterious
energy
-
possibly
a
scalar
field
-
called
dark
energy
the
density
of
which
does
not
change
over
time
after
about
98
billion
years
the
universe
had
expanded
sufficiently
so
that
the
density
of
matter
was
less
than
the
density
of
dark
energy
marking
the
beginning
of
the
present
dark-energy-dominated
era
in
this
era
the
expansion
of
the
universe
is
accelerating
due
to
dark
energy
of
the
four
fundamental
interactions
gravitation
is
the
dominant
at
astronomical
length
scales
gravity's
effects
are
cumulative;
by
contrast
the
effects
of
positive
and
negative
charges
tend
to
cancel
one
another
making
electromagnetism
relatively
insignificant
on
astronomical
length
scales
the
remaining
two
interactions
the
weak
and
strong
nuclear
forces
decline
very
rapidly
with
distance;
their
effects
are
confined
mainly
to
sub-atomic
length
scales
the
universe
appears
to
have
much
more
matter
than
antimatter
an
asymmetry
possibly
related
to
the
cp
violation
this
imbalance
between
matter
and
antimatter
is
partially
responsible
for
the
existence
of
all
matter
existing
today
since
matter
and
antimatter
if
equally
produced
at
the
big
bang
would
have
completely
annihilated
each
other
and
left
only
photons
as
a
result
of
their
interaction
the
universe
also
appears
to
have
neither
net
momentum
nor
angular
momentum
which
follows
accepted
physical
laws
if
the
universe
is
finite
these
laws
are
the
gauss's
law
and
the
non-divergence
of
the
stress-energy-momentum
pseudotensor
the
size
of
the
universe
is
somewhat
difficult
to
define
according
to
the
general
theory
of
relativity
far
regions
of
space
may
never
interact
with
ours
even
in
the
lifetime
of
the
universe
due
to
the
finite
speed
of
light
and
the
ongoing
expansion
of
space
for
example
radio
messages
sent
from
earth
may
never
reach
some
regions
of
space
even
if
the
universe
were
to
exist
forever:
space
may
expand
faster
than
light
can
traverse
it
distant
regions
of
space
are
assumed
to
exist
and
to
be
part
of
reality
as
much
as
we
are
even
though
we
can
never
interact
with
them
the
spatial
region
that
we
can
affect
and
be
affected
by
is
the
observable
universe
the
observable
universe
depends
on
the
location
of
the
observer
by
traveling
an
observer
can
come
into
contact
with
a
greater
region
of
spacetime
than
an
observer
who
remains
still
nevertheless
even
the
most
rapid
traveler
will
not
be
able
to
interact
with
all
of
space
typically
the
observable
universe
is
taken
to
mean
the
portion
of
the
universe
that
is
observable
from
our
vantage
point
in
the
milky
way
the
proper
distance—the
distance
as
would
be
measured
at
a
specific
time
including
the
present—between
earth
and
the
edge
of
the
observable
universe
is
46
billion
light-years
(14
billion
parsecs)
making
the
diameter
of
the
observable
universe
about
93
billion
light-years
(28
billion
parsecs)
the
distance
the
light
from
the
edge
of
the
observable
universe
has
travelled
is
very
close
to
the
age
of
the
universe
times
the
speed
of
light
but
this
does
not
represent
the
distance
at
any
given
time
because
the
edge
of
the
observable
universe
and
the
earth
have
since
moved
further
apart
for
comparison
the
diameter
of
a
typical
galaxy
is
30000
light-years
(9198
parsecs)
and
the
typical
distance
between
two
neighboring
galaxies
is
3
million
light-years
(9198
kiloparsecs)
as
an
example
the
milky
way
is
roughly
100000–180000
light-years
in
diameter
and
the
nearest
sister
galaxy
to
the
milky
way
the
andromeda
galaxy
is
located
roughly
25
million
light-years
away
because
we
cannot
observe
space
beyond
the
edge
of
the
observable
universe
it
is
unknown
whether
the
size
of
the
universe
in
its
totality
is
finite
or
infinite
estimates
for
the
total
size
of
the
universe
if
finite
reach
as
high
as
formula_1
megaparsecs
implied
by
one
resolution
of
the
no-boundary
proposal
astronomers
calculate
the
age
of
the
universe
by
assuming
that
the
lambda-cdm
model
accurately
describes
the
evolution
of
the
universe
from
a
very
uniform
hot
dense
primordial
state
to
its
present
state
and
measuring
the
cosmological
parameters
which
constitute
the
model
this
model
is
well
understood
theoretically
and
supported
by
recent
high-precision
astronomical
observations
such
as
wmap
and
planck
commonly
the
set
of
observations
fitted
includes
the
cosmic
microwave
background
anisotropy
the
brightness/redshift
relation
for
type
ia
supernovae
and
large-scale
galaxy
clustering
including
the
baryon
acoustic
oscillation
feature
other
observations
such
as
the
hubble
constant
the
abundance
of
galaxy
clusters
weak
gravitational
lensing
and
globular
cluster
ages
are
generally
consistent
with
these
providing
a
check
of
the
model
but
are
less
accurately
measured
at
present
assuming
that
the
lambda-cdm
model
is
correct
the
measurements
of
the
parameters
using
a
variety
of
techniques
by
numerous
experiments
yield
a
best
value
of
the
age
of
the
universe
as
of
2015
of
13799
±
0021
billion
years
over
time
the
universe
and
its
contents
have
evolved;
for
example
the
relative
population
of
quasars
and
galaxies
has
changed
and
space
itself
has
expanded
due
to
this
expansion
scientists
on
earth
can
observe
the
light
from
a
galaxy
30
billion
light-years
away
even
though
that
light
has
traveled
for
only
13
billion
years;
the
very
space
between
them
has
expanded
this
expansion
is
consistent
with
the
observation
that
the
light
from
distant
galaxies
has
been
redshifted;
the
photons
emitted
have
been
stretched
to
longer
wavelengths
and
lower
frequency
during
their
journey
analyses
of
type
ia
supernovae
indicate
that
the
spatial
expansion
is
accelerating
the
more
matter
there
is
in
the
universe
the
stronger
the
mutual
gravitational
pull
of
the
matter
if
the
universe
were
"too"
dense
then
it
would
re-collapse
into
a
gravitational
singularity
however
if
the
universe
contained
too
"little"
matter
then
the
self-gravity
would
be
too
weak
for
astronomical
structures
like
galaxies
or
planets
to
form
since
the
big
bang
the
universe
has
expanded
monotonically
perhaps
unsurprisingly
our
universe
has
just
the
right
mass-energy
density
equivalent
to
about
5
protons
per
cubic
meter
which
has
allowed
it
to
expand
for
the
last
138
billion
years
giving
time
to
form
the
universe
as
observed
today
there
are
dynamical
forces
acting
on
the
particles
in
the
universe
which
affect
the
expansion
rate
before
1998
it
was
expected
that
the
expansion
rate
would
be
decreasing
as
time
went
on
due
to
the
influence
of
gravitational
interactions
in
the
universe;
and
thus
there
is
an
additional
observable
quantity
in
the
universe
called
the
deceleration
parameter
which
most
cosmologists
expected
to
be
positive
and
related
to
the
matter
density
of
the
universe
in
1998
the
deceleration
parameter
was
measured
by
two
different
groups
to
be
negative
approximately
-055
which
technically
implies
that
the
second
derivative
of
the
cosmic
scale
factor
formula_2
has
been
positive
in
the
last
5-6
billion
years
this
acceleration
does
not
however
imply
that
the
hubble
parameter
is
currently
increasing;
see
deceleration
parameter
for
details
spacetimes
are
the
arenas
in
which
all
physical
events
take
place
the
basic
elements
of
spacetimes
are
events
in
any
given
spacetime
an
event
is
defined
as
a
unique
position
at
a
unique
time
a
spacetime
is
the
union
of
all
events
(in
the
same
way
that
a
line
is
the
union
of
all
of
its
points)
formally
organized
into
a
manifold
the
universe
appears
to
be
a
smooth
spacetime
continuum
consisting
of
three
spatial
dimensions
and
one
temporal
(time)
dimension
(an
event
in
the
spacetime
of
the
physical
universe
can
therefore
be
identified
by
a
set
of
four
coordinates:
("x"
"y"
"z"
"t")
)
on
the
average
space
is
observed
to
be
very
nearly
flat
(with
a
curvature
close
to
zero)
meaning
that
euclidean
geometry
is
empirically
true
with
high
accuracy
throughout
most
of
the
universe
spacetime
also
appears
to
have
a
simply
connected
topology
in
analogy
with
a
sphere
at
least
on
the
length-scale
of
the
observable
universe
however
present
observations
cannot
exclude
the
possibilities
that
the
universe
has
more
dimensions
(which
is
postulated
by
theories
such
as
the
string
theory)
and
that
its
spacetime
may
have
a
multiply
connected
global
topology
in
analogy
with
the
cylindrical
or
toroidal
topologies
of
two-dimensional
spaces
the
spacetime
of
the
universe
is
usually
interpreted
from
a
euclidean
perspective
with
space
as
consisting
of
three
dimensions
and
time
as
consisting
of
one
dimension
the
"fourth
dimension"
by
combining
space
and
time
into
a
single
manifold
called
minkowski
space
physicists
have
simplified
a
large
number
of
physical
theories
as
well
as
described
in
a
more
uniform
way
the
workings
of
the
universe
at
both
the
supergalactic
and
subatomic
levels
spacetime
events
are
not
absolutely
defined
spatially
and
temporally
but
rather
are
known
to
be
relative
to
the
motion
of
an
observer
minkowski
space
approximates
the
universe
without
gravity;
the
pseudo-riemannian
manifolds
of
general
relativity
describe
spacetime
with
matter
and
gravity
general
relativity
describes
how
spacetime
is
curved
and
bent
by
mass
and
energy
(gravity)
the
topology
or
geometry
of
the
universe
includes
both
local
geometry
in
the
observable
universe
and
global
geometry
cosmologists
often
work
with
a
given
space-like
slice
of
spacetime
called
the
comoving
coordinates
the
section
of
spacetime
which
can
be
observed
is
the
backward
light
cone
which
delimits
the
cosmological
horizon
the
cosmological
horizon
(also
called
the
particle
horizon
or
the
light
horizon)
is
the
maximum
distance
from
which
particles
can
have
traveled
to
the
observer
in
the
age
of
the
universe
this
horizon
represents
the
boundary
between
the
observable
and
the
unobservable
regions
of
the
universe
the
existence
properties
and
significance
of
a
cosmological
horizon
depend
on
the
particular
cosmological
model
an
important
parameter
determining
the
future
evolution
of
the
universe
theory
is
the
density
parameter
omega
(ω)
defined
as
the
average
matter
density
of
the
universe
divided
by
a
critical
value
of
that
density
this
selects
one
of
three
possible
geometries
depending
on
whether
ω
is
equal
to
less
than
or
greater
than
1
these
are
called
respectively
the
flat
open
and
closed
universes
observations
including
the
cosmic
background
explorer
(cobe)
wilkinson
microwave
anisotropy
probe
(wmap)
and
planck
maps
of
the
cmb
suggest
that
the
universe
is
infinite
in
extent
with
a
finite
age
as
described
by
the
friedmann–lemaître–robertson–walker
(flrw)
models
these
flrw
models
thus
support
inflationary
models
and
the
standard
model
of
cosmology
describing
a
flat
homogeneous
universe
presently
dominated
by
dark
matter
and
dark
energy
the
universe
may
be
"fine-tuned";
the
fine-tuned
universe
hypothesis
is
the
proposition
that
the
conditions
that
allow
the
existence
of
observable
life
in
the
universe
can
only
occur
when
certain
universal
fundamental
physical
constants
lie
within
a
very
narrow
range
of
values
so
that
if
any
of
several
fundamental
constants
were
only
slightly
different
the
universe
would
have
been
unlikely
to
be
conducive
to
the
establishment
and
development
of
matter
astronomical
structures
elemental
diversity
or
life
as
it
is
understood
the
proposition
is
discussed
among
philosophers
scientists
theologians
and
proponents
of
creationism
the
universe
is
composed
almost
completely
of
dark
energy
dark
matter
and
ordinary
matter
other
contents
are
electromagnetic
radiation
(estimated
to
constitute
from
0005%
to
close
to
001%
of
the
total
mass
of
the
universe)
and
antimatter
the
proportions
of
all
types
of
matter
and
energy
have
changed
over
the
history
of
the
universe
the
total
amount
of
electromagnetic
radiation
generated
within
the
universe
has
decreased
by
1/2
in
the
past
2
billion
years
today
ordinary
matter
which
includes
atoms
stars
galaxies
and
life
accounts
for
only
49%
of
the
contents
of
the
universe
the
present
overall
density
of
this
type
of
matter
is
very
low
roughly
45
×
10
grams
per
cubic
centimetre
corresponding
to
a
density
of
the
order
of
only
one
proton
for
every
four
cubic
meters
of
volume
the
nature
of
both
dark
energy
and
dark
matter
is
unknown
dark
matter
a
mysterious
form
of
matter
that
has
not
yet
been
identified
accounts
for
268%
of
the
cosmic
contents
dark
energy
which
is
the
energy
of
empty
space
and
is
causing
the
expansion
of
the
universe
to
accelerate
accounts
for
the
remaining
683%
of
the
contents
matter
dark
matter
and
dark
energy
are
distributed
homogeneously
throughout
the
universe
over
length
scales
longer
than
300
million
light-years
or
so
however
over
shorter
length-scales
matter
tends
to
clump
hierarchically;
many
atoms
are
condensed
into
stars
most
stars
into
galaxies
most
galaxies
into
clusters
superclusters
and
finally
large-scale
galactic
filaments
the
observable
universe
contains
approximately
300
sextillion
(3)
stars
and
more
than
100
billion
(10)
galaxies
typical
galaxies
range
from
dwarfs
with
as
few
as
ten
million
(10)
stars
up
to
giants
with
one
trillion
(10)
stars
between
the
larger
structures
are
voids
which
are
typically
10–150
mpc
(33
million–490
million
ly)
in
diameter
the
milky
way
is
in
the
local
group
of
galaxies
which
in
turn
is
in
the
laniakea
supercluster
this
supercluster
spans
over
500
million
light-years
while
the
local
group
spans
over
10
million
light-years
the
universe
also
has
vast
regions
of
relative
emptiness;
the
largest
known
void
measures
18
billion
ly
(550
mpc)
across
the
observable
universe
is
isotropic
on
scales
significantly
larger
than
superclusters
meaning
that
the
statistical
properties
of
the
universe
are
the
same
in
all
directions
as
observed
from
earth
the
universe
is
bathed
in
highly
isotropic
microwave
radiation
that
corresponds
to
a
thermal
equilibrium
blackbody
spectrum
of
roughly
272548
kelvins
the
hypothesis
that
the
large-scale
universe
is
homogeneous
and
isotropic
is
known
as
the
cosmological
principle
a
universe
that
is
both
homogeneous
and
isotropic
looks
the
same
from
all
vantage
points
and
has
no
center
an
explanation
for
why
the
expansion
of
the
universe
is
accelerating
remains
elusive
it
is
often
attributed
to
"dark
energy"
an
unknown
form
of
energy
that
is
hypothesized
to
permeate
space
on
a
mass–energy
equivalence
basis
the
density
of
dark
energy
(~
7
×
10
g/cm)
is
much
less
than
the
density
of
ordinary
matter
or
dark
matter
within
galaxies
however
in
the
present
dark-energy
era
it
dominates
the
mass–energy
of
the
universe
because
it
is
uniform
across
space
two
proposed
forms
for
dark
energy
are
the
cosmological
constant
a
"constant"
energy
density
filling
space
homogeneously
and
scalar
fields
such
as
quintessence
or
moduli
"dynamic"
quantities
whose
energy
density
can
vary
in
time
and
space
contributions
from
scalar
fields
that
are
constant
in
space
are
usually
also
included
in
the
cosmological
constant
the
cosmological
constant
can
be
formulated
to
be
equivalent
to
vacuum
energy
scalar
fields
having
only
a
slight
amount
of
spatial
inhomogeneity
would
be
difficult
to
distinguish
from
a
cosmological
constant
dark
matter
is
a
hypothetical
kind
of
matter
that
is
invisible
to
the
entire
electromagnetic
spectrum
but
which
accounts
for
most
of
the
matter
in
the
universe
the
existence
and
properties
of
dark
matter
are
inferred
from
its
gravitational
effects
on
visible
matter
radiation
and
the
large-scale
structure
of
the
universe
other
than
neutrinos
a
form
of
hot
dark
matter
dark
matter
has
not
been
detected
directly
making
it
one
of
the
greatest
mysteries
in
modern
astrophysics
dark
matter
neither
emits
nor
absorbs
light
or
any
other
electromagnetic
radiation
at
any
significant
level
dark
matter
is
estimated
to
constitute
268%
of
the
total
mass–energy
and
845%
of
the
total
matter
in
the
universe
the
remaining
49%
of
the
mass–energy
of
the
universe
is
ordinary
matter
that
is
atoms
ions
electrons
and
the
objects
they
form
this
matter
includes
stars
which
produce
nearly
all
of
the
light
we
see
from
galaxies
as
well
as
interstellar
gas
in
the
interstellar
and
intergalactic
media
planets
and
all
the
objects
from
everyday
life
that
we
can
bump
into
touch
or
squeeze
as
a
matter
of
fact
the
great
majority
of
ordinary
matter
in
the
universe
is
unseen
since
visible
stars
and
gas
inside
galaxies
and
clusters
account
for
less
than
10
per
cent
of
the
ordinary
matter
contribution
to
the
mass-energy
density
of
the
universe
ordinary
matter
commonly
exists
in
four
states
(or
phases):
solid
liquid
gas
and
plasma
however
advances
in
experimental
techniques
have
revealed
other
previously
theoretical
phases
such
as
bose–einstein
condensates
and
fermionic
condensates
ordinary
matter
is
composed
of
two
types
of
elementary
particles:
quarks
and
leptons
for
example
the
proton
is
formed
of
two
up
quarks
and
one
down
quark;
the
neutron
is
formed
of
two
down
quarks
and
one
up
quark;
and
the
electron
is
a
kind
of
lepton
an
atom
consists
of
an
atomic
nucleus
made
up
of
protons
and
neutrons
and
electrons
that
orbit
the
nucleus
because
most
of
the
mass
of
an
atom
is
concentrated
in
its
nucleus
which
is
made
up
of
baryons
astronomers
often
use
the
term
"baryonic
matter"
to
describe
ordinary
matter
although
a
small
fraction
of
this
"baryonic
matter"
is
electrons
soon
after
the
big
bang
primordial
protons
and
neutrons
formed
from
the
quark–gluon
plasma
of
the
early
universe
as
it
cooled
below
two
trillion
degrees
a
few
minutes
later
in
a
process
known
as
big
bang
nucleosynthesis
nuclei
formed
from
the
primordial
protons
and
neutrons
this
nucleosynthesis
formed
lighter
elements
those
with
small
atomic
numbers
up
to
lithium
and
beryllium
but
the
abundance
of
heavier
elements
dropped
off
sharply
with
increasing
atomic
number
some
boron
may
have
been
formed
at
this
time
but
the
next
heavier
element
carbon
was
not
formed
in
significant
amounts
big
bang
nucleosynthesis
shut
down
after
about
20
minutes
due
to
the
rapid
drop
in
temperature
and
density
of
the
expanding
universe
subsequent
formation
of
heavier
elements
resulted
from
stellar
nucleosynthesis
and
supernova
nucleosynthesis
ordinary
matter
and
the
forces
that
act
on
matter
can
be
described
in
terms
of
elementary
particles
these
particles
are
sometimes
described
as
being
fundamental
since
they
have
an
unknown
substructure
and
it
is
unknown
whether
or
not
they
are
composed
of
smaller
and
even
more
fundamental
particles
of
central
importance
is
the
standard
model
a
theory
that
is
concerned
with
electromagnetic
interactions
and
the
weak
and
strong
nuclear
interactions
the
standard
model
is
supported
by
the
experimental
confirmation
of
the
existence
of
particles
that
compose
matter:
quarks
and
leptons
and
their
corresponding
"antimatter"
duals
as
well
as
the
force
particles
that
mediate
interactions:
the
photon
the
w
and
z
bosons
and
the
gluon
the
standard
model
predicted
the
existence
of
the
recently
discovered
higgs
boson
a
particle
that
is
a
manifestation
of
a
field
within
the
universe
that
can
endow
particles
with
mass
because
of
its
success
in
explaining
a
wide
variety
of
experimental
results
the
standard
model
is
sometimes
regarded
as
a
"theory
of
almost
everything"
the
standard
model
does
not
however
accommodate
gravity
a
true
force-particle
"theory
of
everything"
has
not
been
attained
a
hadron
is
a
composite
particle
made
of
quarks
held
together
by
the
strong
force
hadrons
are
categorized
into
two
families:
baryons
(such
as
protons
and
neutrons)
made
of
three
quarks
and
mesons
(such
as
pions)
made
of
one
quark
and
one
antiquark
of
the
hadrons
protons
are
stable
and
neutrons
bound
within
atomic
nuclei
are
stable
other
hadrons
are
unstable
under
ordinary
conditions
and
are
thus
insignificant
constituents
of
the
modern
universe
from
approximately
10
seconds
after
the
big
bang
during
a
period
is
known
as
the
hadron
epoch
the
temperature
of
the
universe
had
fallen
sufficiently
to
allow
quarks
to
bind
together
into
hadrons
and
the
mass
of
the
universe
was
dominated
by
hadrons
initially
the
temperature
was
high
enough
to
allow
the
formation
of
hadron/anti-hadron
pairs
which
kept
matter
and
antimatter
in
thermal
equilibrium
however
as
the
temperature
of
the
universe
continued
to
fall
hadron/anti-hadron
pairs
were
no
longer
produced
most
of
the
hadrons
and
anti-hadrons
were
then
eliminated
in
particle-antiparticle
annihilation
reactions
leaving
a
small
residual
of
hadrons
by
the
time
the
universe
was
about
one
second
old
a
lepton
is
an
elementary
half-integer
spin
particle
that
does
not
undergo
strong
interactions
but
is
subject
to
the
pauli
exclusion
principle;
no
two
leptons
of
the
same
species
can
be
in
exactly
the
same
state
at
the
same
time
two
main
classes
of
leptons
exist:
charged
leptons
(also
known
as
the
"electron-like"
leptons)
and
neutral
leptons
(better
known
as
neutrinos)
electrons
are
stable
and
the
most
common
charged
lepton
in
the
universe
whereas
muons
and
taus
are
unstable
particle
that
quickly
decay
after
being
produced
in
high
energy
collisions
such
as
those
involving
cosmic
rays
or
carried
out
in
particle
accelerators
charged
leptons
can
combine
with
other
particles
to
form
various
composite
particles
such
as
atoms
and
positronium
the
electron
governs
nearly
all
of
chemistry
as
it
is
found
in
atoms
and
is
directly
tied
to
all
chemical
properties
neutrinos
rarely
interact
with
anything
and
are
consequently
rarely
observed
neutrinos
stream
throughout
the
universe
but
rarely
interact
with
normal
matter
the
lepton
epoch
was
the
period
in
the
evolution
of
the
early
universe
in
which
the
leptons
dominated
the
mass
of
the
universe
it
started
roughly
1
second
after
the
big
bang
after
the
majority
of
hadrons
and
anti-hadrons
annihilated
each
other
at
the
end
of
the
hadron
epoch
during
the
lepton
epoch
the
temperature
of
the
universe
was
still
high
enough
to
create
lepton/anti-lepton
pairs
so
leptons
and
anti-leptons
were
in
thermal
equilibrium
approximately
10
seconds
after
the
big
bang
the
temperature
of
the
universe
had
fallen
to
the
point
where
lepton/anti-lepton
pairs
were
no
longer
created
most
leptons
and
anti-leptons
were
then
eliminated
in
annihilation
reactions
leaving
a
small
residue
of
leptons
the
mass
of
the
universe
was
then
dominated
by
photons
as
it
entered
the
following
photon
epoch
a
photon
is
the
quantum
of
light
and
all
other
forms
of
electromagnetic
radiation
it
is
the
force
carrier
for
the
electromagnetic
force
even
when
static
via
virtual
photons
the
effects
of
this
force
are
easily
observable
at
the
microscopic
and
at
the
macroscopic
level
because
the
photon
has
zero
rest
mass;
this
allows
long
distance
interactions
like
all
elementary
particles
photons
are
currently
best
explained
by
quantum
mechanics
and
exhibit
wave–particle
duality
exhibiting
properties
of
waves
and
of
particles
the
photon
epoch
started
after
most
leptons
and
anti-leptons
were
annihilated
at
the
end
of
the
lepton
epoch
about
10
seconds
after
the
big
bang
atomic
nuclei
were
created
in
the
process
of
nucleosynthesis
which
occurred
during
the
first
few
minutes
of
the
photon
epoch
for
the
remainder
of
the
photon
epoch
the
universe
contained
a
hot
dense
plasma
of
nuclei
electrons
and
photons
about
380000
years
after
the
big
bang
the
temperature
of
the
universe
fell
to
the
point
where
nuclei
could
combine
with
electrons
to
create
neutral
atoms
as
a
result
photons
no
longer
interacted
frequently
with
matter
and
the
universe
became
transparent
the
highly
redshifted
photons
from
this
period
form
the
cosmic
microwave
background
tiny
variations
in
temperature
and
density
detectable
in
the
cmb
were
the
early
"seeds"
from
which
all
subsequent
structure
formation
took
place
general
relativity
is
the
geometric
theory
of
gravitation
published
by
albert
einstein
in
1915
and
the
current
description
of
gravitation
in
modern
physics
it
is
the
basis
of
current
cosmological
models
of
the
universe
general
relativity
generalizes
special
relativity
and
newton's
law
of
universal
gravitation
providing
a
unified
description
of
gravity
as
a
geometric
property
of
space
and
time
or
spacetime
in
particular
the
curvature
of
spacetime
is
directly
related
to
the
energy
and
momentum
of
whatever
matter
and
radiation
are
present
the
relation
is
specified
by
the
einstein
field
equations
a
system
of
partial
differential
equations
in
general
relativity
the
distribution
of
matter
and
energy
determines
the
geometry
of
spacetime
which
in
turn
describes
the
acceleration
of
matter
therefore
solutions
of
the
einstein
field
equations
describe
the
evolution
of
the
universe
combined
with
measurements
of
the
amount
type
and
distribution
of
matter
in
the
universe
the
equations
of
general
relativity
describe
the
evolution
of
the
universe
over
time
with
the
assumption
of
the
cosmological
principle
that
the
universe
is
homogeneous
and
isotropic
everywhere
a
specific
solution
of
the
field
equations
that
describes
the
universe
is
the
metric
tensor
called
the
friedmann–lemaître–robertson–walker
metric
where
("r"
θ
φ)
correspond
to
a
spherical
coordinate
system
this
metric
has
only
two
undetermined
parameters
an
overall
dimensionless
length
scale
factor
"r"
describes
the
size
scale
of
the
universe
as
a
function
of
time;
an
increase
in
"r"
is
the
expansion
of
the
universe
a
curvature
index
"k"
describes
the
geometry
the
index
"k"
is
defined
so
that
it
can
take
only
one
of
three
values:
0
corresponding
to
flat
euclidean
geometry;
1
corresponding
to
a
space
of
positive
curvature;
or
−1
corresponding
to
a
space
of
positive
or
negative
curvature
the
value
of
"r"
as
a
function
of
time
"t"
depends
upon
"k"
and
the
cosmological
constant
"λ"
the
cosmological
constant
represents
the
energy
density
of
the
vacuum
of
space
and
could
be
related
to
dark
energy
the
equation
describing
how
"r"
varies
with
time
is
known
as
the
friedmann
equation
after
its
inventor
alexander
friedmann
the
solutions
for
"r(t)"
depend
on
"k"
and
"λ"
but
some
qualitative
features
of
such
solutions
are
general
first
and
most
importantly
the
length
scale
"r"
of
the
universe
can
remain
constant
"only"
if
the
universe
is
perfectly
isotropic
with
positive
curvature
("k"=1)
and
has
one
precise
value
of
density
everywhere
as
first
noted
by
albert
einstein
however
this
equilibrium
is
unstable:
because
the
universe
is
known
to
be
inhomogeneous
on
smaller
scales
"r"
must
change
over
time
when
"r"
changes
all
the
spatial
distances
in
the
universe
change
in
tandem;
there
is
an
overall
expansion
or
contraction
of
space
itself
this
accounts
for
the
observation
that
galaxies
appear
to
be
flying
apart;
the
space
between
them
is
stretching
the
stretching
of
space
also
accounts
for
the
apparent
paradox
that
two
galaxies
can
be
40
billion
light-years
apart
although
they
started
from
the
same
point
138
billion
years
ago
and
never
moved
faster
than
the
speed
of
light
second
all
solutions
suggest
that
there
was
a
gravitational
singularity
in
the
past
when
"r"
went
to
zero
and
matter
and
energy
were
infinitely
dense
it
may
seem
that
this
conclusion
is
uncertain
because
it
is
based
on
the
questionable
assumptions
of
perfect
homogeneity
and
isotropy
(the
cosmological
principle)
and
that
only
the
gravitational
interaction
is
significant
however
the
penrose–hawking
singularity
theorems
show
that
a
singularity
should
exist
for
very
general
conditions
hence
according
to
einstein's
field
equations
"r"
grew
rapidly
from
an
unimaginably
hot
dense
state
that
existed
immediately
following
this
singularity
(when
"r"
had
a
small
finite
value);
this
is
the
essence
of
the
big
bang
model
of
the
universe
understanding
the
singularity
of
the
big
bang
likely
requires
a
quantum
theory
of
gravity
which
has
not
yet
been
formulated
third
the
curvature
index
"k"
determines
the
sign
of
the
mean
spatial
curvature
of
spacetime
averaged
over
sufficiently
large
length
scales
(greater
than
about
a
billion
light-years)
if
"k"=1
the
curvature
is
positive
and
the
universe
has
a
finite
volume
a
universe
with
positive
curvature
is
often
visualized
as
a
three-dimensional
sphere
embedded
in
a
four-dimensional
space
conversely
if
"k"
is
zero
or
negative
the
universe
has
an
infinite
volume
it
may
seem
counter-intuitive
that
an
infinite
and
yet
infinitely
dense
universe
could
be
created
in
a
single
instant
at
the
big
bang
when
"r"=0
but
exactly
that
is
predicted
mathematically
when
"k"
does
not
equal
1
by
analogy
an
infinite
plane
has
zero
curvature
but
infinite
area
whereas
an
infinite
cylinder
is
finite
in
one
direction
and
a
torus
is
finite
in
both
a
toroidal
universe
could
behave
like
a
normal
universe
with
periodic
boundary
conditions
the
ultimate
fate
of
the
universe
is
still
unknown
because
it
depends
critically
on
the
curvature
index
"k"
and
the
cosmological
constant
"λ"
if
the
universe
were
sufficiently
dense
"k"
would
equal
+1
meaning
that
its
average
curvature
throughout
is
positive
and
the
universe
will
eventually
recollapse
in
a
big
crunch
possibly
starting
a
new
universe
in
a
big
bounce
conversely
if
the
universe
were
insufficiently
dense
"k"
would
equal
0
or
−1
and
the
universe
would
expand
forever
cooling
off
and
eventually
reaching
the
big
freeze
and
the
heat
death
of
the
universe
modern
data
suggests
that
the
rate
of
expansion
of
the
universe
is
not
decreasing
as
originally
expected
but
increasing;
if
this
continues
indefinitely
the
universe
may
eventually
reach
a
big
rip
observationally
the
universe
appears
to
be
flat
("k"
=
0)
with
an
overall
density
that
is
very
close
to
the
critical
value
between
recollapse
and
eternal
expansion
some
speculative
theories
have
proposed
that
our
universe
is
but
one
of
a
set
of
disconnected
universes
collectively
denoted
as
the
multiverse
challenging
or
enhancing
more
limited
definitions
of
the
universe
scientific
multiverse
models
are
distinct
from
concepts
such
as
alternate
planes
of
consciousness
and
simulated
reality
max
tegmark
developed
a
four-part
classification
scheme
for
the
different
types
of
multiverses
that
scientists
have
suggested
in
response
to
various
physics
problems
an
example
of
such
multiverses
is
the
one
resulting
from
the
chaotic
inflation
model
of
the
early
universe
another
is
the
multiverse
resulting
from
the
many-worlds
interpretation
of
quantum
mechanics
in
this
interpretation
parallel
worlds
are
generated
in
a
manner
similar
to
quantum
superposition
and
decoherence
with
all
states
of
the
wave
functions
being
realized
in
separate
worlds
effectively
in
the
many-worlds
interpretation
the
multiverse
evolves
as
a
universal
wavefunction
if
the
big
bang
that
created
our
multiverse
created
an
ensemble
of
multiverses
the
wave
function
of
the
ensemble
would
be
entangled
in
this
sense
the
least
controversial
category
of
multiverse
in
tegmark's
scheme
is
the
multiverses
of
this
level
are
composed
by
distant
spacetime
events
"in
our
own
universe"
if
space
is
infinite
or
sufficiently
large
and
uniform
identical
instances
of
the
history
of
earth's
entire
hubble
volume
occur
every
so
often
simply
by
chance
tegmark
calculated
that
our
nearest
so-called
doppelgänger
is
10
meters
away
from
us
(a
double
exponential
function
larger
than
a
googolplex)
in
principle
it
would
be
impossible
to
scientifically
verify
the
existence
of
an
identical
hubble
volume
however
this
existence
does
follow
as
a
fairly
straightforward
consequence
it
is
possible
to
conceive
of
disconnected
spacetimes
each
existing
but
unable
to
interact
with
one
another
an
easily
visualized
metaphor
of
this
concept
is
a
group
of
separate
soap
bubbles
in
which
observers
living
on
one
soap
bubble
cannot
interact
with
those
on
other
soap
bubbles
even
in
principle
according
to
one
common
terminology
each
"soap
bubble"
of
spacetime
is
denoted
as
a
"universe"
whereas
our
particular
spacetime
is
denoted
as
"the
universe"
just
as
we
call
our
moon
"the
moon"
the
entire
collection
of
these
separate
spacetimes
is
denoted
as
the
multiverse
with
this
terminology
different
"universes"
are
not
causally
connected
to
each
other
in
principle
the
other
unconnected
"universes"
may
have
different
dimensionalities
and
topologies
of
spacetime
different
forms
of
matter
and
energy
and
different
physical
laws
and
physical
constants
although
such
possibilities
are
purely
speculative
others
consider
each
of
several
bubbles
created
as
part
of
chaotic
inflation
to
be
separate
"universes"
though
in
this
model
these
universes
all
share
a
causal
origin
historically
there
have
been
many
ideas
of
the
cosmos
(cosmologies)
and
its
origin
(cosmogonies)
theories
of
an
impersonal
universe
governed
by
physical
laws
were
first
proposed
by
the
greeks
and
indians
ancient
chinese
philosophy
encompassed
the
notion
of
the
universe
including
both
all
of
space
and
all
of
time
over
the
centuries
improvements
in
astronomical
observations
and
theories
of
motion
and
gravitation
led
to
ever
more
accurate
descriptions
of
the
universe
the
modern
era
of
cosmology
began
with
albert
einstein's
1915
general
theory
of
relativity
which
made
it
possible
to
quantitatively
predict
the
origin
evolution
and
conclusion
of
the
universe
as
a
whole
most
modern
accepted
theories
of
cosmology
are
based
on
general
relativity
and
more
specifically
the
predicted
big
bang
many
cultures
have
stories
describing
the
origin
of
the
world
and
universe
cultures
generally
regard
these
stories
as
having
some
truth
there
are
however
many
differing
beliefs
in
how
these
stories
apply
amongst
those
believing
in
a
supernatural
origin
ranging
from
a
god
directly
creating
the
universe
as
it
is
now
to
a
god
just
setting
the
"wheels
in
motion"
(for
example
via
mechanisms
such
as
the
big
bang
and
evolution)
ethnologists
and
anthropologists
who
study
myths
have
developed
various
classification
schemes
for
the
various
themes
that
appear
in
creation
stories
for
example
in
one
type
of
story
the
world
is
born
from
a
world
egg;
such
stories
include
the
finnish
epic
poem
"kalevala"
the
chinese
story
of
pangu
or
the
indian
brahmanda
purana
in
related
stories
the
universe
is
created
by
a
single
entity
emanating
or
producing
something
by
him-
or
herself
as
in
the
tibetan
buddhism
concept
of
adi-buddha
the
ancient
greek
story
of
gaia
(mother
earth)
the
aztec
goddess
coatlicue
myth
the
ancient
egyptian
god
atum
story
and
the
judeo-christian
genesis
creation
narrative
in
which
the
abrahamic
god
created
the
universe
in
another
type
of
story
the
universe
is
created
from
the
union
of
male
and
female
deities
as
in
the
maori
story
of
rangi
and
papa
in
other
stories
the
universe
is
created
by
crafting
it
from
pre-existing
materials
such
as
the
corpse
of
a
dead
god —
as
from
tiamat
in
the
babylonian
epic
"enuma
elish"
or
from
the
giant
ymir
in
norse
mythology –
or
from
chaotic
materials
as
in
izanagi
and
izanami
in
japanese
mythology
in
other
stories
the
universe
emanates
from
fundamental
principles
such
as
brahman
and
prakrti
the
creation
myth
of
the
serers
or
the
yin
and
yang
of
the
tao
the
pre-socratic
greek
philosophers
and
indian
philosophers
developed
some
of
the
earliest
philosophical
concepts
of
the
universe
the
earliest
greek
philosophers
noted
that
appearances
can
be
deceiving
and
sought
to
understand
the
underlying
reality
behind
the
appearances
in
particular
they
noted
the
ability
of
matter
to
change
forms
(eg
ice
to
water
to
steam)
and
several
philosophers
proposed
that
all
the
physical
materials
in
the
world
are
different
forms
of
a
single
primordial
material
or
"arche"
the
first
to
do
so
was
thales
who
proposed
this
material
to
be
water
thales'
student
anaximander
proposed
that
everything
came
from
the
limitless
"apeiron"
anaximenes
proposed
the
primordial
material
to
be
air
on
account
of
its
perceived
attractive
and
repulsive
qualities
that
cause
the
"arche"
to
condense
or
dissociate
into
different
forms
anaxagoras
proposed
the
principle
of
"nous"
(mind)
while
heraclitus
proposed
fire
(and
spoke
of
"logos")
empedocles
proposed
the
elements
to
be
earth
water
air
and
fire
his
four-element
model
became
very
popular
like
pythagoras
plato
believed
that
all
things
were
composed
of
number
with
empedocles'
elements
taking
the
form
of
the
platonic
solids
democritus
and
later
philosophers—most
notably
leucippus—proposed
that
the
universe
is
composed
of
indivisible
atoms
moving
through
a
void
(vacuum)
although
aristotle
did
not
believe
that
to
be
feasible
because
air
like
water
offers
resistance
to
motion
air
will
immediately
rush
in
to
fill
a
void
and
moreover
without
resistance
it
would
do
so
indefinitely
fast
although
heraclitus
argued
for
eternal
change
his
contemporary
parmenides
made
the
radical
suggestion
that
all
change
is
an
illusion
that
the
true
underlying
reality
is
eternally
unchanging
and
of
a
single
nature
parmenides
denoted
this
reality
as
(the
one)
parmenides'
idea
seemed
implausible
to
many
greeks
but
his
student
zeno
of
elea
challenged
them
with
several
famous
paradoxes
aristotle
responded
to
these
paradoxes
by
developing
the
notion
of
a
potential
countable
infinity
as
well
as
the
infinitely
divisible
continuum
unlike
the
eternal
and
unchanging
cycles
of
time
he
believed
that
the
world
is
bounded
by
the
celestial
spheres
and
that
cumulative
stellar
magnitude
is
only
finitely
multiplicative
the
indian
philosopher
kanada
founder
of
the
vaisheshika
school
developed
a
notion
of
atomism
and
proposed
that
light
and
heat
were
varieties
of
the
same
substance
in
the
5th
century
ad
the
buddhist
atomist
philosopher
dignāga
proposed
atoms
to
be
point-sized
durationless
and
made
of
energy
they
denied
the
existence
of
substantial
matter
and
proposed
that
movement
consisted
of
momentary
flashes
of
a
stream
of
energy
the
notion
of
temporal
finitism
was
inspired
by
the
doctrine
of
creation
shared
by
the
three
abrahamic
religions:
judaism
christianity
and
islam
the
christian
philosopher
john
philoponus
presented
the
philosophical
arguments
against
the
ancient
greek
notion
of
an
infinite
past
and
future
philoponus'
arguments
against
an
infinite
past
were
used
by
the
early
muslim
philosopher
al-kindi
(alkindus);
the
jewish
philosopher
saadia
gaon
(saadia
ben
joseph);
and
the
muslim
theologian
al-ghazali
(algazel)
astronomical
models
of
the
universe
were
proposed
soon
after
astronomy
began
with
the
babylonian
astronomers
who
viewed
the
universe
as
a
flat
disk
floating
in
the
ocean
and
this
forms
the
premise
for
early
greek
maps
like
those
of
anaximander
and
hecataeus
of
miletus
later
greek
philosophers
observing
the
motions
of
the
heavenly
bodies
were
concerned
with
developing
models
of
the
universe-based
more
profoundly
on
empirical
evidence
the
first
coherent
model
was
proposed
by
eudoxus
of
cnidos
according
to
aristotle's
physical
interpretation
of
the
model
celestial
spheres
eternally
rotate
with
uniform
motion
around
a
stationary
earth
normal
matter
is
entirely
contained
within
the
terrestrial
sphere
"de
mundo"
(composed
before
250
bc
or
between
350
and
200
bc)
stated
"five
elements
situated
in
spheres
in
five
regions
the
less
being
in
each
case
surrounded
by
the
greater—namely
earth
surrounded
by
water
water
by
air
air
by
fire
and
fire
by
ether—make
up
the
whole
universe"
this
model
was
also
refined
by
callippus
and
after
concentric
spheres
were
abandoned
it
was
brought
into
nearly
perfect
agreement
with
astronomical
observations
by
ptolemy
the
success
of
such
a
model
is
largely
due
to
the
mathematical
fact
that
any
function
(such
as
the
position
of
a
planet)
can
be
decomposed
into
a
set
of
circular
functions
(the
fourier
modes)
other
greek
scientists
such
as
the
pythagorean
philosopher
philolaus
postulated
(according
to
stobaeus
account)
that
at
the
center
of
the
universe
was
a
"central
fire"
around
which
the
earth
sun
moon
and
planets
revolved
in
uniform
circular
motion
the
greek
astronomer
aristarchus
of
samos
was
the
first
known
individual
to
propose
a
heliocentric
model
of
the
universe
though
the
original
text
has
been
lost
a
reference
in
archimedes'
book
"the
sand
reckoner"
describes
aristarchus's
heliocentric
model
archimedes
wrote:
you
king
gelon
are
aware
the
universe
is
the
name
given
by
most
astronomers
to
the
sphere
the
center
of
which
is
the
center
of
the
earth
while
its
radius
is
equal
to
the
straight
line
between
the
center
of
the
sun
and
the
center
of
the
earth
this
is
the
common
account
as
you
have
heard
from
astronomers
but
aristarchus
has
brought
out
a
book
consisting
of
certain
hypotheses
wherein
it
appears
as
a
consequence
of
the
assumptions
made
that
the
universe
is
many
times
greater
than
the
universe
just
mentioned
his
hypotheses
are
that
the
fixed
stars
and
the
sun
remain
unmoved
that
the
earth
revolves
about
the
sun
on
the
circumference
of
a
circle
the
sun
lying
in
the
middle
of
the
orbit
and
that
the
sphere
of
fixed
stars
situated
about
the
same
center
as
the
sun
is
so
great
that
the
circle
in
which
he
supposes
the
earth
to
revolve
bears
such
a
proportion
to
the
distance
of
the
fixed
stars
as
the
center
of
the
sphere
bears
to
its
surface
aristarchus
thus
believed
the
stars
to
be
very
far
away
and
saw
this
as
the
reason
why
stellar
parallax
had
not
been
observed
that
is
the
stars
had
not
been
observed
to
move
relative
each
other
as
the
earth
moved
around
the
sun
the
stars
are
in
fact
much
farther
away
than
the
distance
that
was
generally
assumed
in
ancient
times
which
is
why
stellar
parallax
is
only
detectable
with
precision
instruments
the
geocentric
model
consistent
with
planetary
parallax
was
assumed
to
be
an
explanation
for
the
unobservability
of
the
parallel
phenomenon
stellar
parallax
the
rejection
of
the
heliocentric
view
was
apparently
quite
strong
as
the
following
passage
from
plutarch
suggests
("on
the
apparent
face
in
the
orb
of
the
moon"):
cleanthes
[a
contemporary
of
aristarchus
and
head
of
the
stoics]
thought
it
was
the
duty
of
the
greeks
to
indict
aristarchus
of
samos
on
the
charge
of
impiety
for
putting
in
motion
the
hearth
of
the
universe
[ie
the
earth]
supposing
the
heaven
to
remain
at
rest
and
the
earth
to
revolve
in
an
oblique
circle
while
it
rotates
at
the
same
time
about
its
own
axis
the
only
other
astronomer
from
antiquity
known
by
name
who
supported
aristarchus's
heliocentric
model
was
seleucus
of
seleucia
a
hellenistic
astronomer
who
lived
a
century
after
aristarchus
according
to
plutarch
seleucus
was
the
first
to
prove
the
heliocentric
system
through
reasoning
but
it
is
not
known
what
arguments
he
used
seleucus'
arguments
for
a
heliocentric
cosmology
were
probably
related
to
the
phenomenon
of
tides
according
to
strabo
(119)
seleucus
was
the
first
to
state
that
the
tides
are
due
to
the
attraction
of
the
moon
and
that
the
height
of
the
tides
depends
on
the
moon's
position
relative
to
the
sun
alternatively
he
may
have
proved
heliocentricity
by
determining
the
constants
of
a
geometric
model
for
it
and
by
developing
methods
to
compute
planetary
positions
using
this
model
like
what
nicolaus
copernicus
later
did
in
the
16th
century
during
the
middle
ages
heliocentric
models
were
also
proposed
by
the
indian
astronomer
aryabhata
and
by
the
persian
astronomers
albumasar
and
al-sijzi
the
aristotelian
model
was
accepted
in
the
western
world
for
roughly
two
millennia
until
copernicus
revived
aristarchus's
perspective
that
the
astronomical
data
could
be
explained
more
plausibly
if
the
earth
rotated
on
its
axis
and
if
the
sun
were
placed
at
the
center
of
the
universe
as
noted
by
copernicus
himself
the
notion
that
the
earth
rotates
is
very
old
dating
at
least
to
philolaus
(c
450
bc)
heraclides
ponticus
(c
350
bc)
and
ecphantus
the
pythagorean
roughly
a
century
before
copernicus
the
christian
scholar
nicholas
of
cusa
also
proposed
that
the
earth
rotates
on
its
axis
in
his
book
"on
learned
ignorance"
(1440)
al-sijzi
also
proposed
that
the
earth
rotates
on
its
axis
empirical
evidence
for
the
earth's
rotation
on
its
axis
using
the
phenomenon
of
comets
was
given
by
tusi
(1201–1274)
and
ali
qushji
(1403–1474)
this
cosmology
was
accepted
by
isaac
newton
christiaan
huygens
and
later
scientists
edmund
halley
(1720)
and
jean-philippe
de
chéseaux
(1744)
noted
independently
that
the
assumption
of
an
infinite
space
filled
uniformly
with
stars
would
lead
to
the
prediction
that
the
nighttime
sky
would
be
as
bright
as
the
sun
itself;
this
became
known
as
olbers'
paradox
in
the
19th
century
newton
believed
that
an
infinite
space
uniformly
filled
with
matter
would
cause
infinite
forces
and
instabilities
causing
the
matter
to
be
crushed
inwards
under
its
own
gravity
this
instability
was
clarified
in
1902
by
the
jeans
instability
criterion
one
solution
to
these
paradoxes
is
the
charlier
universe
in
which
the
matter
is
arranged
hierarchically
(systems
of
orbiting
bodies
that
are
themselves
orbiting
in
a
larger
system
"ad
infinitum")
in
a
fractal
way
such
that
the
universe
has
a
negligibly
small
overall
density;
such
a
cosmological
model
had
also
been
proposed
earlier
in
1761
by
johann
heinrich
lambert
a
significant
astronomical
advance
of
the
18th
century
was
the
realization
by
thomas
wright
immanuel
kant
and
others
of
nebulae
in
1919
when
hooker
telescope
was
completed
the
prevailing
view
still
was
that
the
universe
consisted
entirely
of
the
milky
way
galaxy
using
the
hooker
telescope
edwin
hubble
identified
cepheid
variables
in
several
spiral
nebulae
and
in
1922–1923
proved
conclusively
that
andromeda
nebula
and
triangulum
among
others
were
entire
galaxies
outside
our
own
thus
proving
that
universe
consists
of
multitude
of
galaxies
the
modern
era
of
physical
cosmology
began
in
1917
when
albert
einstein
first
applied
his
general
theory
of
relativity
to
model
the
structure
and
dynamics
of
the
universe
outer
space
outer
space
or
just
space
is
the
expanse
that
exists
beyond
the
earth
and
outside
of
any
astronomical
object
outer
space
is
not
completely
empty—it
is
a
hard
vacuum
containing
a
low
density
of
particles
predominantly
a
plasma
of
hydrogen
and
helium
as
well
as
electromagnetic
radiation
magnetic
fields
neutrinos
dust
and
cosmic
rays
the
baseline
temperature
as
set
by
the
background
radiation
from
the
big
bang
is
the
plasma
between
galaxies
accounts
for
about
half
of
the
baryonic
(ordinary)
matter
in
the
universe;
it
has
a
number
density
of
less
than
one
hydrogen
atom
per
cubic
metre
and
a
temperature
of
millions
of
kelvins;
local
concentrations
of
this
plasma
have
condensed
into
stars
and
galaxies
studies
indicate
that
90%
of
the
mass
in
most
galaxies
is
in
an
unknown
form
called
dark
matter
which
interacts
with
other
matter
through
gravitational
but
not
electromagnetic
forces
observations
suggest
that
the
majority
of
the
mass-energy
in
the
observable
universe
is
a
poorly
understood
vacuum
energy
of
space
which
astronomers
label
"dark
energy"
intergalactic
space
takes
up
most
of
the
volume
of
the
universe
but
even
galaxies
and
star
systems
consist
almost
entirely
of
empty
space
outer
space
does
not
begin
at
a
definite
altitude
above
the
earth's
surface
however
the
kármán
line
an
altitude
of
above
sea
level
is
conventionally
used
as
the
start
of
outer
space
in
space
treaties
and
for
aerospace
records
keeping
the
framework
for
international
space
law
was
established
by
the
outer
space
treaty
which
entered
into
force
on
10
october
1967
this
treaty
precludes
any
claims
of
national
sovereignty
and
permits
all
states
to
freely
explore
outer
space
despite
the
drafting
of
un
resolutions
for
the
peaceful
uses
of
outer
space
anti-satellite
weapons
have
been
tested
in
earth
orbit
humans
began
the
physical
exploration
of
space
during
the
20th
century
with
the
advent
of
high-altitude
balloon
flights
followed
by
manned
rocket
launches
earth
orbit
was
first
achieved
by
yuri
gagarin
of
the
soviet
union
in
1961
and
unmanned
spacecraft
have
since
reached
all
of
the
known
planets
in
the
solar
system
due
to
the
high
cost
of
getting
into
space
manned
spaceflight
has
been
limited
to
low
earth
orbit
and
the
moon
outer
space
represents
a
challenging
environment
for
human
exploration
because
of
the
hazards
of
vacuum
and
radiation
microgravity
also
has
a
negative
effect
on
human
physiology
that
causes
both
muscle
atrophy
and
bone
loss
in
addition
to
these
health
and
environmental
issues
the
economic
cost
of
putting
objects
including
humans
into
space
is
very
high
in
350
bce
greek
philosopher
aristotle
suggested
that
"nature
abhors
a
vacuum"
a
principle
that
became
known
as
the
"horror
vacui"
this
concept
built
upon
a
5th-century
bce
ontological
argument
by
the
greek
philosopher
parmenides
who
denied
the
possible
existence
of
a
void
in
space
based
on
this
idea
that
a
vacuum
could
not
exist
in
the
west
it
was
widely
held
for
many
centuries
that
space
could
not
be
empty
as
late
as
the
17th
century
the
french
philosopher
rené
descartes
argued
that
the
entirety
of
space
must
be
filled
in
ancient
china
the
2nd-century
astronomer
zhang
heng
became
convinced
that
space
must
be
infinite
extending
well
beyond
the
mechanism
that
supported
the
sun
and
the
stars
the
surviving
books
of
the
hsüan
yeh
school
said
that
the
heavens
were
boundless
"empty
and
void
of
substance"
likewise
the
"sun
moon
and
the
company
of
stars
float
in
the
empty
space
moving
or
standing
still"
the
italian
scientist
galileo
galilei
knew
that
air
had
mass
and
so
was
subject
to
gravity
in
1640
he
demonstrated
that
an
established
force
resisted
the
formation
of
a
vacuum
however
it
would
remain
for
his
pupil
evangelista
torricelli
to
create
an
apparatus
that
would
produce
a
partial
vacuum
in
1643
this
experiment
resulted
in
the
first
mercury
barometer
and
created
a
scientific
sensation
in
europe
the
french
mathematician
blaise
pascal
reasoned
that
if
the
column
of
mercury
was
supported
by
air
then
the
column
ought
to
be
shorter
at
higher
altitude
where
the
air
pressure
is
lower
in
1648
his
brother-in-law
florin
périer
repeated
the
experiment
on
the
puy
de
dôme
mountain
in
central
france
and
found
that
the
column
was
shorter
by
three
inches
this
decrease
in
pressure
was
further
demonstrated
by
carrying
a
half-full
balloon
up
a
mountain
and
watching
it
gradually
expand
then
contract
upon
descent
in
1650
german
scientist
otto
von
guericke
constructed
the
first
vacuum
pump:
a
device
that
would
further
refute
the
principle
of
"horror
vacui"
he
correctly
noted
that
the
atmosphere
of
the
earth
surrounds
the
planet
like
a
shell
with
the
density
gradually
declining
with
altitude
he
concluded
that
there
must
be
a
vacuum
between
the
earth
and
the
moon
back
in
the
15th
century
german
theologian
nicolaus
cusanus
speculated
that
the
universe
lacked
a
center
and
a
circumference
he
believed
that
the
universe
while
not
infinite
could
not
be
held
as
finite
as
it
lacked
any
bounds
within
which
it
could
be
contained
these
ideas
led
to
speculations
as
to
the
infinite
dimension
of
space
by
the
italian
philosopher
giordano
bruno
in
the
16th
century
he
extended
the
copernican
heliocentric
cosmology
to
the
concept
of
an
infinite
universe
filled
with
a
substance
he
called
aether
which
did
not
resist
the
motion
of
heavenly
bodies
english
philosopher
william
gilbert
arrived
at
a
similar
conclusion
arguing
that
the
stars
are
visible
to
us
only
because
they
are
surrounded
by
a
thin
aether
or
a
void
this
concept
of
an
aether
originated
with
ancient
greek
philosophers
including
aristotle
who
conceived
of
it
as
the
medium
through
which
the
heavenly
bodies
move
the
concept
of
a
universe
filled
with
a
luminiferous
aether
retained
support
among
some
scientists
until
the
early
20th
century
this
form
of
aether
was
viewed
as
the
medium
through
which
light
could
propagate
in
1887
the
michelson–morley
experiment
tried
to
detect
the
earth's
motion
through
this
medium
by
looking
for
changes
in
the
speed
of
light
depending
on
the
direction
of
the
planet's
motion
however
the
null
result
indicated
something
was
wrong
with
the
concept
the
idea
of
the
luminiferous
aether
was
then
abandoned
it
was
replaced
by
albert
einstein's
theory
of
special
relativity
which
holds
that
the
speed
of
light
in
a
vacuum
is
a
fixed
constant
independent
of
the
observer's
motion
or
frame
of
reference
the
first
professional
astronomer
to
support
the
concept
of
an
infinite
universe
was
the
englishman
thomas
digges
in
1576
but
the
scale
of
the
universe
remained
unknown
until
the
first
successful
measurement
of
the
distance
to
a
nearby
star
in
1838
by
the
german
astronomer
friedrich
bessel
he
showed
that
the
star
61
cygni
had
a
parallax
of
just
031 arcseconds
(compared
to
the
modern
value
of
0287″)
this
corresponds
to
a
distance
of
over
10
light
years
in
1917
heber
curtis
noted
that
novae
in
spiral
nebulae
were
on
average
10
magnitudes
fainter
than
galactic
novae
suggesting
that
the
former
are
100
times
further
away
the
distance
to
the
andromeda
galaxy
was
determined
in
1923
by
american
astronomer
edwin
hubble
by
measuring
the
brightness
of
cepheid
variables
in
that
galaxy
a
new
technique
discovered
by
henrietta
leavitt
this
established
that
the
andromeda
galaxy
and
by
extension
all
galaxies
lay
well
outside
the
milky
way
the
modern
concept
of
outer
space
is
based
on
the
"big
bang"
cosmology
first
proposed
in
1931
by
the
belgian
physicist
georges
lemaître
this
theory
holds
that
the
universe
originated
from
a
very
dense
form
that
has
since
undergone
continuous
expansion
the
earliest
known
estimate
of
the
temperature
of
outer
space
was
by
the
swiss
physicist
charles
é
guillaume
in
1896
using
the
estimated
radiation
of
the
background
stars
he
concluded
that
space
must
be
heated
to
a
temperature
of
5–6 k
british
physicist
arthur
eddington
made
a
similar
calculation
to
derive
a
temperature
of
318 k
in
1926
german
physicist
erich
regener
used
the
total
measured
energy
of
cosmic
rays
to
estimate
an
intergalactic
temperature
of
28 k
in
1933
american
physicists
ralph
alpher
and
robert
herman
predicted
5 k
for
the
temperature
of
space
in
1948
based
on
the
gradual
decrease
in
background
energy
following
the
then-new
big
bang
theory
the
modern
measurement
of
the
cosmic
microwave
background
is
about
27k
the
term
"outward
space"
was
used
in
1842
by
the
english
poet
lady
emmeline
stuart-wortley
in
her
poem
"the
maiden
of
moscow"
the
expression
"outer
space"
was
used
as
an
astronomical
term
by
alexander
von
humboldt
in
1845
it
was
later
popularized
in
the
writings
of
h
g
wells
in
1901
the
shorter
term
"space"
is
older
first
used
to
mean
the
region
beyond
earth's
sky
in
john
milton's
"paradise
lost"
in
1667
according
to
the
big
bang
theory
the
very
early
universe
was
an
extremely
hot
and
dense
state
about
138 billion
years
ago
which
rapidly
expanded
about
380000
years
later
the
universe
had
cooled
sufficiently
to
allow
protons
and
electrons
to
combine
and
form
hydrogen—the
so-called
recombination
epoch
when
this
happened
matter
and
energy
became
decoupled
allowing
photons
to
travel
freely
through
the
continually
expanding
space
matter
that
remained
following
the
initial
expansion
has
since
undergone
gravitational
collapse
to
create
stars
galaxies
and
other
astronomical
objects
leaving
behind
a
deep
vacuum
that
forms
what
is
now
called
outer
space
as
light
has
a
finite
velocity
this
theory
also
constrains
the
size
of
the
directly
observable
universe
this
leaves
open
the
question
as
to
whether
the
universe
is
finite
or
infinite
the
present
day
shape
of
the
universe
has
been
determined
from
measurements
of
the
cosmic
microwave
background
using
satellites
like
the
wilkinson
microwave
anisotropy
probe
these
observations
indicate
that
the
spatial
geometry
of
the
observable
universe
is
"flat"
meaning
that
photons
on
parallel
paths
at
one
point
remain
parallel
as
they
travel
through
space
to
the
limit
of
the
observable
universe
except
for
local
gravity
the
flat
universe
combined
with
the
measured
mass
density
of
the
universe
and
the
accelerating
expansion
of
the
universe
indicates
that
space
has
a
non-zero
vacuum
energy
which
is
called
dark
energy
estimates
put
the
average
energy
density
of
the
present
day
universe
at
the
equivalent
of
59
protons
per
cubic
meter
including
dark
energy
dark
matter
and
baryonic
matter
(ordinary
matter
composed
of
atoms)
the
atoms
account
for
only
46%
of
the
total
energy
density
or
a
density
of
one
proton
per
four
cubic
meters
the
density
of
the
universe
however
is
clearly
not
uniform;
it
ranges
from
relatively
high
density
in
galaxies—including
very
high
density
in
structures
within
galaxies
such
as
planets
stars
and
black
holes—to
conditions
in
vast
voids
that
have
much
lower
density
at
least
in
terms
of
visible
matter
unlike
matter
and
dark
matter
dark
energy
seems
not
to
be
concentrated
in
galaxies:
although
dark
energy
may
account
for
a
majority
of
the
mass-energy
in
the
universe
dark
energy's
influence
is
5
orders
of
magnitude
smaller
than
the
influence
of
gravity
from
matter
and
dark
matter
within
the
milky
way
outer
space
is
the
closest
known
approximation
to
a
perfect
vacuum
it
has
effectively
no
friction
allowing
stars
planets
and
moons
to
move
freely
along
their
ideal
orbits
following
the
initial
formation
stage
however
even
the
deep
vacuum
of
intergalactic
space
is
not
devoid
of
matter
as
it
contains
a
few
hydrogen
atoms
per
cubic
meter
by
comparison
the
air
humans
breathe
contains
about
10
molecules
per
cubic
meter
the
low
density
of
matter
in
outer
space
means
that
electromagnetic
radiation
can
travel
great
distances
without
being
scattered:
the
mean
free
path
of
a
photon
in
intergalactic
space
is
about
10 km
or
10 billion
light
years
in
spite
of
this
extinction
which
is
the
absorption
and
scattering
of
photons
by
dust
and
gas
is
an
important
factor
in
galactic
and
intergalactic
astronomy
stars
planets
and
moons
retain
their
atmospheres
by
gravitational
attraction
atmospheres
have
no
clearly
delineated
upper
boundary:
the
density
of
atmospheric
gas
gradually
decreases
with
distance
from
the
object
until
it
becomes
indistinguishable
from
outer
space
the
earth's
atmospheric
pressure
drops
to
about
pa
at
of
altitude
compared
to
100000
pa
for
the
international
union
of
pure
and
applied
chemistry
(iupac)
definition
of
standard
pressure
above
this
altitude
isotropic
gas
pressure
rapidly
becomes
insignificant
when
compared
to
radiation
pressure
from
the
sun
and
the
dynamic
pressure
of
the
solar
wind
the
thermosphere
in
this
range
has
large
gradients
of
pressure
temperature
and
composition
and
varies
greatly
due
to
space
weather
the
temperature
of
outer
space
is
measured
in
terms
of
the
kinetic
activity
of
the
gas
as
it
is
on
earth
however
the
radiation
of
outer
space
has
a
different
temperature
than
the
kinetic
temperature
of
the
gas
meaning
that
the
gas
and
radiation
are
not
in
thermodynamic
equilibrium
all
of
the
observable
universe
is
filled
with
photons
that
were
created
during
the
big
bang
which
is
known
as
the
cosmic
microwave
background
radiation
(cmb)
(there
is
quite
likely
a
correspondingly
large
number
of
neutrinos
called
the
cosmic
neutrino
background)
the
current
black
body
temperature
of
the
background
radiation
is
about
the
gas
temperatures
in
outer
space
are
always
at
least
the
temperature
of
the
cmb
but
can
be
much
higher
for
example
the
corona
of
the
sun
reaches
temperatures
over
12–26 million k
magnetic
fields
have
been
detected
in
the
space
around
just
about
every
class
of
celestial
object
star
formation
in
spiral
galaxies
can
generate
small-scale
dynamos
creating
turbulent
magnetic
field
strengths
of
around
5–10 μg
the
davis–greenstein
effect
causes
elongated
dust
grains
to
align
themselves
with
a
galaxy's
magnetic
field
resulting
in
weak
optical
polarization
this
has
been
used
to
show
ordered
magnetic
fields
exist
in
several
nearby
galaxies
magneto-hydrodynamic
processes
in
active
elliptical
galaxies
produce
their
characteristic
jets
and
radio
lobes
non-thermal
radio
sources
have
been
detected
even
among
the
most
distant
high-z
sources
indicating
the
presence
of
magnetic
fields
outside
a
protective
atmosphere
and
magnetic
field
there
are
few
obstacles
to
the
passage
through
space
of
energetic
subatomic
particles
known
as
cosmic
rays
these
particles
have
energies
ranging
from
about
10 ev
up
to
an
extreme
10 ev
of
ultra-high-energy
cosmic
rays
the
peak
flux
of
cosmic
rays
occurs
at
energies
of
about
10 ev
with
approximately
87%
protons
12%
helium
nuclei
and
1%
heavier
nuclei
in
the
high
energy
range
the
flux
of
electrons
is
only
about
1%
of
that
of
protons
cosmic
rays
can
damage
electronic
components
and
pose
a
health
threat
to
space
travelers
according
to
astronauts
like
don
pettit
space
has
a
burned/metallic
odor
that
clings
to
their
suits
and
equipment
similar
to
the
scent
of
an
arc
welding
torch
despite
the
harsh
environment
several
life
forms
have
been
found
that
can
withstand
extreme
space
conditions
for
extended
periods
species
of
lichen
carried
on
the
esa
biopan
facility
survived
exposure
for
ten
days
in
2007
seeds
of
"arabidopsis
thaliana"
and
"nicotiana
tabacum"
germinated
after
being
exposed
to
space
for
15
years
a
strain
of
"bacillus
subtilis"
has
survived
559
days
when
exposed
to
low-earth
orbit
or
a
simulated
martian
environment
the
lithopanspermia
hypothesis
suggests
that
rocks
ejected
into
outer
space
from
life-harboring
planets
may
successfully
transport
life
forms
to
another
habitable
world
a
conjecture
is
that
just
such
a
scenario
occurred
early
in
the
history
of
the
solar
system
with
potentially
microorganism-bearing
rocks
being
exchanged
between
venus
earth
and
mars
even
at
relatively
low
altitudes
in
the
earth's
atmosphere
conditions
are
hostile
to
the
human
body
the
altitude
where
atmospheric
pressure
matches
the
vapor
pressure
of
water
at
the
temperature
of
the
human
body
is
called
the
armstrong
line
named
after
american
physician
harry
g
armstrong
it
is
located
at
an
altitude
of
around
at
or
above
the
armstrong
line
fluids
in
the
throat
and
lungs
boil
away
more
specifically
exposed
bodily
liquids
such
as
saliva
tears
and
liquids
in
the
lungs
boil
away
hence
at
this
altitude
human
survival
requires
a
pressure
suit
or
a
pressurized
capsule
once
in
space
sudden
exposure
of
unprotected
humans
to
very
low
pressure
such
as
during
a
rapid
decompression
can
cause
pulmonary
barotrauma—a
rupture
of
the
lungs
due
to
the
large
pressure
differential
between
inside
and
outside
the
chest
even
if
the
subject's
airway
is
fully
open
the
flow
of
air
through
the
windpipe
may
be
too
slow
to
prevent
the
rupture
rapid
decompression
can
rupture
eardrums
and
sinuses
bruising
and
blood
seep
can
occur
in
soft
tissues
and
shock
can
cause
an
increase
in
oxygen
consumption
that
leads
to
hypoxia
as
a
consequence
of
rapid
decompression
oxygen
dissolved
in
the
blood
empties
into
the
lungs
to
try
to
equalize
the
partial
pressure
gradient
once
the
deoxygenated
blood
arrives
at
the
brain
humans
lose
consciousness
after
a
few
seconds
and
die
of
hypoxia
within
minutes
blood
and
other
body
fluids
boil
when
the
pressure
drops
below
63 kpa
and
this
condition
is
called
ebullism
the
steam
may
bloat
the
body
to
twice
its
normal
size
and
slow
circulation
but
tissues
are
elastic
and
porous
enough
to
prevent
rupture
ebullism
is
slowed
by
the
pressure
containment
of
blood
vessels
so
some
blood
remains
liquid
swelling
and
ebullism
can
be
reduced
by
containment
in
a
pressure
suit
the
crew
altitude
protection
suit
(caps)
a
fitted
elastic
garment
designed
in
the
1960s
for
astronauts
prevents
ebullism
at
pressures
as
low
as
2
kpa
supplemental
oxygen
is
needed
at
to
provide
enough
oxygen
for
breathing
and
to
prevent
water
loss
while
above
pressure
suits
are
essential
to
prevent
ebullism
most
space
suits
use
around
30–39 kpa
of
pure
oxygen
about
the
same
as
on
the
earth's
surface
this
pressure
is
high
enough
to
prevent
ebullism
but
evaporation
of
nitrogen
dissolved
in
the
blood
could
still
cause
decompression
sickness
and
gas
embolisms
if
not
managed
humans
evolved
for
life
in
earth
gravity
and
exposure
to
weightlessness
has
been
shown
to
have
deleterious
effects
on
human
health
initially
more
than
50%
of
astronauts
experience
space
motion
sickness
this
can
cause
nausea
and
vomiting
vertigo
headaches
lethargy
and
overall
malaise
the
duration
of
space
sickness
varies
but
it
typically
lasts
for
1–3
days
after
which
the
body
adjusts
to
the
new
environment
longer-term
exposure
to
weightlessness
results
in
muscle
atrophy
and
deterioration
of
the
skeleton
or
spaceflight
osteopenia
these
effects
can
be
minimized
through
a
regimen
of
exercise
other
effects
include
fluid
redistribution
slowing
of
the
cardiovascular
system
decreased
production
of
red
blood
cells
balance
disorders
and
a
weakening
of
the
immune
system
lesser
symptoms
include
loss
of
body
mass
nasal
congestion
sleep
disturbance
and
puffiness
of
the
face
for
long-duration
space
travel
radiation
can
pose
an
acute
health
hazard
exposure
to
high-energy
ionizing
cosmic
rays
can
result
in
fatigue
nausea
vomiting
as
well
as
damage
to
the
immune
system
and
changes
to
the
white
blood
cell
count
over
longer
durations
symptoms
include
an
increased
risk
of
cancer
plus
damage
to
the
eyes
nervous
system
lungs
and
the
gastrointestinal
tract
on
a
round-trip
mars
mission
lasting
three
years
a
large
fraction
of
the
cells
in
an
astronaut's
body
would
be
traversed
and
potentially
damaged
by
high
energy
nuclei
the
energy
of
such
particles
is
significantly
diminished
by
the
shielding
provided
by
the
walls
of
a
spacecraft
and
can
be
further
diminished
by
water
containers
and
other
barriers
however
the
impact
of
the
cosmic
rays
upon
the
shielding
produces
additional
radiation
that
can
affect
the
crew
further
research
is
needed
to
assess
the
radiation
hazards
and
determine
suitable
countermeasures
there
is
no
clear
boundary
between
earth's
atmosphere
and
space
as
the
density
of
the
atmosphere
gradually
decreases
as
the
altitude
increases
there
are
several
standard
boundary
designations
namely:
in
2009
scientists
reported
detailed
measurements
with
a
supra-thermal
ion
imager
(an
instrument
that
measures
the
direction
and
speed
of
ions)
which
allowed
them
to
establish
a
boundary
at
above
earth
the
boundary
represents
the
midpoint
of
a
gradual
transition
over
tens
of
kilometers
from
the
relatively
gentle
winds
of
the
earth's
atmosphere
to
the
more
violent
flows
of
charged
particles
in
space
which
can
reach
speeds
well
over
the
outer
space
treaty
provides
the
basic
framework
for
international
space
law
it
covers
the
legal
use
of
outer
space
by
nation
states
and
includes
in
its
definition
of
"outer
space"
the
moon
and
other
celestial
bodies
the
treaty
states
that
outer
space
is
free
for
all
nation
states
to
explore
and
is
not
subject
to
claims
of
national
sovereignty
it
also
prohibits
the
deployment
of
nuclear
weapons
in
outer
space
the
treaty
was
passed
by
the
united
nations
general
assembly
in
1963
and
signed
in
1967
by
the
ussr
the
united
states
of
america
and
the
united
kingdom
as
of
2017
105
state
parties
have
either
ratified
or
acceded
to
the
treaty
an
additional
25
states
signed
the
treaty
without
ratifying
it
since
1958
outer
space
has
been
the
subject
of
multiple
united
nations
resolutions
of
these
more
than
50
have
been
concerning
the
international
co-operation
in
the
peaceful
uses
of
outer
space
and
preventing
an
arms
race
in
space
four
additional
space
law
treaties
have
been
negotiated
and
drafted
by
the
un's
committee
on
the
peaceful
uses
of
outer
space
still
there
remains
no
legal
prohibition
against
deploying
conventional
weapons
in
space
and
anti-satellite
weapons
have
been
successfully
tested
by
the
us
ussr
and
china
the
1979
moon
treaty
turned
the
jurisdiction
of
all
heavenly
bodies
(including
the
orbits
around
such
bodies)
over
to
the
international
community
however
this
treaty
has
not
been
ratified
by
any
nation
that
currently
practices
manned
spaceflight
in
1976
eight
equatorial
states
(ecuador
colombia
brazil
congo
zaire
uganda
kenya
and
indonesia)
met
in
bogotá
colombia
with
their
"declaration
of
the
first
meeting
of
equatorial
countries"
or
"the
bogotá
declaration"
they
claimed
control
of
the
segment
of
the
geosynchronous
orbital
path
corresponding
to
each
country
these
claims
are
not
internationally
accepted
a
spacecraft
enters
orbit
when
its
centripetal
acceleration
due
to
gravity
is
less
than
or
equal
to
the
centrifugal
acceleration
due
to
the
horizontal
component
of
its
velocity
for
a
low
earth
orbit
this
velocity
is
about
;
by
contrast
the
fastest
manned
airplane
speed
ever
achieved
(excluding
speeds
achieved
by
deorbiting
spacecraft)
was
in
1967
by
the
north
american
x-15
to
achieve
an
orbit
a
spacecraft
must
travel
faster
than
a
sub-orbital
spaceflight
the
energy
required
to
reach
earth
orbital
velocity
at
an
altitude
of
is
about
36 mj/kg
which
is
six
times
the
energy
needed
merely
to
climb
to
the
corresponding
altitude
spacecraft
with
a
perigee
below
about
are
subject
to
drag
from
the
earth's
atmosphere
which
decreases
the
orbital
altitude
the
rate
of
orbital
decay
depends
on
the
satellite's
cross-sectional
area
and
mass
as
well
as
variations
in
the
air
density
of
the
upper
atmosphere
below
about
decay
becomes
more
rapid
with
lifetimes
measured
in
days
once
a
satellite
descends
to
it
has
only
hours
before
it
vaporizes
in
the
atmosphere
the
escape
velocity
required
to
pull
free
of
earth's
gravitational
field
altogether
and
move
into
interplanetary
space
is
about
space
is
a
partial
vacuum:
its
different
regions
are
defined
by
the
various
atmospheres
and
"winds"
that
dominate
within
them
and
extend
to
the
point
at
which
those
winds
give
way
to
those
beyond
geospace
extends
from
earth's
atmosphere
to
the
outer
reaches
of
earth's
magnetic
field
whereupon
it
gives
way
to
the
solar
wind
of
interplanetary
space
interplanetary
space
extends
to
the
heliopause
whereupon
the
solar
wind
gives
way
to
the
winds
of
the
interstellar
medium
interstellar
space
then
continues
to
the
edges
of
the
galaxy
where
it
fades
into
the
intergalactic
void
geospace
is
the
region
of
outer
space
near
earth
including
the
upper
atmosphere
and
magnetosphere
the
van
allen
radiation
belts
lie
within
the
geospace
the
outer
boundary
of
geospace
is
the
magnetopause
which
forms
an
interface
between
the
earth's
magnetosphere
and
the
solar
wind
the
inner
boundary
is
the
ionosphere
the
variable
space-weather
conditions
of
geospace
are
affected
by
the
behavior
of
the
sun
and
the
solar
wind;
the
subject
of
geospace
is
interlinked
with
heliophysics—the
study
of
the
sun
and
its
impact
on
the
planets
of
the
solar
system
the
day-side
magnetopause
is
compressed
by
solar-wind
pressure—the
subsolar
distance
from
the
center
of
the
earth
is
typically
10
earth
radii
on
the
night
side
the
solar
wind
stretches
the
magnetosphere
to
form
a
magnetotail
that
sometimes
extends
out
to
more
than
100–200
earth
radii
for
roughly
four
days
of
each
month
the
lunar
surface
is
shielded
from
the
solar
wind
as
the
moon
passes
through
the
magnetotail
geospace
is
populated
by
electrically
charged
particles
at
very
low
densities
the
motions
of
which
are
controlled
by
the
earth's
magnetic
field
these
plasmas
form
a
medium
from
which
storm-like
disturbances
powered
by
the
solar
wind
can
drive
electrical
currents
into
the
earth's
upper
atmosphere
geomagnetic
storms
can
disturb
two
regions
of
geospace
the
radiation
belts
and
the
ionosphere
these
storms
increase
fluxes
of
energetic
electrons
that
can
permanently
damage
satellite
electronics
interfering
with
shortwave
radio
communication
and
gps
location
and
timing
magnetic
storms
can
also
be
a
hazard
to
astronauts
even
in
low
earth
orbit
they
also
create
aurorae
seen
at
high
latitudes
in
an
oval
surrounding
the
geomagnetic
poles
although
it
meets
the
definition
of
outer
space
the
atmospheric
density
within
the
first
few
hundred
kilometers
above
the
kármán
line
is
still
sufficient
to
produce
significant
drag
on
satellites
this
region
contains
material
left
over
from
previous
manned
and
unmanned
launches
that
are
a
potential
hazard
to
spacecraft
some
of
this
debris
re-enters
earth's
atmosphere
periodically
earth's
gravity
keeps
the
moon
in
orbit
at
an
average
distance
of
the
region
outside
earth's
atmosphere
and
extending
out
to
just
beyond
the
moon's
orbit
including
the
lagrangian
points
is
sometimes
referred
to
as
cislunar
space
the
region
of
space
where
earth's
gravity
remains
dominant
against
gravitational
perturbations
from
the
sun
is
called
the
hill
sphere
this
extends
well
out
into
translunar
space
to
a
distance
of
roughly
1%
of
the
mean
distance
from
earth
to
the
sun
or
deep
space
has
different
definitions
as
to
where
it
starts
it
has
been
defined
by
the
united
states
government
and
others
as
any
region
beyond
cislunar
space
the
international
telecommunication
union
responsible
for
radio
communication
(including
satellites)
defines
the
beginning
of
deep
space
at
about
5
times
that
distance
()
interplanetary
space
is
defined
by
the
solar
wind
a
continuous
stream
of
charged
particles
emanating
from
the
sun
that
creates
a
very
tenuous
atmosphere
(the
heliosphere)
for
billions
of
kilometers
into
space
this
wind
has
a
particle
density
of
5–10
protons/cm
and
is
moving
at
a
velocity
of
interplanetary
space
extends
out
to
the
heliopause
where
the
influence
of
the
galactic
environment
starts
to
dominate
over
the
magnetic
field
and
particle
flux
from
the
sun
the
distance
and
strength
of
the
heliopause
varies
depending
on
the
activity
level
of
the
solar
wind
the
heliopause
in
turn
deflects
away
low-energy
galactic
cosmic
rays
with
this
modulation
effect
peaking
during
solar
maximum
the
volume
of
interplanetary
space
is
a
nearly
total
vacuum
with
a
mean
free
path
of
about
one
astronomical
unit
at
the
orbital
distance
of
the
earth
however
this
space
is
not
completely
empty
and
is
sparsely
filled
with
cosmic
rays
which
include
ionized
atomic
nuclei
and
various
subatomic
particles
there
is
also
gas
plasma
and
dust
small
meteors
and
several
dozen
types
of
organic
molecules
discovered
to
date
by
microwave
spectroscopy
a
cloud
of
interplanetary
dust
is
visible
at
night
as
a
faint
band
called
the
zodiacal
light
interplanetary
space
contains
the
magnetic
field
generated
by
the
sun
there
are
also
magnetospheres
generated
by
planets
such
as
jupiter
saturn
mercury
and
the
earth
that
have
their
own
magnetic
fields
these
are
shaped
by
the
influence
of
the
solar
wind
into
the
approximation
of
a
teardrop
shape
with
the
long
tail
extending
outward
behind
the
planet
these
magnetic
fields
can
trap
particles
from
the
solar
wind
and
other
sources
creating
belts
of
charged
particles
such
as
the
van
allen
radiation
belts
planets
without
magnetic
fields
such
as
mars
have
their
atmospheres
gradually
eroded
by
the
solar
wind
interstellar
space
is
the
physical
space
within
a
galaxy
beyond
the
influence
each
star
has
upon
the
encompassed
plasma
the
contents
of
interstellar
space
are
called
the
interstellar
medium
approximately
70%
of
the
mass
of
the
interstellar
medium
consists
of
lone
hydrogen
atoms;
most
of
the
remainder
consists
of
helium
atoms
this
is
enriched
with
trace
amounts
of
heavier
atoms
formed
through
stellar
nucleosynthesis
these
atoms
are
ejected
into
the
interstellar
medium
by
stellar
winds
or
when
evolved
stars
begin
to
shed
their
outer
envelopes
such
as
during
the
formation
of
a
planetary
nebula
the
cataclysmic
explosion
of
a
supernova
generates
an
expanding
shock
wave
consisting
of
ejected
materials
that
further
enrich
the
medium
the
density
of
matter
in
the
interstellar
medium
can
vary
considerably:
the
average
is
around
10
particles
per
m
but
cold
molecular
clouds
can
hold
10–10
per
m
a
number
of
molecules
exist
in
interstellar
space
as
can
tiny
01 μm
dust
particles
the
tally
of
molecules
discovered
through
radio
astronomy
is
steadily
increasing
at
the
rate
of
about
four
new
species
per
year
large
regions
of
higher
density
matter
known
as
molecular
clouds
allow
chemical
reactions
to
occur
including
the
formation
of
organic
polyatomic
species
much
of
this
chemistry
is
driven
by
collisions
energetic
cosmic
rays
penetrate
the
cold
dense
clouds
and
ionize
hydrogen
and
helium
resulting
for
example
in
the
trihydrogen
cation
an
ionized
helium
atom
can
then
split
relatively
abundant
carbon
monoxide
to
produce
ionized
carbon
which
in
turn
can
lead
to
organic
chemical
reactions
the
local
interstellar
medium
is
a
region
of
space
within
100 parsecs
(pc)
of
the
sun
which
is
of
interest
both
for
its
proximity
and
for
its
interaction
with
the
solar
system
this
volume
nearly
coincides
with
a
region
of
space
known
as
the
local
bubble
which
is
characterized
by
a
lack
of
dense
cold
clouds
it
forms
a
cavity
in
the
orion
arm
of
the
milky
way
galaxy
with
dense
molecular
clouds
lying
along
the
borders
such
as
those
in
the
constellations
of
ophiuchus
and
taurus
(the
actual
distance
to
the
border
of
this
cavity
varies
from
60
to
250 pc
or
more)
this
volume
contains
about
10–10
stars
and
the
local
interstellar
gas
counterbalances
the
astrospheres
that
surround
these
stars
with
the
volume
of
each
sphere
varying
depending
on
the
local
density
of
the
interstellar
medium
the
local
bubble
contains
dozens
of
warm
interstellar
clouds
with
temperatures
of
up
to
7000 k
and
radii
of
05–5 pc
when
stars
are
moving
at
sufficiently
high
peculiar
velocities
their
astrospheres
can
generate
bow
shocks
as
they
collide
with
the
interstellar
medium
for
decades
it
was
assumed
that
the
sun
had
a
bow
shock
in
2012
data
from
interstellar
boundary
explorer
(ibex)
and
nasa's
voyager
probes
showed
that
the
sun's
bow
shock
does
not
exist
instead
these
authors
argue
that
a
subsonic
bow
wave
defines
the
transition
from
the
solar
wind
flow
to
the
interstellar
medium
a
bow
shock
is
the
third
boundary
of
an
astrosphere
after
the
termination
shock
and
the
astropause
(called
the
heliopause
in
the
solar
system)
intergalactic
space
is
the
physical
space
between
galaxies
studies
of
the
large
scale
distribution
of
galaxies
show
that
the
universe
has
a
foam-like
structure
with
clusters
and
groups
of
galaxies
lying
along
filaments
that
occupy
about
a
tenth
of
the
total
space
the
remainder
forms
huge
voids
that
are
mostly
empty
of
galaxies
typically
a
void
spans
a
distance
of
(10–40)
"h"
mpc
where
"h"
is
the
hubble
constant
in
units
of
surrounding
and
stretching
between
galaxies
there
is
a
rarefied
plasma
that
is
organized
in
a
galactic
filamentary
structure
this
material
is
called
the
intergalactic
medium
(igm)
the
density
of
the
igm
is
5–200
times
the
average
density
of
the
universe
it
consists
mostly
of
ionized
hydrogen;
ie
a
plasma
consisting
of
equal
numbers
of
electrons
and
protons
as
gas
falls
into
the
intergalactic
medium
from
the
voids
it
heats
up
to
temperatures
of
10 k
to
10 k
which
is
high
enough
so
that
collisions
between
atoms
have
enough
energy
to
cause
the
bound
electrons
to
escape
from
the
hydrogen
nuclei;
this
is
why
the
igm
is
ionized
at
these
temperatures
it
is
called
the
warm–hot
intergalactic
medium
(whim)
(although
the
plasma
is
very
hot
by
terrestrial
standards
10
k
is
often
called
"warm"
in
astrophysics)
computer
simulations
and
observations
indicate
that
up
to
half
of
the
atomic
matter
in
the
universe
might
exist
in
this
warm–hot
rarefied
state
when
gas
falls
from
the
filamentary
structures
of
the
whim
into
the
galaxy
clusters
at
the
intersections
of
the
cosmic
filaments
it
can
heat
up
even
more
reaching
temperatures
of
10 k
and
above
in
the
so-called
intracluster
medium
for
the
majority
of
human
history
space
was
explored
by
observations
made
from
the
earth's
surface—initially
with
the
unaided
eye
and
then
with
the
telescope
prior
to
the
advent
of
reliable
rocket
technology
the
closest
that
humans
had
come
to
reaching
outer
space
was
through
the
use
of
balloon
flights
in
1935
the
us
"explorer
ii"
manned
balloon
flight
had
reached
an
altitude
of
this
was
greatly
exceeded
in
1942
when
the
third
launch
of
the
german
a-4
rocket
climbed
to
an
altitude
of
about
in
1957
the
unmanned
satellite
sputnik
1
was
launched
by
a
russian
r-7
rocket
achieving
earth
orbit
at
an
altitude
of
this
was
followed
by
the
first
human
spaceflight
in
1961
when
yuri
gagarin
was
sent
into
orbit
on
vostok
1
the
first
humans
to
escape
low-earth
orbit
were
frank
borman
jim
lovell
and
william
anders
in
1968
on
board
the
us
apollo
8
which
achieved
lunar
orbit
and
reached
a
maximum
distance
of
from
the
earth
the
first
spacecraft
to
reach
escape
velocity
was
the
soviet
luna
1
which
performed
a
fly-by
of
the
moon
in
1959
in
1961
venera
1
became
the
first
planetary
probe
it
revealed
the
presence
of
the
solar
wind
and
performed
the
first
fly-by
of
venus
although
contact
was
lost
before
reaching
venus
the
first
successful
planetary
mission
was
the
1962
fly-by
of
venus
by
mariner
2
the
first
fly-by
of
mars
was
by
mariner
4
in
1964
since
that
time
unmanned
spacecraft
have
successfully
examined
each
of
the
solar
system's
planets
as
well
their
moons
and
many
minor
planets
and
comets
they
remain
a
fundamental
tool
for
the
exploration
of
outer
space
as
well
as
observation
of
the
earth
in
august
2012
"voyager
1"
became
the
first
man-made
object
to
leave
the
solar
system
and
enter
interstellar
space
the
absence
of
air
makes
outer
space
an
ideal
location
for
astronomy
at
all
wavelengths
of
the
electromagnetic
spectrum
this
is
evidenced
by
the
spectacular
pictures
sent
back
by
the
hubble
space
telescope
allowing
light
from
more
than
13 billion
years
ago—almost
to
the
time
of
the
big
bang—to
be
observed
however
not
every
location
in
space
is
ideal
for
a
telescope
the
interplanetary
zodiacal
dust
emits
a
diffuse
near-infrared
radiation
that
can
mask
the
emission
of
faint
sources
such
as
extrasolar
planets
moving
an
infrared
telescope
out
past
the
dust
increases
its
effectiveness
likewise
a
site
like
the
daedalus
crater
on
the
far
side
of
the
moon
could
shield
a
radio
telescope
from
the
radio
frequency
interference
that
hampers
earth-based
observations
unmanned
spacecraft
in
earth
orbit
are
an
essential
technology
of
modern
civilization
they
allow
direct
monitoring
of
weather
conditions
relay
long-range
communications
like
television
provide
a
means
of
precise
navigation
and
allow
remote
sensing
of
the
earth
the
latter
role
serves
a
wide
variety
of
purposes
including
tracking
soil
moisture
for
agriculture
prediction
of
water
outflow
from
seasonal
snow
packs
detection
of
diseases
in
plants
and
trees
and
surveillance
of
military
activities
the
deep
vacuum
of
space
could
make
it
an
attractive
environment
for
certain
industrial
processes
such
as
those
requiring
ultraclean
surfaces
however
like
asteroid
mining
space
manufacturing
requires
significant
investment
with
little
prospect
of
immediate
return
an
important
factor
in
the
total
expense
is
the
high
cost
of
placing
mass
into
earth
orbit:
$–
per
kg
in
inflation-adjusted
dollars
according
to
a
2006
estimate
proposed
concepts
for
addressing
this
issue
include
non-rocket
spacelaunch
momentum
exchange
tethers
and
space
elevators
interstellar
travel
for
a
human
crew
remains
at
present
only
a
theoretical
possibility
the
distances
to
the
nearest
stars
will
require
new
technological
developments
and
the
ability
to
safely
sustain
crews
for
journeys
lasting
several
decades
for
example
the
daedalus
project
study
which
proposed
a
spacecraft
powered
by
the
fusion
of
deuterium
and
he
would
require
36
years
to
reach
the
nearby
alpha
centauri
system
other
proposed
interstellar
propulsion
systems
include
light
sails
ramjets
and
beam-powered
propulsion
more
advanced
propulsion
systems
could
use
antimatter
as
a
fuel
potentially
reaching
relativistic
velocities
biophysical
environment
a
biophysical
environment
is
a
biotic
and
abiotic
surrounding
of
an
organism
or
population
and
consequently
includes
the
factors
that
have
an
influence
in
their
survival
development
and
evolution
a
biophysical
environment
can
vary
in
scale
from
microscopic
to
global
in
extent
it
can
also
be
subdivided
according
to
its
attributes
examples
include
the
marine
environment
the
atmospheric
environment
and
the
terrestrial
environment
the
number
of
biophysical
environments
is
countless
given
that
each
living
organism
has
its
own
environment
the
term
"environment"
can
refer
to
a
singular
global
environment
in
relation
to
humanity
or
a
local
biophysical
environment
eg
the
uk's
environment
agency
all
life
that
has
survived
must
have
adapted
to
conditions
of
its
environment
temperature
light
humidity
soil
nutrients
etc
all
influence
any
species
within
any
environment
however
life
in
turn
modifies
in
various
forms
its
conditions
some
long
term
modifications
along
the
history
of
our
planet
have
been
significant
such
as
the
incorporation
of
oxygen
to
the
atmosphere
this
process
consisted
in
the
breakdown
of
carbon
dioxide
by
anaerobic
microorganisms
that
used
the
carbon
in
their
metabolism
and
released
the
oxygen
to
the
atmosphere
this
led
to
the
existence
of
oxygen-based
plant
and
animal
life
the
great
oxygenation
event
other
interactions
are
more
immediate
and
simple
such
as
the
smoothing
effect
that
forests
have
on
the
temperature
cycle
compared
to
neighboring
unforested
areas
environmental
science
is
the
study
of
the
interactions
within
the
biophysical
environment
part
of
this
scientific
discipline
is
the
investigation
of
the
effect
of
human
activity
on
the
environment
ecology
a
sub-discipline
of
biology
and
a
part
of
environmental
sciences
is
often
mistaken
as
a
study
of
human
induced
effects
on
the
environment
environmental
studies
is
a
broader
academic
discipline
that
is
the
systematic
study
of
interaction
of
humans
with
their
environment
it
is
a
broad
field
of
study
that
includes
the
natural
environment
built
environments
and
social
environments
environmentalism
is
a
broad
social
and
philosophical
movement
that
in
a
large
part
seeks
to
minimise
and
compensate
the
negative
effect
of
human
activity
on
the
biophysical
environment
the
issues
of
concern
for
environmentalists
usually
relate
to
the
natural
environment
with
the
more
important
ones
being
climate
change
species
extinction
pollution
and
old
growth
forest
loss
one
of
the
studies
related
include
employing
geographic
information
science
to
study
the
biophysical
environment
regional
policy
regional
policy
aims
to
improve
economic
conditions
in
regions
of
relative
disadvantage
either
within
a
nation
or
within
a
supranational
grouping
such
as
the
european
union
although
the
european
union
is
one
of
the
richest
parts
of
the
world
there
are
large
internal
disparities
of
income
and
opportunity
between
its
regions
the
may
2004
enlargement
followed
by
accession
of
bulgaria
and
romania
in
january
2007
has
widened
these
gaps
regional
policy
transfers
resources
from
richer
to
poorer
regions
the
argument
for
regional
policy
is
that
it
is
both
an
instrument
of
financial
solidarity
and
a
powerful
force
for
economic
integration
the
major
italian
experience
of
regional
policy
is
the
cassa
per
il
mezzogiorno
set
up
in
the
mid-1950s
to
foster
economic
development
in
southern
italy
originally
intended
to
last
for
six
months
it
survived
until
1984
new
roads
irrigation
projects
and
developments
in
infrastructure
were
built
in
an
area
where
local
communities
had
suffered
seriously
from
poverty
de-population
and
high
levels
of
emigration
tourism
projects
attempted
to
exploit
calabria’s
beaches
uk
regional
policy
was
born
during
the
economic
depression
of
the
1930s
when
heavy
industries
in
the
north
were
devastated
"assisted
areas"
were
established
within
which
companies
could
acquire
grants
or
capital
allowances
–
known
as
regional
selective
assistance
(rsa)
–
in
return
for
protecting
jobs
the
overall
pattern
of
policy
changed
little
in
the
next
forty
years
despite
criticism
by
a
1970s
royal
commission
that
it
was
"empiricism
run
mad;
a
game
of
hit
and
miss
played
with
more
enthusiasm
than
success"
governments
of
both
parties
maintained
assisted
areas
under
the
1980s
thatcher
government
regional
policy
was
significantly
rolled
back
with
assisted
areas
substantially
reduced
in
size
the
post-1997
labour
administration
reorganised
regional
policy
with
rsa
replaced
by
selective
finance
for
investment
in
england
and
scotland
uk
policy
has
been
subject
to
eu
regional
policy
framework
with
its
strong
injunctions
against
unfair
competition
(generally
meaning
state
aid)
ws-policy
ws-policy
is
a
specification
that
allows
web
services
to
use
xml
to
advertise
their
policies
(on
security
quality
of
service
etc)
and
for
web
service
consumers
to
specify
their
policy
requirements
ws-policy
is
a
w3c
recommendation
as
of
september
2007
ws-policy
represents
a
set
of
specifications
that
describe
the
capabilities
and
constraints
of
the
security
(and
other
business)
policies
on
intermediaries
and
end
points
(for
example
required
security
tokens
supported
encryption
algorithms
and
privacy
rules)
and
how
to
associate
policies
with
services
and
end
points
assertions
can
either
be
requirements
put
upon
a
web
service
or
an
advertisement
of
the
policies
of
a
web
service
two
"operators"
(xml
tags)
are
used
to
make
statements
about
policy
combinations:
logically
an
empty
"wsp:all"
tag
makes
no
assertions
if
both
provider
and
consumer
specify
a
policy
an
effective
policy
will
be
computed
which
usually
consists
of
the
intersection
of
both
policies
the
new
policy
contains
those
assertions
made
by
both
sides
which
do
not
contradict
each
other
however
synonymous
assertions
are
considered
incompatible
by
a
policy
intersection
this
can
easily
be
explained
by
the
fact
that
policy
intersection
is
a
syntactic
approach
which
does
not
incorporate
the
semantics
of
the
assertions
furthermore
it
ignores
the
assertion
parameters
opposed
to
what
the
name
might
suggest
a
policy
intersection
is
(although
quite
similar)
not
a
set-intersection
haldane
principle
in
british
research
policy
the
haldane
principle
is
the
idea
that
decisions
about
what
to
spend
research
funds
on
should
be
made
by
researchers
rather
than
politicians
it
is
named
after
richard
burdon
haldane
who
in
1904
and
from
1909
to
1918
chaired
committees
and
commissions
which
recommended
this
policy
the
1904
committee
recommended
the
creation
of
the
university
grants
committee
which
has
evolved
via
the
universities
funding
council
into
the
current
higher
education
funding
councils:
research
councils
uk
higher
education
funding
council
for
england
scottish
funding
council
and
higher
education
funding
council
for
wales
in
1918
haldane's
committee
produced
the
"haldane
report"
the
report
suggested
that
research
required
by
government
departments
could
be
separated
into
that
required
by
specific
departments
and
that
which
was
more
general
it
recommended
that
departments
should
oversee
the
specific
research
but
the
general
research
should
be
under
the
control
of
autonomous
research
councils
which
would
be
free
from
political
and
administrative
pressures
that
might
discourage
research
in
certain
areas
the
principle
of
the
autonomy
of
the
research
councils
is
now
referred
to
as
the
haldane
principle
the
first
research
council
to
be
created
as
a
result
of
the
haldane
report
was
the
medical
research
council
the
principle
has
remained
enshrined
in
british
government
policy
but
has
been
criticised
and
altered
over
the
years
in
1939
jd
bernal
argued
that
social
good
was
more
important
than
researchers'
freedom
in
deciding
the
direction
of
research
solly
zuckerman
criticised
it
in
1971
for
its
artificial
separation
of
basic
and
applied
science
and
the
consequent
elevation
of
the
status
of
the
former
a
major
revision
to
the
application
of
the
haldane
principle
in
british
research
funding
came
in
the
early
1970s
with
the
rothschild
report
of
1971
and
its
implementation
which
transferred
about
25%
of
the
then
research
council
funds
and
the
decisions
on
the
research
to
be
funded
with
them
back
to
government
departments
a
move
later
undone
by
margaret
thatcher's
government
there
is
currently
a
debate
about
the
extent
to
which
the
principle
is
still
applied
in
practice
the
higher
education
and
research
act
2017
which
merged
the
research
councils
and
the
research
part
of
the
higher
education
funding
council
for
england
into
uk
research
and
innovation
enacted
the
haldane
principle
as
section
103(3):
"the
“haldane
principle”
is
the
principle
that
decisions
on
individual
research
proposals
are
best
taken
following
an
evaluation
of
the
quality
and
likely
impact
of
the
proposals
(such
as
a
peer
review
process)"
eightfold
path
(policy
analysis)
the
eightfold
path
is
a
method
of
policy
analysis
assembled
by
eugene
bardach
a
professor
at
the
goldman
school
of
public
policy
at
the
university
of
california
berkeley
it
is
outlined
in
his
book
"a
practical
guide
for
policy
analysis:
the
eightfold
path
to
more
effective
problem
solving"
which
is
now
in
its
fourth
edition
the
book
is
commonly
referenced
in
public
policy
and
public
administration
scholarship
bardach's
procedure
is
as
follows:
a
possible
ninth
step
based
on
bardach's
own
writing
might
be
"repeat
steps
1
-
8
as
necessary"
the
new
york
taxi
driver
test
is
a
technique
for
evaluating
the
effectiveness
of
communication
between
policy
makers
and
analysts
bardach
contends
that
policy
explanations
must
be
clear
and
down-to-earth
enough
for
a
taxi
driver
to
be
able
to
understand
the
premise
during
a
trip
through
city
streets
the
new
york
taxi
driver
is
presumed
to
be
both
a
non-specialist
and
a
tough
customer
multifunctionality
in
agriculture
multifunctionality
in
agriculture
(often
simply
"multifunctionality")
refers
to
the
numerous
benefits
that
agricultural
policies
may
provide
for
a
country
or
region
generally
speaking
multifunctionality
refers
to
the
"non-trade"
benefits
of
agriculture
that
is
benefits
other
than
commerce
and
food
production
these
include
in
the
wto
definition
of
multifunctionality
environmental
protection
landscape
preservation
rural
employment
and
food
security
these
can
be
broadly
classified
as
benefits
to
society
culture
a
national
economy
as
a
whole
national
security
and
other
concerns
for
example
in
addition
to
providing
food
and
plant-derived
products
for
the
population
agriculture
may
also
provide
jobs
for
rural
people
and
contribute
to
the
viability
of
the
area
create
a
more
stable
food
supply
and
provide
other
desired
environmental
and
rural
outputs
the
numerous
externalities
both
positive
and
negative
which
are
associated
with
agriculture
are
important
considerations
for
policy
makers
sometimes
current
agricultural
practices
and
markets
produce
too
much
of
an
undesired
effect
or
not
enough
of
a
desired
one
governments
may
step
in
to
correct
such
market
failures
with
policies
designed
to
either
encourage
or
discourage
a
certain
practice
however
individual
policies
may
carry
consequences
for
other
policies
and
for
other
countries
such
policies
are
therefore
a
major
topic
of
discussion
in
the
international
community
removing
protectionist
policies
on
agriculture
is
one
step
that
may
need
to
be
taken
for
a
country
to
maximize
positive
externalities
minimize
negative
ones
and
make
sure
that
the
mixture
of
outputs
derived
from
agriculture
corresponds
to
the
needs
of
society
however
removing
agricultural
supports
is
often
cause
for
consternation
among
public
officials
who
may
predict
the
loss
of
certain
positive
externalities
of
the
policies
already
in
place
at
the
same
time
officials
may
fear
the
implementation
of
new
market
protections
in
other
countries
which
are
trying
to
promote
the
production
of
such
outputs
of
agriculture
in
such
cases
advocates
for
free
trade
such
as
oecd
recommend
that
countries
reduce
as
much
as
possible
their
agricultural
protections
and
institute
policies
which
specifically
target
the
production
of
the
positive
non-commodity
outputs
to
help
countries
formulate
their
agricultural
policies
oecd
has
established
a
framework
for
analyzing
non-commodity
outputs
of
agricultural
activities
when
analyzing
the
multifunctionality
of
agriculture
and
the
appropriate
policies
to
implement
there
are
several
concepts
that
need
to
be
considered
the
first
of
these
is
jointness
or
the
extent
to
which
the
intended
agricultural
product
and
the
incidental
non-commodity
outputs
of
agricultural
activity
are
linked
the
production
of
some
non-commodity
outputs
may
be
inseparable
from
agricultural
commodity
outputs
while
others
may
be
produced
independently
of
agricultural
activity
the
goal
is
to
separate
agricultural
commodities
and
non-commodity
outputs
as
much
as
possible
the
next
issue
to
be
addressed
is
whether
or
not
the
production
or
non-production
of
the
non-commodity
output
in
question
constitutes
a
market
failure
if
there
is
no
market
failure
there
is
no
need
for
a
policy
to
correct
it
finally
policy
makers
should
examine
the
characteristics
of
the
output
in
question
since
it
may
have
both
a
degree
of
market
failure
and
jointness
associated
with
it
after
considering
the
matter
from
these
three
perspectives
policy
makers
may
find
non-governmental
ways
of
addressing
dealing
with
non-commodity
outputs
or
make
changes
in
their
agricultural
policies
in
agricultural
trade
discussions
in
the
wto
the
eu
and
japan
among
others
argue
that
multifunctionality
justifies
continued
protection
and
subsidization
of
agriculture
the
united
states
and
the
cairns
group
argue
that
support
of
multifunctionality
should
be
specific
targeted
and
provided
in
a
non-trade
distorting
manner
courtesy
resolution
courtesy
resolution
is
a
non-controversial
resolution
in
the
nature
of
congratulations
on
the
birth
of
a
child
celebration
of
a
wedding
anniversary
congratulations
of
an
outstanding
citizen
achievement
or
a
similar
event
it
is
"a
resolution
expressing
thanks
for
assistance
or
commending
meritorious
accomplishments"
an
example
of
a
courtesy
resolution
is
the
resolution
at
the
end
of
the
political
convention
thanking
everyone
for
their
time
for
a
courtesy
resolution
only
the
affirmative
vote
is
taken
and
this
is
usually
a
voice
vote
association
for
public
policy
analysis
and
management
the
association
for
public
policy
analysis
and
management
(appam)
is
an
american
organization
whose
focus
is
improving
public
policy
and
management
by
fostering
excellence
in
research
analysis
and
education
appam
founded
the
"journal
of
policy
analysis
and
management"
("jpam")
in
1981
the
president
is
brendan
sullivan
health
administration
health
administration
or
healthcare
administration
is
the
field
relating
to
leadership
management
and
administration
of
public
health
systems
health
care
systems
hospitals
and
hospital
networks
health
systems
management
or
health
care
systems
management
describes
the
leadership
and
general
management
of
hospitals
hospital
networks
and/or
health
care
systems
in
international
use
the
term
refers
to
management
at
all
levels
in
the
united
states
management
of
a
single
institution
(eg
a
hospital)
is
also
referred
to
as
"medical
and
health
services
management"
"healthcare
management"
or
"health
administration"
health
systems
management
ensures
that
specific
outcomes
are
attained
that
departments
within
a
health
facility
are
running
smoothly
that
the
right
people
are
in
the
right
jobs
that
people
know
what
is
expected
of
them
that
resources
are
used
efficiently
and
that
all
departments
are
working
towards
a
common
goal
hospital
administrators
are
individuals
or
groups
of
people
who
act
as
the
central
point
of
control
within
hospitals
these
individuals
may
be
previous
or
current
clinicians
or
individuals
with
other
backgrounds
there
are
two
types
of
administrators
generalists
and
specialists
generalists
are
individuals
who
are
responsible
for
managing
or
helping
to
manage
an
entire
facility
specialists
are
individuals
who
are
responsible
for
the
efficient
operations
of
a
specific
department
such
as
policy
analysis
finance
accounting
budgeting
human
resources
or
marketing
it
was
reported
in
september
2014
that
the
united
states
spends
roughly
$218
billion
per
year
on
hospital's
administration
costs
which
is
equivalent
to
143
percent
of
the
total
us
economy
hospital
administration
has
grown
as
a
percent
of
the
us
economy
from
9
percent
in
2000
to
143
percent
in
2012
according
to
"health
affairs"
in
11
different
countries
hospitals
allocate
approximately
12
percent
of
their
budget
toward
administrative
costs
in
the
united
states
hospitals
spend
25
percent
on
administrative
costs
nchl
competencies
that
require
to
engage
with
credibility
creativity
and
motivation
in
complex
and
dynamic
health
care
environments
health
care
management
is
usually
studied
through
healthcare
administration
or
healthcare
management
programs
in
a
business
school
or
in
some
institutions
in
a
school
of
public
health
although
many
colleges
and
universities
are
offering
a
bachelor's
degree
in
healthcare
administration
or
human
resources
a
master's
degree
is
considered
the
"standard
credential"
for
most
health
administrators
in
the
united
states
research
and
academic-based
doctorate
level
degrees
such
as
the
doctor
of
philosophy
(phd)
in
health
administration
and
the
doctor
of
health
administration
(dha)
degree
prepare
health
care
professionals
to
turn
their
clinical
or
administrative
experiences
into
opportunities
to
develop
new
knowledge
and
practice
teach
shape
public
policy
and/or
lead
complex
organizations
there
are
multiple
recognized
degree
types
that
are
considered
equivalent
from
the
perspective
of
professional
preparation
the
commission
on
the
accreditation
of
healthcare
management
education
(cahme)
is
the
accrediting
body
overseeing
master's-level
programs
in
the
united
states
and
canada
on
behalf
of
the
united
states
department
of
education
it
accredits
several
degree
program
types
including
master
of
hospital
administration
(mha)
master
of
health
services
administration
(mhsa)
master
of
business
administration
in
hospital
management
(mba-hm)
master
of
health
administration
(mha)
master
of
public
health
(mph
msph
mshpm)
master
of
science
(ms-hsm
ms-ha)
and
master
of
public
administration
(mpa)
health
care
management
study
is
a
new
discipline
in
nepal
pokhara
university
offers
a
hospital
management
course
national
open
college
launched
a
four-year
bachelor's
level
(bhcm)
course
in
september
2000
with
an
enrolment
of
40
students
and
the
next
year
it
also
started
a
one-year
postgraduate
diploma
(pgdhcm)
and
a
two-year
master's
course
(mhcm)
in
health
care
management
nobel
college
at
sinamangal
has
also
been
offering
a
bachelor’s
level
(bhcm)
course
since
2006
md
hospital
administration
(mdha)
and
master
in
hospital
management
(mhm)
are
being
started
from
2013
it
is
uncertain
how
many
citizens
of
nepal
are
gaining
healthcare
management
qualifications
in
other
countries
there
is
an
absence
of
professional
organization
and
regulation
in
the
health
care
management
profession
in
nepal
there
are
a
variety
of
different
professional
associations
related
to
health
systems
management
which
can
be
subcategorized
as
either
personal
or
institutional
membership
groups
personal
membership
groups
are
joined
by
individuals
and
typically
have
individual
skills
and
career
development
as
their
focus
larger
personal
membership
groups
include
the
healthcare
financial
management
association
and
the
healthcare
information
and
management
systems
society
institutional
membership
groups
are
joined
by
organizations;
whereas
they
typically
focus
on
organizational
effectiveness
and
may
also
include
data-sharing
agreements
and
other
medical
related
or
administrative
practice
sharing
vehicles
for
member
organizations
prominent
examples
include
the
american
hospital
association
and
the
university
healthsystems
consortium
early
hospital
administrators
were
called
patient
directors
or
superintendents
at
the
time
many
were
nurses
who
had
taken
on
administrative
responsibilities
over
half
of
the
members
of
the
american
hospital
association
were
graduate
nurses
in
1916
other
superintendents
were
medical
doctors
laymen
and
members
of
the
clergy
in
the
united
states
the
first
degree
granting
program
in
the
united
states
was
established
at
marquette
university
in
milwaukee
wisconsin
by
1927
the
first
two
students
received
their
degrees
the
original
idea
is
credited
to
father
moulinier
associated
with
the
catholic
hospital
association
the
first
modern
health
systems
management
program
was
established
in
1934
at
the
university
of
chicago
at
the
time
programs
were
completed
in
two
years
–
one
year
of
formal
graduate
study
and
one
year
of
practicing
internship
in
1958
the
sloan
program
at
cornell
university
began
offering
a
special
program
requiring
two
years
of
formal
study
which
remains
the
dominant
structure
in
the
united
states
and
canada
today
(see
also
"academic
preparation")
health
systems
management
has
been
described
as
a
"hidden"
health
profession
because
of
the
relatively
low-profile
role
managers
take
in
health
systems
in
comparison
to
direct-care
professions
such
as
nursing
and
medicine
however
the
visibility
of
the
management
profession
within
healthcare
has
been
rising
in
recent
years
due
largely
to
the
widespread
problems
developed
countries
are
having
in
balancing
cost
access
and
quality
in
their
hospitals
and
health
systems
overton
window
the
overton
window
is
the
range
of
ideas
tolerated
in
public
discourse
also
known
as
the
window
of
discourse
the
term
is
named
after
political
scientist
joseph
p
overton
who
claimed
that
an
idea's
political
viability
depends
mainly
on
whether
it
falls
within
a
range
acceptable
to
the
public
rather
than
on
politicians'
individual
preferences
according
to
overton
the
window
contains
the
range
of
policies
that
a
politician
can
recommend
without
appearing
too
extreme
to
gain
or
keep
public
office
in
the
current
climate
of
public
opinion
overton
described
a
spectrum
from
"more
free"
to
"less
free"
with
regard
to
government
intervention
oriented
vertically
on
an
axis
to
avoid
comparison
with
the
left-right
political
spectrum
as
the
spectrum
moves
or
expands
an
idea
at
a
given
location
may
become
more
or
less
politically
acceptable
political
commentator
joshua
treviño
postulated
that
the
degrees
of
acceptance
of
public
ideas
are
roughly:
the
overton
window
is
an
approach
to
identifying
which
ideas
define
the
domain
of
acceptability
within
a
democracy's
possible
governmental
policies
proponents
of
policies
outside
the
window
seek
to
convince
or
persuade
the
public
in
order
to
move
and/or
expand
the
window
proponents
of
current
policies
or
similar
ones
within
the
window
seek
to
convince
people
that
policies
outside
it
should
be
deemed
unacceptable
after
overton's
death
others
have
examined
the
concept
of
adjusting
the
window
by
the
deliberate
promotion
of
ideas
outside
of
it
or
"outer
fringe"
ideas
with
the
intention
of
making
less
fringe
ideas
acceptable
by
comparison
the
"door-in-the-face"
technique
of
persuasion
is
similar
the
idea
echoes
several
earlier
expressions
the
most
recent
and
similarly
academic
being
hallin's
spheres
in
his
1986
book
"the
uncensored
war"
communication
scholar
daniel
c
hallin
posits
three
areas
of
media
coverage
into
which
a
topic
may
fall
the
areas
are
diagrammed
as
concentric
circles
called
spheres
from
innermost
to
outermost
they
are
the
sphere
of
consensus
the
sphere
of
legitimate
controversy
and
the
sphere
of
deviance
proposals
and
positions
can
be
placed
at
varying
degrees
of
distance
from
the
metaphorical
center
and
political
actors
can
fight
over
and
help
change
these
positions
hallin's
theory
is
developed
and
applied
primarily
as
a
theory
that
explains
varying
levels
of
objectivity
in
media
coverage
but
it
also
accounts
for
the
ongoing
contest
among
media
and
other
political
actors
about
what
counts
as
legitimate
disagreement
potentially
leading
to
changes
in
the
boundaries
between
spheres
as
one
study
that
applies
hallin's
theory
explains
"the
borders
between
the
three
spheres
are
dynamic
depending
on
the
political
climate
and
on
the
editorial
line
of
the
various
media
outlets"
in
this
way
the
idea
also
captures
the
tug-of-war
over
the
boundaries
between
normal
and
deviant
political
discourse
an
idea
similar
to
the
overton
window
was
expressed
by
anthony
trollope
in
1868
in
his
novel
"phineas
finn":
in
his
"west
india
emancipation"
speech
at
canandaigua
new
york
in
1857
abolitionist
leader
frederick
douglass
described
how
public
opinion
limits
the
ability
of
those
in
power
to
act
with
impunity:
culture
change
culture
change
is
a
term
used
in
public
policy
making
that
emphasizes
the
influence
of
cultural
capital
on
individual
and
community
behavior
it
has
been
sometimes
called
repositioning
of
culture
which
means
the
reconstruction
of
the
cultural
concept
of
a
society
it
places
stress
on
the
social
and
cultural
capital
determinants
of
decision
making
and
the
manner
in
which
these
interact
with
other
factors
like
the
availability
of
information
or
the
financial
incentives
facing
individuals
to
drive
behavior
these
cultural
capital
influences
include
the
role
of
parenting
families
and
close
associates;
organizations
such
as
schools
and
workplaces;
communities
and
neighborhoods;
and
wider
social
influences
such
as
the
media
it
is
argued
that
this
cultural
capital
manifests
into
specific
values
attitudes
or
social
norms
which
in
turn
guide
the
behavioral
"intentions"
that
individuals
adopt
in
regard
to
particular
decisions
or
courses
of
action
these
behavioral
intentions
interact
with
other
factors
driving
behavior
such
as
financial
incentives
regulation
and
legislation
or
levels
of
information
to
drive
actual
behavior
and
ultimately
feed
back
into
underlying
cultural
capital
in
general
cultural
stereotypes
present
great
resistance
to
change
and
to
their
own
redefinition
culture
often
appears
fixed
to
the
observer
at
any
one
point
in
time
because
cultural
mutations
occur
incrementally
cultural
change
is
a
long-term
process
policymakers
need
to
make
a
great
effort
to
improve
some
basics
aspects
of
a
society’s
cultural
traits
the
term
is
used
by
knott
et
al
of
the
prime
minister's
strategy
unit
in
the
publication:
"achieving
culture
change:
a
policy
framework"
(knott
et
al
2008)
the
paper
sets
out
how
public
policy
can
achieve
social
and
cultural
change
through
'downstream'
interventions
including
fiscal
incentives
legislation
regulation
and
information
provision
and
also
'upstream'
interventions
such
as
parenting
peer
and
mentoring
programs
or
development
of
social
and
community
networks
the
key
concepts
the
paper
is
based
on
include:
knott
et
al
use
examples
from
a
range
of
policy
areas
to
demonstrate
how
the
culture
change
framework
can
be
applied
to
policymaking
for
example:
security
policy
security
policy
is
a
definition
of
what
it
means
to
"be
secure"
for
a
system
organization
or
other
entity
for
an
organization
it
addresses
the
constraints
on
behavior
of
its
members
as
well
as
constraints
imposed
on
adversaries
by
mechanisms
such
as
doors
locks
keys
and
walls
for
systems
the
security
policy
addresses
constraints
on
functions
and
flow
among
them
constraints
on
access
by
external
systems
and
adversaries
including
programs
and
access
to
data
by
people
if
it
is
important
to
be
secure
then
it
is
important
to
be
sure
all
of
the
security
policy
is
enforced
by
mechanisms
that
are
strong
enough
there
are
many
organized
methodologies
and
risk
assessment
strategies
to
assure
completeness
of
security
policies
and
assure
that
they
are
completely
enforced
in
complex
systems
such
as
information
systems
policies
can
be
decomposed
into
sub-policies
to
facilitate
the
allocation
of
security
mechanisms
to
enforce
sub-policies
however
this
practice
has
pitfalls
it
is
too
easy
to
simply
go
directly
to
the
sub-policies
which
are
essentially
the
rules
of
operation
and
dispense
with
the
top
level
policy
that
gives
the
false
sense
that
the
rules
of
operation
address
some
overall
definition
of
security
when
they
do
not
because
it
is
so
difficult
to
think
clearly
with
completeness
about
security
rules
of
operation
stated
as
"sub-policies"
with
no
"super-policy"
usually
turn
out
to
be
rambling
rules
that
fail
to
enforce
anything
with
completeness
consequently
a
top-level
security
policy
is
essential
to
any
serious
security
scheme
and
sub-policies
and
rules
of
operation
are
meaningless
without
it
science
policy
science
policy
is
concerned
with
the
allocation
of
resources
for
the
conduct
of
science
towards
the
goal
of
best
serving
the
public
interest
topics
include
the
funding
of
science
the
careers
of
scientists
and
the
translation
of
scientific
discoveries
into
technological
innovation
to
promote
commercial
product
development
competitiveness
economic
growth
and
economic
development
science
policy
focuses
on
knowledge
production
and
role
of
knowledge
networks
collaborations
and
the
complex
distributions
of
expertise
equipment
and
know-how
understanding
the
processes
and
organizational
context
of
generating
novel
and
innovative
science
and
engineering
ideas
is
a
core
concern
of
science
policy
science
policy
topics
include
weapons
development
health
care
and
environmental
monitoring
science
policy
thus
deals
with
the
entire
domain
of
issues
that
involve
science
a
large
and
complex
web
of
factors
influences
the
development
of
science
and
engineering
that
includes
government
science
policy
makers
private
firms
(including
both
national
and
multi-national
firms)
social
movements
media
non-governmental
organizations
universities
and
other
research
institutions
in
addition
science
policy
is
increasingly
international
as
defined
by
the
global
operations
of
firms
and
research
institutions
as
well
as
by
the
collaborative
networks
of
non-governmental
organizations
and
of
the
nature
of
scientific
inquiry
itself
state
policy
has
influenced
the
funding
of
public
works
and
science
for
thousands
of
years
dating
at
least
from
the
time
of
the
mohists
who
inspired
the
study
of
logic
during
the
period
of
the
hundred
schools
of
thought
and
the
study
of
defensive
fortifications
during
the
warring
states
period
in
china
general
levies
of
labor
and
grain
were
collected
to
fund
great
public
works
in
china
including
the
accumulation
of
grain
for
distribution
in
times
of
famine
for
the
building
of
levees
to
control
flooding
by
the
great
rivers
of
china
for
the
building
of
canals
and
locks
to
connect
rivers
of
china
some
of
which
flowed
in
opposite
directions
to
each
other
and
for
the
building
of
bridges
across
these
rivers
these
projects
required
a
civil
service
the
scholars
some
of
whom
demonstrated
great
mastery
of
hydraulics
in
italy
galileo
noted
that
individual
taxation
of
minute
amounts
could
fund
large
sums
to
the
state
which
could
then
fund
his
research
on
the
trajectory
of
cannonballs
noting
that
"each
individual
soldier
was
being
paid
from
coin
collected
by
a
general
tax
of
pennies
and
farthings
while
even
a
million
of
gold
would
not
suffice
to
pay
the
entire
army"
in
great
britain
lord
chancellor
sir
francis
bacon
had
a
formative
effect
on
science
policy
with
his
identification
of
"experiments
of
light
more
penetrating
into
nature
[than
what
others
know]"
which
today
we
call
the
crucial
experiment
governmental
approval
of
the
royal
society
recognized
a
scientific
community
which
exists
to
this
day
british
prizes
for
research
spurred
the
development
of
an
accurate
portable
chronometer
which
directly
enabled
reliable
navigation
and
sailing
on
the
high
seas
and
also
funded
babbage's
computer
the
professionalization
of
science
begun
in
the
nineteenth
century
was
partly
enabled
by
the
creation
of
scientific
organizations
such
as
the
national
academy
of
sciences
the
kaiser
wilhelm
institute
and
state
funding
of
universities
of
their
respective
nations
in
the
united
states
a
member
of
the
national
academy
of
sciences
can
sponsor
a
direct
submission
for
publication
in
the
"proceedings
of
the
national
academy
of
sciences"
"pnas"
serves
as
a
channel
to
recognize
research
of
importance
to
at
least
one
member
of
the
national
academy
of
sciences
public
policy
can
directly
affect
the
funding
of
capital
equipment
intellectual
infrastructure
for
industrial
research
by
providing
tax
incentives
to
those
organizations
who
fund
research
vannevar
bush
director
of
the
office
of
scientific
research
and
development
for
the
us
government
in
july
1945
wrote
"science
is
a
proper
concern
of
government"
vannevar
bush
directed
the
forerunner
of
the
national
science
foundation
and
his
writings
directly
inspired
researchers
to
invent
the
hyperlink
and
the
computer
mouse
the
darpa
initiative
to
support
computing
was
the
impetus
for
the
internet
protocol
stack
in
the
same
way
that
scientific
consortiums
like
cern
for
high-energy
physics
have
a
commitment
to
public
knowledge
access
to
this
public
knowledge
in
physics
led
directly
to
cern's
sponsorship
of
development
of
the
world
wide
web
and
standard
internet
access
for
all
the
programs
that
are
funded
are
often
divided
into
four
basic
categories:
basic
research
applied
research
development
and
facilities
and
equipment
translational
research
is
a
newer
concept
that
seeks
to
bridge
the
gap
between
basic
science
and
practical
applications
basic
science
attempts
to
stimulate
breakthroughs
breakthroughs
often
lead
to
an
explosion
of
new
technologies
and
approaches
once
the
basic
result
is
developed
it
is
widely
published;
however
conversion
into
a
practical
product
is
left
for
the
free
market
however
many
governments
have
developed
risk-taking
research
and
development
organizations
to
take
basic
theoretical
research
over
the
edge
into
practical
engineering
in
the
us
this
function
is
performed
by
darpa
on
the
other
hand
technology
development
is
a
policy
in
which
engineering
the
application
of
science
is
supported
rather
than
basic
science
the
emphasis
is
usually
given
to
projects
that
increase
important
strategic
or
commercial
engineering
knowledge
the
most
extreme
success
story
is
doubtless
the
manhattan
project
that
developed
nuclear
weapons
another
remarkable
success
story
was
the
"x-vehicle"
studies
that
gave
the
us
a
lasting
lead
in
aerospace
technologies
these
exemplify
two
disparate
approaches:
the
manhattan
project
was
huge
and
spent
unblinkingly
on
the
most
risky
alternative
approaches
the
project
members
believed
that
failure
would
result
in
their
enslavement
or
destruction
by
nazi
germany
each
x-project
built
an
aircraft
whose
only
purpose
was
to
develop
a
particular
technology
the
plan
was
to
build
a
few
cheap
aircraft
of
each
type
fly
a
test
series
often
to
the
destruction
of
an
aircraft
and
never
design
an
aircraft
for
a
practical
mission
the
only
mission
was
technology
development
a
number
of
high-profile
technology
developments
have
failed
the
us
space
shuttle
failed
to
meet
its
cost
or
flight
schedule
goals
most
observers
explain
the
project
as
over
constrained:
the
cost
goals
too
aggressive
the
technology
and
mission
too
underpowered
and
undefined
the
japanese
fifth
generation
computer
systems
project
met
every
technological
goal
but
failed
to
produce
commercially
important
artificial
intelligence
many
observers
believe
that
the
japanese
tried
to
force
engineering
beyond
available
science
by
brute
investment
half
the
amount
spent
on
basic
research
rather
might
have
produced
ten
times
the
result
utilitarian
policies
prioritize
scientific
projects
that
significantly
reduce
suffering
for
larger
numbers
of
people
this
approach
would
mainly
consider
the
numbers
of
people
that
can
be
helped
by
a
research
policy
research
is
more
likely
to
be
supported
when
it
costs
less
and
has
greater
benefits
utilitarian
research
often
pursues
incremental
improvements
rather
than
dramatic
advancements
in
knowledge
or
break-through
solutions
which
are
more
commercially
viable/feasible
in
contrast
monumental
science
is
a
policy
in
which
science
is
supported
for
the
sake
of
a
greater
understanding
of
the
universe
rather
than
for
specific
short-term
practical
goals
this
designation
covers
both
large
projects
often
with
large
facilities
and
smaller
research
that
does
not
have
obvious
practical
applications
and
are
often
overlooked
while
these
projects
may
not
always
have
obvious
practical
outcomes
they
provide
education
of
future
scientists
and
advancement
of
scientific
knowledge
of
lasting
worth
about
the
basic
building
blocks
of
science
practical
outcomes
do
result
from
many
of
these
"monumental"
science
programs
sometimes
these
practical
outcomes
are
foreseeable
and
sometimes
they
are
not
a
classic
example
of
a
monumental
science
program
focused
towards
a
practical
outcome
is
the
manhattan
project
an
example
of
a
monumental
science
program
that
produces
unexpected
practical
outcome
is
the
laser
coherent
light
the
principle
behind
lasing
was
first
predicted
by
einstein
in
1916
but
not
created
until
1954
by
charles
h
townes
with
the
maser
the
breakthrough
with
the
maser
led
to
the
creation
of
the
laser
in
1960
by
theodore
maiman
the
delay
between
the
theory
of
coherent
light
and
the
production
of
the
laser
was
partially
due
to
the
assumption
that
it
would
be
of
no
practical
use
this
policy
approach
prioritizes
efficiently
teaching
all
available
science
to
those
who
can
use
it
rather
than
investing
in
new
science
in
particular
the
goal
is
not
to
"lose"
any
existing
knowledge
and
to
find
new
practical
ways
to
apply
the
available
knowledge
the
classic
success
stories
of
this
method
occurred
in
the
19th
century
us
land-grant
universities
which
established
a
strong
tradition
of
research
in
practical
agricultural
and
engineering
methods
more
recently
the
green
revolution
prevented
mass
famine
over
the
last
thirty
years
the
focus
unsurprisingly
is
usually
on
developing
a
robust
curriculum
and
inexpensive
practical
methods
to
meet
local
needs
most
developed
countries
usually
have
a
specific
national
body
overseeing
national
science
(including
technology
and
innovation)
policy
in
the
case
developing
countries
many
follow
the
same
fashion
many
governments
of
developed
countries
provide
considerable
funds
(primarily
to
universities)
for
scientific
research
(in
fields
such
as
physics
and
geology)
as
well
as
social
science
research
(in
fields
such
as
economics
and
history)
much
of
this
is
not
intended
to
provide
concrete
results
that
may
be
commercialisable
although
research
in
scientific
fields
may
lead
to
results
that
have
such
potential
most
university
research
is
aimed
at
gaining
publication
in
peer
reviewed
academic
journals
a
funding
body
is
an
organisation
that
provides
research
funding
in
the
form
of
research
grants
or
scholarships
research
councils
are
the
funding
bodies
that
are
government-funded
agencies
engaged
in
the
support
of
research
in
different
disciplines
and
postgraduate
funding
funding
from
research
councils
is
typically
competitive
as
a
general
rule
more
funding
is
available
in
science
and
engineering
disciplines
than
in
the
arts
and
social
sciences
in
australia
the
two
main
research
councils
are
the
australian
research
council
and
the
national
health
and
medical
research
council
in
canada
the
three
main
research
councils
("tri-council")
are
the
social
sciences
and
humanities
research
council
(sshrc)
the
natural
sciences
and
engineering
research
council
(nserc)
and
the
canadian
institutes
of
health
research
(cihr)
additional
research
funding
agencies
include
the
canada
foundation
for
innovation
genome
canada
sustainable
development
technology
canada
and
several
tri-council
supported
networks
of
centres
of
excellence
in
brazil
two
important
research
agencies
are
the
national
council
for
scientific
and
technological
development
(cnpq
portuguese:
conselho
nacional
de
desenvolvimento
científico
e
tecnológico)
an
organization
of
the
brazilian
federal
government
under
the
ministry
of
science
and
technology
and
são
paulo
research
foundation
(fapesp
portuguese:
fundação
de
amparo
à
pesquisa
do
estado
de
são
paulo)
a
public
foundation
located
in
the
state
of
são
paulo
brazil
the
science
policy
of
the
european
union
is
carried
out
through
the
european
research
area
which
is
a
system
that
integrates
the
scientific
resources
of
member
nations
and
acts
as
a
"common
market"
for
research
and
innovation
purpose
the
european
union's
executive
body
the
european
commission
has
a
directorate-general
for
research;
which
is
responsible
for
the
union's
science
policy
in
addition
the
joint
research
centre
provides
independent
scientific
and
technical
advice
to
the
european
commission
and
member
states
of
the
european
union
(eu)
in
support
of
eu
policies
there
is
also
the
recently
established
european
research
council
the
first
european
union
funding
body
set
up
to
support
investigator-driven
research
there
are
also
european
science
agencies
that
operate
independently
of
the
european
union
such
as
the
european
science
foundation
european
space
agency
and
the
european
higher
education
area;
which
are
created
by
the
bologna
process
on
science
policy
and
on
the
european
research
area
is
grounded
the
european
environmental
research
and
innovation
policy
which
addresses
global
challenges
of
pivotal
importance
for
the
well-being
of
the
european
citizens
within
the
context
of
sustainable
development
and
environmental
protection
research
and
innovation
in
europe
is
financially
supported
by
the
programme
horizon
2020
which
is
also
open
to
participation
worldwide
german
research
funding
agencies
include
the
deutsche
forschungsgemeinschaft
which
covers
both
science
and
humanities
research
funding
by
the
government
of
india
comes
from
a
number
of
sources
for
basic
science
and
technology
research
these
include
the
council
for
scientific
and
industrial
research
(csir)
department
of
science
and
technology
(dst)
and
university
grants
commission
(ugc)
for
medical
research
these
include
the
indian
council
for
medical
research
(icmr)
csir
dst
and
department
of
biotechnology
(dbt)
for
applied
research
these
include
the
csir
dbt
and
science
and
engineering
research
council
(serc)
other
funding
authorities
are
the
defence
research
development
organisation
(drdo)
the
indian
council
of
agricultural
research
(icar)
the
indian
space
research
organisation
(isro)
the
department
of
ocean
development
(dod)
the
indian
council
for
social
science
research
(icssr)
and
the
ministry
of
environment
and
forests
(mef)
etc
irish
funding
councils
include
the
irish
research
council
(irc)
and
the
science
foundation
ireland
the
prior
irish
research
council
for
science
engineering
and
technology
(ircset)
and
the
irish
research
council
for
the
humanities
and
social
sciences
(irchss)
were
merged
to
form
the
irc
in
march
2012
dutch
research
funding
agencies
include
nederlandse
organisatie
voor
wetenschappelijk
onderzoek
(nwo)
and
agentschap
nl
the
government
of
pakistan
has
mandated
that
a
certain
percentage
of
gross
revenue
generated
by
all
telecom
service
providers
be
allocated
to
development
and
research
of
information
and
communication
technologies
the
national
ict
rd
fund
was
established
in
january
2007
under
the
soviet
union
much
research
was
routinely
suppressed
now
science
in
russia
is
supported
by
state
and
private
funds
from
the
state:
the
russian
humanitarian
scientific
foundation
(http://wwwrfhru)
the
russian
foundation
for
basic
research
(wwwrfbrru)
the
russian
science
foundation
(http://rscfru)
swiss
research
funding
agencies
include
the
swiss
national
science
foundation
(snsf)
the
innovation
promotion
agency
cti
(cti/kti)
ressortforschung
des
bundes
and
eidgenössische
stiftungsaufsicht
in
the
united
kingdom
the
haldane
principle
that
decisions
about
what
to
spend
research
funds
on
should
be
made
by
researchers
rather
than
politicians
is
still
influential
in
research
policy
there
are
several
university
departments
with
a
focus
on
science
policy
such
as
the
science
policy
research
unit
there
are
seven
grant-awarding
research
councils:
the
united
states
has
a
long
history
of
government
support
specially
for
science
and
technology
science
policy
in
the
united
states
is
the
responsibility
of
many
organizations
throughout
the
federal
government
much
of
the
large-scale
policy
is
made
through
the
legislative
budget
process
of
enacting
the
yearly
federal
budget
further
decisions
are
made
by
the
various
federal
agencies
which
spend
the
funds
allocated
by
congress
either
on
in-house
research
or
by
granting
funds
to
outside
organizations
and
researchers
research
funding
agencies
in
the
united
states
are
spread
among
many
different
departments
which
include:
public
policy
of
the
united
states
public
policy
decisions
are
often
decided
by
a
group
of
individuals
with
different
beliefs
and
interests
the
policies
of
the
united
states
of
america
comprise
all
actions
taken
by
its
federal
government
the
executive
branch
is
the
primary
entity
through
which
policies
are
enacted
however
the
policies
are
derived
from
a
collection
of
laws
executive
decisions
and
legal
precedents
the
policies
of
the
united
states
the
almanac
of
policy
issues
which
provides
background
information
archived
documents
and
links
to
major
us
public
policy
issues
organized
the
public
policy
of
the
united
states
into
nine
categories
the
following
lists
these
categories
followed
by
a
few
examples
of
specific
respective
policies:
agricultural
policy
of
the
united
states
is
the
governing
policy
for
agriculture
in
the
united
states
and
is
composed
primarily
of
the
periodically
renewed
federal
us
farm
bills
in
"a
new
agricultural
policy
for
the
united
states"
authors
dennis
keeney
and
long
kemp
summarize
the
agricultural
policy
of
the
united
states
as
follows:
"because
of
its
unique
geography
weather
history
and
policies
the
united
states
has
an
agriculture
that
has
been
dominated
by
production
of
commodity
crops
for
use
in
animal
industrial
and
export
enterprises
over
time
agricultural
policies
evolved
to
support
an
industrialized
commodity-based
agriculture
this
evolution
resulted
in
farmers
leaving
the
land
with
agriculture
moving
to
an
industrial
structure"
in
parallel
with
the
industrialization
of
agriculture
in
the
united
states
the
federal
government
also
developed
the
dietary
guidelines
for
americans
which
emphasize
consumption
of
foods
that
are
produced
by
large-scale
farming
the
drug
policy
of
the
united
states
is
established
by
the
office
of
national
drug
control
policy
a
former
cabinet-level
component
of
the
executive
office
of
the
president
of
the
united
states
which
was
established
by
the
anti-drug
abuse
act
of
1988
its
stated
goal
is
to
establish
policies
priorities
and
objectives
to
eradicate
illicit
drug
use
drug
manufacturing
and
trafficking
drug-related
crime
and
violence
and
drug-related
health
consequences
in
the
us
the
office
of
national
drug
control
policy's
two
current
specific
goals
are
to
"curtail
illicit
drug
consumption
in
america"
and
to
"improve
the
public
health
and
public
safety
of
the
american
people
by
reducing
the
consequences
of
drug
abuse"
they
plan
to
achieve
these
goals
by
taking
the
following
actions:
the
energy
policy
of
the
united
states
addresses
issues
of
energy
production
distribution
and
consumption
such
as
building
codes
and
gas
mileage
standards
the
united
states
department
of
energy
plays
a
major
role
and
its
mission
is
"to
ensure
america's
security
and
prosperity
by
addressing
its
energy
environmental
and
nuclear
challenges
through
transformative
science
and
technology
solutions"
moreover
the
white
house
provides
a
summary
of
the
united
states'
current
condition
regarding
its
energy
policy:
"for
decades
it
has
been
clear
that
the
way
americans
produce
and
consume
energy
is
not
sustainable
our
addiction
to
foreign
oil
and
fossil
fuels
puts
our
economy
our
national
security
and
our
environment
at
risk
to
take
this
country
in
a
new
direction
the
president
is
working
with
congress
to
pass
comprehensive
energy
and
climate
legislation
to
protect
our
nation
from
the
serious
economic
and
strategic
risks
associated
with
our
reliance
on
foreign
oil
to
create
jobs
and
to
cut
down
on
the
carbon
pollution
that
contributes
to
the
destabilizing
effects
of
climate
change"
the
following
is
a
snapshot
of
the
united
states'
current
energy
policy
goals:
the
environmental
policy
of
the
united
states
addresses
and
regulates
activities
that
impact
the
environment
its
general
goal
is
to
protect
the
environment
for
the
welfare
of
future
generations
the
environmental
policy
goals
are
detailed
below:
the
foreign
policy
of
the
united
states
defines
how
the
united
states
interacts
with
foreign
nations
it
only
addresses
the
security
of
the
american
people
and
promotes
international
order
the
following
are
the
most
prominent
foreign
policies
of
the
united
states:
the
federal
reserve
treasury
and
securities
and
exchange
commission
took
several
steps
on
september
19
to
intervene
in
the
crisis
to
stop
the
potential
run
on
money
market
mutual
funds
the
treasury
also
announced
on
september
19
a
new
$50
billion
program
to
insure
the
investments
similar
to
the
federal
deposit
insurance
corporation
(fdic)
program
part
of
the
announcements
included
temporary
exceptions
to
section
23a
and
23b
(regulation
w)
allowing
financial
groups
to
more
easily
share
funds
within
their
group
the
exceptions
would
expire
on
january
30
2009
unless
extended
by
the
federal
reserve
board
the
securities
and
exchange
commission
announced
termination
of
short-selling
of
799
financial
stocks
as
well
as
action
against
naked
short
selling
as
part
of
its
reaction
to
the
mortgage
crisis
policy
alienation
policy
alienation
refers
to
a
framework
which
examines
the
experiences
of
governmental
employees
with
new
policies
they
have
to
implement
it
has
been
used
to
describe
the
experiences
of
front-line
public
professionals
with
new
policies
it
is
defined
"as
a
general
cognitive
state
of
psychological
disconnection
from
the
policy
programme
being
implemented"
a
number
of
examples
can
clarify
the
concept
of
policy
alienation
for
example
bottery
(1998:40)
examining
the
pressures
on
professionals
stemming
from
new
policies
in
education
and
health
care
in
great
britain
cites
a
teacher
arguing
that:
“the
changes
have
been
outrageous
and
have
produced
a
culture
of
meritocracy
and
high
flyers
there’s
massive
paperwork
because
the
politicians
don’t
believe
teachers
are
to
be
trusted”
this
indicates
that
professionals
had
difficulties
identifying
with
the
policies
they
had
to
implement
a
second
example
refers
to
the
introduction
of
a
new
reimbursement
policy
in
mental
healthcare
in
the
netherlands
in
one
large-scale
survey
as
many
as
nine
out
of
ten
professionals
wanted
to
abandon
this
new
policy
(palm
et
al
2008)
psychologists
even
went
as
far
as
to
openly
demonstrate
on
the
street
against
this
policy
a
major
reason
for
this
was
that
many
could
not
align
their
professional
values
with
the
content
of
the
policy
as
one
professional
noted:
""within
the
new
healthcare
system
economic
values
are
dominant
too
little
attention
is
being
paid
to
the
content:
professionals
helping
patients
the
result
is
that
professionals
become
more
aware
of
the
costs
and
revenues
of
their
behavior
this
comes
at
the
expense
of
acting
according
to
professional
standards”"
overall
a
number
of
studies
show
an
increasing
discontent
among
public
professionals
toward
public
policies
(see
also
hebson
et
al
2003;
white
1996)
although
more
positive
experiences
can
also
be
found
(ruiter
2007)
the
policy
alienation
framework
was
developed
to
better
understand
the
experiences
of
front-line
public
professionals
with
new
policies
currently
there
is
an
intense
debate
concerning
professionals
in
the
public
sector
many
of
the
pressures
that
professionals
face
are
related
to
the
difficulties
they
have
with
the
policies
they
have
to
implement
when
implementers
are
unable
to
identify
with
a
policy
this
can
negatively
influence
policy
effectiveness
furthermore
a
high
degree
of
policy
alienation
can
affect
the
quality
of
interactions
between
professionals
and
citizens
which
may
eventually
influence
the
output
legitimacy
of
government
the
policy
alienation
framework
is
used
to
analyze
this
topic
it
has
been
shown
that
policy
alienation
increases
resistance
to
a
new
policy
lowers
behavioral
support
for
the
policy
and
decreases
job
satisfaction
of
public
professionals
hence
it
has
both
influences
on
the
individual
professional
as
well
as
on
policy
effectiveness
alienation
broadly
refers
to
a
sense
of
social
estrangement
an
absence
of
social
support
or
meaningful
social
connection
sociologists
public
administration
scholars
and
other
social
scientists
have
used
the
alienation
concept
in
various
studies
as
a
result
a
number
of
meanings
have
been
attributed
to
the
term
in
an
attempt
to
provide
clarity
seeman
broke
these
meanings
down
into
five
alienation
dimensions:
powerlessness
meaninglessness
normlessness
social
isolation
and
self-estrangement
many
scholars
have
used
these
dimensions
to
devise
operational
measures
for
alienation
so
that
they
can
examine
the
concept
in
a
range
of
settings
mau
for
example
used
four
dimensions
in
examining
student
alienation
rayce
et
al
when
investigating
adolescent
alienation
used
three
of
the
five
dimensions
further
many
other
researchers
have
used
seeman’s
classification
in
examining
the
concept
of
work
alienation
blauner
devised
operational
measures
for
three
of
the
dimensions:
powerlessness
meaninglessness
and
social
isolation
the
policy
alienation
framework
was
conceptualized
based
on
the
works
of
sociologists
such
as
hegel
marx
seeman
and
blauner
furthermore
works
of
public
administration
scholars
were
used
particularly
on
lipsky
(street-level
bureaucracy)
like
work
alienation
policy
alienation
is
multidimensional
consisting
of
policy
powerlessness
and
policy
meaninglessness
dimensions
in
the
work
alienation
literature
the
dimensions
of
powerlessness
and
meaninglessness
are
also
considered
very
important
in
essence
powerlessness
is
a
person's
lack
of
control
over
events
in
their
life
in
the
realm
of
policy
formulation
and
implementation
policy
powerlessness
relates
to
the
degree
of
influence
public
professionals
have
over
shaping
a
policy
program
powerlessness
can
occur
when
a
new
policy
is
drafted
without
the
help
of
the
professionals
by
for
example
not
consulting
their
professionals
associations
or
labor
unions
furthermore
on
an
operational
level
professionals
can
feel
powerless
when
they
have
to
adhere
to
tight
procedures
and
rules
when
implementing
a
policy
(see
also
lipsky)
this
kind
of
powerlessness
may
be
particularly
pronounced
in
professionals
whose
expectations
of
discretion
and
autonomy
contradict
notions
of
bureaucratic
control
(see
also
profession)
the
second
dimension
of
policy
alienation
is
meaninglessness
in
the
realm
of
policy
making
and
implementation
policy
meaninglessness
refers
to
a
professional’s
perception
of
the
contribution
that
the
policy
makes
to
a
greater
purpose
most
notably
to
society
or
to
their
own
clients
for
instance
a
professional
can
feel
that
implementing
a
policy
is
meaningless
if
it
does
not
deliver
any
apparent
beneficial
outcomes
for
society
such
as
more
safety
on
the
streets
to
make
the
dimensions
more
specific
five
sub-dimensions
were
identified:
strategic
tactical
and
operational
powerlessness
societal
and
client
meaninglessness
this
is
shown
in
the
table
below
five
sub-dimensions
of
policy
alienation
nosokinetics
nosokinetics
is
the
science/subject
of
measuring
and
modelling
the
process
of
care
in
health
and
social
care
systems"
nosokinetics"
brings
together
the
greek
words
for
"noso":
disease
and
"kinetics":
movement
black
box
models
are
currently
used
to
plan
changes
in
health
and
social
care
systems
these
input-output
models
overlook
the
process
of
inpatient
care
as
a
result
suboptimal
decisions
are
made
nosokinetics
(analogous
to
pharmacokinetics)
seeks
to
develop
dynamic
methods
which
measure
and
model
the
process
of
inpatient
care
the
aim
is
to
develop
a
scientific
base
to
underpin
the
planning
of
sustainable
health
and
social
care
systems
nosokinetics
is
a
new
"science"
that
was
established
in
the
uk
in
the
early
1990s
by
prof
peter
h
millard
after
publishing
his
phd
thesis
in
2004
nosokinetics
group
newsletter
was
established
prof
peter
h
millard
writes
about
nosokinetics
:
"if
the
random
forces
of
wind
and
tide
can
make
such
a
beautiful
statue
(referring
to
an
iceberg)
how
much
better
could
mankind
do
if
a
new
science
was
developed
which
explains
the
complex
processes
of
health
and
social
care
until
new
methods
of
planning
health
and
social
care
services
to
meet
the
needs
of
an
ageing
population
are
introduced
service
delivery
will
stumble
on
from
crisis
to
crisis
the
world
population
is
ageing
and
sustainable
systems
of
health
care
need
to
be
developed"
he
has
established
the
nosokinetics
group
of
interested
researchers
the
group
collaborates
to
organize
conferences
and
disseminates
news
of
nosokinetics
and
other
researchers'
research
and
practical
use
of
modelling
to
enhance
decision
making
in
health
and
social
care
systems
the
nosokinetics
group
has
succeeded
in
attracting
a
lot
of
researchers
nosokinetics
interested
people
are
present
in
many
countries
including
australia
uk
egypt
they
are
from
different
disciplines
ranging
from
health
care
providers
to
management
scientists
the
news
related
to
nosokinetics
is
shared
to
the
network
through
the
bimonthly
newsletter
"nosokinetics
news"
which
helps
to
communicate
papers
conferences
and
events
of
interest
to
the
nosokinetics
network
policy
studies
policy
studies
is
a
subdisicipline
of
political
science
that
includes
the
analysis
of
the
process
of
policymaking
(the
policy
process)
and
the
contents
of
policy
(policy
analysis)
policy
analysis
includes
substantive
area
research
(such
as
health
or
education
policy)
program
evaluation
and
impact
studies
and
policy
design
it
"involves
systematically
studying
the
nature
causes
and
effects
of
alternative
public
policies
with
particular
emphasis
on
determining
the
policies
that
will
achieve
given
goals"
it
emerged
in
the
united
states
in
the
1960s
and
1970s
policy
studies
also
examines
the
conflicts
and
conflict
resolution
that
arise
from
the
making
of
policies
in
civil
society
the
private
sector
or
more
commonly
in
the
public
sector
(eg
government)
it
frequently
focuses
on
the
public
sector
but
is
equally
applicable
to
other
kinds
of
organizations
(eg
the
not-for-profit
sector)
some
policy
study
experts
graduate
from
public
policy
schools
with
public
policy
degrees
alternatively
experts
may
have
backgrounds
in
policy
analysis
program
evaluation
sociology
psychology
philosophy
economics
anthropology
geography
law
political
science
social
work
environmental
planning
and
public
administration
traditionally
the
field
of
policy
studies
focused
on
domestic
policy
with
the
notable
exceptions
of
foreign
and
defense
policies
however
the
wave
of
economic
globalization
which
ensued
in
the
late
20th
and
early
21st
centuries
created
a
need
for
a
subset
of
policy
studies
that
focuses
on
global
governance
especially
as
it
relates
to
issues
that
transcend
national
borders
such
as
climate
change
terrorism
nuclear
proliferation
and
economic
development
this
subset
of
policy
studies
which
is
often
referred
to
as
international
policy
studies
typically
requires
mastery
of
a
second
language
and
attention
to
cross-cultural
issues
in
order
to
address
national
and
cultural
biases
for
example
the
monterey
institute
of
international
studies
at
middlebury
college
offers
master
of
arts
programs
that
focus
exclusively
on
international
policy
through
a
mix
of
interdisciplinary
and
cross-cultural
analysis
called
the
"monterey
way"
examples
of
academic
programs
in
policy
studies
include
the
harvard
kennedy
school
and
the
lbj
school
of
public
affairs
blanket
policy
blanket
policy
is
a
policy
which
behaves
similarly
to
a
variety
of
things
based
on
webster's
dictionary
it
"covers
a
group
or
class
of
things
or
properties
instead
of
one
or
more
things
mentioned
individually
as
where
a
mortgage
secures
various
debts
as
a
group
or
subjects
a
group
or
class
of
different
pieces
of
property
to
one
general
lien"
webster
1913
suppl
open
educational
resources
policy
open
educational
resource
policies
(oer
policies)
are
principles
or
tenets
adopted
by
governing
bodies
in
support
of
the
use
of
open
content—specifically
open
educational
resources
(oer)
--
and
practices
in
educational
institutions
such
policies
are
emerging
increasingly
at
the
national
state/province
and
local
levels
creative
commons
defines
(oer)
policies
as
"legislation
institutional
policies
and/or
funder
mandates
that
lead
to
the
creation
increased
use
and/or
support
for
improving
oer"
oer
are
learning
materials
that
reside
in
the
public
domain
or
have
been
released
under
an
intellectual
property
license
that
permits
their
free
use
and
re-purposing
by
others
creative
commons
hosts
an
open
educational
resources
policy
registry
which
lists
112
current
and
proposed
open
education
policies
from
around
the
world
another
resource
for
finding
oer
policies
is
the
open
educational
quality
initiative
opal
best
practice
clearing
house
the
opal
initiative
is
a
partnership
between
seven
organizations
including
the
international
council
for
open
and
distance
education
(icde)
unesco
european
foundation
for
quality
the
open
university
uk
aalto
university
and
the
catholic
university
portugal
led
by
the
university
of
duisburg-essen
in
germany
it
is
partly
funded
by
the
european
commission
on
friday
22
june
2012
the
unesco
world
open
educational
resources
(oer)
congress
released
the
2012
paris
oer
declaration
which
called
on
governments
to
openly
license
publicly
funded
educational
materials
unesco
member
states
unanimously
approved
the
declaration
which
highlights
the
importance
of
open
educational
resources
and
gives
recommendations
to
governments
and
institutions
around
the
globe
on
january
17
2014
the
council
on
higher
education
in
south
africa
published
a
"white
paper
for
post-school
education
and
training"
this
paper
emphasized
open
learning
principles
and
set
the
stage
for
supporting
national
efforts
to
design
and
develop
high-quality
open
educational
resources
in
response
the
university
of
south
africa
(unisa)—one
of
the
founding
partners
of
the
oeru
network
and
a
member
of
the
2012
unesco
oer
conference
in
paris—approved
an
open
educational
resource
(oer)
strategy
in
march
2014
an
open-access
policy
enacted
by
the
faculty
of
a
research
university
can
empower
them
in
choosing
how
to
distribute
their
own
scholarly
work
if
a
faculty
member
wishes
to
grant
exclusive
rights
to
a
publisher
they
would
first
need
to
request
a
waiver
from
their
faculty
governance
body
some
reasons
to
implement
this
kind
of
policy
institution-wide
are
to:
this
kind
of
blanket
policy
provides
support
to
those
whose
research
is
not
part
of
a
project
that
requires
open
access
to
the
research
done
for
example
since
the
february
2013
directive
from
the
united
states
office
of
science
and
technology
policy
us
federal
agencies
have
been
developing
their
own
policies
on
making
research
freely
available
within
a
year
of
publication
sparc
the
scholarly
publishing
and
academic
resources
coalition
led
the
collaborative
and
open
effort
to
create
an
"open
access
spectrum"
that
demonstrates
a
more
sophisticated
approach
is
needed
in
discussions
about
the
concept
of
openness
in
research
communications
the
"howopenisit?
guide
(as
well
as
an
faq
document
and
slide
deck)
is
available
for
download
on
the
sparc
website
another
useful
guide
has
been
developed
by
members
of
the
harvard
office
for
scholarly
communication
the
harvard
open
access
project
and
the
berkman
center
for
internet
and
society
this
online
guide
"good
practices
for
university
open-access
policies"
is
built
on
a
wiki
and
is
designed
to
evolve
over
time
according
to
the
co-authors:
emily
kilcer
stuart
shieber
and
peter
suber
on
june
10
2013
the
faculty
board
of
the
california
institute
of
technology
(caltech)
created
an
institution-wide
open
access
policy
the
ruling
stated
that
as
of
january
1
2014
all
caltech
faculty
must
agree
to
grant
nonexclusive
rights
to
caltech
to
disseminate
their
scholarly
papers
either
via
the
authors'
own
sites
or
to
caltech
authors
the
online
repository
the
goal
is
to
encourage
wider
distribution
of
their
work
and
to
simplify
the
copyright
process
when
posting
research
on
faculty
or
institutional
web
sites
the
initiative
was
put
in
place
to
prevent
publishers
of
those
journals
from
threatening
legal
action
or
issuing
takedown
notices
to
authors
who
have
posted
their
content
on
their
own
sites
or
to
caltechauthors
an
online
repository
for
research
papers
authored
by
caltech
faculty
and
other
researchers
at
caltech
on
march
21
2010
the
duke
university
academic
council
voted
to
support
the
university
library's
new
data
repository
dukespace
with
a
blanket
policy
to
provide
open
access
to
their
scholarly
writings
the
policy
allows
for
faculty
members
to
opt
out
at
any
time
and
it
is
regularly
reviewed
to
determine
its
effectiveness
duke
also
in
2010
joined
the
compact
for
open-access
publishing
equity
(cope)
and
established
a
fund
to
help
duke
faculty
members
to
cover
any
author
fees
required
to
publish
in
open
access
journals
on
february
12
2008
the
faculty
of
arts
and
sciences
of
harvard
university
approved
their
open
access
policy
granting
to
the
president
and
fellows
of
harvard
to
"make
available
his
or
her
scholarly
articles
and
to
exercise
the
copyright
in
those
articles
in
a
nonexclusive
irrevocable
paid-up
worldwide
license"
since
then
several
other
schools
within
the
university
now
participate
in
the
open
access
policies
supported
by
the
office
for
scholarly
communication:
the
graduate
school
of
design
the
school
of
education
the
business
school
the
law
school
the
kennedy
school
of
government
the
divinity
school
and
the
school
of
public
health
the
university's
open-access
repository
is
called
dash
(digital
access
to
scholarship
at
harvard)
which
is
where
the
faculty
upload
their
scholarly
articles
for
access
by
all
adopted
by
a
unanimous
vote
on
march
18
2009
the
massachusetts
institute
of
technology
(mit)
faculty
adopted
an
open
access
policy
the
policy
applies
to
"all
scholarly
articles
written
while
the
person
is
a
member
of
the
faculty
except
for
any
articles
completed
before
the
adoption
of
this
policy
and
any
articles
for
which
the
faculty
member
entered
into
an
incompatible
licensing
or
assignment
agreement
before
the
adoption
of
this
policy"
the
mit
online
repository
is
called
dspace@mit
and
it
was
designed
to
work
seamlessly
with
google
scholar
the
faculty
revised
and
updated
the
policy
in
2010
to
take
into
consideration
the
various
issues
associated
with
the
mit
librarians'
discussions
with
publishers
in
2010
the
dean
of
the
faculty
of
princeton
university
appointed
an
ad-hoc
committee
of
faculty
and
the
university
librarian
to
study
the
question
of
open
access
to
faculty
publications
-
and
in
march
2011
the
committee
recommended
several
changes
to
the
faculty
rules
to
allow
for
a
blanket
policy
for
open
access
to
princeton
faculty
scholarship
the
faculty
approved
an
open
access
policy
on
september
19
2011
which
was
last
revised
in
january
2012
on
june
26
2008
the
stanford
university
graduate
school
of
education
(gse)
were
the
first
in
that
school
to
grant
permission
to
the
university
to
make
their
scholarly
articles
publicly
accessible
and
to
exercise
the
copyright
in
a
"nonexclusive
irrevocable
worldwide
license
provided
that
the
articles
are
properly
attributed
to
the
authors
not
sold
for
a
profit"
the
gse
open
archive
houses
and
makes
publicly
available
the
gse
authors'
working
papers
as
well
as
published
articles
between
may
21-24th
2013
the
stanford
gse
doctoral
students
voted
in
favor
of
a
motion
to
enact
an
open
access
policy
at
this
time
however
despite
the
strong
case
made
by
professors
john
willinsky
and
juan
pablo
alperin
no
other
stanford
academic
units
have
stepped
forward
on
july
24
2013
the
academic
senate
of
the
university
of
california
(uc)
approved
the
uc
open
access
policy
for
all
8000
plus
faculty
at
their
ten
campuses
some
confusion
at
the
local
campuses
led
to
online
postings
of
journal
articles
whose
copyright
was
already
owned
by
publishers
for
example
in
december
2013
the
academic
publishing
company
elsevier
sent
several
uc
faculty
notices
to
take
down
certain
journal
articles
posted
openly
on
their
campus
webpages
eg
on
the
department
websites
or
faculty
profiles
the
uc
open
access
policy
protected
those
faculty
who
had
correctly
uploaded
their
articles
to
the
uc
escholarship
repository
in
another
case
of
misunderstanding
by
the
faculty
about
open
access
in
march
2014
the
university
received
a
digital
millennium
copyright
act
(dmca)
takedown
notice
for
nine
articles
owned
by
the
american
society
for
civil
engineers
(asce)
the
uc
faculty
authors
had
uploaded
to
escholarship
the
publisher-formatted
articles
between
2004
and
2008
before
the
uc
open
access
policy
had
been
enacted
and
in
violation
of
the
publisher's
agreement
with
the
authors
when
they
gave
their
copyrights
to
the
asce
in
2014
the
faculty
assembly
of
the
university
of
colorado
boulder
approved
the
cu
boulder
open
access
policy
"in
order
to
allow
for
broad
dissemination
of
their
research"
they
granted
to
the
regents
of
the
university
of
colorado
"a
nonexclusive
irrevocable
worldwide
license
to
exercise
any
and
all
rights
under
copyright
relating
to
their
scholarly
work
as
long
as
the
works
are
properly
attributed
to
the
authors
and
not
used
for
commercial
purposes"—and
that
the
individual
faculty
would
retain
full
ownership
of
the
material
authors
at
uc
boulder
are
expected
to
inform
publishers
about
the
university's
policy
and
that
they
"have
granted
a
pre-existing
license"
the
digital
repository
cu
scholar
is
maintained
by
the
university
libraries
and
functions
under
a
set
of
policies
derived
from
the
open
access
policy
contributions
from
the
cu
boulder
community
can
include
working
papers
and
technical
reports
published
scholarly
research
articles
completed
manuscripts
digital
art
or
multimedia
conference
papers
and
proceedings
theses
and
dissertations
undergraduate
honors
theses
journals
published
on
campus
faculty
course-related
output
primarily
of
scholarly
interest
and
data
sets
the
chancellor's
executive
committee
recently
approved
the
new
policy
following
the
lead
of
the
council
of
deans
and
the
office
of
the
provost
and
executive
vice
chancellor
in
2005
the
university
of
kansas
(ku)
created
ku
scholarworks
a
digital
repository
for
scholarly
work
created
by
ku
faculty
and
staff
faculty
senate
president
lisa
wolf-wendel
professor
of
education
leadership
and
policy
studies
approved
a
new
policy
"open
access
policy
for
university
of
kansas
scholarship"
on
april
30
2009
in
order
to
provide
the
broadest
possible
access
to
the
journal
literature
authored
by
ku
faculty"
in
june
2009
under
a
faculty-initiated
policy
approved
by
chancellor
robert
hemenway
ku
became
the
first
us
public
university
to
implement
an
open
access
policy
unless
a
ku
author
sought
a
waiver
all
articles
must
be
submitted
to
ku
scholarworks
"processes
to
implement
the
ku
open
access
policy"
were
endorsed
by
the
faculty
senate
in
february
2010
theses
and
dissertations
at
the
university
of
kansas
are
also
openly
available
however
in
2010
ku
graduate
studies
established
a
policy
that
a
student
may
request
permission
to
embargo
its
publication
for
six
months
one
year
or
two
years
graduates
earning
the
ku
master
of
fine
arts
in
creative
writing
or
phd
in
english
(literature
and
creative
writing
track)
may
request
a
permanent
embargo
in
the
united
kingdom
the
higher
education
funding
council
for
england
(hefce)
subsidized
the
jisc
academy
open
educational
resources
programme
jisc
refers
to
a
membership
organization
that
provides
digital
solutions
for
united
kingdom
education
and
research
initiatives
the
jisc/he
oer
programme
(phase
3
from
october
2011
–
october
2012)
was
meant
to
build
on
sustainable
procedure
indicated
in
the
first
two
phases
eventually
expanding
in
new
directions
that
connect
open
educational
resources
to
other
fields
of
work
this
third
phase
involved
important
stakeholders
emphasizing
fresh
challenges
and
insights
about
the
effect
of
oer
and
open
educational
practice
during
this
stage
the
concept
of
electronic
books
and
massive
open
online
courses
(mooc)
also
emerged
moocs
offer
courses
at
the
university
level
without
having
to
finish
the
whole
programme
many
students
get
the
chance
to
study
premium
courses
online
frequently
at
no
cost
hefce
made
significant
investments
through
the
jisc
and
academy
from
2009
until
2012
the
objective
was
to
encourage
sharing
and
reusing
of
resources
which
provide
benefits
to
higher
education
in
the
united
kingdom
more
than
80
projects
obtained
funding
during
the
uk
oer
programme
substantial
investments
were
channelled
towards
the
development
of
open
educational
resources
even
as
the
benefits
for
stakeholders
have
not
been
explained
properly
sufficient
evidence
is
needed
to
prove
this
point
one
criticism
is
that
many
such
programmes
are
not
technically
and
educationally
accessible
to
a
worldwide
audience
policy
transfer
policy
transfer
policy
transfer
is
a
process
in
which
knowledge
about
policies
administrative
arrangements
institutions
and
ideas
in
one
political
setting
(past
or
present)
is
used
in
the
development
of
policies
administrative
arrangements
institutions
and
ideas
in
another
political
setting
policy
transfer
has
a
long
history
with
a
variety
of
policies
such
as
zero
tolerance
policing
welfare-to-work
and
business
improvement
districts
moving
between
different
nation
states
policy
transfer
has
been
the
subject
of
considerable
academic
research
led
primarily
by
political
scientists
since
the
late
1990s
since
the
mid-2000s
geographers
have
also
played
an
important
role
in
these
debates
(often
use
the
term
"policy
mobilities"
instead
of
policy
transfer)
since
david
dolowitz
and
david
marsh's
(2000)
paper
'learning
from
abroad:
the
role
of
policy
transfer
in
contemporary
policy-making'
academic
research
has
focused
on
the
issues
of
who
is
involved
in
policy
transfer
what
is
transferred
from
and
to
where
policy
is
transferred
the
degrees
of
and
constraints
on
transfer
and
its
success
once
transferred
more
recently
there
have
been
attempts
to
explicate
the
role
of
two
way
communication
and
particularly
feedback
from
policy
stakeholders
for
successful
policy
transfer
along
with
efforts
to
acknowledge
the
"indigenization"
of
policies
as
they
are
modified
and
adapted
to
context
the
debates
in
geography
have
focused
on
the
technologies
and
methods
through
which
policies
and
ideas
circulate
–
such
as
study
tours
conferences
and
best
practice
guides
–
as
well
as
looking
at
how
and
why
the
policies
change
form
as
they
circulate
asia-pacific
network
for
global
change
research
the
asia-pacific
network
for
global
change
research
(apn)
is
an
intergovernmental
network
that
promotes
policy-oriented
research
and
capacity-building
activities
related
to
global
change
in
the
region
apn
receives
financial
contribution
from
the
governments
of
the
united
states
japan
republic
of
korea
and
new
zealand
with
in-kind
contribution
from
all
it
22
member
countries
the
apn
secretariat
is
based
in
kobe
japan
hosted
by
the
hyogo
prefectural
government
the
history
of
apn
dates
back
to
the
1990
white
house
conference
on
science
and
economics
research
related
to
global
change
17–18
april
1990
at
which
then
us
president
george
bush
invited
countries
of
the
world
to
join
the
united
states
in
creating
regional
networks
for
north-south
scientific
cooperation
at
the
intergovernmental
level
to
deal
with
global
environmental
change
research
later
in
1992
president
bush
and
then
prime
minister
of
japan
kiichi
miyazawa
signed
the
1992
us-japan
global
partnership
agreement
which
among
other
things
reaffirmed
and
strengthened
japan-us
commitment
to
global
change
research
discussions
along
these
lines
ultimately
resulted
in
the
establishment
of
three
global
change
research
networks:
enrich
for
europe
and
africa
apn
for
asia
and
the
pacific
and
iai
for
the
americas
apn
was
formally
launched
in
1996
at
its
first
intergovernmental
meeting
held
at
chiang
mai
thailand
in
1997
a
competitive
process
was
in
place
open
to
funding
applications
for
scientific
research
projects
relating
to
global
environmental
change
starting
from
12
countries
in
1996
apn
membership
has
grown
to
22
as
of
april
2013
in
addition
to
the
22
full
members
institutions
and
individuals
from
a
number
of
“approved
countries”
are
eligible
for
apn
funding
policy
monitoring
policy
monitoring
comes
a
range
of
activities
describing
and
analyzing
the
development
and
implementation
of
policies
identifying
potential
gaps
in
the
process
outlining
areas
for
improvement
and
holding
policy
implementers
accountable
for
their
activities
monitoring
policy
development
and
implementation
is
an
integral
component
of
the
policy
cycle
and
can
be
applied
in
sectors
including
agriculture
health
education
and
finance
policy
monitoring
can
improve
policy
informatioation
among
stakeholders
and
the
use
of
evaluation
techniques
to
provide
feedback
to
reframe
and
revise
policies
waterman
and
wood
derived
policy
monitoring
from
agency
theory
describing
a
process
where
policymakers
monitor
the
actions
of
their
bureaucratic
agents
who
implement
and
enforce
policies
this
monitoring
allows
policymakers
to
compensate
for
their
agents’
greater
knowledge
of
the
policy
process
and
enables
them
to
be
well-informed
decision
makers
thus
policy
monitoring
allows
policymakers
and
interested
actors
to
systematically
examine
the
process
of
creating
a
policy
implementing
it
and
evaluating
its
effects
policy
monitoring
activities
can
be
used
to
collect
and
analyze
data
related
to
the
development
and
implementation
of
specific
policies
it
can
also
help
link
policies
to
specific
outcomes
and
help
identify
and
evaluate
policy
impacts
policy
impacts
can
include
specific
changes
in
behavior
(eg
increased
number
of
people
wearing
seatbelts)
finances
(eg
increased
tax
revenue)
health
status
or
epidemiology
(eg
reduced
number
of
new
hiv
infections)
or
other
social
indicators
(eg
reduced
crime
rates
reduced
levels
of
pollution)
data
from
policy
monitoring
can
be
used
to
support
advocacy
efforts
and
guide
the
development
of
new
timely
and
relevant
policies
policy
monitoring
should
also
include
the
identification
of
operational
policy
barriers
that
can
be
addressed
through
policy
and
program
reform
and
findings
can
support
improved
implementation
of
existing
policies
numerous
actors
and
stakeholders
can
influence
the
movement
of
policy
from
inception
to
implementation
well-maintained
documentation
and
review
of
all
key
stakeholders
involved
in
a
policy
can
help
advocates
for
a
given
policy—such
as
military
reform
water
rights
or
disability
legislation—prepare
to
address
different
ideologies
capacities
or
interests
of
key
actors
limiting
stakeholder
analysis
only
to
government
and
official
policymakers
may
ignore
major
groups
that
can
support
policy
development
policy
monitoring
coalitions
should
agree
on
what
they
are
monitoring
and
be
succinct
in
their
recommendations
to
policymakers
policy
initiatives
themselves
are
often
controversial
and
policy
monitoring
can
be
contentious
because
it
shows
how
well
policy
implementers
and
enforcers
are
doing
their
jobs
those
conducting
policy
monitoring
should
be
thorough
in
their
data
collection
and
unbiased
in
their
presentation
of
facts
robust
trainings
on
policy
monitoring
work
can
help
organizations
be
systematic
and
effective
in
their
policy
monitoring
efforts
the
united
states
president’s
emergency
plan
for
aids
relief
(pepfar)’s
monitoring
policy
reform
tool
outlines
the
progression
of
policy
development
related
to
hiv
from
problem
identification
to
monitoring
and
evaluation
the
tool
supports
a
relatively
simple
and
uniform
monitoring
process
which
can
be
applied
to
any
policy
area
this
tool
can
guide
policy
monitoring
efforts
throughout
the
policy
reform
process
the
fao’s
food
and
agriculture
policy
decision
analysis
(fapda)
is
a
policy
monitoring
tool
that
provides
a
working
cycle
technique
to
identify
policy
problems
and
improve
analysis
of
policy
issues
by
incorporating
fapda
outputs
such
as
a
web-based
tool
country
policy
review
and
policy
analysis
report
policy
dialogue
can
be
more
systematic
and
encompass
different
actors
interested
in
fapda
data
the
world
health
organization
has
started
to
develop
dedicated
monitoring
systems
for
policy
interventions
on
the
social
determinants
of
health
that
improve
health
equity
such
as
social
protection
and
gender
equity
policies
policy
monitoring
can
be
performed
through
different
issue-driven
lenses
such
as
gender
sensitivity
or
gender
equality
gender-sensitive
policy
monitoring
analyzes
any
gender
aspects
of
a
policy
or
policy
issue
and
considers
the
impact
of
the
policy
on
both
men
and
women
as
well
as
its
impact
on
gender
relations
for
example
a
policy
that
is
shown
to
have
improved
the
welfare
of
a
household
may
not
necessarily
affect
all
household
members
positively
or
equally
and
may
have
even
exacerbated
gender
inequity
gender-sensitive
policy
monitoring
can
help
advance
gender
equity
and
improve
policy
implementation
civil
society
and
other
stakeholders
can
use
policy
monitoring
techniques
to
systematically
gather
data
on
the
gender
aspects
of
policies
and
use
these
data
to
influence
policymakers
to
favor
gender-equitable
health
policies¬¬¬—these
processes
are
essential
to
facilitating
gender
mainstreaming
a
2012
study
analyzed
planned
policy
interventions
across
the
22
publicly
accessible
pepfar
partnership
frameworks
to
understand
how
the
interventions
are
related
to
pepfar
and
country
or
regional
priorities
the
study
found
that
“policy
monitoring
by
donors
partner
country
governments
and
civil
society
stakeholders
can
help
measure
whether
policy
interventions
are
occurring
as
planned
in
order
to
further
hiv
prevention
care
and
treatment
and
health
system
goals
and
if
not
can
point
to
needed
changes"
mandate
(politics)
in
politics
a
mandate
is
the
authority
granted
by
a
constituency
to
act
as
its
representative
the
concept
of
a
government
having
a
legitimate
mandate
to
govern
via
the
fair
winning
of
a
democratic
election
is
a
central
idea
of
representative
democracy
new
governments
who
attempt
to
introduce
policies
that
they
did
not
make
public
during
an
election
campaign
are
said
not
to
have
a
legitimate
"mandate"
to
implement
such
policies
elections
especially
ones
with
a
large
margin
of
victory
and
are
often
said
to
give
the
newly
elected
government
or
elected
official
an
implicit
mandate
to
put
into
effect
certain
policies
when
a
government
seeks
re-election
they
may
introduce
new
policies
as
part
of
the
campaign
and
are
hoping
for
approval
from
the
voters
and
say
they
are
seeking
a
"new
mandate"
in
some
languages
a
'mandate'
can
mean
a
parliamentary
seat
won
in
an
election
rather
than
the
electoral
victory
itself
in
case
such
a
mandate
is
bound
to
the
wishes
of
the
electorate
it
is
an
imperative
mandate
otherwise
it
is
called
"free"
political
philosophy:
policy
analysis
policy
analysis
is
a
technique
used
in
public
administration
to
enable
civil
servants
activists
and
others
to
examine
and
evaluate
the
available
options
to
implement
the
goals
of
laws
and
elected
officials
the
process
is
also
used
in
the
administration
of
large
organizations
with
complex
policies
it
has
been
defined
as
the
process
of
"determining
which
of
various
policies
will
achieve
a
given
set
of
goals
in
light
of
the
relations
between
the
policies
and
the
goals"
policy
analysis
can
be
divided
into
two
major
fields:
the
areas
of
interest
and
the
purpose
of
analysis
determine
what
types
of
analysis
are
conducted
a
combination
of
two
kinds
of
policy
analyses
together
with
program
evaluation
is
defined
as
"policy
studies"
policy
analysis
is
frequently
deployed
in
the
public
sector
but
is
equally
applicable
elsewhere
such
as
nonprofit
organizations
and
non-governmental
organizations
policy
analysis
has
its
roots
in
systems
analysis
an
approach
used
by
united
states
secretary
of
defense
robert
mcnamara
in
the
1960s
various
approaches
to
policy
analysis
exist
the
analysis
"for"
policy
(and
analysis
"of"
policy)
is
the
central
approach
in
social
science
and
educational
policy
studies
it
is
linked
to
two
different
traditions
of
policy
analysis
and
research
frameworks
the
approach
of
analysis
"for"
policy
refers
to
research
conducted
for
actual
policy
development
often
commissioned
by
policymakers
inside
the
bureaucracy
(eg
senior
civil
servants)
within
which
the
policy
is
developed
analysis
"of"
policy
is
more
of
an
academic
exercise
conducted
by
academic
researchers
professors
and
think
tank
researchers
who
are
often
seeking
to
understand
why
a
particular
policy
was
developed
at
a
particular
time
and
assess
the
effects
intended
or
otherwise
of
that
policy
when
it
was
implemented
there
are
three
approaches
that
can
be
distinguished:
the
analysis-centric
the
policy
process
and
the
meta-policy
approach
the
analysis-centric
(or
"analycentric")
approach
focuses
on
individual
problems
and
their
solutions
its
scope
is
the
micro-scale
and
its
problem
interpretation
or
problem
resolution
usually
involves
a
technical
solution
the
primary
aim
is
to
identify
the
most
effective
and
efficient
solution
in
technical
and
economic
terms
(eg
the
most
efficient
allocation
of
resources)
the
policy
process
approach
puts
its
focal
point
onto
political
processes
and
involved
stakeholders;
its
scope
is
the
broader
meso-scale
and
it
interprets
problems
using
a
political
lens
(ie
the
interests
and
goals
of
elected
officials)
it
aims
at
determining
what
processes
means
and
policy
instruments
(eg
regulation
legislation
subsidy)
are
used
as
well
it
tries
to
explain
the
role
and
influence
of
stakeholders
within
the
policy
process
in
the
2010s
"stakeholders"
is
defined
broadly
to
include
citizens
community
groups
non-governmental
organizations
businesses
and
even
opposing
political
parties
by
changing
the
relative
power
and
influence
of
certain
groups
(eg
enhancing
public
participation
and
consultation)
solutions
to
problems
may
be
identified
that
have
more
"buy
in"
from
a
wider
group
one
way
of
doing
this
followed
a
heuristic
model
called
the
"policy
cycle"
in
its
simplest
form
the
policy
cycle
which
is
often
depicted
visually
as
a
loop
or
circle
starts
with
the
identification
of
the
problem
proceeds
to
an
examination
of
the
different
policy
tools
that
could
be
used
to
respond
to
that
problem
then
goes
on
to
the
implementation
stage
in
which
one
or
more
policies
are
put
into
practice
(eg
a
new
regulation
or
subsidy
is
set
in
place)
and
then
finally
once
the
policy
has
been
implemented
and
run
for
a
certain
period
the
policy
is
evaluated
a
number
of
different
viewpoints
can
be
used
during
evaluation
including
looking
at
a
policy's
effectiveness
cost-effectiveness
value
for
money
outcomes
or
outputs
the
meta-policy
approach
is
a
systems
and
context
approach;
ie
its
scope
is
the
macro-scale
and
its
problem
interpretation
is
usually
of
a
structural
nature
it
aims
at
explaining
the
contextual
factors
of
the
policy
process;
ie
what
the
political
economic
and
socio-cultural
factors
are
that
influence
it
as
problems
may
result
because
of
structural
factors
(eg
a
certain
economic
system
or
political
institution)
solutions
may
entail
changing
the
structure
itself
policy
analysis
uses
both
qualitative
methods
and
quantitative
methods
qualitative
research
includes
case
studies
and
interviews
with
community
members
quantitative
research
includes
survey
research
statistical
analysis
(also
called
"data
analysis")
and
model
building
a
common
practice
is
to
define
the
problem
and
evaluation
criteria;
identify
and
evaluate
alternatives;
and
recommend
a
certain
policy
accordingly
promotion
of
the
best
agendas
are
the
product
of
careful
"back-room"
analysis
of
policies
by
"a
priori"
assessment
and
"a
posteriori"
evaluation
there
are
six
dimensions
to
policy
analysis
categorized
as
the
effects
and
implementation
of
the
policy
across
a
period
of
time
also
collectively
known
as
"durability"
of
the
policy
which
means
the
capacity
in
content
of
the
policy
to
produce
visible
effective
compatible
change
or
results
over
time
with
robustness
effects
implementation
the
strategic
effects
dimensions
can
pose
certain
limitations
due
to
data
collection
however
the
analytical
dimensions
of
effects
directly
influences
acceptability
the
degree
of
acceptability
is
based
upon
the
plausible
definitions
of
actors
involved
in
feasibility
if
the
feasibility
dimension
is
compromised
it
will
put
the
implementation
at
risk
which
will
entail
additional
costs
finally
implementation
dimensions
collectively
influence
a
policy's
ability
to
produce
results
or
impacts
one
model
of
policy
analysis
is
the
"five-e
approach"
which
consists
of
examining
a
policy
in
terms
of:
policies
are
considered
as
frameworks
that
can
optimize
the
general
well-being
these
are
commonly
analyzed
by
legislative
bodies
and
lobbyists
every
policy
analysis
is
intended
to
bring
an
evaluative
outcome
a
systemic
policy
analysis
is
meant
for
in
depth
study
for
addressing
a
social
problem
following
are
steps
in
a
policy
analysis:
many
models
exist
to
analyze
the
development
and
implementation
of
public
policy
analysts
use
these
models
to
identify
important
aspects
of
policy
as
well
as
explain
and
predict
policy
and
its
consequences
each
of
these
models
are
based
upon
the
types
of
policies
some
evidence
supported
models
are:
public
policy
is
determined
by
a
range
of
political
institutions
which
give
policy
legitimacy
to
policy
measures
in
general
the
government
applies
policy
to
all
citizens
and
monopolizes
the
use
of
force
in
applying
or
implementing
policy
(through
government
control
of
law
enforcement
court
systems
imprisonment
and
armed
forces)
the
legislature
executive
and
judicial
branches
of
government
are
examples
of
institutions
that
give
policy
legitimacy
many
countries
also
have
independent
quasi-independent
or
arm's
length
bodies
which
while
funded
by
government
are
independent
from
elected
officials
and
political
leaders
these
organizations
may
include
government
commissions
tribunals
regulatory
agencies
and
electoral
commissions
policy
creation
is
a
process
that
typically
follows
a
sequence
of
steps
or
stages:
this
model
however
has
been
criticized
for
being
overly
linear
and
simplistic
in
reality
stages
of
the
policy
process
may
overlap
or
never
happen
also
this
model
fails
to
take
into
account
the
multiple
factors
attempting
to
influence
the
process
itself
as
well
as
each
other
and
the
complexity
this
entails
one
of
the
most
widely
used
model
for
public
institutions
are
of
herbert
a
simon
the
father
of
rational
models
it
is
also
used
by
private
corporations
however
many
criticise
the
model
due
to
characteristics
of
the
model
being
impractical
and
relying
on
unrealistic
assumptions
for
instance
it
is
a
difficult
model
to
apply
in
the
public
sector
because
social
problems
can
be
very
complex
ill-defined
and
interdependent
the
problem
lies
in
the
thinking
procedure
implied
by
the
model
which
is
linear
and
can
face
difficulties
in
extraordinary
problems
or
social
problems
which
have
no
sequences
of
happenings
the
rational
model
of
decision-making
is
a
process
for
making
sound
decisions
in
policy-making
in
the
public
sector
rationality
is
defined
as
“a
style
of
behavior
that
is
appropriate
to
the
achievement
of
given
goals
within
the
limits
imposed
by
given
conditions
and
constraints”
it
is
important
to
note
the
model
makes
a
series
of
assumptions
such
as:
'the
model
must
be
applied
in
a
system
that
is
stable';
'the
government
is
a
rational
and
unitary
actor
and
that
its
actions
are
perceived
as
rational
choices';
'the
policy
problem
is
unambiguous';
'there
are
no
limitations
of
time
or
cost'
furthermore
in
the
context
of
the
public
sector
policy
models
are
intended
to
achieve
maximum
social
gain
simon
identifies
an
outline
of
a
step
by
step
mode
of
analysis
to
achieve
rational
decisions
ian
thomas
describes
simon's
steps
as
follows:
the
model
of
rational
decision-making
has
also
proven
to
be
very
useful
to
several
decision
making
processes
in
industries
outside
the
public
sphere
nonetheless
there
are
some
who
criticize
the
rational
model
due
to
the
major
problems
which
can
be
faced
which
tend
to
arise
in
practice
because
social
and
environmental
values
can
be
difficult
to
quantify
and
forge
consensus
around
furthermore
the
assumptions
stated
by
simon
are
never
fully
valid
in
a
real
world
context
further
criticism
of
the
rational
model
include:
leaving
a
gap
between
planning
and
implementation
ignoring
of
the
role
of
people
entrepreneurs
leadership
etc
the
insufficiency
of
technical
competence
(ie
ignoring
the
human
factor)
reflecting
too
mechanical
an
approach
(ie
the
organic
nature
of
organizations)
requiring
of
multidimensional
and
complex
models
generation
of
predictions
which
are
often
wrong
(ie
simple
solutions
may
be
overlooked)
incurring
of
cost
(ie
costs
of
rational-comprehensive
planning
may
outweigh
the
cost
savings
of
the
policy)
however
thomas
r
dye
the
president
of
the
lincoln
center
for
public
service
states
the
rational
model
provides
a
good
perspective
since
in
modern
society
rationality
plays
a
central
role
and
everything
that
is
rational
tends
to
be
prized
thus
it
does
not
seem
strange
that
“we
ought
to
be
trying
for
rational
decision-making”
an
incremental
policy
model
relies
on
features
of
incremental
decision-making
such
as:
satisfying
organizational
drift
bounded
rationality
and
limited
cognition
among
others
such
policies
are
often
called
"muddling
through"
represent
a
conservative
tendency:
new
policies
are
only
slightly
different
from
old
policies
policy-makers
are
too
short
on
time
resources
and
brains
to
make
totally
new
policies;
as
such
past
policies
are
accepted
as
having
some
legitimacy
when
existing
policies
have
sunk
costs
which
discourage
innovation
incrementalism
is
an
easier
approach
than
rationalism
and
the
policies
are
more
politically
expedient
because
they
don't
necessitate
any
radical
redistribution
of
values
such
models
necessarily
struggle
to
improve
the
acceptability
of
public
policy
criticisms
of
such
a
policy
approach
include:
challenges
to
bargaining
(ie
not
successful
with
limited
resources)
downplaying
useful
quantitative
information
obscuring
real
relationships
between
political
entities
an
anti-intellectual
approach
to
problems
(ie
the
preclusion
of
imagination)
and
a
bias
towards
conservatism
(ie
bias
against
far-reaching
solutions)
there
are
many
contemporary
policies
relevant
to
gender
and
workplace
issues
actors
analyze
contemporary
gender-related
employment
issues
ranging
from
parental
leave
and
maternity
programs
sexual
harassment
and
work/life
balance
to
gender
mainstreaming
it
is
by
the
juxtaposition
of
a
variety
of
research
methodologies
focused
on
a
common
theme
the
richness
of
understanding
is
gained
this
integrates
what
are
usually
separate
bodies
of
evaluation
on
the
role
of
gender
in
welfare
state
developments
employment
transformations
workplace
policies
and
work
experience
this
policy
is
formed
as
a
result
of
forces
and
pressures
from
influential
groups
pressure
groups
are
informally
co-opted
into
the
policy
making
process
regulatory
agencies
are
captured
by
those
they
are
supposed
to
regulate
no
one
group
is
dominant
all
the
time
on
all
issues
the
group
is
the
bridge
between
the
individual
and
the
administration
the
executive
is
thus
pressured
by
interest
groups
the
task
of
the
system
is
to:
there
are
several
other
major
types
of
policy
analysis
broadly
groupable
into
competing
approaches:
the
success
of
a
policy
can
be
measured
by
changes
in
the
behavior
of
the
target
population
and
active
support
from
various
actors
and
institutions
involved
a
public
policy
is
an
authoritative
communication
prescribing
an
unambiguous
course
of
action
for
specified
individuals
or
groups
in
certain
situations
there
must
be
an
authority
or
leader
charged
with
the
implementation
and
monitoring
of
the
policy
with
a
sound
social
theory
underlying
the
program
and
the
target
group
evaluations
can
help
estimate
what
effects
will
be
produced
by
program
objectives/alternatives
however
claims
of
causality
can
only
be
made
with
randomized
control
trials
in
which
the
policy
change
is
applied
to
one
group
and
not
applied
to
a
control
group
and
individuals
are
randomly
assigned
to
these
groups
to
obtain
compliance
of
the
actors
involved
the
government
can
resort
to
positive
sanctions
such
as
favorable
publicity
price
supports
tax
credits
grants-in-aid
direct
services
or
benefits;
declarations;
rewards;
voluntary
standards;
mediation;
education;
demonstration
programs;
training
contracts;
subsidies;
loans;
general
expenditures;
informal
procedures
bargaining;
franchises;
sole-source
provider
awardsetc
policy
evaluation
is
used
to
examine
content
implementation
or
impact
of
the
policy
which
helps
to
understand
the
merit
worth
and
the
utility
of
the
policy
following
are
national
collaborating
centre
for
healthy
public
policy's
(ncchpp)
10
steps:
end
use
energy
demand
centres
the
end
use
energy
demand
(eued)
centres
carry
out
interdisciplinary
research
and
advise
policy
on
reducing
energy
demand
to
help
achieve
the
uk
government's
co2
emissions
targets
the
centres
are
a
£30m
investment
of
the
research
councils
uk
energy
programme
(with
additional
funding
from
industrial
partners)
that
run
from
2013-2018
the
six
large
centres
are
based
across
25
institutions
and
encompass
over
200
researchers
the
centres
are:
centre
for
energy
epidemiology
(cee)
-
based
at
university
college
london
and
directed
by
professor
tadj
oreszczyn
centre
on
innovation
and
energy
demand
(cied)
-
based
at
the
university
of
sussex
and
directed
by
professor
benjamin
sovacool
centre
for
industrial
energy
materials
and
products
(cie-map)
-
based
at
the
university
of
leeds
and
directed
by
professor
john
barrett
centre
for
sustainable
energy
use
in
food
chains
(csef)
-
based
at
brunel
university
and
led
by
professor
savvas
tassou
dynamics
of
energy
mobility
and
demand
(demand)
-
based
at
the
university
of
lancaster
and
led
by
professor
elizabeth
shove
interdisciplinary
centre
for
storage
transformation
and
upgrading
of
thermal
energy
(i-stute)
-
based
at
the
university
of
warwick
and
directed
by
professor
robert
critoph
the
uk
is
committed
to
achieving
large-scale
reductions
(80%)
in
greenhouse
gas
emissions
by
2050
(climate
change
act
(1990))
end
use
energy
demand
(eued)
research
is
about
reducing
energy
use
on
the
scale
needed
to
reach
this
target
whilst
maintaining
current
or
achieving
better
standards
of
living
and
economic
growth
the
eued
centres
were
funded
in
2013
following
a
2011
call
for
proposals
from
the
research
councils
uk
energy
programme
to
address
these
issues
centralisation
centralisation
(british)
or
centralization
(both
british
and
american)
is
the
process
by
which
the
activities
of
an
organization
particularly
those
regarding
planning
and
decision-making
become
concentrated
within
a
particular
geographical
location
group
this
moves
the
important
decision-making
and
planning
powers
within
the
center
of
the
organisation
the
term
has
a
variety
of
meanings
in
several
fields
in
political
science
centralisation
refers
to
the
concentration
of
a
government's
power—both
geographically
and
politically—into
a
centralized
government
"centralisation
of
authority"
is
defined
as
the
systematic
and
consistent
concentration
of
authority
at
a
central
point
or
in
a
person
within
the
organization
this
idea
was
first
introduced
in
the
qin
dynasty
of
china
the
qin
government
was
highly
bureaucratic
and
was
administered
by
a
hierarchy
of
officials
all
serving
the
first
emperor
qin
shi
huang
the
qin
dynasty
practised
all
the
things
that
han
feizi
taught
allowing
qin
shi
huang
to
own
and
control
all
his
territories
including
those
conquered
from
other
countries
zheng
and
his
advisers
ended
feudalism
in
china
by
setting
up
new
laws
and
regulations
under
a
centralized
and
bureaucratic
government
with
a
rigid
centralization
of
authority
under
this
system
both
the
military
and
government
thrived
this
was
because
talented
individuals
were
more
easily
identified
and
picked
out
to
be
trained
for
specialized
functions
the
acts
for
the
implementation
are
needed
after
delegation
therefore
the
authority
for
taking
the
decisions
can
be
spread
with
the
help
of
the
delegation
of
the
authority
the
centralisation
of
authority
can
be
done
immediately
if
complete
concentration
is
given
at
the
decision-making
stage
for
any
position
the
centralisation
can
be
done
with
a
position
or
at
a
level
in
an
organisation
ideally
the
decision-making
power
is
held
by
a
few
individuals
centralisation
of
authority
has
several
advantages
and
disadvantages
the
benefits
include:
disadvantages
on
the
other
hand
are
as
follows:
as
written
in
vi
lenin’s
book
"imperialism
the
highest
stage
of
capitalism"
"the
remarkably
rapid
concentration
of
production
in
ever-larger
enterprises
are
one
of
the
most
characteristic
features
of
capitalism"
he
researched
the
development
of
production
and
decided
to
develop
the
concept
of
production
as
a
centralised
framework
from
individual
and
scattered
small
workshops
into
large
factories
leading
the
capitalism
to
the
world
this
is
guided
by
the
idea
that
once
concentration
of
production
develops
into
a
particular
level
it
will
become
a
monopoly
like
party
organisations
of
cartel
syndicate
and
trust
most
businesses
deal
with
issues
relating
to
the
specifics
of
centralization
or
decentralization
of
decision-making
the
key
question
is
either
whether
the
authority
should
manage
all
the
things
at
the
centre
of
a
business
(centralised)
or
whether
it
should
be
delegated
far
away
from
the
centre
(decentralised)
the
choice
between
centralised
or
decentralised
varies
many
large
businesses
necessarily
involve
some
extent
of
decentralisation
and
some
extent
of
centralisation
when
it
begins
to
operate
from
several
places
or
any
new
units
and
markets
added
protected
area
downgrading
downsizing
and
degazettement
protected
area
downgrading
downsizing
and
degazettement
(paddd)
are
processes
that
change
the
legal
status
of
national
parks
and
other
protected
areas
"downgrading"
is
"a
decrease
in
legal
restrictions
on
the
number
magnitude
or
extent
of
human
activities
within
a
protected
area
(ie
legal
authorization
for
increased
human
use)"
"downsizing"
refers
to
a
"decrease
in
size
of
a
protected
area
as
a
result
of
excision
of
land
or
sea
area
through
a
legal
boundary
change"
"degazettement"
is
defined
as
a
loss
of
legal
protection
for
an
entire
national
park
or
other
protected
area
collectively
paddd
represents
legal
processes
that
temper
regulations
shrink
boundaries
or
eliminate
all
legal
protections
originally
associated
with
establishment
of
a
protected
area
paddd
is
a
phenomenon
that
has
recently
gained
attention
among
scientists
and
policymakers
scientific
publications
have
identified
more
than
600
enacted
paddd
events
in
57
countries
encompassing
a
total
of
more
than
550000
km2
of
protected
lands
paddd
was
a
topic
of
discussion
at
the
world
parks
congress
in
sydney
australia
in
november
2014
scientists
have
suggested
that
the
global
paddd
trend
could
be
combatted
via
a
systematic
programme
of
protected
area
"upgrading"
whereby
conserved
wild
areas
are
expanded
via
the
purchase
or
gazetting
of
surrounding
territory
successful
examples
of
protected-area
upgrading
include
gorongosa
national
park
in
mozambique
and
the
guanacaste
conservation
area
in
costa
rica
policy
learning
policy
learning
is
the
increased
understanding
that
occurs
when
policymakers
compare
one
set
of
policy
problems
to
others
within
their
own
or
in
other
jurisdictions
it
can
aid
in
understanding
why
a
policy
was
implemented
the
policy's
effects
and
how
the
policy
could
apply
to
the
policymakers'
jurisdiction
before
a
policy
is
adopted
it
goes
through
a
process
that
involves
various
combinations
of
elected
official(s)
political
parties
civil
servants
advocacy
groups
policy
experts
or
consultants
corporations
think
tanks
and
multiple
levels
of
government
policies
can
be
challenged
in
various
ways
including
questioning
its
legality
ideally
policymakers
develop
complete
knowledge
about
the
policy;
the
policy
should
achieve
its
intent
and
efficiently
use
resources
policy
learning
through
globalization
has
helped
government
organizations
become
more
competitive
policymakers
have
easy
access
to
global
policy
knowledge
through
the
internet
access
to
think
tanks
international
institutions
such
as
the
united
nations
international
monetary
fund
(imf)
or
the
world
bank
and
individual
experts
in
the
1960s
academics
started
to
study
how
policymakers
learn
about
policies
during
that
time
countries
were
experiencing
social
political
economic
and
technological
change
researchers
discovered
that
governments
in
different
countries
faced
similar
problems
in
policies
and
programs
amidst
uncertainty
on
how
to
handle
problems
in
financing
its
welfare
programs
policymakers
start
to
learn
about
policy
through
facts
first-hand
experiences
or
from
the
experiences
of
others
policy
instruments
and
policy
implementation
are
the
steps
to
policy
learning
policymakers
review
policy
objectives
tools
and
implementation
strategies
when
implementations
fail
reviews
look
for
the
cause(s)
adjustments
in
objectives
tools
and
implementation
are
considered
instrumental
policy
learning
is
the
acquisition
of
knowledge
about
the
effectiveness
of
various
policy
instruments
and
implementatiions
policymakers
must
make
choices
about
the
appropriate
policy
intervention
tool(s)
to
use
the
intent
is
to
discover
the
most
effective
tool(s)
that
consume
the
least
resources
policymakers
can
employ
seven
major
policy
instrument
types
after
choosing
a
policy
instrument
the
details
for
employing
the
instrument
must
be
arranged
implementation
carries
risks
of
failing
in
various
ways
such
as
ineffectiveness
unacceptable
delays
and
excessive
costs
practices
that
improve
success
rates
include
setting
reasonable
expectations
allowing
adequate
time
and
sufficient
resources
having
clear
communication
and
u
nderstanding
policy
objectives
minimizing
the
number
of
approvals
simplifying
management
structures
and
aligning
all
relevant
groups
around
the
implementation
along
with
mechanisms
to
adapt
the
implementation
in
accord
with
subsequent
experience
to
correct
problems
and
take
advantage
of
new
opportunities
the
top-down
approach
involves
allowing
high-level
policymakers
set
objectives
and
define
implementation
strategies
lower
level
implementers
carry
out
the
policy
objectives
must
be
clearly
defined
and
the
implementation
tools
must
be
selected
based
on
the
implementation
strategy
policy
designers
need
to
assess
the
commitment
of
policy
implementers
who
could
be
teachers
police
officers
social
workers
or
private
sector
workers
one
example
of
the
top-down
approach
was
in
1973
when
the
us
congress
passed
a
policy
limiting
the
driving
speed
to
55 mph
on
america’s
freeways
under
the
national
maximum
speed
law
the
policy
objective
was
to
reduce
gasoline
consumption
in
addition
to
increased
travel
times
a
side
effect
was
the
reduction
of
freeway
fatalities
the
bottom-up
approach
helps
policymakers
to
evaluate
whether
policy
goals
are
open
to
more
than
one
interpretation
does
the
policy
implement
a
statute
or
reflect
rules
practices
and/or
norms
such
as
energy
policy
or
criminal
procedure?
are
the
policy
goals
internally
consistent?
how
will
the
policy
affect
the
activities
of
workers
who
directly
provide
services?
bottom-up
approaches
require
policymakers
to
involve
both
service
providers
and
service
recipients
in
refining
goals
strategies
and
activities
this
bottom-up
approach
starts
from
consumer-facing
bureaucrats
and
moves
up
to
the
top
policymakers
should
the
policy
face
pushback
policymakers
must
be
open
to
negotiations
for
a
compromise
approach
the
bottom-up
approach
emphasizes
low
level
policy
implementers
but
policy
learners
must
not
attempt
to
frustrate
the
goals
of
top
policymakers
in
america
the
no
child
left
behind
act
(nclb)
adopted
policyies
that
would
have
benefited
from
bottom-up
perspectives
when
nclb
was
passed
many
states
struggled
to
figure
out
what
was
required
all
states
had
to
get
their
education
plans
approved
by
us
department
of
education
once
the
education
plan
was
approved
each
state
had
to
incorporate
nclb
into
the
state's
framework
of
educational
governance
and
to
use
the
legislation
to
achieve
the
state's
own
goals
if
the
us
federal
government
had
consulted
with
each
state
about
its
education
policies
performances
and
future
goals
teachers
and
the
government
would
have
had
a
better
understanding
on
what
policy
objectives
were
achievable
policy
learning
has
not
been
embraced
in
some
countries
some
countries
that
were
once
colonized
fear
that
embracing
policies
recommended
by
outsiders
will
allow
other
countries
to
exploit
their
resources
rockefeller
(1966)
claimed
that
in
latin
america
in
the
early
1960s
free-market
policies
were
in
competition
with
communist
propaganda
in
latin
american
countries
at
the
time
american
business
were
claimed
to
be
exploiting
the
people
and
their
resources
however
companies
such
as
chase
manhattan
bank
launched
a
program
in
panama
to
improve
cattle
raising
by
training
ranchersr
to
follow
the
scientific
advances
of
seeding
feeding
and
breeding
cattle
more
effectively
this
process
improved
the
quality
of
beef
which
encouraged
higher
meat
consumption
improved
dietary
standards
and
made
panama
a
beef
exporter
european
countries
created
the
euro
to
simplify
trading
between
european
union
countries
adopting
the
euro
would
remove
currency
risk
and
the
cost
of
currency
conversion
and
provide
a
common
monetary
policy
among
members
policy
learning
took
places
as
more
european
countries
learned
that
joining
the
eurozone
would
give
them
access
to
other
markets
citizens
of
eu
member
countries
could
travel
to
other
eu
countries
within
the
schengen
area
without
transiting
a
border
checkpoint
the
learning
did
not
reach
all
policy
sectors
some
eu
countries
kept
their
budgets
in
near
balance
amid
strong
growth
and
employment
while
others'
budgets
were
so
far
out
of
balance
that
their
overall
debt
created
fears
about
their
ability
to
make
their
payments
index
of
articles
related
to
terms
of
service
and
privacy
policies
this
is
a
list
of
articles
about
terms
of
service
and
privacy
policies
these
are
also
called
terms
of
use
and
are
rules
one
must
agree
to
in
order
to
use
a
service
the
articles
fall
in
two
main
categories:
descriptions
of
terms
used
for
specific
companies
or
products
and
descriptions
of
different
kinds
of
terms
in
general
articles
on
companies
vary
widely
in
the
amount
of
detail
they
give
on
terms
of
service
annotations
show
what
is
available
in
the
article
on
each
company
and
need
to
be
updated
as
those
articles
are
improved
terms
of
service
are
regularly
the
subject
of
news
articles
throughout
the
english-language
press
such
as
in
the
us
uk
africa
india
singapore
and
australia
terms
of
service
are
also
addressed
in
a
widely
reviewed
documentary
academic
research
and
legal
research
imperative
mandate
the
imperative
mandate
is
a
political
system
in
which
"representatives
enact
policies
in
accordance
with
mandates
and
can
be
recalled
by
people’s
assemblies"
it
requires
a
context
in
which
"power
is
not
monopolized
by
the
state
but
distributed
in
a
plurality
of
municipalities
and
assemblies
with
specific
political
authority"
the
imperative
mandate
goes
back
to
the
middle
ages
it
was
embraced
by
the
revolutionary
assemblies
in
paris
in
1793
but
then
banned
by
the
royalist
members
of
the
french
national
assembly
of
1789
to
block
greater
influence
by
the
people
it
was
also
rejected
in
the
american
revolution
it
was
embraced
in
the
paris
commune
and
by
the
council
communism
movement
the
imperative
mandate
has
been
used
by
the
united
democratic
front
and
abahlali
basemjondolo
in
south
africa
as
well
as
the
zapatistas
in
mexico
wicked
problem
a
wicked
problem
is
a
problem
that
is
difficult
or
impossible
to
solve
because
of
incomplete
contradictory
and
changing
requirements
that
are
often
difficult
to
recognize
the
use
of
the
term
"wicked"
here
has
come
to
denote
resistance
to
resolution
rather
than
evil
another
definition
is
"a
problem
whose
social
complexity
means
that
it
has
no
determinable
stopping
point"
moreover
because
of
complex
interdependencies
the
effort
to
solve
one
aspect
of
a
wicked
problem
may
reveal
or
create
other
problems
the
phrase
was
originally
used
in
social
planning
its
modern
sense
was
introduced
in
1967
by
c
west
churchman
in
a
guest
editorial
churchman
wrote
in
the
journal
"management
science"
responding
to
a
previous
use
of
the
term
by
horst
rittel
churchman
discussed
the
moral
responsibility
of
operations
research
"to
inform
the
manager
in
what
respect
our
'solutions'
have
failed
to
tame
his
wicked
problems"
rittel
and
melvin
m
webber
formally
described
the
concept
of
wicked
problems
in
a
1973
treatise
contrasting
"wicked"
problems
with
relatively
"tame"
soluble
problems
in
mathematics
chess
or
puzzle
solving
rittel
and
webber's
1973
formulation
of
wicked
problems
in
social
policy
planning
specified
ten
characteristics:
conklin
later
generalized
the
concept
of
problem
wickedness
to
areas
other
than
planning
and
policy;
conklin's
defining
characteristics
are:
classic
examples
of
wicked
problems
include
economic
environmental
and
political
issues
a
problem
whose
solution
requires
a
great
number
of
people
to
change
their
mindsets
and
behavior
is
likely
to
be
a
wicked
problem
therefore
many
standard
examples
of
wicked
problems
come
from
the
areas
of
public
planning
and
policy
these
include
global
climate
change
natural
hazards
healthcare
the
aids
epidemic
pandemic
influenza
international
drug
trafficking
nuclear
weapons
nuclear
energy
waste
and
social
injustice
in
recent
years
problems
in
many
areas
have
been
identified
as
exhibiting
elements
of
wickedness;
examples
range
from
aspects
of
design
decision
making
and
knowledge
management
to
business
strategy
rittel
and
webber
coined
the
term
in
the
context
of
problems
of
social
policy
an
arena
in
which
a
purely
scientific-engineering
approach
cannot
be
applied
because
of
the
lack
of
a
clear
problem
definition
and
differing
perspectives
of
stakeholders
in
their
words
thus
wicked
problems
are
also
characterised
by
the
following:
although
rittel
and
webber
framed
the
concept
in
terms
of
social
policy
and
planning
wicked
problems
occur
in
any
domain
involving
stakeholders
with
differing
perspectives
recognising
this
rittel
and
kunz
developed
a
technique
called
issue-based
information
system
(ibis)
which
facilitates
documentation
of
the
rationale
behind
a
group
decision
in
an
objective
manner
a
recurring
theme
in
research
and
industry
literature
is
the
connection
between
wicked
problems
and
design
design
problems
are
typically
wicked
because
they
are
often
ill
defined
(no
prescribed
way
forward)
involve
stakeholders
with
different
perspectives
and
have
no
"right"
or
"optimal"
solution
thus
wicked
problems
cannot
be
solved
by
the
application
of
standard
(or
known)
methods;
they
demand
creative
solutions
wicked
problems
cannot
be
tackled
by
the
traditional
approach
in
which
problems
are
defined
analysed
and
solved
in
sequential
steps
the
main
reason
for
this
is
that
there
is
no
clear
problem
definition
of
wicked
problems
in
a
paper
published
in
2000
nancy
roberts
identified
the
following
strategies
to
cope
with
wicked
problems:
in
his
1972
paper
rittel
hints
at
a
collaborative
approach;
one
which
attempts
"to
make
those
people
who
are
being
affected
into
participants
of
the
planning
process
they
are
not
merely
asked
but
actively
involved
in
the
planning
process"
a
disadvantage
of
this
approach
is
that
achieving
a
shared
understanding
and
commitment
to
solving
a
wicked
problem
is
a
time-consuming
process
another
difficulty
is
that
in
some
matters
at
least
one
group
of
people
may
hold
an
absolute
belief
that
necessarily
contradicts
other
absolute
beliefs
held
by
other
groups
collaboration
then
becomes
impossible
until
one
set
of
beliefs
is
relativized
or
abandoned
entirely
research
over
the
last
two
decades
has
shown
the
value
of
computer-assisted
argumentation
techniques
in
improving
the
effectiveness
of
cross-stakeholder
communication
the
technique
of
dialogue
mapping
has
been
used
in
tackling
wicked
problems
in
organizations
using
a
collaborative
approach
more
recently
in
a
four-year
study
of
interorganizational
collaboration
across
public
private
and
voluntary
sectors
steering
by
government
was
found
to
perversely
undermine
a
successful
collaboration
producing
an
organizational
crisis
which
led
to
the
collapse
of
a
national
initiative
in
"wholesome
design
for
wicked
problems"
robert
knapp
stated
that
there
are
ways
forward
in
dealing
with
wicked
problems:
examining
networks
designed
to
tackle
wicked
problems
in
health
care
such
as
caring
for
older
people
or
reducing
sexually
transmitted
infections
ferlie
and
colleagues
suggest
that
managed
networks
may
be
the
"least
bad"
way
of
"making
wicked
problems
governable"
a
range
of
approaches
called
"problem
structuring
methods"
(psms)
have
been
developed
in
operations
research
since
the
1970s
to
address
problems
involving
complexity
uncertainty
and
conflict
psms
are
usually
used
by
a
group
of
people
in
collaboration
(rather
than
by
a
solitary
individual)
to
create
a
consensus
about
or
at
least
to
facilitate
negotiations
about
what
needs
to
change
some
widely
adopted
psms
include
soft
systems
methodology
the
strategic
choice
approach
and
strategic
options
development
and
analysis
(soda)
russell
l
ackoff
wrote
about
complex
problems
as
messes:
"every
problem
interacts
with
other
problems
and
is
therefore
part
of
a
set
of
interrelated
problems
a
system
of
problems
i
choose
to
call
such
a
system
a
mess"
extending
ackoff
robert
horn
says
that
"a
social
mess
is
a
set
of
interrelated
problems
and
other
messes
complexity—systems
of
systems—is
among
the
factors
that
makes
social
messes
so
resistant
to
analysis
and
more
importantly
to
resolution"
according
to
horn
the
defining
characteristics
of
a
social
mess
are:
e
f
schumacher
distinguishes
between
"divergent
and
convergent
problems"
in
his
book
"a
guide
for
the
perplexed"
convergent
problems
are
those
for
which
attempted
solutions
gradually
converge
on
one
solution
or
answer
divergent
problems
are
those
for
which
different
answers
appear
to
increasingly
contradict
each
other
all
the
more
they
are
elaborated
requiring
a
different
approach
involving
faculties
of
a
higher
order
like
love
and
empathy
in
1990
degrace
and
stahl
introduced
the
concept
of
wicked
problems
to
software
development
in
the
last
decade
other
computer
scientists
have
pointed
out
that
software
development
shares
many
properties
with
other
design
practices
(particularly
that
people-
process-
and
technology-problems
have
to
be
considered
equally)
and
have
incorporated
rittel's
concepts
into
their
software
design
methodologies
the
design
and
integration
of
complex
software-defined
services
that
use
the
web
(web
services)
can
be
construed
as
an
evolution
from
previous
models
of
software
design
and
therefore
becomes
a
wicked
problem
also
kelly
levin
benjamin
cashore
graeme
auld
and
steven
bernstein
introduced
the
distinction
between
"wicked
problems"
and
"super
wicked
problems"
in
a
2007
conference
paper
which
was
followed
by
a
2012
journal
article
in
"policy
sciences"
in
their
discussion
of
global
climate
change
they
define
super
wicked
problems
as
having
the
following
additional
characteristics:
while
the
items
that
define
a
wicked
problem
relate
to
the
problem
itself
the
items
that
define
a
super
wicked
problem
relate
to
the
agent
trying
to
solve
it
global
warming
is
a
super
wicked
problem
and
the
need
to
intervene
to
tend
to
our
longer
term
interests
has
also
been
taken
up
by
others
including
richard
lazarus
sexuality
policy
watch
sexuality
policy
watch
(spw)
is
a
global
forum
of
researchers
and
activists
working
on
sexual
rights
issues
and
policies
across
the
world
the
forum
was
launched
in
2002
as
the
international
working
group
on
sexuality
and
social
policy
(iwgssp)
but
changed
its
name
to
sexuality
policy
watch
in
2006
since
its
inception
spw
has
conducted
research
on
trends
in
sexuality
advocated
to
prevent
violence
against
women
built
partnerships
with
sexual
rights
groups
and
published
key
policy
analyses
thus
together
with
the
latin
american
committee
for
the
rights
of
women
/brazil
(cladem)
the
commission
for
citizenship
and
reproduction
(ccr)
promsex
-
center
for
the
promotion
and
defense
of
sexual
and
reproductive
rights
and
the
national
rapporteurship
for
the
human
right
to
sexual
and
reproductive
health
in
brazil
spw
published
a
report
investigating
press
claims
in
2012
about
the
earlier
forced
sterilisation
campaigns
in
peru
spw
is
hosted
at
the
brazilian
interdisciplinary
aids
association
or
associação
brasileira
interdisciplinar
de
aids
(abia)
in
rio
de
janeiro
brazil
the
spw
co-chairs
are
sonia
corrêa
from
brazil
and
richard
parker
from
usa
policy
a
policy
is
a
deliberate
system
of
principles
to
guide
decisions
and
achieve
rational
outcomes
a
policy
is
a
statement
of
intent
and
is
implemented
as
a
procedure
or
protocol
policies
are
generally
adopted
by
a
governance
body
within
an
organization
policies
can
assist
in
both
"subjective"
and
"objective"
decision
making
policies
to
assist
in
subjective
decision
making
usually
assist
senior
management
with
decisions
that
must
be
based
on
the
relative
merits
of
a
number
of
factors
and
as
a
result
are
often
hard
to
test
objectively
eg
work-life
balance
policy
in
contrast
policies
to
assist
in
objective
decision
making
are
usually
operational
in
nature
and
can
be
objectively
tested
eg
password
policy
the
term
may
apply
to
government
private
sector
organizations
and
groups
as
well
as
individuals
presidential
executive
orders
corporate
privacy
policies
and
parliamentary
rules
of
order
are
all
examples
of
policy
policy
differs
from
rules
or
law
while
law
can
compel
or
prohibit
behaviors
(eg
a
law
requiring
the
payment
of
taxes
on
income)
policy
merely
guides
actions
toward
those
that
are
most
likely
to
achieve
a
desired
outcome
policy
or
policy
study
may
also
refer
to
the
process
of
making
important
organizational
decisions
including
the
identification
of
different
alternatives
such
as
programs
or
spending
priorities
and
choosing
among
them
on
the
basis
of
the
impact
they
will
have
policies
can
be
understood
as
political
managerial
financial
and
administrative
mechanisms
arranged
to
reach
explicit
goals
in
public
corporate
finance
a
critical
accounting
policy
is
a
policy
for
a
firm/company
or
an
industry
that
is
considered
to
have
a
notably
high
subjective
element
and
that
has
a
material
impact
on
the
financial
statements
the
intended
effects
of
a
policy
vary
widely
according
to
the
organization
and
the
context
in
which
they
are
made
broadly
policies
are
typically
instituted
to
avoid
some
negative
effect
that
has
been
noticed
in
the
organization
or
to
seek
some
positive
benefit
corporate
purchasing
policies
provide
an
example
of
how
organizations
attempt
to
avoid
negative
effects
many
large
companies
have
policies
that
all
purchases
above
a
certain
value
must
be
performed
through
a
purchasing
process
by
requiring
this
standard
purchasing
process
through
policy
the
organization
can
limit
waste
and
standardize
the
way
purchasing
is
done
the
state
of
california
provides
an
example
of
benefit-seeking
policy
in
recent
years
the
numbers
of
hybrid
cars
in
california
has
increased
dramatically
in
part
because
of
policy
changes
in
federal
law
that
provided
usd
$1500
in
tax
credits
(since
phased
out)
as
well
as
the
use
of
high-occupancy
vehicle
lanes
to
hybrid
owners
(no
loew
hybrid
vehicles)
in
this
case
the
organization
(state
and/or
federal
government)
created
an
effect
(increased
ownership
and
use
of
hybrid
vehicles)
through
policy
(tax
breaks
highway
lanes)
policies
frequently
have
side
effects
or
unintended
consequences
because
the
environments
that
policies
seek
to
influence
or
manipulate
are
typically
complex
adaptive
systems
(eg
governments
societies
large
companies)
making
a
policy
change
can
have
counterintuitive
results
for
example
a
government
may
make
a
policy
decision
to
raise
taxes
in
hopes
of
increasing
overall
tax
revenue
depending
on
the
size
of
the
tax
increase
this
may
have
the
overall
effect
of
reducing
tax
revenue
by
causing
capital
flight
or
by
creating
a
rate
so
high
that
citizens
are
deterred
from
earning
the
money
that
is
taxed
(see
the
laffer
curve)
the
policy
formulation
process
theoretically
includes
an
attempt
to
assess
as
many
areas
of
potential
policy
impact
as
possible
to
lessen
the
chances
that
a
given
policy
will
have
unexpected
or
unintended
consequences
in
political
science
the
policy
cycle
is
a
tool
used
for
the
analyzing
of
the
development
of
a
policy
item
it
can
also
be
referred
to
as
a
"stagist
approach"
"stages
heuristic"
or
"stages
approach"
it
is
thus
a
rule
of
thumb
rather
than
the
actual
reality
of
how
policy
is
created
but
has
been
influential
in
how
political
scientists
looked
at
policy
in
general
it
was
developed
as
a
theory
from
harold
lasswell's
work
one
version
by
james
e
anderson
in
his
"public
policy-making"
(1974)
has
the
following
stages:
an
eight
step
policy
cycle
is
developed
in
detail
in
"the
australian
policy
handbook"
by
peter
bridgman
and
glyn
davis:
(now
with
catherine
althaus
in
its
4th
and
5th
editions)
the
althaus
bridgman
davis
model
is
heuristic
and
iterative
it
is
and
not
meant
to
be
or
predictive
policy
cycles
are
typically
characterized
as
adopting
a
classical
approach
and
tend
to
describe
processes
from
the
perspective
of
policy
decision
makers
accordingly
some
postpositivist
academics
challenge
cyclical
models
as
unresponsive
and
unrealistic
preferring
systemic
and
more
complex
models
they
consider
a
broader
range
of
actors
involved
in
the
policy
space
that
includes
civil
society
organisations
the
media
intellectuals
think
tanks
or
policy
research
institutes
corporations
lobbyists
etc
policies
are
typically
promulgated
through
official
written
documents
policy
documents
often
come
with
the
endorsement
or
signature
of
the
executive
powers
within
an
organization
to
legitimize
the
policy
and
demonstrate
that
it
is
considered
in
force
such
documents
often
have
standard
formats
that
are
particular
to
the
organization
issuing
the
policy
while
such
formats
differ
in
form
policy
documents
usually
contain
certain
standard
components
including
:
some
policies
may
contain
additional
sections
including:
the
american
political
scientist
theodore
j
lowi
proposed
four
types
of
policy
namely
distributive
redistributive
regulatory
and
constituent
in
his
article
'four
systems
of
policy
politics
and
choice'
and
in
'american
business
public
policy
case
studies
and
political
theory'
policy
addresses
the
intent
of
the
organization
whether
government
business
professional
or
voluntary
policy
is
intended
to
affect
the
'real'
world
by
guiding
the
decisions
that
are
made
whether
they
are
formally
written
or
not
most
organizations
have
identified
policies
policies
may
be
classified
in
many
different
ways
the
following
is
a
sample
of
several
different
types
of
policies
broken
down
by
their
effect
on
members
of
the
organization
distributive
policies
extend
goods
and
services
to
members
of
an
organization
as
well
as
distributing
the
costs
of
the
goods/services
amongst
the
members
of
the
organization
examples
include
government
policies
that
impact
spending
for
welfare
public
education
highways
and
public
safety
or
a
professional
organization's
benefits
plan
regulatory
policies
or
mandates
limit
the
discretion
of
individuals
and
agencies
or
otherwise
compel
certain
types
of
behavior
these
policies
are
generally
thought
to
be
best
applied
when
good
behavior
can
be
easily
defined
and
bad
behavior
can
be
easily
regulated
and
punished
through
fines
or
sanctions
an
example
of
a
fairly
successful
public
regulatory
policy
is
that
of
a
highway
speed
limit
constituent
policies
create
executive
power
entities
or
deal
with
laws
constituent
policies
also
deal
with
fiscal
policy
in
some
circumstances
policies
are
dynamic;
they
are
not
just
static
lists
of
goals
or
laws
policy
blueprints
have
to
be
implemented
often
with
unexpected
results
social
policies
are
what
happens
'on
the
ground'
when
they
are
implemented
as
well
as
what
happens
at
the
decision
making
or
legislative
stage
when
the
term
policy
is
used
it
may
also
refer
to:
the
actions
the
organization
actually
takes
may
often
vary
significantly
from
stated
policy
this
difference
is
sometimes
caused
by
political
compromise
over
policy
while
in
other
situations
it
is
caused
by
lack
of
policy
implementation
and
enforcement
implementing
policy
may
have
unexpected
results
stemming
from
a
policy
whose
reach
extends
further
than
the
problem
it
was
originally
crafted
to
address
additionally
unpredictable
results
may
arise
from
selective
or
idiosyncratic
enforcement
of
policy
types
of
policy
analysis
include:
these
qualifiers
can
be
combined
so
one
could
for
example
have
a
stationary-memoryless-index
policy
media
policy
media
policy
/
m
politics
is
a
term
describing
all
legislation
and
political
action
directed
towards
regulating
the
media
especially
mass
media
and
the
media
industry
those
actions
will
usually
be
prompted
by
pressures
from
public
opinion
or
from
industry
interest
groups
print
media
public
radio
and
television
broadcasting
mobile
communications
all
converge
in
the
digital
infrastructure
this
digitalisation
produces
markets
that
still
lack
consistent
and
rigorous
regulation
in
instances
where
regulations
exist
technical
innovations
outpace
and
overtake
existing
rules
and
give
rise
to
illegal
activities
like
copyright
violations
this
has
to
be
dealt
with
to
defend
intellectual
property
rights
(see
eg
digital
economy
act
2010)
media
politics
is
the
subject
of
studies
in
media
research
and
cultural
studies
liberal
media
policy
is
adversely
affected
by
the
fact
that
political
success
itself
hinges
critically
on
favorable
comments
in
the
media
see
politico-media
complex
reshaping
cultural
policies
reshaping
cultural
policies
(styled
as
re|shaping
cultural
policies)
is
a
report
series
published
by
unesco
which
monitors
the
implementation
of
the
unesco
convention
on
the
protection
and
promotion
of
the
diversity
of
cultural
expressions
(2005)
the
2005
unesco
convention
encourages
its
146
parties
to
introduce
policies
for
culture
within
a
global
context
and
commitment
to
protect
and
promote
the
diversity
of
cultural
expressions
the
second
and
most
recent
report
(2018)
subtitled
“advancing
creativity
for
development”
follows
the
first
report
(2015)
with
the
subtitle
“a
decade
promoting
the
diversity
of
cultural
expressions
for
development”
primarily
the
report
series
draws
on
reports
of
all
parties
to
the
convention
submitted
every
four
years
in
which
they
present
and
describe
the
actions
they
have
taken
in
order
to
implement
the
convention
these
reports
are
called
quadrennial
periodic
reports
(qprs)
in
addition
the
report
series
includes
the
analysis
of
other
both
governmental
and
non-governmental
sources
in
general
the
report
investigates
how
implementing
the
convention
reshapes
cultural
policies
additionally
it
provides
evidence
of
how
the
implementation
process
contributes
to
attaining
the
united
nations
2030
sustainable
development
goals
(sdgs)
to
end
poverty
protect
the
planet
and
ensure
prosperity
for
every
human
being
the
report
series
also
analyses
trends
and
issues
concerning
the
creative
economy
which
currently
is
worth
$2250
billion
and
employs
30
million
people
worldwide
the
report
puts
forward
a
set
of
policy
recommendations
for
the
future
addressing
the
adaptation
of
cultural
policies
to
rapid
change
in
the
digital
environment
based
on
human
rights
and
fundamental
freedoms
of
expression
“each
report
is
not
an
end-result
but
a
tool
to
be
used
in
a
long-term
process
that
includes
the
forging
of
spaces
for
policy
dialogue
reinforcing
stakeholders’
capacities
to
work
together
to
generate
data
and
information
and
advocate
for
policy
innovation
both
nationally
and
globally”
the
reports
are
published
in
english
french
spanish
russian
portuguese
arabic
chinese
indonesian
vietnamese
and
german
unesco
is
the
lead
institutional
author
of
the
global
report
series
and
coordinates
a
broader
network
of
independent
experts
who
author
chapters
in
line
with
the
parties’
quadrennial
periodic
reporting
the
series
is
produced
every
four
years
the
first
cycle
spanned
the
years
2012-2015
and
the
second
runs
from
2016
to
2019
accordingly
the
third
publication
will
take
place
in
december
2021
the
2005
convention
is
an
international
standard
setting
instrument
providing
a
framework
for
the
governance
of
culture
in
this
context
governance
of
culture
refers
to
policies
and
measures
governments
establish
to
regulate
to
promote
and
to
protect
all
forms
of
creativity
and
artistic
expressions
the
most
recent
unesco
convention
in
the
field
of
culture
and
ratified
by
146
parties
it
is
the
first
international
legal
tool
to
encourage
governments
to
invest
in
creativity
it
frames
the
formulation
and
implementation
of
different
types
of
legislative
regulatory
institutional
and
financial
interventions
to
promote
the
emergence
of
dynamic
cultural
and
creative
industry
sectors
around
the
world
within
the
context
of
the
2005
convention
the
diversity
of
cultural
expressions
"″"refers
to
the
manifold
ways
in
which
the
cultures
of
groups
and
societies
find
expression
these
expressions
are
passed
on
within
and
among
groups
and
societies"″"
specifically
the
convention
understands
cultural
expressions
as
all
forms
of
creativity
and
artistic
expressions
such
as
in
cinema/audiovisual
arts
design
digital
arts
music
performing
arts
publishing
and
the
visual
arts
the
2005
convention
was
"since
its
beginnings
permeated
by
a
material
and
economic
perspective
of
cultural
expressions
focused
on
the
production
and
consumption
of
cultural
goods
and
services
with
a
view
to
promote
more
balanced
exchanges
and
sustainable
development
that
takes
into
account
cultural
diversity
concerns"
the
implementation
of
the
2005
convention
aims
to
contribute
to
achieving
several
sustainable
development
goals
(sdgs)
precisely
sdg
4
(quality
education)
sdg
5
(gender
equality)
sdg
8
(decent
work
and
economic
growth)
sdg
10
(reduced
inequalities)
sdg
16
(peace
justice
and
strong
institutions)
and
sdg
17
(partnerships
for
the
goals)
the
implementation
process
identifies
investing
in
creativity
as
a
priority
for
sustainable
development
at
the
global
level
the
convention
calls
for
countries
to
provide
financial
assistance
for
creativity
through
their
official
development
assistance
(oda)
by
investing
in
the
convention’s
international
fund
for
cultural
diversity
additionally
unesco
through
the
2005
convention
offers
technical
assistance
to
strengthen
human
and
institutional
capacities
in
developing
countries
the
director
general
of
unesco
audrey
azoulay
referring
to
the
unesco
general
conference's
conviction
that
cultural
activities
goods
and
services
have
both
an
economic
and
a
cultural
nature
stated:
″[c]ulture
is
not
a
commodity:
it
carries
values
and
identities
it
gives
markers
to
live
together
in
a
globalized
world
our
role
is
to
encourage
question
collect
data
to
understand
and
energize
creative
channels
to
encourage
the
mobility
of
artists
to
stimulate
a
rapidly
changing
sector
in
the
new
digital
environment″
annika
markovic
ambassador
and
permanent
delegate
of
sweden
to
unesco
in
2018
claimed
that
the
report
is
"“the
only
global
document
that
presents
an
overview
of
cultural
development
world-wide
and
monitors
state
action
to
protect
and
promote
the
diversity
of
cultural
expressions
at
all
levels”"
the
following
aspects
thematically
summarize
the
core
findings
identified
by
the
2018
report
with
regard
to
the
implementation
of
the
2005
unesco
convention
for
the
first
time
national
development
plans
and
strategies
integrate
culture
mainly
of
countries
in
the
global
south
as
a
result
cities
seem
to
invest
more
and
more
in
cultural
industries
for
development
the
un’s
2030
agenda
recognized
the
role
of
creativity
in
sustainable
development
in
the
implementation
of
the
sdgs
however
the
share
of
development
aid
spent
on
culture
today
is
the
lowest
it
has
been
in
over
10
years
according
to
the
report
digital
revenues
make
up
50%
of
the
recorded
music
market
growing
almost
18%
over
the
past
year
due
to
a
sharp
increase
in
the
share
of
streaming
revenues
the
report
states
that
the
internet
transforms
the
cultural
value
chain
into
a
network
platform
e-commerce
challenges
both
culture
and
trade
policies
that
intend
to
promote
the
diversity
of
cultural
expressions
it
articulates
the
urgency
to
improve
data
collection
on
revenues
generated
through
digital
channels
in
order
to
design
better
policies
and
negotiate
fair
trade
agreements
the
report
claims
that
monitoring
the
relationship
between
large
platforms
big
data
artificial
intelligence
and
the
diversity
of
cultural
expressions
is
crucial
to
ensure
that
a
variety
of
distribution
platforms
and
providers
promote
and
protect
future
artistic
creations
as
informed
by
the
report
attacks
against
artists
have
increased
in
the
past
years
including
in
the
digital
environment
where
surveillance
and
online
trolling
pose
new
threats
to
artistic
freedom
in
2016
430
cases
were
reported
around
the
world
(compared
to
340
in
2015
and
90
in
2014)
musicians
are
the
most
threatened
group
while
authors
also
often
become
a
target
in
2016
attacks
against
authors
occurred
most
often
in
the
asia-pacific
region
(80
cases)
the
middle
east
and
north
africa
(51
cases)
and
europe
(47
cases)
the
report
reveals
that
meanwhile
there
exists
an
increased
awareness
with
regard
to
such
threats
leading
to
a
larger
number
of
initiatives
to
support
the
social
and
economic
rights
of
artists
particularly
in
african
countries
while
there
exists
legal
action
to
affirm
the
freedom
of
expression
for
artists
other
laws
addressing
terrorism
and
state
security
repress
artistic
expressions
the
report
states
that
half
of
the
persons
working
in
the
cultural
and
creative
industries
are
female
however
a
gender
gap
persists
worldwide
concerning
equal
pay
access
to
funding
and
prices
charged
for
creative
works
consequently
women
remain
under-represented
in
key
creative
roles
and
are
outnumbered
in
decision-making
positions
women
make
up
only
34%
of
ministers
for
culture
(compared
to
24%
in
2005)
and
only
31%
of
national
arts
program
directors
generally
women
are
represented
in
specific
cultural
fields
such
as
arts
education
and
training
(60%)
book
publishing
and
press
(54%)
audiovisual
and
interactive
media
(26%)
as
well
as
design
and
creative
services
(33%)
the
report
demonstrates
that
predominantly
restrictions
in
terms
of
mobility
represent
great
challenges
to
persons
pursuing
careers
in
the
cultural
and
creative
industries
specifically
to
those
from
the
global
south
it
reveals
that
a
holder
of
a
german
passport
can
travel
to
176
countries
without
a
visa
while
a
holder
of
an
afghan
passport
can
only
travel
to
24
countries
without
a
visa
as
a
matter
of
fact
artists
and
cultural
professionals
need
to
travel
to
perform
to
reach
new
audiences
or
to
attend
a
residency
or
to
engage
in
networking
the
report
exposes
that
travel
restrictions
including
difficulties
in
obtaining
visas
oftentimes
impedes
artists
from
the
global
south
to
participate
in
art
biennales
or
film
festivals
even
when
invited
to
receive
an
award
or
to
promote
their
works
as
stated
in
the
report
the
2005
convention
provides
legitimacy
for
the
formulation
of
cultural
policies
and
their
adaptation
to
changing
circumstances
and
needs
the
report
underscores
that
collaborative
governance
and
multi-stakeholder
policy
making
have
progressed
notably
in
some
developing
countries
particularly
in
the
creative
economy
and
cultural
education
as
a
result
parties
to
the
convention
have
made
considerable
progress
in
fostering
digital
arts
creation
supporting
creative
entrepreneurship
accelerating
the
modernization
of
cultural
sectors
promoting
distribution
and
updating
copyright
legislation
however
the
report
also
reveals
a
lack
in
civil
society
participation
in
policy
making
it
underlines
the
urgency
for
more
effort
to
ensure
the
creation
of
open
transparent
and
participatory
policy
processes
in
order
to
involve
civil
society
participation
in
policy
making
in
accordance
with
the
report
the
2005
convention
formally
recognizes
that
cultural
goods
and
services
not
only
have
important
economic
value
but
also
convey
identities
meanings
and
values
as
a
consequence
at
least
eight
bilateral
and
regional
free
trade
agreements
concluded
between
2015
and
2017
have
introduced
cultural
clauses
or
list
of
commitments
that
promote
the
objectives
and
principles
of
the
2005
convention
despite
the
lack
of
the
promotion
of
the
objectives
and
principles
of
the
2005
convention
with
regard
to
the
negotiation
of
mega-regional
partnership
agreements
some
parties
to
the
trans
pacific
partnership
(ttp)
have
succeeded
in
introducing
important
cultural
reservations
to
protect
and
promote
the
diversity
of
cultural
expressions
the
report’s
primary
objective
is
“to
provide
key
actors
with
better
knowledge
on
how
to
support
evidence-based
policy
and
to
strengthen
informed
transparent
and
participatory
systems
of
governance
for
culture”
it
aims
to
motivate
governments
and
civil
society
actors
to
integrate
findings
and
recommendations
into
their
national
cultural
policy
and
development
strategies
and
frameworks
following
the
findings
presented
above
the
implementation
of
the
2005
convention
"introduce[s]
a
range
of
different
policy
strategies
for
integrating
culture
into
development
processes"
and
culture
is
increasingly
regarded
as
"an
economic
asset
in
pursuing
sustainable
development"
based
on
its
analysis
and
findings
the
global
report
of
2018
suggests
the
following
road
map
for
the
parties
to
the
2005
convention
accordingly
parties
could
tackle
major
challenges
in
the
implementation
of
the
2005
convention
by:
speaking
about
the
visibility
of
the
progress
in
cultural
policies
shown
by
the
report
series
bárbara
lovrinić
stated
that
"“[u]nfortunately
where
unesco
is
concerned
there
is
a
lack
of
promotion
in
the
media
in
general
in
the
long
term
the
report
could
have
a
positive
impact
on
these
issues
which
would
be
enhanced
if
the
public
were
made
more
aware
of
such
work”"
she
also
points
out
that
there
is
""a
risk
that
many
people
will
not
dwell
on
the
2005
convention
and
the
sustainable
development
goals
unless
they
are
already
somewhat
familiar
with
the
topic”"
with
reference
to
the
title
of
the
report
series
she
concludes
that
""cultural
policy-making
is
still
far
from
being
reshaped
for
it
takes
a
serious
amount
of
time
to
yield
valuable
results”"
outline
of
education
the
following
outline
is
provided
as
an
overview
of
and
topical
guide
to
education:
education
–
in
the
general
sense
is
any
act
or
experience
that
has
a
formative
effect
on
the
mind
character
or
physical
ability
of
an
individual
in
its
technical
sense
education
is
the
process
by
which
society
deliberately
transmits
its
accumulated
knowledge
skills
and
values
from
one
generation
to
another
education
can
also
be
defined
as
the
process
of
becoming
an
educated
person
history
of
education
education
education
is
the
process
of
facilitating
learning
or
the
acquisition
of
knowledge
skills
values
beliefs
and
habits
educational
methods
include
storytelling
discussion
teaching
training
and
directed
research
education
frequently
takes
place
under
the
guidance
of
educators
and
also
learners
may
also
educate
themselves
education
can
take
place
in
formal
or
informal
settings
and
any
experience
that
has
a
formative
effect
on
the
way
one
thinks
feels
or
acts
may
be
considered
educational
the
methodology
of
teaching
is
called
pedagogy
formal
education
is
commonly
divided
formally
into
such
stages
as
preschool
or
kindergarten
primary
school
secondary
school
and
then
college
university
or
apprenticeship
a
right
to
education
has
been
recognized
by
some
governments
and
the
united
nations
in
most
regions
education
is
compulsory
up
to
a
certain
age
etymologically
the
word
"education"
is
derived
from
the
latin
word
"ēducātiō"
("a
breeding
a
bringing
up
a
rearing")
from
"ēducō"
("i
educate
i
train")
which
is
related
to
the
homonym
"ēdūcō"
("i
lead
forth
i
take
out;
i
raise
up
i
erect")
from
"ē-"
("from
out
of")
and
"dūcō"
("i
lead
i
conduct")
education
began
in
prehistory
as
adults
trained
the
young
in
the
knowledge
and
skills
deemed
necessary
in
their
society
in
pre-literate
societies
this
was
achieved
orally
and
through
imitation
story-telling
passed
knowledge
values
and
skills
from
one
generation
to
the
next
as
cultures
began
to
extend
their
knowledge
beyond
skills
that
could
be
readily
learned
through
imitation
formal
education
developed
schools
existed
in
egypt
at
the
time
of
the
middle
kingdom
plato
founded
the
academy
in
athens
the
first
institution
of
higher
learning
in
europe
the
city
of
alexandria
in
egypt
established
in
330
bce
became
the
successor
to
athens
as
the
intellectual
cradle
of
ancient
greece
there
the
great
library
of
alexandria
was
built
in
the
3rd
century
bce
european
civilizations
suffered
a
collapse
of
literacy
and
organization
following
the
fall
of
rome
in
ce
476
in
china
confucius
(551–479
bce)
of
the
state
of
lu
was
the
country's
most
influential
ancient
philosopher
whose
educational
outlook
continues
to
influence
the
societies
of
china
and
neighbours
like
korea
japan
and
vietnam
confucius
gathered
disciples
and
searched
in
vain
for
a
ruler
who
would
adopt
his
ideals
for
good
governance
but
his
analects
were
written
down
by
followers
and
have
continued
to
influence
education
in
east
asia
into
the
modern
era
the
aztecs
also
had
a
well-developed
theory
about
education
which
has
an
equivalent
word
in
nahuatl
called
"tlacahuapahualiztli"
it
means
"the
art
of
raising
or
educating
a
person"
or
"the
art
of
strengthening
or
bringing
up
men"
this
was
a
broad
conceptualization
of
education
which
prescribed
that
it
begins
at
home
supported
by
formal
schooling
and
reinforced
by
community
living
historians
cite
that
formal
education
was
mandatory
for
everyone
regardless
of
social
class
and
gender
there
was
also
the
word
"neixtlamachiliztli"
which
is
"the
act
of
giving
wisdom
to
the
face"
these
concepts
underscore
a
complex
set
of
educational
practices
which
was
oriented
towards
communicating
to
the
next
generation
the
experience
and
intellectual
heritage
of
the
past
for
the
purpose
of
individual
development
and
his
integration
into
the
community
after
the
fall
of
rome
the
catholic
church
became
the
sole
preserver
of
literate
scholarship
in
western
europe
the
church
established
cathedral
schools
in
the
early
middle
ages
as
centres
of
advanced
education
some
of
these
establishments
ultimately
evolved
into
medieval
universities
and
forebears
of
many
of
europe's
modern
universities
during
the
high
middle
ages
chartres
cathedral
operated
the
famous
and
influential
chartres
cathedral
school
the
medieval
universities
of
western
christendom
were
well-integrated
across
all
of
western
europe
encouraged
freedom
of
inquiry
and
produced
a
great
variety
of
fine
scholars
and
natural
philosophers
including
thomas
aquinas
of
the
university
of
naples
robert
grosseteste
of
the
university
of
oxford
an
early
expositor
of
a
systematic
method
of
scientific
experimentation
and
saint
albert
the
great
a
pioneer
of
biological
field
research
founded
in
1088
the
university
of
bologne
is
considered
the
first
and
the
oldest
continually
operating
university
elsewhere
during
the
middle
ages
islamic
science
and
mathematics
flourished
under
the
islamic
caliphate
which
was
established
across
the
middle
east
extending
from
the
iberian
peninsula
in
the
west
to
the
indus
in
the
east
and
to
the
almoravid
dynasty
and
mali
empire
in
the
south
the
renaissance
in
europe
ushered
in
a
new
age
of
scientific
and
intellectual
inquiry
and
appreciation
of
ancient
greek
and
roman
civilizations
around
1450
johannes
gutenberg
developed
a
printing
press
which
allowed
works
of
literature
to
spread
more
quickly
the
european
age
of
empires
saw
european
ideas
of
education
in
philosophy
religion
arts
and
sciences
spread
out
across
the
globe
missionaries
and
scholars
also
brought
back
new
ideas
from
other
civilizations –
as
with
the
jesuit
china
missions
who
played
a
significant
role
in
the
transmission
of
knowledge
science
and
culture
between
china
and
europe
translating
works
from
europe
like
euclid's
elements
for
chinese
scholars
and
the
thoughts
of
confucius
for
european
audiences
the
enlightenment
saw
the
emergence
of
a
more
secular
educational
outlook
in
europe
in
most
countries
today
full-time
education
whether
at
school
or
otherwise
is
compulsory
for
all
children
up
to
a
certain
age
due
to
this
the
proliferation
of
compulsory
education
combined
with
population
growth
unesco
has
calculated
that
in
the
next
30 years
more
people
will
receive
formal
education
than
in
all
of
human
history
thus
far
formal
education
occurs
in
a
structured
environment
whose
explicit
purpose
is
teaching
students
usually
formal
education
takes
place
in
a
school
environment
with
classrooms
of
multiple
students
learning
together
with
a
trained
certified
teacher
of
the
subject
most
school
systems
are
designed
around
a
set
of
values
or
ideals
that
govern
all
educational
choices
in
that
system
such
choices
include
curriculum
organizational
models
design
of
the
physical
learning
spaces
(eg
classrooms)
student-teacher
interactions
methods
of
assessment
class
size
educational
activities
and
more
preschools
provide
education
from
ages
approximately
three
to
seven
depending
on
the
country
when
children
enter
primary
education
these
are
also
known
as
nursery
schools
and
as
kindergarten
except
in
the
us
where
kindergarten
is
a
term
often
used
to
describe
the
earliest
levels
of
primary
education
kindergarten
"provide[s]
a
child-centred
preschool
curriculum
for
three-
to
seven-year-old
children
that
aim[s]
at
unfolding
the
child's
physical
intellectual
and
moral
nature
with
balanced
emphasis
on
each
of
them"
primary
(or
elementary)
education
consists
of
the
first
five
to
seven
years
of
formal
structured
education
in
general
primary
education
consists
of
six
to
eight
years
of
schooling
starting
at
the
age
of
five
or
six
although
this
varies
between
and
sometimes
within
countries
globally
around
89%
of
children
aged
six
to
twelve
are
enrolled
in
primary
education
and
this
proportion
is
rising
under
the
education
for
all
programs
driven
by
unesco
most
countries
have
committed
to
achieving
universal
enrollment
in
primary
education
by
2015
and
in
many
countries
it
is
compulsory
the
division
between
primary
and
secondary
education
is
somewhat
arbitrary
but
it
generally
occurs
at
about
eleven
or
twelve
years
of
age
some
education
systems
have
separate
middle
schools
with
the
transition
to
the
final
stage
of
secondary
education
taking
place
at
around
the
age
of
fourteen
schools
that
provide
primary
education
are
mostly
referred
to
as
"primary
schools
"or
"elementary
schools"
primary
schools
are
often
subdivided
into
infant
schools
and
junior
school
in
india
for
example
compulsory
education
spans
over
twelve
years
with
eight
years
of
elementary
education
five
years
of
primary
schooling
and
three
years
of
upper
primary
schooling
various
states
in
the
republic
of
india
provide
12
years
of
compulsory
school
education
based
on
a
national
curriculum
framework
designed
by
the
national
council
of
educational
research
and
training
in
most
contemporary
educational
systems
of
the
world
secondary
education
comprises
the
formal
education
that
occurs
during
adolescence
it
is
characterized
by
transition
from
the
typically
compulsory
comprehensive
primary
education
for
minors
to
the
optional
selective
tertiary
"postsecondary"
or
"higher"
education
(eg
university
vocational
school)
for
adults
depending
on
the
system
schools
for
this
period
or
a
part
of
it
may
be
called
secondary
or
high
schools
gymnasiums
lyceums
middle
schools
colleges
or
vocational
schools
the
exact
meaning
of
any
of
these
terms
varies
from
one
system
to
another
the
exact
boundary
between
primary
and
secondary
education
also
varies
from
country
to
country
and
even
within
them
but
is
generally
around
the
seventh
to
the
tenth
year
of
schooling
secondary
education
occurs
mainly
during
the
teenage
years
in
the
united
states
canada
and
australia
primary
and
secondary
education
together
are
sometimes
referred
to
as
k-12
education
and
in
new
zealand
year
1–13
is
used
the
purpose
of
secondary
education
can
be
to
give
common
knowledge
to
prepare
for
higher
education
or
to
train
directly
in
a
profession
secondary
education
in
the
united
states
did
not
emerge
until
1910
with
the
rise
of
large
corporations
and
advancing
technology
in
factories
which
required
skilled
workers
in
order
to
meet
this
new
job
demand
high
schools
were
created
with
a
curriculum
focused
on
practical
job
skills
that
would
better
prepare
students
for
white
collar
or
skilled
blue
collar
work
this
proved
beneficial
for
both
employers
and
employees
since
the
improved
human
capital
lowered
costs
for
the
employer
while
skilled
employees
received
higher
wages
secondary
education
has
a
longer
history
in
europe
where
grammar
schools
or
academies
date
from
as
early
as
the
16th
century
in
the
form
of
public
schools
fee-paying
schools
or
charitable
educational
foundations
which
themselves
date
even
further
back
community
colleges
offer
another
option
at
this
transitional
stage
of
education
they
provide
nonresidential
junior
college
courses
to
people
living
in
a
particular
area
higher
education
also
called
tertiary
third
stage
or
postsecondary
education
is
the
non-compulsory
educational
level
that
follows
the
completion
of
a
school
such
as
a
high
school
or
secondary
school
tertiary
education
is
normally
taken
to
include
undergraduate
and
postgraduate
education
as
well
as
vocational
education
and
training
colleges
and
universities
mainly
provide
tertiary
education
collectively
these
are
sometimes
known
as
tertiary
institutions
individuals
who
complete
tertiary
education
generally
receive
certificates
diplomas
or
academic
degrees
higher
education
typically
involves
work
towards
a
degree-level
or
foundation
degree
qualification
in
most
developed
countries
a
high
proportion
of
the
population
(up
to
50%)
now
enter
higher
education
at
some
time
in
their
lives
higher
education
is
therefore
very
important
to
national
economies
both
as
a
significant
industry
in
its
own
right
and
as
a
source
of
trained
and
educated
personnel
for
the
rest
of
the
economy
university
education
includes
teaching
research
and
social
services
activities
and
it
includes
both
the
undergraduate
level
(sometimes
referred
to
as
tertiary
education)
and
the
graduate
(or
postgraduate)
level
(sometimes
referred
to
as
graduate
school)
some
universities
are
composed
of
several
colleges
one
type
of
university
education
is
a
liberal
arts
education
which
can
be
defined
as
a
"college
or
university
curriculum
aimed
at
imparting
broad
general
knowledge
and
developing
general
intellectual
capacities
in
contrast
to
a
professional
vocational
or
technical
curriculum"
although
what
is
known
today
as
liberal
arts
education
began
in
europe
the
term
"liberal
arts
college"
is
more
commonly
associated
with
institutions
in
the
united
states
such
as
williams
college
or
barnard
college
vocational
education
is
a
form
of
education
focused
on
direct
and
practical
training
for
a
specific
trade
or
craft
vocational
education
may
come
in
the
form
of
an
apprenticeship
or
internship
as
well
as
institutions
teaching
courses
such
as
carpentry
agriculture
engineering
medicine
architecture
and
the
arts
in
the
past
those
who
were
disabled
were
often
not
eligible
for
public
education
children
with
disabilities
were
repeatedly
denied
an
education
by
physicians
or
special
tutors
these
early
physicians
(people
like
itard
seguin
howe
gallaudet)
set
the
foundation
for
special
education
today
they
focused
on
individualized
instruction
and
functional
skills
in
its
early
years
special
education
was
only
provided
to
people
with
severe
disabilities
but
more
recently
it
has
been
opened
to
anyone
who
has
experienced
difficulty
learning
while
considered
"alternative"
today
most
alternative
systems
have
existed
since
ancient
times
after
the
public
school
system
was
widely
developed
beginning
in
the
19th
century
some
parents
found
reasons
to
be
discontented
with
the
new
system
alternative
education
developed
in
part
as
a
reaction
to
perceived
limitations
and
failings
of
traditional
education
a
broad
range
of
educational
approaches
emerged
including
alternative
schools
self
learning
homeschooling
and
unschooling
example
alternative
schools
include
montessori
schools
waldorf
schools
(or
steiner
schools)
friends
schools
sands
school
summerhill
school
walden's
path
the
peepal
grove
school
sudbury
valley
school
krishnamurti
schools
and
open
classroom
schools
charter
schools
are
another
example
of
alternative
education
which
have
in
the
recent
years
grown
in
numbers
in
the
us
and
gained
greater
importance
in
its
public
education
system
in
time
some
ideas
from
these
experiments
and
paradigm
challenges
may
be
adopted
as
the
norm
in
education
just
as
friedrich
fröbel's
approach
to
early
childhood
education
in
19th-century
germany
has
been
incorporated
into
contemporary
kindergarten
classrooms
other
influential
writers
and
thinkers
have
included
the
swiss
humanitarian
johann
heinrich
pestalozzi;
the
american
transcendentalists
amos
bronson
alcott
ralph
waldo
emerson
and
henry
david
thoreau;
the
founders
of
progressive
education
john
dewey
and
francis
parker;
and
educational
pioneers
such
as
maria
montessori
and
rudolf
steiner
and
more
recently
john
caldwell
holt
paul
goodman
frederick
mayer
george
dennison
and
ivan
illich
indigenous
education
refers
to
the
inclusion
of
indigenous
knowledge
models
methods
and
content
within
formal
and
non-formal
educational
systems
often
in
a
post-colonial
context
the
growing
recognition
and
use
of
indigenous
education
methods
can
be
a
response
to
the
erosion
and
loss
of
indigenous
knowledge
and
language
through
the
processes
of
colonialism
furthermore
it
can
enable
indigenous
communities
to
"reclaim
and
revalue
their
languages
and
cultures
and
in
so
doing
improve
the
educational
success
of
indigenous
students"
informal
learning
is
one
of
three
forms
of
learning
defined
by
the
organisation
for
economic
co-operation
and
development
(oecd)
informal
learning
occurs
in
a
variety
of
places
such
as
at
home
work
and
through
daily
interactions
and
shared
relationships
among
members
of
society
for
many
learners
this
includes
language
acquisition
cultural
norms
and
manners
in
informal
learning
there
is
often
a
reference
person
a
peer
or
expert
to
guide
the
learner
if
learners
have
a
personal
interest
in
what
they
are
informally
being
taught
learners
tend
to
expand
their
existing
knowledge
and
conceive
new
ideas
about
the
topic
being
learned
for
example
a
museum
is
traditionally
considered
an
informal
learning
environment
as
there
is
room
for
free
choice
a
diverse
and
potentially
non-standardized
range
of
topics
flexible
structures
socially
rich
interaction
and
no
externally
imposed
assessments
while
informal
learning
often
takes
place
outside
educational
establishments
and
does
not
follow
a
specified
curriculum
it
can
also
occur
within
educational
settings
and
even
during
formal
learning
situations
educators
can
structure
their
lessons
to
directly
utilize
their
students
informal
learning
skills
within
the
education
setting
in
the
late
19th
century
education
through
play
began
to
be
recognized
as
making
an
important
contribution
to
child
development
in
the
early
20th
century
the
concept
was
broadened
to
include
young
adults
but
the
emphasis
was
on
physical
activities
lp
jacks
also
an
early
proponent
of
lifelong
learning
described
education
through
recreation:
"a
master
in
the
art
of
living
draws
no
sharp
distinction
between
his
work
and
his
play
his
labour
and
his
leisure
his
mind
and
his
body
his
education
and
his
recreation
he
hardly
knows
which
is
which
he
simply
pursues
his
vision
of
excellence
through
whatever
he
is
doing
and
leaves
others
to
determine
whether
he
is
working
or
playing
to
himself
he
always
seems
to
be
doing
both
enough
for
him
that
he
does
it
well"
education
through
recreation
is
the
opportunity
to
learn
in
a
seamless
fashion
through
all
of
life's
activities
the
concept
has
been
revived
by
the
university
of
western
ontario
to
teach
anatomy
to
medical
students
autodidacticism
(also
autodidactism)
is
a
term
used
to
describe
self-directed
learning
one
may
become
an
autodidact
at
nearly
any
point
in
one's
life
notable
autodidacts
include
abraham
lincoln
(us
president)
srinivasa
ramanujan
(mathematician)
michael
faraday
(chemist
and
physicist)
charles
darwin
(naturalist)
thomas
alva
edison
(inventor)
tadao
ando
(architect)
george
bernard
shaw
(playwright)
frank
zappa
(composer
recording
engineer
film
director)
and
leonardo
da
vinci
(engineer
scientist
mathematician)
many
large
university
institutions
are
now
starting
to
offer
free
or
almost
free
full
courses
such
as
harvard
mit
and
berkeley
teaming
up
to
form
edx
other
universities
offering
open
education
are
prestigious
private
universities
such
as
stanford
princeton
duke
johns
hopkins
the
university
of
pennylvania
and
caltech
as
well
as
notable
public
universities
including
tsinghua
peking
edinburgh
university
of
michigan
and
university
of
virginia
open
education
has
been
called
the
biggest
change
in
the
way
people
learn
since
the
printing
press
despite
favourable
studies
on
effectiveness
many
people
may
still
desire
to
choose
traditional
campus
education
for
social
and
cultural
reasons
many
open
universities
are
working
to
have
the
ability
to
offer
students
standardized
testing
and
traditional
degrees
and
credentials
the
conventional
merit-system
degree
is
currently
not
as
common
in
open
education
as
it
is
in
campus
universities
although
some
open
universities
do
already
offer
conventional
degrees
such
as
the
open
university
in
the
united
kingdom
presently
many
of
the
major
open
education
sources
offer
their
own
form
of
certificate
due
to
the
popularity
of
open
education
these
new
kind
of
academic
certificates
are
gaining
more
respect
and
equal
"academic
value"
to
traditional
degrees
out
of
182
colleges
surveyed
in
2009
nearly
half
said
tuition
for
online
courses
was
higher
than
for
campus-based
ones
a
recent
meta-analysis
found
that
online
and
blended
educational
approaches
had
better
outcomes
than
methods
that
used
solely
face-to-face
interaction
the
education
sector
or
education
system
is
a
group
of
institutions
(ministries
of
education
local
educational
authorities
teacher
training
institutions
schools
universities
etc)
whose
primary
purpose
is
to
provide
education
to
children
and
young
people
in
educational
settings
it
involves
a
wide
range
of
people
(curriculum
developers
inspectors
school
principals
teachers
school
nurses
students
etc)
these
institutions
can
vary
according
to
different
contexts
schools
deliver
education
with
support
from
the
rest
of
the
education
system
through
various
elements
such
as
education
policies
and
guidelines
–
to
which
school
policies
can
refer
–
curricula
and
learning
materials
as
well
as
pre-
and
in-service
teacher
training
programmes
the
school
environment
–
both
physical
(infrastructures)
and
psychological
(school
climate)
–
is
also
guided
by
school
policies
that
should
ensure
the
well-being
of
students
when
they
are
in
school
the
organisation
for
economic
co-operation
and
development
has
found
that
schools
tend
to
perform
best
when
principals
have
full
authority
and
responsibility
for
ensuring
that
students
are
proficient
in
core
subjects
upon
graduation
they
must
also
seek
feedback
from
students
for
quality-assurance
and
improvement
governments
should
limit
themselves
to
monitoring
student
proficiency
the
education
sector
is
fully
integrated
into
society
through
interactions
with
a
large
number
of
stakeholders
and
other
sectors
these
include
parents
local
communities
religious
leaders
ngos
stakeholders
involved
in
health
child
protection
justice
and
law
enforcement
(police)
media
and
political
leadership
several
un
agencies
have
asserted
that
comprehensive
sexuality
education
should
be
integrated
into
school
curriculum
chimombo
pointed
out
education's
role
as
a
policy
instrument
capable
of
instilling
social
change
and
economic
advancement
in
developing
countries
by
giving
communities
the
opportunity
to
take
control
of
their
destinies
the
2030
agenda
for
sustainable
development
adopted
by
the
united
nations
(un)
general
assembly
in
september
2015
calls
for
a
new
vision
to
address
the
environmental
social
and
economic
concerns
facing
the
world
today
the
agenda
includes
17
sustainable
development
goals
(sdgs)
including
sdg
4
on
education
since
1909
the
ratio
of
children
in
the
developing
world
attending
school
has
increased
before
then
a
small
minority
of
boys
attended
school
by
the
start
of
the
21st
century
the
majority
of
all
children
in
most
regions
of
the
world
attended
school
universal
primary
education
is
one
of
the
eight
international
millennium
development
goals
towards
which
progress
has
been
made
in
the
past
decade
though
barriers
still
remain
securing
charitable
funding
from
prospective
donors
is
one
particularly
persistent
problem
researchers
at
the
overseas
development
institute
have
indicated
that
the
main
obstacles
to
funding
for
education
include
conflicting
donor
priorities
an
immature
aid
architecture
and
a
lack
of
evidence
and
advocacy
for
the
issue
additionally
transparency
international
has
identified
corruption
in
the
education
sector
as
a
major
stumbling
block
to
achieving
universal
primary
education
in
africa
furthermore
demand
in
the
developing
world
for
improved
educational
access
is
not
as
high
as
foreigners
have
expected
indigenous
governments
are
reluctant
to
take
on
the
ongoing
costs
involved
there
is
also
economic
pressure
from
some
parents
who
prefer
their
children
to
earn
money
in
the
short
term
rather
than
work
towards
the
long-term
benefits
of
education
a
study
conducted
by
the
unesco
international
institute
for
educational
planning
indicates
that
stronger
capacities
in
educational
planning
and
management
may
have
an
important
spill-over
effect
on
the
system
as
a
whole
sustainable
capacity
development
requires
complex
interventions
at
the
institutional
organizational
and
individual
levels
that
could
be
based
on
some
foundational
principles:
nearly
every
country
now
has
universal
primary
education
similarities
–
in
systems
or
even
in
ideas
–
that
schools
share
internationally
have
led
to
an
increase
in
international
student
exchanges
the
european
socrates-erasmus
program
facilitates
exchanges
across
european
universities
the
soros
foundation
provides
many
opportunities
for
students
from
central
asia
and
eastern
europe
programs
such
as
the
international
baccalaureate
have
contributed
to
the
internationalization
of
education
the
global
campus
online
led
by
american
universities
allows
free
access
to
class
materials
and
lecture
files
recorded
during
the
actual
classes
the
programme
for
international
student
assessment
and
the
international
association
for
the
evaluation
of
educational
achievement
objectively
monitor
and
compare
the
proficiency
of
students
from
a
wide
range
of
different
nations
the
internationalization
of
education
is
sometimes
equated
by
critics
with
the
westernization
of
education
these
critics
say
that
the
internationalization
of
education
leads
to
the
erosion
of
local
education
systems
and
indigenous
values
and
norms
which
are
replaced
with
western
systems
and
cultural
and
ideological
values
and
orientation
technology
plays
an
increasingly
significant
role
in
improving
access
to
education
for
people
living
in
impoverished
areas
and
developing
countries
however
lack
of
technological
advancement
is
still
causing
barriers
with
regards
to
quality
and
access
to
education
in
developing
countries
charities
like
one
laptop
per
child
are
dedicated
to
providing
infrastructures
through
which
the
disadvantaged
may
access
educational
materials
the
olpc
foundation
a
group
out
of
mit
media
lab
and
supported
by
several
major
corporations
has
a
stated
mission
to
develop
a
$100
laptop
for
delivering
educational
software
the
laptops
were
widely
available
as
of
2008
they
are
sold
at
cost
or
given
away
based
on
donations
in
africa
the
new
partnership
for
africa's
development
(nepad)
has
launched
an
"e-school
program"
to
provide
all
600000
primary
and
high
schools
with
computer
equipment
learning
materials
and
internet
access
within
10
years
an
international
development
agency
project
called
nabuurcom
started
with
the
support
of
former
american
president
bill
clinton
uses
the
internet
to
allow
co-operation
by
individuals
on
issues
of
social
development
india
is
developing
technologies
that
will
bypass
land-based
telephone
and
internet
infrastructure
to
deliver
distance
learning
directly
to
its
students
in
2004
the
indian
space
research
organisation
launched
edusat
a
communications
satellite
providing
access
to
educational
materials
that
can
reach
more
of
the
country's
population
at
a
greatly
reduced
cost
research
into
lcps
(low-cost
private
schools)
found
that
over
5
years
to
july
2013
debate
around
lcpss
to
achieving
education
for
all
(efa)
objectives
was
polarized
and
finding
growing
coverage
in
international
policy
the
polarization
was
due
to
disputes
around
whether
the
schools
are
affordable
for
the
poor
reach
disadvantaged
groups
provide
quality
education
support
or
undermine
equality
and
are
financially
sustainable
the
report
examined
the
main
challenges
encountered
by
development
organizations
which
support
lcpss
surveys
suggest
these
types
of
schools
are
expanding
across
africa
and
asia
this
success
is
attributed
to
excess
demand
these
surveys
found
concern
for:
the
report
showed
some
cases
of
successful
voucher
and
subsidy
programs;
evaluations
of
international
support
to
the
sector
are
not
widespread
addressing
regulatory
ineffectiveness
is
a
key
challenge
emerging
approaches
stress
the
importance
of
understanding
the
political
economy
of
the
market
for
lcps
specifically
how
relationships
of
power
and
accountability
between
users
government
and
private
providers
can
produce
better
education
outcomes
for
the
poor
educational
psychology
is
the
study
of
how
humans
learn
in
educational
settings
the
effectiveness
of
educational
interventions
the
psychology
of
teaching
and
the
social
psychology
of
schools
as
organizations
although
the
terms
"educational
psychology"
and
"school
psychology"
are
often
used
interchangeably
researchers
and
theorists
are
likely
to
be
identified
as
whereas
practitioners
in
schools
or
school-related
settings
are
identified
as
school
psychologists
educational
psychology
is
concerned
with
the
processes
of
educational
attainment
in
the
general
population
and
in
sub-populations
such
as
gifted
children
and
those
with
specific
disabilities
educational
psychology
can
in
part
be
understood
through
its
relationship
with
other
disciplines
it
is
informed
primarily
by
psychology
bearing
a
relationship
to
that
discipline
analogous
to
the
relationship
between
medicine
and
biology
educational
psychology
in
turn
informs
a
wide
range
of
specialties
within
educational
studies
including
instructional
design
educational
technology
curriculum
development
organizational
learning
special
education
and
classroom
management
educational
psychology
both
draws
from
and
contributes
to
cognitive
science
and
the
learning
sciences
in
universities
departments
of
educational
psychology
are
usually
housed
within
faculties
of
education
possibly
accounting
for
the
lack
of
representation
of
educational
psychology
content
in
introductory
psychology
textbooks
(lucas
blazek
raley
2006)
intelligence
is
an
important
factor
in
how
the
individual
responds
to
education
those
who
have
higher
intelligence
tend
to
perform
better
at
school
and
go
on
to
higher
levels
of
education
this
effect
is
also
observable
in
the
opposite
direction
in
that
education
increases
measurable
intelligence
studies
have
shown
that
while
educational
attainment
is
important
in
predicting
intelligence
in
later
life
intelligence
at
53
is
more
closely
correlated
to
intelligence
at
8
years
old
than
to
educational
attainment
there
has
been
much
interest
in
learning
modalities
and
styles
over
the
last
two
decades
the
most
commonly
employed
learning
modalities
are:
other
commonly
employed
modalities
include
musical
interpersonal
verbal
logical
and
intrapersonal
dunn
and
dunn
focused
on
identifying
relevant
stimuli
that
may
influence
learning
and
manipulating
the
school
environment
at
about
the
same
time
as
joseph
renzulli
recommended
varying
teaching
strategies
howard
gardner
identified
a
wide
range
of
modalities
in
his
multiple
intelligences
theories
the
myers-briggs
type
indicator
and
keirsey
temperament
sorter
based
on
the
works
of
jung
focus
on
understanding
how
people's
personality
affects
the
way
they
interact
personally
and
how
this
affects
the
way
individuals
respond
to
each
other
within
the
learning
environment
the
work
of
david
kolb
and
anthony
gregorc's
type
delineator
follows
a
similar
but
more
simplified
approach
some
theories
propose
that
all
individuals
benefit
from
a
variety
of
learning
modalities
while
others
suggest
that
individuals
may
have
preferred
learning
styles
learning
more
easily
through
visual
or
kinesthetic
experiences
a
consequence
of
the
latter
theory
is
that
effective
teaching
should
present
a
variety
of
teaching
methods
which
cover
all
three
learning
modalities
so
that
different
students
have
equal
opportunities
to
learn
in
a
way
that
is
effective
for
them
guy
claxton
has
questioned
the
extent
that
learning
styles
such
as
visual
auditory
and
kinesthetic(vak)
are
helpful
particularly
as
they
can
have
a
tendency
to
label
children
and
therefore
restrict
learning
recent
research
has
argued
"there
is
no
adequate
evidence
base
to
justify
incorporating
learning
styles
assessments
into
general
educational
practice"
educational
neuroscience
is
an
emerging
scientific
field
that
brings
together
researchers
in
cognitive
neuroscience
developmental
cognitive
neuroscience
educational
psychology
educational
technology
education
theory
and
other
related
disciplines
to
explore
the
interactions
between
biological
processes
and
education
researchers
in
educational
neuroscience
investigate
the
neural
mechanisms
of
reading
numerical
cognition
attention
and
their
attendant
difficulties
including
dyslexia
dyscalculia
and
adhd
as
they
relate
to
education
several
academic
institutions
around
the
world
are
beginning
to
devote
resources
to
the
establishment
of
educational
neuroscience
research
as
an
academic
field
philosophy
of
education
is
"the
philosophical
study
of
education
and
its
problems ()
its
central
subject
matter
is
education
and
its
methods
are
those
of
philosophy"
"the
philosophy
of
education
may
be
either
the
philosophy
of
the
process
of
education
or
the
philosophy
of
the
discipline
of
education
that
is
it
may
be
part
of
the
discipline
in
the
sense
of
being
concerned
with
the
aims
forms
methods
or
results
of
the
process
of
educating
or
being
educated;
or
it
may
be
metadisciplinary
in
the
sense
of
being
concerned
with
the
concepts
aims
and
methods
of
the
discipline"
as
such
it
is
both
part
of
the
field
of
education
and
a
field
of
applied
philosophy
drawing
from
fields
of
metaphysics
epistemology
axiology
and
the
philosophical
approaches
(speculative
prescriptive
or
analytic)
to
address
questions
in
and
about
pedagogy
education
policy
and
curriculum
as
well
as
the
process
of
learning
to
name
a
few
for
example
it
might
study
what
constitutes
upbringing
and
education
the
values
and
norms
revealed
through
upbringing
and
educational
practices
the
limits
and
legitimization
of
education
as
an
academic
discipline
and
the
relation
between
education
theory
and
practice
there
is
no
broad
consensus
as
to
what
education's
chief
aim
or
aims
are
or
should
be
different
places
and
at
different
times
have
used
educational
systems
for
different
purposes
the
prussian
education
system
in
the
19th
century
for
example
wanted
to
turn
boys
and
girls
into
adults
who
would
serve
the
state's
political
goals
some
authors
stress
its
value
to
the
individual
emphasizing
its
potential
for
positively
influencing
students'
personal
development
promoting
autonomy
forming
a
cultural
identity
or
establishing
a
career
or
occupation
other
authors
emphasize
education's
contributions
to
societal
purposes
including
good
citizenship
shaping
students
into
productive
members
of
society
thereby
promoting
society's
general
economic
development
and
preserving
cultural
values
the
purpose
of
education
in
a
given
time
and
place
affects
who
is
taught
what
is
taught
and
how
the
education
system
behaves
for
example
in
the
21st
century
many
countries
treat
education
as
a
positional
good
in
this
competitive
approach
people
want
their
own
students
to
get
a
better
education
than
other
students
this
approach
can
lead
to
unfair
treatment
of
some
students
especially
those
from
disadvantaged
or
marginalized
groups
for
example
in
this
system
a
city's
school
system
may
draw
school
district
boundaries
so
that
nearly
all
the
students
in
one
school
are
from
low-income
families
and
that
nearly
all
the
students
in
the
neighboring
schools
come
from
more
affluent
families
even
though
concentrating
low-income
students
in
one
school
results
in
worse
educational
achievement
for
the
entire
school
system
in
formal
education
a
curriculum
is
the
set
of
courses
and
their
content
offered
at
a
school
or
university
as
an
idea
curriculum
stems
from
the
latin
word
for
"race
course"
referring
to
the
course
of
deeds
and
experiences
through
which
children
grow
to
become
mature
adults
a
curriculum
is
prescriptive
and
is
based
on
a
more
general
syllabus
which
merely
specifies
what
topics
must
be
understood
and
to
what
level
to
achieve
a
particular
grade
or
standard
an
academic
discipline
is
a
branch
of
knowledge
which
is
formally
taught
either
at
the
university
–
or
via
some
other
such
method
each
discipline
usually
has
several
sub-disciplines
or
branches
and
distinguishing
lines
are
often
both
arbitrary
and
ambiguous
examples
of
broad
areas
of
academic
disciplines
include
the
natural
sciences
mathematics
computer
science
social
sciences
humanities
and
applied
sciences
educational
institutions
may
incorporate
fine
arts
as
part
of
k-12
grade
curricula
or
within
majors
at
colleges
and
universities
as
electives
the
various
types
of
fine
arts
are
music
dance
and
theatre
the
sudbury
valley
school
offers
a
model
of
education
without
a
curricula
instruction
is
the
facilitation
of
another's
learning
instructors
in
primary
and
secondary
institutions
are
often
called
teachers
and
they
direct
the
education
of
students
and
might
draw
on
many
subjects
like
reading
writing
mathematics
science
and
history
instructors
in
post-secondary
institutions
might
be
called
teachers
instructors
or
professors
depending
on
the
type
of
institution;
and
they
primarily
teach
only
their
specific
discipline
studies
from
the
united
states
suggest
that
the
quality
of
teachers
is
the
single
most
important
factor
affecting
student
performance
and
that
countries
which
score
highly
on
international
tests
have
multiple
policies
in
place
to
ensure
that
the
teachers
they
employ
are
as
effective
as
possible
with
the
passing
of
nclb
in
the
united
states
(no
child
left
behind)
teachers
must
be
highly
qualified
a
popular
way
to
gauge
teaching
performance
is
to
use
student
evaluations
of
teachers
(sets)
but
these
evaluations
have
been
criticized
for
being
counterproductive
to
learning
and
inaccurate
due
to
student
bias
college
basketball
coach
john
wooden
the
wizard
of
westwood
would
teach
through
quick
"this
not
that"
technique
he
would
show
(a)
the
correct
way
to
perform
an
action
(b)
the
incorrect
way
the
player
performed
it
and
again
(c)
the
correct
way
to
perform
an
action
this
helped
him
to
be
a
responsive
teacher
and
fix
errors
on
the
fly
also
less
communication
from
him
meant
more
time
that
the
player
could
practice
it
has
been
argued
that
high
rates
of
education
are
essential
for
countries
to
be
able
to
achieve
high
levels
of
economic
growth
empirical
analyses
tend
to
support
the
theoretical
prediction
that
poor
countries
should
grow
faster
than
rich
countries
because
they
can
adopt
cutting
edge
technologies
already
tried
and
tested
by
rich
countries
however
technology
transfer
requires
knowledgeable
managers
and
engineers
who
are
able
to
operate
new
machines
or
production
practices
borrowed
from
the
leader
in
order
to
close
the
gap
through
imitation
therefore
a
country's
ability
to
learn
from
the
leader
is
a
function
of
its
stock
of
"human
capital"
recent
study
of
the
determinants
of
aggregate
economic
growth
have
stressed
the
importance
of
fundamental
economic
institutions
and
the
role
of
cognitive
skills
at
the
level
of
the
individual
there
is
a
large
literature
generally
related
to
the
work
of
jacob
mincer
on
how
earnings
are
related
to
the
schooling
and
other
human
capital
this
work
has
motivated
a
large
number
of
studies
but
is
also
controversial
the
chief
controversies
revolve
around
how
to
interpret
the
impact
of
schooling
some
students
who
have
indicated
a
high
potential
for
learning
by
testing
with
a
high
intelligence
quotient
may
not
achieve
their
full
academic
potential
due
to
financial
difficulties
economists
samuel
bowles
and
herbert
gintis
argued
in
1976
that
there
was
a
fundamental
conflict
in
american
schooling
between
the
egalitarian
goal
of
democratic
participation
and
the
inequalities
implied
by
the
continued
profitability
of
capitalist
production
many
countries
are
now
drastically
changing
the
way
they
educate
their
citizens
the
world
is
changing
at
an
ever
quickening
rate
which
means
that
a
lot
of
knowledge
becomes
obsolete
and
inaccurate
more
quickly
the
emphasis
is
therefore
shifting
to
teaching
the
skills
of
learning:
to
picking
up
new
knowledge
quickly
and
in
as
agile
a
way
as
possible
finnish
schools
have
even
begun
to
move
away
from
the
regular
subject-focused
curricula
introducing
instead
developments
like
phenomenon-based
learning
where
students
study
concepts
like
climate
change
instead
there
are
also
active
educational
interventions
to
implement
programs
and
paths
specific
to
non-traditional
students
such
as
first
generation
students
education
is
also
becoming
a
commodity
no
longer
reserved
for
children
adults
need
it
too
some
governmental
bodies
like
the
finnish
innovation
fund
sitra
in
finland
have
even
proposed
compulsory
lifelong
education
united
states
military
veteran
suicide
united
states
military
veteran
suicide
is
an
ongoing
phenomenon
regarding
a
reportedly
high
rate
of
suicide
among
us
military
veterans
in
comparison
to
the
general
public
according
to
the
most
recent
report
published
by
the
united
states
department
of
veterans
affairs
(va)
in
2016
which
analyzed
55
million
veterans'
records
from
1979
to
2014
the
current
analysis
indicates
that
an
average
of
20
veterans
a
day
die
from
suicide
in
2012
alone
an
estimated
6500
former
military
personnel
died
by
suicide
more
active
duty
veterans
177
succumbed
to
suicide
that
year
than
were
killed
in
combat
176
the
army
suffered
52%
of
the
suicides
from
all
branches
in
2013
the
va
released
a
study
that
covered
suicides
from
1999
to
2010
which
showed
that
roughly
22
veterans
were
dying
by
suicide
per
day
or
one
every
65
minutes
some
sources
suggest
that
this
rate
may
be
undercounting
suicides
a
recent
analysis
found
a
suicide
rate
among
veterans
of
about
30
per
100000
population
per
year
compared
with
the
civilian
rate
of
14
per
100000
however
the
comparison
was
not
adjusted
for
age
and
sex
the
total
number
of
suicides
differs
by
age
group;
31%
of
these
suicides
were
by
veterans
49
and
younger
while
69%
were
by
veterans
aged
50
and
older
as
with
suicides
in
general
suicide
of
veterans
is
primarily
male
with
about
97
percent
of
the
suicides
being
male
in
the
states
that
reported
gender
in
2015
the
clay
hunt
veterans
suicide
prevention
act
passed
in
the
senate
and
was
then
enacted
as
on
february
12
2015
in
august
2016
the
va
released
a
new
report
which
consisted
of
the
nation's
largest
analysis
of
veteran
suicide
the
report
reviewed
more
than
55
million
veterans'
records
from
1979
to
2014
from
every
state
in
the
nation
the
previous
report
from
2012
was
primarily
limited
to
data
on
veterans
who
used
vha
health
services
or
from
mortality
records
obtained
directly
from
20
states
and
approximately
3
million
records
compared
to
the
data
from
the
2012
report
which
estimated
the
number
of
veteran
deaths
by
suicide
to
be
22
per
day
the
current
analysis
indicates
that
in
2014
an
average
of
20
veterans
a
day
died
from
suicide
the
first
suicide
prevention
center
in
the
united
states
was
opened
in
los
angeles
in
1958
with
funding
from
the
us
public
health
service
in
1966
the
center
for
studies
of
suicide
prevention
(later
the
suicide
research
unit)
was
established
at
the
national
institute
of
mental
health
(nimh)
of
the
national
institutes
of
health
(nih)
later
on
in
1970
the
nimh
pushed
in
phoenix
the
discussion
about
the
status
of
suicide
prevention
presented
relevant
findings
about
suicide
rate
and
identified
the
future
directions
and
priorities
of
the
topic
however
it
wasn't
until
mid-1990s
when
suicide
started
being
the
central
issue
of
the
political-social
agenda
of
the
united
states
survivors
from
suicide
began
to
mobilize
encouraging
the
development
of
a
national
strategy
for
suicide
prevention
finally
two
congressional
resolutions—s
res
84
and
h
res
212
of
the
105th
congress—recognized
suicide
as
a
national
problem
and
suicide
prevention
as
a
national
priority
as
recommended
in
the
un
guidelines
these
groups
set
out
to
establish
a
public
and
private
partnership
that
would
be
responsible
for
promoting
suicide
prevention
in
the
united
states
this
innovative
public-private
partnership
jointly
sponsored
a
national
consensus
conference
on
suicide
prevention
in
reno
nevada
which
developed
a
list
of
81
recommendations
one
of
the
most
important
laws
about
veterans'
suicide
prevention
is
the
joshua
omvig
veterans
suicide
prevention
act
(jovspa)
of
2007
supporting
the
creation
of
a
comprehensive
program
to
reduce
the
incidence
of
suicide
among
veterans
named
for
a
veteran
of
operation
iraqi
freedom
who
died
by
suicide
in
2005
the
act
directed
the
secretary
of
the
us
department
of
veterans
affairs
(va)
to
implement
a
comprehensive
suicide
prevention
program
for
veterans
components
include
staff
education
mental
health
assessments
as
part
of
overall
health
assessments
a
suicide
prevention
coordinator
at
each
va
medical
facility
research
efforts
24-hour
mental
health
care
a
toll-free
crisis
line
and
outreach
to
and
education
for
veterans
and
their
families
in
the
summer
of
2009
va
added
a
one-to-one
“chat
service”
for
veterans
who
prefer
to
reach
out
for
assistance
using
the
internet
in
2010
the
national
action
alliance
for
suicide
prevention
was
created
and
in
2012
the
national
strategy
was
revised
with
obama’s
administration
suicide
prevention
strategies
for
veterans
expanded
and
a
goal
was
formed
to
make
the
process
of
finding
and
obtaining
mental
health
resources
easier
for
veterans
work
to
retain
and
recruit
mental
health
professionals
and
make
the
government
programs
more
accountable
for
the
people
they
serve
in
2011
the
national
veterans
suicide
prevention
hotline
was
renamed
the
veterans
crisis
line
(vcl)
the
primary
mission
of
the
vcl
is
“to
provide
24/7
world-class
suicide
prevention
and
crisis
intervention
services
to
veterans
servicemembers
and
their
family
members”
the
vcl
faces
a
number
of
challenges
it
must
meet
the
operational
and
business
demands
of
responding
to
over
500000
calls
per
year
along
with
thousands
of
electronic
chats
and
text
messages
and
initiating
rescue
processes
when
indicated
it
must
also
train
staff
to
respond
to
veterans
and
their
family
members
in
individual
encounters
during
which
a
responder
must
make
an
accurate
assessment
of
the
needs
of
the
caller
under
stressful
time-sensitive
conditions
since
its
inception
in
july
2007
the
vcl
has
answered
over
3
million
calls
and
initiated
the
dispatch
of
emergency
services
to
callers
in
imminent
crisis
over
84000
times
since
launching
chat
in
2009
and
text
services
in
november
2011
the
vcl
has
answered
nearly
359000
and
nearly
78000
requests
for
chat
and
text
services
respectively
in
addition
staff
has
forwarded
more
than
504000
referrals
to
local
va
suicide
prevention
coordinators
on
behalf
of
veterans
to
ensure
continuity
of
care
with
veterans’
local
va
providers
for
fy
2016
more
than
51000
chats
and
17000
texts
were
answered
by
vcl
responders
for
fy
2017
nearly
54000
chats
and
nearly
16000
texts
were
answered
by
vcl
responders
emergency
services
were
dispatched
to
over
12000
callers
in
immediate
crisis
in
fy
2016
and
nearly
19000
callers
in
immediate
crisis
in
fy
2017
for
fy
2016
nearly
87000
referrals
were
made
to
local
suicide
prevention
coordinators
for
follow-up
care
and
over
95000
referrals
were
made
in
fy
2017
the
2018
federal
budget
expanded
mental
health
screenings
for
veterans
a
study
published
in
the
"cleveland
clinic
journal
of
medicine"
found
that
the
same
study
also
found
that
in
veterans
with
ptsd
related
to
combat
experience
combat-related
guilt
may
be
a
significant
predictor
of
suicidal
ideation
and
attempts
craig
bryan
of
the
university
of
utah
national
center
for
veterans
studies
said
that
veterans
have
the
same
risk
factors
for
suicide
as
the
general
population
including
feelings
of
depression
hopelessness
post-traumatic
stress
disorder
a
history
of
trauma
and
access
to
firearms
a
study
done
by
the
"department
of
veterans
affairs"
discovered
that
veterans
are
more
likely
to
develop
symptoms
of
ptsd
for
a
number
of
reasons
such
as:
the
"department
of
veterans
affairs"
also
discovered
that
where
you
were
deployed
and
which
branch
of
military
you
are
with
can
also
have
drastic
effects
on
your
mental
status
after
returning
from
service
as
in
most
combat
wars
your
experiences
will
vary
depending
on
where
you
are
stationed
critics
of
this
reporting
such
as
author
tim
worstall
in
feb
2013
claim
that
there
is
no
epidemic
when
comparing
similar
demographic
cohorts
in
the
civilian
population
he
points
out
that
since
vets
are
predominantly
male
the
suicide
rate
to
compare
to
is
not
the
general
civilian
rate
but
the
rate
for
males
veterans
have
difficulty
transitioning
from
the
military
to
civilian
life
many
choose
to
transition
by
utilizing
their
gi
bill
or
other
education
benefits
the
pursuit
of
education
often
facilitates
the
transition
to
civilian
life
the
pursuit
of
education
among
veterans
can
aggravate
post
service
conditions
that
are
linked
to
a
higher
likelihood
of
suicide
but
often
aids
in
the
transition
to
civilian
life
veterans
pursuing
education
especially
those
utilizing
the
post
9/11
gi
bill
are
more
likely
to
have
protective
factors
related
to
socialization
and
reintegration
than
those
who
are
not
although
higher
education
has
presented
many
difficulties
to
returning
veterans
research
supports
that
veterans
often
benefit
from
transitioning
from
the
military
into
higher
education
academic
life
often
requires
student
veterans
to
work
and
interact
with
other
classmates
most
academic
institutions
have
student
veteran
organizations
and
resources
centers
specifically
to
aid
military
veterans
military
education
benefits
primarily
the
post
9/11
gi
bill
pay
the
cost
of
tuition
and
provide
a
housing
stipend
to
student
veterans
education
benefits
often
give
veteran
students
an
income
a
goal
to
continue
to
work
towards
and
socialization
with
the
general
population
national
commission
on
teaching
and
america's
future
the
national
commission
on
teaching
and
america's
future
(nctaf)
is
a
non-profit
non-partisan
education
policy
advocacy
organization
based
in
washington
dc
founded
in
1994
by
then-north
carolina
governor
jim
hunt
and
stanford
university
professor
linda
darling-hammond
the
nctaf
focuses
its
research
on
improving
the
teaching
profession
through
recruitment
development
and
retention
of
skilled
teachers
in
2017
the
nctaf
announced
that
it
will
merge
with
learning
forward
and
will
operate
under
the
learning
forward
name
in
its
1996
report
"what
matters
most:
teaching
for
america's
future"
the
nctaf
issued
broad
recommendations
for
education
leaders
and
state
policymakers
to
among
other
things
overhaul
teacher
education
programs
establish
state
boards
of
professional
teaching
standards
strengthen
teacher
licensure
standards
implement
teacher
mentoring
programs
and
create
teacher
compensation
policies
that
reward
knowledge
and
expertise
the
report
had
wide-reaching
impact
with
seven
states
including
illinois
indiana
kentucky
maine
missouri
north
carolina
and
ohio
signing
on
to
be
partners
in
implementing
the
report's
recommendations
in
2001
the
nctaf
appointed
former
us
federal
election
official
tom
carrol
its
executive
director
carroll
announced
his
retirement
in
2014
he
was
succeeded
by
melinda
george
reach
every
reader
reach
every
reader
is
a
five-year
initiative
supported
by
a
$30
million
grant
from
chan
zuckerberg
initiative
co-founders
priscilla
chan
and
facebook
ceo
mark
zuckerberg
reach
every
reader
was
launched
by
faculty
at
the
harvard
graduate
school
of
education
and
massachusetts
institute
of
technology's
integrated
learning
initiative
and
involves
collaborators
at
the
florida
center
for
reading
research
and
florida
state
university
college
of
communication
and
information
and
the
charlotte-mecklenburg
school
district
in
north
carolina
the
collaboration
consists
of
five
projects:
reach
every
reader
will
develop
a
web-based
screening
tool
for
reading
difficulties
that
diagnoses
underlying
causes
the
diagnostic
screening
tool
will
identify
kindergarteners
who
are
at
high
risk
for
reading
difficulty
the
goal
is
to
make
this
type
of
screening
available
to
all
children
the
collaboration
will
also
examine
which
interventions
work
for
which
students
in
order
to
work
toward
the
development
of
personalized
interventions
researchers
will
work
with
schools
to
deliver
their
interventions
to
kindergarten
students
in
summer
programs
and
eventually
implement
them
in
the
school
curriculum
concern
has
been
expressed
that
the
project
involves
"crisis
talk"
that
creates
pressure
for
children
and
that
parents
may
be
concerned
about
the
tracking
of
their
children's
personal
information
state
college
of
florida
collegiate
school
state
college
of
florida
collegiate
school
(scfcs)
is
a
college
preparatory
school
located
on
state
college
of
florida's
bradenton
campus
it
is
based
on
a
school
in
sweden
with
similar
views
of
having
students
work
on
their
own
pace
classes
are
available
for
grades
6-10
the
school
is
largely
technology
based
utilizing
a
service
canvas
from
instructure
to
assign
and
turn
in
schoolwork
each
student
is
assigned
an
ipad
based
on
their
grade
level
and
apple
laptops
are
available
for
services
not
available
on
the
ipad
each
student
start
classes
on
the
college
campus
in
eleventh
grade
if
they
pass
an
enrollment
test
called
the
pert
and
have
at
least
a
30
gpa
after
completing
the
program
they
are
given
an
associate
degree
at
graduation
alongside
their
high
school
diploma
following
this
for
a
two-year
period
students
can
be
given
a
tuition-paid
scholarship
for
the
florida
gulf
coast
university
the
current
headmaster
is
kelly
monod
state
college
of
florida
collegiate
school
is
recognized
for
its
high
academic
record
and
high
quality
work
in
the
year
2011
scfcs
participated
in
the
florida
comprehensive
assessment
test
(fcat)
in
sixth
grade
reading
scfcs
students
scored
85
compared
to
manatee
county
public
school
district
63
and
all
florida
students’
composite
of
67
sixth
grade
math
scores
were
scfcs
at
72
manatee
county
48
and
florida
57
in
seventh
grade
reading
scfcs
students
scored
83
compared
to
manatee
county
65
and
florida
68
scfcs
scored
73
in
seventh
grade
math
compared
to
manatee
county
at
59
and
florida
at
62
many
scf
collegiate
students
regularly
participate
on
the
college
brain
bowl
team
often
with
great
success
scf
collegiate
students
carlyle
styer
and
christopher
medrano
were
a
part
of
the
"scf
fire
team"
which
won
the
2015
fcsaa
brain
bowl
state
championship
carlyle
styer
was
joined
by
fellow
scfcs
student
kara
stevens
as
well
as
four
other
scf
students
in
winning
the
2015
naqt
community
college
championship
it
is
believed
that
this
may
be
the
only
instance
in
which
high
school
students
have
played
a
significant
role
in
a
quiz
bowl
national
championship
at
the
collegiate
level
freethought
freethought
(or
"free
thought")
is
a
philosophical
viewpoint
which
holds
that
positions
regarding
truth
should
be
formed
on
the
basis
of
logic
reason
and
empiricism
rather
than
authority
tradition
revelation
or
dogma
according
to
the
oxford
english
dictionary
independent
working
class
education
independent
working
class
education
is
an
approach
to
education
particularly
adult
education
developed
by
labour
activists
whereby
the
education
of
working
class
people
is
seen
as
a
specifically
political
process
linked
to
other
aspects
of
class
struggle
the
term
abbreviated
to
(iwce)
is
particularly
linked
to
the
plebs'
league
skill
assessment
competence
assessment
is
a
process
in
which
evidence
is
gathered
by
the
assessor
and
evaluated
against
agreed
criteria
in
order
to
make
a
judgement
of
competence
skill
assessment
is
the
comparison
of
actual
performance
of
a
skill
with
the
specified
standard
for
performance
of
that
skill
under
the
circumstances
specified
by
the
standard
and
evaluation
of
whether
the
performance
meets
or
exceed
the
requirements
assessment
of
a
skill
should
comply
with
the
four
principles
of
validity
reliability
fairness
and
flexibility
formative
assessment
provides
feedback
for
remedial
work
and
coaching
while
summative
assessment
checks
whether
the
competence
has
been
achieved
at
the
end
of
training
assessment
of
combinations
of
skills
and
of
foundational
knowledge
may
provide
greater
efficiency
and
in
some
cases
competence
in
one
skill
my
imply
competence
in
other
skills
the
thoroughness
reqired
of
assessment
may
depend
on
the
consequences
of
occasional
poor
performance
validity
is
the
primary
requirement
if
the
assessment
is
not
valid
then
the
other
characteristics
are
irrelevant
validity
means
that
an
assessment
process
effectively
assesses
what
it
is
claimed
and
intended
to
assess
to
achieve
this
the
assessment
tools
must
address
all
requirements
of
the
standard
to
the
appropriate
depth
(neither
too
much
nor
too
little)
and
be
repeated
often
enough
to
ensure
that
the
required
performance
is
repeatable
the
training
standard
that
specifies
the
competency
is
the
benchmark
for
assessment
and
to
be
valid
the
assessment
must
comply
exactly
with
its
requirements
so
that
nothing
required
by
the
standard
is
omitted
and
nothing
that
is
not
required
is
included
the
assessment
tools
for
a
skill
therefore
need
to
be
designed
so
that
they
allow
the
skill
to
be
tested
in
compliance
with
the
requirements
of
the
standard
it
can
be
useful
to
map
the
assessment
tools
to
the
specific
competences
to
ensure
that
they
cover
the
full
scope
of
the
standard
there
may
be
a
requirement
for
periodical
validation
of
assessment
tools
this
process
generally
involves
mapping
the
tools
against
the
standard
and
checking
that
the
tools
comply
with
the
other
principles
of
assessment
and
the
rules
of
evidence
after
validity
reliability
is
essential
a
reliable
assessment
is
one
where
the
evidence
elicited
and
interpretation
of
evidence
is
consistent
with
the
skill
required
so
that
the
assessment
consistently
produces
outcomes
that
are
compliant
with
the
standard
the
assessment
decision
of
a
given
observed
performance
should
not
vary
for
different
assessors
the
same
evidence
should
lead
to
the
same
outcome
to
achieve
this
the
assessment
tool
must
provide
sufficient
guidance
for
the
assessor
in
practice
the
assessment
instrument
provided
to
the
candidate
should
be
paired
with
an
assessor
guide
which
provides
instructions
to
the
assessor
to
guide
their
judgement
of
satisfactory
performance
or
acceptable
answers
to
questions
to
be
fair
the
assessment
process
must
be
clearly
understood
by
the
candidates
and
there
must
be
agreement
by
both
assessors
and
candidates
that
candidates’
reasonable
needs
circumstances
are
addressed
the
assessment
tool
can
provide
evidence
that
the
process
is
understood
and
accepted
by
the
candidate
by
having
a
place
where
a
statement
to
this
effect
is
signed
by
the
candidate
at
the
start
of
the
assessment
a
further
statement
that
the
assessor
has
checked
with
the
candidate
for
any
special
circumstances
or
requirements
can
also
be
included
reasonable
adjustment
must
not
compromise
the
validity
or
reliability
of
the
assessment
flexibility
of
assessment
is
desirable
where
reasonably
practicable
this
is
a
feature
that
should
be
inherent
in
the
assessment
tools
for
the
skill
and
should
take
into
account
the
expected
variability
of
circumstances
including
variations
in
candidates
equipment
location
environmental
conditions
and
other
things
not
entirely
under
the
control
of
the
assessor
but
within
the
scope
of
the
competence
requirements
flexibility
does
not
imply
bending
the
rules
or
failing
to
comply
with
the
specifications
of
the
standards
all
performance
criteria
must
be
addressed
formative
assessments
are
formal
and
informal
tests
tasks
quizzes
discussions
or
observations
taken
during
the
learning
process
these
assessments
identify
strengths
and
weaknesses
and
provide
feedback
to
modify
the
consequent
learning
activities
to
facilitate
efficient
learning
and
skill
development
summative
assessments
evaluate
skills
at
or
after
the
end
of
an
instructional
unit
to
ensure
that
competence
has
been
achieved
at
this
point
remedial
work
may
no
longer
be
practicable
integrated
assessment
is
part
of
the
learning
and
teaching
process
and
can
take
place
at
various
stages
of
a
learning
programme
assessments
may
combine
assessment
of
theory
and
practice
some
skills
may
need
separate
and
specific
assessment
but
others
can
be
combined
for
efficiency
assessment
is
not
an
event
that
only
occurs
at
the
end
of
training
it
is
most
effective
when
continuous
and
when
providing
constant
feedback
on
progress
and
problems
allowing
timely
intervention
where
useful
in
many
cases
a
sample
of
evidence
is
sufficient
to
infer
competence
over
a
fairly
large
range
as
competence
in
a
skill
that
requires
competence
in
other
skills
may
be
a
proxy
for
those
more
foundational
skills
comprehensive
planning
is
usually
necessary
to
produce
robust
assessment
tools
that
suit
the
training
programme
and
do
justice
to
both
the
training
standard
and
the
learners
assessment
of
practical
skills
is
usually
best
done
by
direct
observation
of
performance
in
conditions
as
close
as
reasonably
practicable
to
the
circumstances
in
which
the
skill
would
normally
be
practiced
where
this
is
not
reasonably
practicable
simulations
may
be
appropriate
to
whatever
level
of
accuracy
is
available
assessment
of
realistic
combinations
of
skills
may
save
a
lot
of
time
and
scenarios
may
be
devised
that
allow
simultaneous
and
sequential
assessments
of
several
skill
in
one
assessment
session
the
number
of
repetitions
required
will
also
depend
on
how
critical
the
skill
is
considered
to
be
a
single
successful
demonstration
may
be
sufficient
to
show
that
the
candidate
can
perform
a
task
when
the
consequences
are
minor
several
sequential
faultless
performances
may
be
required
if
another
person's
life
will
depend
on
correct
performance
assessment
tools
for
practical
skills
may
describe
a
task
to
be
done
and
the
assessors
guide
should
generally
list
the
stages
of
the
task
and
the
details
the
assessor
should
check
off
as
they
are
done
where
order
is
important
this
should
be
mentioned
a
checklist
may
be
provided
as
permanent
record
or
a
video
may
be
taken
in
some
cases
there
will
be
a
product
which
can
be
retained
as
long
term
evidence
along
with
the
paperwork
or
database
records
peacejam
ghana
peacejam
ghana
started
in
2008
by
its
official
chapter
the
west
africa
center
for
peace
foundation
ghana
(wacpf)
peacejam
ghana
has
mentored
and
trained
over
5000
students
since
its
inception
peacejam
ghana
has
produced
many
scholars
some
of
whom
have
received
the
tpg
global
impact
youth
fellows
scholarship
to
pursue
higher
degrees
wisdom
addo
is
the
founder
and
executive
director
of
the
west
africa
center
for
peace
foundation
ghana
peacejam
is
an
annual
youth
leadership
conference
that
is
built
around
the
nobel
peace
prize
laureates
who
work
with
young
people
with
the
aim
of
imparting
their
skills
knowledge
and
wisdom
to
them
for
community
and
sustainable
development
the
conference
usually
draws
students
from
junior
high
and
senior
high
schools
across
the
country
who
are
usually
trained
and
mentored
by
mentors
on
diverse
areas
including
but
not
limited
to
commitment
to
justice
and
peace
social
responsibility
academic
excellence
and
sustainable
development
1
osu
presby
senior
high
school
2
kaneshie
senior
high
secondary
technical
-
accra
3
kraboa
presby
senior
high
technical
4
 half
assini
senior
high
school
5
annor
adjaye
senior
high
school
-
6
 accra
high
senior
high
school
–
accra
7
 st
mary's
senior
high
school
(ghana)
8
st
stephen’s
r/c
-
9
prince
of
peace
-
10 
star
of
the
sea
r/c
-
11 
mataheko
r/c
-
12 
st
kizito
-
13 
bennett
caulley
-
logic
logic
(from
the
)
is
the
systematic
study
of
the
form
of
valid
inference
and
the
most
general
laws
of
truth
a
valid
inference
is
one
where
there
is
a
specific
relation
of
logical
support
between
the
assumptions
of
the
inference
and
its
conclusion
in
ordinary
discourse
inferences
may
be
signified
by
words
such
as
"therefore"
"hence"
"ergo"
and
so
on
there
is
no
universal
agreement
as
to
the
exact
scope
and
subject
matter
of
logic
(see
below)
but
it
has
traditionally
included
the
classification
of
arguments
the
systematic
exposition
of
the
'logical
form'
common
to
all
valid
arguments
the
study
of
proof
and
inference
including
paradoxes
and
fallacies
and
the
study
of
syntax
and
semantics
historically
logic
has
been
studied
in
philosophy
(since
ancient
times)
and
mathematics
(since
the
mid-19th
century)
and
recently
logic
has
been
studied
in
computer
science
linguistics
psychology
and
other
fields
the
concept
of
logical
form
is
central
to
logic
the
validity
of
an
argument
is
determined
by
its
logical
form
not
by
its
content
traditional
aristotelian
syllogistic
logic
and
modern
symbolic
logic
are
examples
of
formal
logic
however
agreement
on
what
logic
is
has
remained
elusive
and
although
the
field
of
universal
logic
has
studied
the
common
structure
of
logics
in
2007
mossakowski
et
al
commented
that
"it
is
embarrassing
that
there
is
no
widely
acceptable
formal
definition
of
'a
logic'"
logic
is
generally
considered
formal
when
it
analyzes
and
represents
the
"form"
of
any
valid
argument
type
the
form
of
an
argument
is
displayed
by
representing
its
sentences
in
the
formal
grammar
and
symbolism
of
a
logical
language
to
make
its
content
usable
in
formal
inference
simply
put
to
formalize
simply
means
to
translate
english
sentences
into
the
language
of
logic
this
is
called
showing
the
"logical
form"
of
the
argument
it
is
necessary
because
indicative
sentences
of
ordinary
language
show
a
considerable
variety
of
form
and
complexity
that
makes
their
use
in
inference
impractical
it
requires
first
ignoring
those
grammatical
features
irrelevant
to
logic
(such
as
gender
and
declension
if
the
argument
is
in
latin)
replacing
conjunctions
irrelevant
to
logic
(such
as
"but")
with
logical
conjunctions
like
"and"
and
replacing
ambiguous
or
alternative
logical
expressions
("any"
"every"
etc)
with
expressions
of
a
standard
type
(such
as
"all"
or
the
universal
quantifier
∀)
second
certain
parts
of
the
sentence
must
be
replaced
with
schematic
letters
thus
for
example
the
expression
"all
ps
are
qs"
shows
the
logical
form
common
to
the
sentences
"all
men
are
mortals"
"all
cats
are
carnivores"
"all
greeks
are
philosophers"
and
so
on
the
schema
can
further
be
condensed
into
the
formula
"a(pq)"
where
the
letter
"a"
indicates
the
judgement
'all
-
are
-'
the
importance
of
form
was
recognised
from
ancient
times
aristotle
uses
variable
letters
to
represent
valid
inferences
in
"prior
analytics"
leading
jan
łukasiewicz
to
say
that
the
introduction
of
variables
was
"one
of
aristotle's
greatest
inventions"
according
to
the
followers
of
aristotle
(such
as
ammonius)
only
the
logical
principles
stated
in
schematic
terms
belong
to
logic
not
those
given
in
concrete
terms
the
concrete
terms
"man"
"mortal"
etc
are
analogous
to
the
substitution
values
of
the
schematic
placeholders
"p"
"q"
"r"
which
were
called
the
"matter"
(greek
"hyle")
of
the
inference
there
is
a
big
difference
between
the
kinds
of
formulas
seen
in
traditional
term
logic
and
the
predicate
calculus
that
is
the
fundamental
advance
of
modern
logic
the
formula
"a(pq)"
(all
ps
are
qs)
of
traditional
logic
corresponds
to
the
more
complex
formula
formula_1
in
predicate
logic
involving
the
logical
connectives
for
universal
quantification
and
implication
rather
than
just
the
predicate
letter
"a"
and
using
variable
arguments
formula_2
where
traditional
logic
uses
just
the
term
letter
"p"
with
the
complexity
comes
power
and
the
advent
of
the
predicate
calculus
inaugurated
revolutionary
growth
of
the
subject
the
validity
of
an
argument
depends
upon
the
meaning
or
"semantics"
of
the
sentences
that
make
it
up
aristotle's
organon
especially
"on
interpretation"
gives
a
cursory
outline
of
semantics
which
the
scholastic
logicians
particularly
in
the
thirteenth
and
fourteenth
century
developed
into
a
complex
and
sophisticated
theory
called
supposition
theory
this
showed
how
the
truth
of
simple
sentences
expressed
schematically
depend
on
how
the
terms
'supposit'
or
"stand
for"
certain
extra-linguistic
items
for
example
in
part
ii
of
his
summa
logicae
william
of
ockham
presents
a
comprehensive
account
of
the
necessary
and
sufficient
conditions
for
the
truth
of
simple
sentences
in
order
to
show
which
arguments
are
valid
and
which
are
not
thus
"every
a
is
b'
is
true
if
and
only
if
there
is
something
for
which
'a'
stands
and
there
is
nothing
for
which
'a'
stands
for
which
'b'
does
not
also
stand"
early
modern
logic
defined
semantics
purely
as
a
relation
between
ideas
antoine
arnauld
in
the
port
royal
logic
says
that
'after
conceiving
things
by
our
ideas
we
compare
these
ideas
and
finding
that
some
belong
together
and
some
do
not
we
unite
or
separate
them
this
is
called
"affirming"
or
"denying"
and
in
general
"judging"
thus
truth
and
falsity
are
no
more
than
the
agreement
or
disagreement
of
ideas
this
suggests
obvious
difficulties
leading
locke
to
distinguish
between
'real'
truth
when
our
ideas
have
'real
existence'
and
'imaginary'
or
'verbal'
truth
where
ideas
like
harpies
or
centaurs
exist
only
in
the
mind
this
view
(psychologism)
was
taken
to
the
extreme
in
the
nineteenth
century
and
is
generally
held
by
modern
logicians
to
signify
a
low
point
in
the
decline
of
logic
before
the
twentieth
century
modern
semantics
is
in
some
ways
closer
to
the
medieval
view
in
rejecting
such
psychological
truth-conditions
however
the
introduction
of
quantification
needed
to
solve
the
problem
of
multiple
generality
rendered
impossible
the
kind
of
subject-predicate
analysis
that
underlies
medieval
semantics
the
main
modern
approach
is
"model-theoretic
semantics"
based
on
alfred
tarski's
semantic
theory
of
truth
the
approach
assumes
that
the
meaning
of
the
various
parts
of
the
propositions
are
given
by
the
possible
ways
we
can
give
a
recursively
specified
group
of
interpretation
functions
from
them
to
some
predefined
domain
of
discourse:
an
interpretation
of
first-order
predicate
logic
is
given
by
a
mapping
from
terms
to
a
universe
of
individuals
and
a
mapping
from
propositions
to
the
truth
values
"true"
and
"false"
model-theoretic
semantics
is
one
of
the
fundamental
concepts
of
model
theory
modern
semantics
also
admits
rival
approaches
such
as
the
proof-theoretic
semantics
that
associates
the
meaning
of
propositions
with
the
roles
that
they
can
play
in
inferences
an
approach
that
ultimately
derives
from
the
work
of
gerhard
gentzen
on
structural
proof
theory
and
is
heavily
influenced
by
ludwig
wittgenstein's
later
philosophy
especially
his
aphorism
"meaning
is
use"
"inference"
is
not
to
be
confused
with
"implication"
an
implication
is
a
sentence
of
the
form
'if
p
then
q'
and
can
be
true
or
false
the
stoic
logician
philo
of
megara
was
the
first
to
define
the
truth
conditions
of
such
an
implication:
false
only
when
the
antecedent
p
is
true
and
the
consequent
q
is
false
in
all
other
cases
true
an
inference
on
the
other
hand
consists
of
two
separately
asserted
propositions
of
the
form
'p
therefore
q'
an
inference
is
not
true
or
false
but
valid
or
invalid
however
there
is
a
connection
between
implication
and
inference
as
follows:
if
the
implication
'if
p
then
q'
is
"true"
the
inference
'p
therefore
q'
is
"valid"
this
was
given
an
apparently
paradoxical
formulation
by
philo
who
said
that
the
implication
'if
it
is
day
it
is
night'
is
true
only
at
night
so
the
inference
'it
is
day
therefore
it
is
night'
is
valid
in
the
night
but
not
in
the
day
the
theory
of
inference
(or
'consequences')
was
systematically
developed
in
medieval
times
by
logicians
such
as
william
of
ockham
and
walter
burley
it
is
uniquely
medieval
though
it
has
its
origins
in
aristotle's
topics
and
boethius'
"de
syllogismis
hypotheticis"
this
is
why
many
terms
in
logic
are
latin
for
example
the
rule
that
licenses
the
move
from
the
implication
'if
p
then
q'
plus
the
assertion
of
its
antecedent
p
to
the
assertion
of
the
consequent
q
is
known
as
modus
ponens
(or
'mode
of
positing')
its
latin
formulation
is
'posito
antecedente
ponitur
consequens'
the
latin
formulations
of
many
other
rules
such
as
'ex
falso
quodlibet'
(anything
follows
from
a
falsehood)
'reductio
ad
absurdum'
(disproof
by
showing
the
consequence
is
absurd)
also
date
from
this
period
however
the
theory
of
consequences
or
of
the
so-called
'hypothetical
syllogism'
was
never
fully
integrated
into
the
theory
of
the
'categorical
syllogism'
this
was
partly
because
of
the
resistance
to
reducing
the
categorical
judgment
'every
s
is
p'
to
the
so-called
hypothetical
judgment
'if
anything
is
s
it
is
p'
the
first
was
thought
to
imply
'some
s
is
p'
the
second
was
not
and
as
late
as
1911
in
the
encyclopædia
britannica
article
on
logic
we
find
the
oxford
logician
th
case
arguing
against
sigwart's
and
brentano's
modern
analysis
of
the
universal
proposition
a
formal
system
is
an
organization
of
terms
used
for
the
analysis
of
deduction
it
consists
of
an
alphabet
a
language
over
the
alphabet
to
construct
sentences
and
a
rule
for
deriving
sentences
among
the
important
properties
that
logical
systems
can
have
are:
some
logical
systems
do
not
have
all
four
properties
as
an
example
kurt
gödel's
incompleteness
theorems
show
that
sufficiently
complex
formal
systems
of
arithmetic
cannot
be
consistent
and
complete;
however
first-order
predicate
logics
not
extended
by
specific
axioms
to
be
arithmetic
formal
systems
with
equality
can
be
complete
and
consistent
as
the
study
of
argument
is
of
clear
importance
to
the
reasons
that
we
hold
things
to
be
true
logic
is
of
essential
importance
to
rationality
here
we
have
defined
logic
to
be
"the
systematic
study
of
the
form
of
arguments";
the
reasoning
behind
argument
is
of
several
sorts
but
only
some
of
these
arguments
fall
under
the
aegis
of
logic
proper
deductive
reasoning
concerns
the
logical
consequence
of
given
premises
and
is
the
form
of
reasoning
most
closely
connected
to
logic
on
a
narrow
conception
of
logic
(see
below)
logic
concerns
just
deductive
reasoning
although
such
a
narrow
conception
controversially
excludes
most
of
what
is
called
informal
logic
from
the
discipline
there
are
other
forms
of
reasoning
that
are
rational
but
that
are
generally
not
taken
to
be
part
of
logic
these
include
inductive
reasoning
which
covers
forms
of
inference
that
move
from
collections
of
particular
judgements
to
universal
judgements
and
abductive
reasoning
which
is
a
form
of
inference
that
goes
from
observation
to
a
hypothesis
that
accounts
for
the
reliable
data
(observation)
and
seeks
to
explain
relevant
evidence
the
american
philosopher
charles
sanders
peirce
(1839–1914)
first
introduced
the
term
as
"guessing"
peirce
said
that
to
"abduce"
a
hypothetical
explanation
formula_3
from
an
observed
surprising
circumstance
formula_4
is
to
surmise
that
formula_3
may
be
true
because
then
formula_4
would
be
a
matter
of
course
thus
to
abduce
formula_3
from
formula_4
involves
determining
that
formula_3
is
sufficient
(or
nearly
sufficient)
but
not
necessary
for
formula_4
while
inductive
and
abductive
inference
are
not
part
of
logic
proper
the
methodology
of
logic
has
been
applied
to
them
with
some
degree
of
success
for
example
the
notion
of
deductive
validity
(where
an
inference
is
deductively
valid
if
and
only
if
there
is
no
possible
situation
in
which
all
the
premises
are
true
but
the
conclusion
false)
exists
in
an
analogy
to
the
notion
of
inductive
validity
or
"strength"
where
an
inference
is
inductively
strong
if
and
only
if
its
premises
give
some
degree
of
probability
to
its
conclusion
whereas
the
notion
of
deductive
validity
can
be
rigorously
stated
for
systems
of
formal
logic
in
terms
of
the
well-understood
notions
of
semantics
inductive
validity
requires
us
to
define
a
reliable
generalization
of
some
set
of
observations
the
task
of
providing
this
definition
may
be
approached
in
various
ways
some
less
formal
than
others;
some
of
these
definitions
may
use
logical
association
rule
induction
while
others
may
use
mathematical
models
of
probability
such
as
decision
trees
logic
arose
(see
below)
from
a
concern
with
correctness
of
argumentation
modern
logicians
usually
wish
to
ensure
that
logic
studies
just
those
arguments
that
arise
from
appropriately
general
forms
of
inference
for
example
thomas
hofweber
writes
in
the
"stanford
encyclopedia
of
philosophy"
that
logic
"does
not
however
cover
good
reasoning
as
a
whole
that
is
the
job
of
the
theory
of
rationality
rather
it
deals
with
inferences
whose
validity
can
be
traced
back
to
the
formal
features
of
the
representations
that
are
involved
in
that
inference
be
they
linguistic
mental
or
other
representations"
logic
has
been
defined
as
"the
study
of
arguments
correct
in
virtue
of
their
form"
this
has
not
been
the
definition
taken
in
this
article
but
the
idea
that
logic
treats
special
forms
of
argument
deductive
argument
rather
than
argument
in
general
has
a
history
in
logic
that
dates
back
at
least
to
logicism
in
mathematics
(19th
and
20th
centuries)
and
the
advent
of
the
influence
of
mathematical
logic
on
philosophy
a
consequence
of
taking
logic
to
treat
special
kinds
of
argument
is
that
it
leads
to
identification
of
special
kinds
of
truth
the
logical
truths
(with
logic
equivalently
being
the
study
of
logical
truth)
and
excludes
many
of
the
original
objects
of
study
of
logic
that
are
treated
as
informal
logic
robert
brandom
has
argued
against
the
idea
that
logic
is
the
study
of
a
special
kind
of
logical
truth
arguing
that
instead
one
can
talk
of
the
logic
of
material
inference
(in
the
terminology
of
wilfred
sellars)
with
logic
making
explicit
the
commitments
that
were
originally
implicit
in
informal
inference
logic
comes
from
the
greek
word
"logos"
originally
meaning
"the
word"
or
"what
is
spoken"
but
coming
to
mean
"thought"
or
"reason"
in
the
western
world
logic
was
first
developed
by
aristotle
who
called
the
subject
'analytics'
aristotelian
logic
became
widely
accepted
in
science
and
mathematics
and
remained
in
wide
use
in
the
west
until
the
early
19th century
aristotle's
system
of
logic
was
responsible
for
the
introduction
of
hypothetical
syllogism
temporal
modal
logic
and
inductive
logic
as
well
as
influential
vocabulary
such
as
terms
predicables
syllogisms
and
propositions
there
was
also
the
rival
stoic
logic
in
europe
during
the
later
medieval
period
major
efforts
were
made
to
show
that
aristotle's
ideas
were
compatible
with
christian
faith
during
the
high
middle
ages
logic
became
a
main
focus
of
philosophers
who
would
engage
in
critical
logical
analyses
of
philosophical
arguments
often
using
variations
of
the
methodology
of
scholasticism
in
1323
william
of
ockham's
influential
"summa
logicae"
was
released
by
the
18th
century
the
structured
approach
to
arguments
had
degenerated
and
fallen
out
of
favour
as
depicted
in
holberg's
satirical
play
"erasmus
montanus"
the
chinese
logical
philosopher
gongsun
long
()
proposed
the
paradox
"one
and
one
cannot
become
two
since
neither
becomes
two"
in
china
the
tradition
of
scholarly
investigation
into
logic
however
was
repressed
by
the
qin
dynasty
following
the
legalist
philosophy
of
han
feizi
in
india
the
anviksiki
school
of
logic
was
founded
by
medhatithi
gautama
(c
6th
century
bce)
innovations
in
the
scholastic
school
called
nyaya
continued
from
ancient
times
into
the
early
18th century
with
the
navya-nyaya
school
by
the
16th century
it
developed
theories
resembling
modern
logic
such
as
gottlob
frege's
"distinction
between
sense
and
reference
of
proper
names"
and
his
"definition
of
number"
as
well
as
the
theory
of
"restrictive
conditions
for
universals"
anticipating
some
of
the
developments
in
modern
set
theory
since
1824
indian
logic
attracted
the
attention
of
many
western
scholars
and
has
had
an
influence
on
important
19th-century
logicians
such
as
charles
babbage
augustus
de
morgan
and
george
boole
in
the
20th century
western
philosophers
like
stanislaw
schayer
and
klaus
glashoff
have
explored
indian
logic
more
extensively
the
syllogistic
logic
developed
by
aristotle
predominated
in
the
west
until
the
mid-19th century
when
interest
in
the
foundations
of
mathematics
stimulated
the
development
of
symbolic
logic
(now
called
mathematical
logic)
in
1854
george
boole
published
"an
investigation
of
the
laws
of
thought
on
which
are
founded
the
mathematical
theories
of
logic
and
probabilities"
introducing
symbolic
logic
and
the
principles
of
what
is
now
known
as
boolean
logic
in
1879
gottlob
frege
published
"begriffsschrift"
which
inaugurated
modern
logic
with
the
invention
of
quantifier
notation
reconciling
the
aristotelian
and
stoic
logics
in
a
broader
system
and
solving
such
problems
for
which
aristotelian
logic
was
impotent
such
as
the
problem
of
multiple
generality
from
1910
to
1913
alfred
north
whitehead
and
bertrand
russell
published
"principia
mathematica"
on
the
foundations
of
mathematics
attempting
to
derive
mathematical
truths
from
axioms
and
inference
rules
in
symbolic
logic
in
1931
gödel
raised
serious
problems
with
the
foundationalist
program
and
logic
ceased
to
focus
on
such
issues
the
development
of
logic
since
frege
russell
and
wittgenstein
had
a
profound
influence
on
the
practice
of
philosophy
and
the
perceived
nature
of
philosophical
problems
(see
analytic
philosophy)
and
philosophy
of
mathematics
logic
especially
sentential
logic
is
implemented
in
computer
logic
circuits
and
is
fundamental
to
computer
science
logic
is
commonly
taught
by
university
philosophy
departments
often
as
a
compulsory
discipline
the
"organon"
was
aristotle's
body
of
work
on
logic
with
the
"prior
analytics"
constituting
the
first
explicit
work
in
formal
logic
introducing
the
syllogistic
the
parts
of
syllogistic
logic
also
known
by
the
name
term
logic
are
the
analysis
of
the
judgements
into
propositions
consisting
of
two
terms
that
are
related
by
one
of
a
fixed
number
of
relations
and
the
expression
of
inferences
by
means
of
syllogisms
that
consist
of
two
propositions
sharing
a
common
term
as
premise
and
a
conclusion
that
is
a
proposition
involving
the
two
unrelated
terms
from
the
premises
aristotle's
work
was
regarded
in
classical
times
and
from
medieval
times
in
europe
and
the
middle
east
as
the
very
picture
of
a
fully
worked
out
system
however
it
was
not
alone:
the
stoics
proposed
a
system
of
propositional
logic
that
was
studied
by
medieval
logicians
also
the
problem
of
multiple
generality
was
recognized
in
medieval
times
nonetheless
problems
with
syllogistic
logic
were
not
seen
as
being
in
need
of
revolutionary
solutions
today
some
academics
claim
that
aristotle's
system
is
generally
seen
as
having
little
more
than
historical
value
(though
there
is
some
current
interest
in
extending
term
logics)
regarded
as
made
obsolete
by
the
advent
of
propositional
logic
and
the
predicate
calculus
others
use
aristotle
in
argumentation
theory
to
help
develop
and
critically
question
argumentation
schemes
that
are
used
in
artificial
intelligence
and
legal
arguments
a
propositional
calculus
or
logic
(also
a
sentential
calculus)
is
a
formal
system
in
which
formulae
representing
propositions
can
be
formed
by
combining
atomic
propositions
using
logical
connectives
and
in
which
a
system
of
formal
proof
rules
establishes
certain
formulae
as
"theorems"
an
example
of
a
theorem
of
propositional
logic
is
formula_11
which
says
that
if
a
holds
then
b
implies
a
predicate
logic
is
the
generic
term
for
symbolic
formal
systems
such
as
first-order
logic
second-order
logic
many-sorted
logic
and
infinitary
logic
it
provides
an
account
of
quantifiers
general
enough
to
express
a
wide
set
of
arguments
occurring
in
natural
language
for
example
bertrand
russell's
famous
barber
paradox
"there
is
a
man
who
shaves
all
and
only
men
who
do
not
shave
themselves"
can
be
formalised
by
the
sentence
formula_12
using
the
non-logical
predicate
formula_13
to
indicate
that
"x"
is
a
man
and
the
non-logical
relation
formula_14
to
indicate
that
"x"
shaves
"y";
all
other
symbols
of
the
formulae
are
logical
expressing
the
universal
and
existential
quantifiers
conjunction
implication
negation
and
biconditional
whilst
aristotelian
syllogistic
logic
specifies
a
small
number
of
forms
that
the
relevant
part
of
the
involved
judgements
may
take
predicate
logic
allows
sentences
to
be
analysed
into
subject
and
argument
in
several
additional
ways—allowing
predicate
logic
to
solve
the
problem
of
multiple
generality
that
had
perplexed
medieval
logicians
the
development
of
predicate
logic
is
usually
attributed
to
gottlob
frege
who
is
also
credited
as
one
of
the
founders
of
analytical
philosophy
but
the
formulation
of
predicate
logic
most
often
used
today
is
the
first-order
logic
presented
in
principles
of
mathematical
logic
by
david
hilbert
and
wilhelm
ackermann
in
1928
the
analytical
generality
of
predicate
logic
allowed
the
formalization
of
mathematics
drove
the
investigation
of
set
theory
and
allowed
the
development
of
alfred
tarski's
approach
to
model
theory
it
provides
the
foundation
of
modern
mathematical
logic
frege's
original
system
of
predicate
logic
was
second-order
rather
than
first-order
second-order
logic
is
most
prominently
defended
(against
the
criticism
of
willard
van
orman
quine
and
others)
by
george
boolos
and
stewart
shapiro
in
languages
modality
deals
with
the
phenomenon
that
sub-parts
of
a
sentence
may
have
their
semantics
modified
by
special
verbs
or
modal
particles
for
example
""we
go
to
the
games"
can
be
modified
to
give
"we
should
go
to
the
games"
and
"we
can
go
to
the
games"
and
perhaps
"we
will
go
to
the
games""
more
abstractly
we
might
say
that
modality
affects
the
circumstances
in
which
we
take
an
assertion
to
be
satisfied
confusing
modality
is
known
as
the
modal
fallacy
aristotle's
logic
is
in
large
parts
concerned
with
the
theory
of
non-modalized
logic
although
there
are
passages
in
his
work
such
as
the
famous
sea-battle
argument
in
"de
interpretatione"
§
9
that
are
now
seen
as
anticipations
of
modal
logic
and
its
connection
with
potentiality
and
time
the
earliest
formal
system
of
modal
logic
was
developed
by
avicenna
who
ultimately
developed
a
theory
of
"temporally
modalized"
syllogistic
while
the
study
of
necessity
and
possibility
remained
important
to
philosophers
little
logical
innovation
happened
until
the
landmark
investigations
of
clarence
irving
lewis
in
1918
who
formulated
a
family
of
rival
axiomatizations
of
the
alethic
modalities
his
work
unleashed
a
torrent
of
new
work
on
the
topic
expanding
the
kinds
of
modality
treated
to
include
deontic
logic
and
epistemic
logic
the
seminal
work
of
arthur
prior
applied
the
same
formal
language
to
treat
temporal
logic
and
paved
the
way
for
the
marriage
of
the
two
subjects
saul
kripke
discovered
(contemporaneously
with
rivals)
his
theory
of
frame
semantics
which
revolutionized
the
formal
technology
available
to
modal
logicians
and
gave
a
new
graph-theoretic
way
of
looking
at
modality
that
has
driven
many
applications
in
computational
linguistics
and
computer
science
such
as
dynamic
logic
the
motivation
for
the
study
of
logic
in
ancient
times
was
clear:
it
is
so
that
one
may
learn
to
distinguish
good
arguments
from
bad
arguments
and
so
become
more
effective
in
argument
and
oratory
and
perhaps
also
to
become
a
better
person
half
of
the
works
of
aristotle's
organon
treat
inference
as
it
occurs
in
an
informal
setting
side
by
side
with
the
development
of
the
syllogistic
and
in
the
aristotelian
school
these
informal
works
on
logic
were
seen
as
complementary
to
aristotle's
treatment
of
rhetoric
this
ancient
motivation
is
still
alive
although
it
no
longer
takes
centre
stage
in
the
picture
of
logic;
typically
dialectical
logic
forms
the
heart
of
a
course
in
critical
thinking
a
compulsory
course
at
many
universities
dialectic
has
been
linked
to
logic
since
ancient
times
but
it
has
not
been
until
recent
decades
that
european
and
american
logicians
have
attempted
to
provide
mathematical
foundations
for
logic
and
dialectic
by
formalising
dialectical
logic
dialectical
logic
is
also
the
name
given
to
the
special
treatment
of
dialectic
in
hegelian
and
marxist
thought
there
have
been
pre-formal
treatises
on
argument
and
dialectic
from
authors
such
as
stephen
toulmin
("the
uses
of
argument")
nicholas
rescher
("dialectics")
and
van
eemeren
and
grootendorst
(pragma-dialectics)
theories
of
defeasible
reasoning
can
provide
a
foundation
for
the
formalisation
of
dialectical
logic
and
dialectic
itself
can
be
formalised
as
moves
in
a
game
where
an
advocate
for
the
truth
of
a
proposition
and
an
opponent
argue
such
games
can
provide
a
formal
game
semantics
for
many
logics
argumentation
theory
is
the
study
and
research
of
informal
logic
fallacies
and
critical
questions
as
they
relate
to
every
day
and
practical
situations
specific
types
of
dialogue
can
be
analyzed
and
questioned
to
reveal
premises
conclusions
and
fallacies
argumentation
theory
is
now
applied
in
artificial
intelligence
and
law
mathematical
logic
comprises
two
distinct
areas
of
research:
the
first
is
the
application
of
the
techniques
of
formal
logic
to
mathematics
and
mathematical
reasoning
and
the
second
in
the
other
direction
the
application
of
mathematical
techniques
to
the
representation
and
analysis
of
formal
logic
the
earliest
use
of
mathematics
and
geometry
in
relation
to
logic
and
philosophy
goes
back
to
the
ancient
greeks
such
as
euclid
plato
and
aristotle
many
other
ancient
and
medieval
philosophers
applied
mathematical
ideas
and
methods
to
their
philosophical
claims
one
of
the
boldest
attempts
to
apply
logic
to
mathematics
was
the
logicism
pioneered
by
philosopher-logicians
such
as
gottlob
frege
and
bertrand
russell
mathematical
theories
were
supposed
to
be
logical
tautologies
and
the
programme
was
to
show
this
by
means
of
a
reduction
of
mathematics
to
logic
the
various
attempts
to
carry
this
out
met
with
failure
from
the
crippling
of
frege's
project
in
his
"grundgesetze"
by
russell's
paradox
to
the
defeat
of
hilbert's
program
by
gödel's
incompleteness
theorems
both
the
statement
of
hilbert's
program
and
its
refutation
by
gödel
depended
upon
their
work
establishing
the
second
area
of
mathematical
logic
the
application
of
mathematics
to
logic
in
the
form
of
proof
theory
despite
the
negative
nature
of
the
incompleteness
theorems
gödel's
completeness
theorem
a
result
in
model
theory
and
another
application
of
mathematics
to
logic
can
be
understood
as
showing
how
close
logicism
came
to
being
true:
every
rigorously
defined
mathematical
theory
can
be
exactly
captured
by
a
first-order
logical
theory;
frege's
proof
calculus
is
enough
to
"describe"
the
whole
of
mathematics
though
not
"equivalent"
to
it
if
proof
theory
and
model
theory
have
been
the
foundation
of
mathematical
logic
they
have
been
but
two
of
the
four
pillars
of
the
subject
set
theory
originated
in
the
study
of
the
infinite
by
georg
cantor
and
it
has
been
the
source
of
many
of
the
most
challenging
and
important
issues
in
mathematical
logic
from
cantor's
theorem
through
the
status
of
the
axiom
of
choice
and
the
question
of
the
independence
of
the
continuum
hypothesis
to
the
modern
debate
on
large
cardinal
axioms
recursion
theory
captures
the
idea
of
computation
in
logical
and
arithmetic
terms;
its
most
classical
achievements
are
the
undecidability
of
the
entscheidungsproblem
by
alan
turing
and
his
presentation
of
the
church–turing
thesis
today
recursion
theory
is
mostly
concerned
with
the
more
refined
problem
of
complexity
classes—when
is
a
problem
efficiently
solvable?—and
the
classification
of
degrees
of
unsolvability
philosophical
logic
deals
with
formal
descriptions
of
ordinary
non-specialist
("natural")
language
that
is
strictly
only
about
the
arguments
within
philosophy's
other
branches
most
philosophers
assume
that
the
bulk
of
everyday
reasoning
can
be
captured
in
logic
if
a
method
or
methods
to
translate
ordinary
language
into
that
logic
can
be
found
philosophical
logic
is
essentially
a
continuation
of
the
traditional
discipline
called
"logic"
before
the
invention
of
mathematical
logic
philosophical
logic
has
a
much
greater
concern
with
the
connection
between
natural
language
and
logic
as
a
result
philosophical
logicians
have
contributed
a
great
deal
to
the
development
of
non-standard
logics
(eg
free
logics
tense
logics)
as
well
as
various
extensions
of
classical
logic
(eg
modal
logics)
and
non-standard
semantics
for
such
logics
(eg
kripke's
supervaluationism
in
the
semantics
of
logic)
logic
and
the
philosophy
of
language
are
closely
related
philosophy
of
language
has
to
do
with
the
study
of
how
our
language
engages
and
interacts
with
our
thinking
logic
has
an
immediate
impact
on
other
areas
of
study
studying
logic
and
the
relationship
between
logic
and
ordinary
speech
can
help
a
person
better
structure
his
own
arguments
and
critique
the
arguments
of
others
many
popular
arguments
are
filled
with
errors
because
so
many
people
are
untrained
in
logic
and
unaware
of
how
to
formulate
an
argument
correctly
logic
cut
to
the
heart
of
computer
science
as
it
emerged
as
a
discipline:
alan
turing's
work
on
the
"entscheidungsproblem"
followed
from
kurt
gödel's
work
on
the
incompleteness
theorems
the
notion
of
the
general
purpose
computer
that
came
from
this
work
was
of
fundamental
importance
to
the
designers
of
the
computer
machinery
in
the
1940s
in
the
1950s
and
1960s
researchers
predicted
that
when
human
knowledge
could
be
expressed
using
logic
with
mathematical
notation
it
would
be
possible
to
create
a
machine
that
reasons
or
artificial
intelligence
this
was
more
difficult
than
expected
because
of
the
complexity
of
human
reasoning
in
logic
programming
a
program
consists
of
a
set
of
axioms
and
rules
logic
programming
systems
such
as
prolog
compute
the
consequences
of
the
axioms
and
rules
in
order
to
answer
a
query
today
logic
is
extensively
applied
in
the
fields
of
artificial
intelligence
and
computer
science
and
these
fields
provide
a
rich
source
of
problems
in
formal
and
informal
logic
argumentation
theory
is
one
good
example
of
how
logic
is
being
applied
to
artificial
intelligence
the
acm
computing
classification
system
in
particular
regards:
furthermore
computers
can
be
used
as
tools
for
logicians
for
example
in
symbolic
logic
and
mathematical
logic
proofs
by
humans
can
be
computer-assisted
using
automated
theorem
proving
the
machines
can
find
and
check
proofs
as
well
as
work
with
proofs
too
lengthy
to
write
out
by
hand
the
logics
discussed
above
are
all
"bivalent"
or
"two-valued";
that
is
they
are
most
naturally
understood
as
dividing
propositions
into
true
and
false
propositions
non-classical
logics
are
those
systems
that
reject
various
rules
of
classical
logic
hegel
developed
his
own
dialectic
logic
that
extended
kant's
transcendental
logic
but
also
brought
it
back
to
ground
by
assuring
us
that
"neither
in
heaven
nor
in
earth
neither
in
the
world
of
mind
nor
of
nature
is
there
anywhere
such
an
abstract
'either–or'
as
the
understanding
maintains
whatever
exists
is
concrete
with
difference
and
opposition
in
itself"
in
1910
nicolai
a
vasiliev
extended
the
law
of
excluded
middle
and
the
law
of
contradiction
and
proposed
the
law
of
excluded
fourth
and
logic
tolerant
to
contradiction
in
the
early
20th century
jan
łukasiewicz
investigated
the
extension
of
the
traditional
true/false
values
to
include
a
third
value
"possible"
so
inventing
ternary
logic
the
first
multi-valued
logic
in
the
western
tradition
logics
such
as
fuzzy
logic
have
since
been
devised
with
an
infinite
number
of
"degrees
of
truth"
represented
by
a
real
number
between
0
and
1
intuitionistic
logic
was
proposed
by
lej
brouwer
as
the
correct
logic
for
reasoning
about
mathematics
based
upon
his
rejection
of
the
law
of
the
excluded
middle
as
part
of
his
intuitionism
brouwer
rejected
formalization
in
mathematics
but
his
student
arend
heyting
studied
intuitionistic
logic
formally
as
did
gerhard
gentzen
intuitionistic
logic
is
of
great
interest
to
computer
scientists
as
it
is
a
constructive
logic
and
sees
many
applications
such
as
extracting
verified
programs
from
proofs
and
influencing
the
design
of
programming
languages
through
the
formulae-as-types
correspondence
modal
logic
is
not
truth
conditional
and
so
it
has
often
been
proposed
as
a
non-classical
logic
however
modal
logic
is
normally
formalized
with
the
principle
of
the
excluded
middle
and
its
relational
semantics
is
bivalent
so
this
inclusion
is
disputable
what
is
the
epistemological
status
of
the
laws
of
logic?
what
sort
of
argument
is
appropriate
for
criticizing
purported
principles
of
logic?
in
an
influential
paper
entitled
"is
logic
empirical?"
hilary
putnam
building
on
a
suggestion
of
w
v
quine
argued
that
in
general
the
facts
of
propositional
logic
have
a
similar
epistemological
status
as
facts
about
the
physical
universe
for
example
as
the
laws
of
mechanics
or
of
general
relativity
and
in
particular
that
what
physicists
have
learned
about
quantum
mechanics
provides
a
compelling
case
for
abandoning
certain
familiar
principles
of
classical
logic:
if
we
want
to
be
realists
about
the
physical
phenomena
described
by
quantum
theory
then
we
should
abandon
the
principle
of
distributivity
substituting
for
classical
logic
the
quantum
logic
proposed
by
garrett
birkhoff
and
john
von
neumann
another
paper
of
the
same
name
by
michael
dummett
argues
that
putnam's
desire
for
realism
mandates
the
law
of
distributivity
distributivity
of
logic
is
essential
for
the
realist's
understanding
of
how
propositions
are
true
of
the
world
in
just
the
same
way
as
he
has
argued
the
principle
of
bivalence
is
in
this
way
the
question
"is
logic
empirical?"
can
be
seen
to
lead
naturally
into
the
fundamental
controversy
in
metaphysics
on
realism
versus
anti-realism
the
notion
of
implication
formalized
in
classical
logic
does
not
comfortably
translate
into
natural
language
by
means
of
"if 
then "
due
to
a
number
of
problems
called
the
paradoxes
of
material
implication
the
first
class
of
paradoxes
involves
counterfactuals
such
as
"if
the
moon
is
made
of
green
cheese
then
2+2=5"
which
are
puzzling
because
natural
language
does
not
support
the
principle
of
explosion
eliminating
this
class
of
paradoxes
was
the
reason
for
ci
lewis's
formulation
of
strict
implication
which
eventually
led
to
more
radically
revisionist
logics
such
as
relevance
logic
the
second
class
of
paradoxes
involves
redundant
premises
falsely
suggesting
that
we
know
the
succedent
because
of
the
antecedent:
thus
"if
that
man
gets
elected
granny
will
die"
is
materially
true
since
granny
is
mortal
regardless
of
the
man's
election
prospects
such
sentences
violate
the
gricean
maxim
of
relevance
and
can
be
modelled
by
logics
that
reject
the
principle
of
monotonicity
of
entailment
such
as
relevance
logic
hegel
was
deeply
critical
of
any
simplified
notion
of
the
law
of
non-contradiction
it
was
based
on
gottfried
wilhelm
leibniz's
idea
that
this
law
of
logic
also
requires
a
sufficient
ground
to
specify
from
what
point
of
view
(or
time)
one
says
that
something
cannot
contradict
itself
a
building
for
example
both
moves
and
does
not
move;
the
ground
for
the
first
is
our
solar
system
and
for
the
second
the
earth
in
hegelian
dialectic
the
law
of
non-contradiction
of
identity
itself
relies
upon
difference
and
so
is
not
independently
assertable
closely
related
to
questions
arising
from
the
paradoxes
of
implication
comes
the
suggestion
that
logic
ought
to
tolerate
inconsistency
relevance
logic
and
paraconsistent
logic
are
the
most
important
approaches
here
though
the
concerns
are
different:
a
key
consequence
of
classical
logic
and
some
of
its
rivals
such
as
intuitionistic
logic
is
that
they
respect
the
principle
of
explosion
which
means
that
the
logic
collapses
if
it
is
capable
of
deriving
a
contradiction
graham
priest
the
main
proponent
of
dialetheism
has
argued
for
paraconsistency
on
the
grounds
that
there
are
in
fact
true
contradictions
the
philosophical
vein
of
various
kinds
of
skepticism
contains
many
kinds
of
doubt
and
rejection
of
the
various
bases
on
which
logic
rests
such
as
the
idea
of
logical
form
correct
inference
or
meaning
typically
leading
to
the
conclusion
that
there
are
no
logical
truths
this
is
in
contrast
with
the
usual
views
in
philosophical
skepticism
where
logic
directs
skeptical
enquiry
to
doubt
received
wisdoms
as
in
the
work
of
sextus
empiricus
friedrich
nietzsche
provides
a
strong
example
of
the
rejection
of
the
usual
basis
of
logic:
his
radical
rejection
of
idealization
led
him
to
reject
truth
as
a
" mobile
army
of
metaphors
metonyms
and
anthropomorphisms—in
short 
metaphors
which
are
worn
out
and
without
sensuous
power;
coins
which
have
lost
their
pictures
and
now
matter
only
as
metal
no
longer
as
coins"
his
rejection
of
truth
did
not
lead
him
to
reject
the
idea
of
either
inference
or
logic
completely
but
rather
suggested
that
"logic
[came]
into
existence
in
man's
head
[out]
of
illogic
whose
realm
originally
must
have
been
immense
innumerable
beings
who
made
inferences
in
a
way
different
from
ours
perished"
thus
there
is
the
idea
that
logical
inference
has
a
use
as
a
tool
for
human
survival
but
that
its
existence
does
not
support
the
existence
of
truth
nor
does
it
have
a
reality
beyond
the
instrumental:
"logic
too
also
rests
on
assumptions
that
do
not
correspond
to
anything
in
the
real
world"
this
position
held
by
nietzsche
however
has
come
under
extreme
scrutiny
for
several
reasons
some
philosophers
such
as
jürgen
habermas
claim
his
position
is
self-refuting—and
accuse
nietzsche
of
not
even
having
a
coherent
perspective
let
alone
a
theory
of
knowledge
georg
lukács
in
his
book
"the
destruction
of
reason"
asserts
that
"were
we
to
study
nietzsche's
statements
in
this
area
from
a
logico-philosophical
angle
we
would
be
confronted
by
a
dizzy
chaos
of
the
most
lurid
assertions
arbitrary
and
violently
incompatible"
bertrand
russell
described
nietzsche's
irrational
claims
with
"he
is
fond
of
expressing
himself
paradoxically
and
with
a
view
to
shocking
conventional
readers"
in
his
book
"a
history
of
western
philosophy"
state
college
of
florida
manatee–sarasota
state
college
of
florida
manatee-sarasota
(scf)
is
a
state
college
with
campuses
located
in
manatee
and
sarasota
county
florida
part
of
the
florida
college
system
it
is
designated
a
"state
college"
because
it
offers
a
greater
number
of
four-year
bachelor's
degrees
than
traditional
two-year
community
colleges
founded
in
1957
as
manatee
junior
college
it
was
known
as
manatee
community
college
from
1985
to
2009
today
scf
operates
three
campuses
in
bradenton
lakewood
ranch
and
venice
the
bradenton
campus
includes
the
family
heritage
house
museum
the
scf
collegiate
school
(scfcs)
the
neel
performing
arts
center
and
the
scf
dental
hygiene
clinic
which
provides
low
cost
dental
care
to
the
public
state
college
of
florida
was
established
on
september
17
1957
by
the
florida
board
of
education
as
manatee
junior
college
the
college
came
into
existence
under
a
plan
of
the
florida
board
of
education
to
provide
accessible
higher
education
to
florida's
population
the
first
classes
were
held
on
september
2
1958
in
what
was
formerly
a
senior
high
school;
enrollment
in
the
first
term
was
502
students
the
college
began
administering
classes
in
its
own
facilities
in
1959
where
the
100
acre
bradenton
campus
stands
today
the
venice
center
was
opened
in
1977
by
mjc's
board
of
trustees
during
this
period
the
center's
functions
were
funded
by
the
donations
of
residents
living
in
the
surrounding
communities
which
included
venice
north
port
and
englewood
it
was
not
until
1983
that
the
college
received
an
appropriation
from
the
florida
legislature
to
expand
the
venice
center
into
what
is
now
the
full-service
venice
campus
it
was
dedicated
on
march
30
1985
and
the
college's
name
was
changed
that
year
to
manatee
community
college
at
the
beginning
of
2003
mcc
opened
the
lakewood
ranch
campus
the
land
appropriated
for
this
was
donated
by
the
schroeder-manatee
ranch
the
lakewood
ranch
campus
offers
credit
and
non-credit
programs
of
study
as
well
as
technical
and
workforce
development
courses
in
2007
the
schroeder-manatee
ranch
donated
an
additional
to
the
lakewood
ranch
campus
mcc
obtained
supplementary
funding
from
the
florida
legislature
which
was
allocated
for
the
construction
of
a
new
classroom/laboratory
building
in
2009
mcc
received
approval
from
the
state
board
of
education
to
offer
baccalaureate
degrees
and
changed
its
name
to
state
college
of
florida
manatee-sarasota
to
reflect
its
new
status
as
a
four
year
state
institution
the
first
bachelor's
degree
offered
at
scf
was
a
bachelor
of
science
in
nursing
which
started
in
january
of
2010
several
other
bachelor's
degrees
are
now
available
students
can
attend
classes
on
campuses
located
in
bradenton
venice
and
lakewood
ranch
as
well
as
many
business
and
public-sector
sites
throughout
the
community
and
from
their
homes
via
online
classes
on
september
29th
2017
state
college
of
florida
purchased
of
land
to
build
a
brand
new
campus
in
parrish
that
will
provide
high
quality
education
to
those
living
north
of
the
manatee
river
the
college
president
is
dr
carol
f
probstfeld
who
was
inaugurated
as
the
sixth
president
of
state
college
of
florida
on
november
8th
2013
on
june
7th
2018
dr
todd
g
fritch
was
named
the
first
executive
vice
president
and
provost
of
scf
state
college
of
florida
is
accredited
by
the
commission
on
colleges
of
the
southern
association
of
colleges
and
schools
to
award
associate
and
baccalaureate
degrees
noncredit
education
is
offered
under
scf's
corporate
community
development
programs
more
than
50
percent
of
the
college-bound
high
school
students
in
manatee
and
sarasota
counties
attend
scf
each
year
with
a
current
enrollment
of
over
30000
students
scf
is
among
the
top
100
producers
of
associate
degrees
in
the
united
states
scf
has
over
50
different
clubs
and
organizations
for
students
to
participate
in
such
as
intramural
sports
phi
theta
kappa
and
a
circle
k
international
club
state
college
of
florida's
athletics
department
consist
of
five
intercollegiate
sports
teams
they
include:
men's
basketball
baseball
softball
women's
tennis
and
woman's
volleyball
scf's
athletic
teams
are
nicknamed
the
manatees
and
they
participate
in
the
suncoast
conference
of
the
florida
state
college
activities
association
(fscaa)
in
division
i
of
njcaa
region
viii
state
college
of
florida's
music
department
is
home
to
over
nine
different
performing
ensembles
consisting
of
the
bradenton
symphony
orchestra
symphonic
band
chamber
choir
concert
choir
jazz
ensemble
jazz
combo
guitar
ensemble
keyboard
studies
presidential
string
quartet
and
the
musical
theatre
ensemble
music
students
from
scf
perform
in
multiple
concerts
throughout
the
semester
as
well
as
various
community
and
state
events
such
as
the
fscaa
symposium
the
theatre
and
musical
theatre
department
at
state
college
of
florida
does
a
total
of
four
productions
per
year
or
two
per
semester
the
theatre
faculty
includes
dean
anthony
craig
smith
james
thaggard
(he
is
also
the
box
office
manager)
and
melodie
dickerson
most
theatre
graduates
have
gone
on
to
four-year
universities
to
receive
ba
or
bfa
degrees
in
theatre
and
performing
arts
the
brain
bowl
team
at
state
college
of
florida
currently
coached
by
christina
dwyer
has
achieved
state
and
national
recognition
for
being
one
of
the
top
quiz
bowl
programs
in
the
country
in
the
2014-2015
competition
season
scf's
"fire
team"
compiled
a
record
of
58-2
against
other
two-year
schools
going
on
to
win
championships
at
tournaments
such
as
the
2014
delta
burke
invitational
2015
fcsaa
west
central
regional
2015
fcsaa
brain
bowl
state
championship
2015
naqt
south
florida
community
college
sectionals
and
the
2015
naqt
community
college
championship
tournament
the
team
was
also
invited
to
compete
in
naqt's
intercollegiate
championship
tournament
(dii)
where
the
team
placed
25th
with
a
record
of
7-6
notably
defeating
four-year
schools
such
as
uc
berkeley
duke
university
university
of
alabama
and
claremont
colleges
in
the
process
the
2015
state
and
national
championship
teams
consisted
of
team
captain
and
club
president
michael
moore
jr
and
players
naimul
chowdhury
leon
hostetler
austin
goode
carlyle
styer
kara
stevens
and
christopher
medrano
in
individual
competition
moore
and
chowdhury
placed
third
and
seventh
in
the
nation
respectively
former
coaches
include
dr
hyun
kim
(co-coach
with
christina
dwyer
during
2014-2015
season)
and
dr
carole
cole
the
2016
state
and
national
championship
teams
consisted
of
team
captain
and
club
president
michael
friedman
and
players
david
espinal
former
championship
winner
austin
goode
paul
forester
and
haley
miller
they
won
first
place
at
state
and
second
at
nationals
scf
player
david
espinal
would
lead
the
2017
team
to
championships
and
states
with
players
lacey
anderson
damien
bobrek
and
nathanael
havlik
the
team
went
on
to
win
first
place
at
erik
korray
third
in
state
and
third
at
nationals
the
current
2018
team
is
lead
by
david
espinal
and
austin
goode
with
fellow
players
ariel
rodriguez
lacey
anderson
justin
reitwiesner
and
sierra
beeson
the
bradenton
campus
is
home
to
the
family
heritage
house
museum
a
gallery
and
resource
center
for
the
study
of
african-american
achievements
exhibits
include
a
timeline
of
significant
events
in
african-american
history
including
slavery
fights
for
freedom
community
building
and
education
the
harlem
renaissance
the
civil
rights
movement
kwanzaa
and
the
modern
era
in
south
africa
there
are
also
displays
about
the
underground
railroad
and
a
collection
of
african
masks
admission
is
free
alumni
status
is
open
to
all
graduates
of
state
college
of
florida
(formerly
manatee
community
college)
all
former
students
of
scf
who
regularly
matriculated
and
left
scf
in
good
standing
inspired
education
group
inspired
education
group
is
a
group
who
operates
and
builds
schools
in
europe
the
middle
east
africa
australia
and
latin
america
inspired
was
founded
by
nadim
m
nsouli
in
2013
when
his
group
acquired
reddam
house
in
south
africa
since
its
founding
reddam
house
has
acquired
and
built
9
schools
in
south
africa
along
with
the
group's
flagship
school
reddam
house
berkshire
in
wokingham
england
the
school
grounds
in
wokingham
were
taken
over
from
bearwood
college
in
2015
nsouli
-
the
founder
ceo
and
chairman
-
has
worked
as
a
lawyer
and
as
an
investment
banker
and
he
has
led
the
group
since
its
founding
inspired’s
president
is
graeme
crawford
a
south
african
educator
and
founder
of
reddam
house
dr
stephen
spurr
is
the
group
education
director
and
was
the
head
master
of
westminster
school
in
london
from
2004
to
2015
he
joined
inspired
in
2014
the
group’s
strategy
has
been
described
as
"buy
and
build"
involving
the
purchase
of
existing
schools
as
well
as
the
building
of
new
schools
it
has
offices
in
london
milan
auckland
bogota
johannesburgand
dubai
schools
in
other
european
countries
that
form
part
of
the
inspired
group
include
st
george's
international
school
in
switzerland
st
john's
international
school
in
belgium
st
louis
school
in
italy
and
sotogrande
international
school
in
spain
in
the
middle
east
the
group
has
acquired
british
school
of
bahrain
inspired's
latin
american
schools
include
blue
valley
school
in
costa
rica
colegio
san
mateo
in
colombia
and
cambridge
college
lima
in
peru
by
2017
inspired
operated
more
than
30
schools
and
as
of
2018
educating
over
35000
students
in
46
schools
inspired
acquired
part
of
new
zealand’s
biggest
private-education
provider
acg
education’s
schools
division
in
2018
from
pacific
equity
partners
for
about
$500
million
principle
a
principle
is
a
proposition
or
value
that
is
a
guide
for
behavior
or
evaluation
in
law
it
is
a
rule
that
has
to
be
or
usually
is
to
be
followed
or
can
be
desirably
followed
or
is
an
inevitable
consequence
of
something
such
as
the
laws
observed
in
nature
or
the
way
that
a
system
is
constructed
the
principles
of
such
a
system
are
understood
by
its
users
as
the
essential
characteristics
of
the
system
or
reflecting
system's
designed
purpose
and
the
effective
operation
or
use
of
which
would
be
impossible
if
any
one
of
the
principles
was
to
be
ignored
a
system
may
be
explicitly
based
on
and
implemented
from
a
document
of
principles
as
was
done
in
ibm's
360/370
"principles
of
operation"
examples
of
principles
are
entropy
in
a
number
of
fields
least
action
in
physics
those
in
descriptive
comprehensive
and
fundamental
law:
doctrines
or
assumptions
forming
normative
rules
of
conduct
separation
of
church
and
state
in
statecraft
the
central
dogma
of
molecular
biology
fairness
in
ethics
etc
in
common
english
it
is
a
substantive
and
collective
term
referring
to
rule
governance
the
absence
of
which
being
"unprincipled"
is
considered
a
character
defect
it
may
also
be
used
to
declare
that
a
reality
has
diverged
from
some
ideal
or
norm
as
when
something
is
said
to
be
true
only
"in
principle"
but
not
in
fact
a
principle
represents
values
that
orient
and
rule
the
conduct
of
persons
in
a
particular
society
to
"act
on
principle"
is
to
act
in
accordance
with
one's
moral
ideals
principles
are
absorbed
in
childhood
through
a
process
of
socialization
there
is
a
presumption
of
liberty
of
individuals
that
is
restrained
exemplary
principles
include
first
do
no
harm
the
golden
rule
and
the
doctrine
of
the
mean
it
represents
a
set
of
values
that
inspire
the
written
norms
that
organize
the
life
of
a
society
submitting
to
the
powers
of
an
authority
generally
the
state
the
law
establishes
a
legal
obligation
in
a
coercive
way;
it
therefore
acts
as
principle
conditioning
of
the
action
that
limits
the
liberty
of
the
individuals
see
for
examples
the
territorial
principle
homestead
principle
and
precautionary
principle
archimedes
principle
relating
buoyancy
to
the
weight
of
displaced
water
is
an
early
example
of
a
law
in
science
another
early
one
developed
by
malthus
is
the
"population
principle"
now
called
the
malthusian
principle
freud
also
wrote
on
principles
especially
the
reality
principle
necessary
to
keep
the
id
and
pleasure
principle
in
check
biologists
use
the
principle
of
priority
and
principle
of
binominal
nomenclature
for
precision
in
naming
species
there
are
many
principles
observed
in
physics
notably
in
cosmology
which
observes
the
mediocrity
principle
the
anthropic
principle
the
principle
of
relativity
and
the
cosmological
principle
other
well-known
principles
include
the
uncertainty
principle
in
quantum
mechanics
and
the
pigeonhole
principle
and
superposition
principle
in
mathematics
the
principle
states
that
every
event
has
a
rational
explanation
the
principle
has
a
variety
of
expressions
all
of
which
are
perhaps
best
summarized
by
the
following:
however
one
realizes
that
in
every
sentence
there
is
a
direct
relation
between
the
predicate
and
the
subject
to
say
that
"the
earth
is
round"
corresponds
to
a
direct
relation
between
the
subject
and
the
predicate
according
to
aristotle
“it
is
impossible
for
the
same
thing
to
belong
and
not
to
belong
at
the
same
time
to
the
same
thing
and
in
the
same
respect”
for
example
it
is
not
possible
that
in
exactly
the
same
moment
and
place
it
rains
and
doesn't
rain
the
principle
of
the
excluding
third
or
"principium
tertium
exclusum"
is
a
principle
of
the
traditional
logic
formulated
canonically
by
leibniz
as:
either
"a"
is
"b"
or
"a"
isn't
"b"
it
is
read
the
following
way:
either
"p"
is
true
or
its
denial
¬"p"
is
it
is
also
known
as
"tertium
non
datur"
('a
third
(thing)
is
not)
classically
it
is
considered
to
be
one
of
the
most
important
fundamental
principles
or
laws
of
thought
(along
with
the
principles
of
identity
no
contradiction
and
sufficient
reason)
preternatural
the
preternatural
or
praeternatural
is
that
which
appears
outside
or
beside
(latin
"")
the
natural
it
is
"suspended
between
the
mundane
and
the
miraculous"
in
theology
the
term
is
often
used
to
distinguish
marvels
or
deceptive
trickery
often
attributed
to
witchcraft
or
demons
from
the
purely
divine
power
of
the
genuinely
supernatural
to
violate
the
laws
of
nature
in
the
early
modern
period
the
term
was
used
by
scientists
to
refer
to
abnormalities
and
strange
phenomena
of
various
kinds
that
seemed
to
depart
from
the
norms
of
nature
medieval
theologians
made
a
clear
distinction
between
the
natural
the
preternatural
and
the
supernatural
thomas
aquinas
argued
that
the
supernatural
consists
in
"god’s
unmediated
actions";
the
natural
is
"what
happens
always
or
most
of
the
time";
and
the
preternatural
is
"what
happens
rarely
but
nonetheless
by
the
agency
of
created
beings 
marvels
belong
properly
speaking
to
the
realm
of
the
preternatural"
theologians
following
aquinas
argued
that
only
god
had
the
power
to
disregard
the
laws
of
nature
that
he
has
created
but
that
demons
could
manipulate
the
laws
of
nature
by
a
form
of
trickery
to
deceive
the
unwary
into
believing
they
had
experienced
real
miracles
according
to
historian
lorraine
daston
by
the
16th
century
the
term
"preternatural"
was
increasingly
used
to
refer
to
demonic
activity
comparable
to
the
use
of
magic
by
human
adepts:
the
devil
"being
a
natural
magician 
may
perform
many
acts
in
ways
above
our
knowledge
though
not
transcending
our
natural
power"
according
to
the
philosophy
of
the
time
preternatural
phenomena
were
not
contrary
to
divine
law
but
used
hidden
or
occult
powers
that
violated
the
"normal"
pattern
of
natural
phenomena
with
the
emergence
of
early
modern
science
the
concept
of
the
preternatural
increasingly
came
to
be
used
to
refer
to
strange
or
abnormal
phenomena
that
seemed
to
violate
the
normal
working
of
nature
but
which
were
not
associated
with
magic
and
witchcraft
this
was
a
development
of
the
idea
that
preternatural
phenomena
were
fake
miracles
as
daston
puts
it
"to
simplify
the
historical
sequence
somewhat:
first
preternatural
phenomena
were
demonized
and
thereby
incidentally
naturalized;
then
the
demons
were
deleted
leaving
only
the
natural
causes"
the
use
of
the
term
was
especially
common
in
medicine
for
example
in
john
brown's
"a
compleat
treatise
of
preternatural
tumours"
(1678)
or
william
smellie's
"a
collection
of
preternatural
cases
and
observations
in
midwifery"
(1754)
in
the
19th
century
the
term
was
appropriated
in
anthropology
to
refer
to
folk
beliefs
about
fairies
trolls
and
other
such
creatures
which
were
not
thought
of
as
demonic
but
which
were
perceived
to
affect
the
natural
world
in
unpredictable
ways
according
to
thorstein
veblen
such
preternatural
agents
were
often
thought
of
as
forces
somewhere
between
supernatural
beings
and
material
processes
"the
preternatural
agency
is
not
necessarily
conceived
to
be
a
personal
agent
in
the
full
sense
but
it
is
an
agency
which
partakes
of
the
attributes
of
personality
to
the
extent
of
somewhat
arbitrarily
influencing
the
outcome
of
any
enterprise
and
especially
of
any
contest"
the
linguistic
association
between
individual
agents
and
unexplained
or
unfortunate
circumstances
remains
many
people
attribute
occurrences
that
are
known
to
be
material
processes
such
as
"gremlins
in
the
engine"
a
"ghost
in
the
machine"
or
attributing
motives
to
objects:
"the
clouds
are
threatening"
the
anthropomorphism
in
our
daily
life
is
a
combination
of
the
above
cultural
stems
as
well
as
the
manifestation
of
our
pattern-projecting
minds
in
2011
penn
state
press
began
publishing
a
learned
journal
titled
"preternature:
critical
and
historical
studies
on
the
preternatural"
edited
by
kirsten
uszkalo
and
richard
raiswell
the
journal
is
dedicated
to
publishing
articles
reviews
and
short
editions
of
original
texts
that
deal
with
conceptions
and
perceptions
of
the
preternatural
in
any
culture
and
in
any
historical
period
the
journal
covers
"magics
witchcraft
spiritualism
occultism
prophecy
monstrophy
demonology
and
folklore"
nature
nature
in
the
broadest
sense
is
the
natural
physical
or
material
world
or
universe
"nature"
can
refer
to
the
phenomena
of
the
physical
world
and
also
to
life
in
general
the
study
of
nature
is
a
large
if
not
the
only
part
of
science
although
humans
are
part
of
nature
human
activity
is
often
understood
as
a
separate
category
from
other
natural
phenomena
the
word
"nature"
is
derived
from
the
latin
word
"natura"
or
"essential
qualities
innate
disposition"
and
in
ancient
times
literally
meant
"birth"
"natura"
is
a
latin
translation
of
the
greek
word
"physis"
(φύσις)
which
originally
related
to
the
intrinsic
characteristics
that
plants
animals
and
other
features
of
the
world
develop
of
their
own
accord
the
concept
of
nature
as
a
whole
the
physical
universe
is
one
of
several
expansions
of
the
original
notion;
it
began
with
certain
core
applications
of
the
word
φύσις
by
pre-socratic
philosophers
and
has
steadily
gained
currency
ever
since
this
usage
continued
during
the
advent
of
modern
scientific
method
in
the
last
several
centuries
within
the
various
uses
of
the
word
today
"nature"
often
refers
to
geology
and
wildlife
nature
can
refer
to
the
general
realm
of
living
plants
and
animals
and
in
some
cases
to
the
processes
associated
with
inanimate
objects—the
way
that
particular
types
of
things
exist
and
change
of
their
own
accord
such
as
the
weather
and
geology
of
the
earth
it
is
often
taken
to
mean
the
"natural
environment"
or
wilderness—wild
animals
rocks
forest
and
in
general
those
things
that
have
not
been
substantially
altered
by
human
intervention
or
which
persist
despite
human
intervention
for
example
manufactured
objects
and
human
interaction
generally
are
not
considered
part
of
nature
unless
qualified
as
for
example
"human
nature"
or
"the
whole
of
nature"
this
more
traditional
concept
of
natural
things
which
can
still
be
found
today
implies
a
distinction
between
the
natural
and
the
artificial
with
the
artificial
being
understood
as
that
which
has
been
brought
into
being
by
a
human
consciousness
or
a
human
mind
depending
on
the
particular
context
the
term
"natural"
might
also
be
distinguished
from
the
or
the
supernatural
earth
is
the
only
planet
known
to
support
life
and
its
natural
features
are
the
subject
of
many
fields
of
scientific
research
within
the
solar
system
it
is
third
closest
to
the
sun;
it
is
the
largest
terrestrial
planet
and
the
fifth
largest
overall
its
most
prominent
climatic
features
are
its
two
large
polar
regions
two
relatively
narrow
temperate
zones
and
a
wide
equatorial
tropical
to
subtropical
region
precipitation
varies
widely
with
location
from
several
metres
of
water
per
year
to
less
than
a
millimetre
71
percent
of
the
earth's
surface
is
covered
by
salt-water
oceans
the
remainder
consists
of
continents
and
islands
with
most
of
the
inhabited
land
in
the
northern
hemisphere
earth
has
evolved
through
geological
and
biological
processes
that
have
left
traces
of
the
original
conditions
the
outer
surface
is
divided
into
several
gradually
migrating
tectonic
plates
the
interior
remains
active
with
a
thick
layer
of
plastic
mantle
and
an
iron-filled
core
that
generates
a
magnetic
field
this
iron
core
is
composed
of
a
solid
inner
phase
and
a
fluid
outer
phase
convective
motion
in
the
core
generates
electric
currents
through
dynamo
action
and
these
in
turn
generate
the
geomagnetic
field
the
atmospheric
conditions
have
been
significantly
altered
from
the
original
conditions
by
the
presence
of
life-forms
which
create
an
ecological
balance
that
stabilizes
the
surface
conditions
despite
the
wide
regional
variations
in
climate
by
latitude
and
other
geographic
factors
the
long-term
average
global
climate
is
quite
stable
during
interglacial
periods
and
variations
of
a
degree
or
two
of
average
global
temperature
have
historically
had
major
effects
on
the
ecological
balance
and
on
the
actual
geography
of
the
earth
geology
is
the
science
and
study
of
the
solid
and
liquid
matter
that
constitutes
the
earth
the
field
of
geology
encompasses
the
study
of
the
composition
structure
physical
properties
dynamics
and
history
of
earth
materials
and
the
processes
by
which
they
are
formed
moved
and
changed
the
field
is
a
major
academic
discipline
and
is
also
important
for
mineral
and
hydrocarbon
extraction
knowledge
about
and
mitigation
of
natural
hazards
some
geotechnical
engineering
fields
and
understanding
past
climates
and
environments
the
geology
of
an
area
evolves
through
time
as
rock
units
are
deposited
and
inserted
and
deformational
processes
change
their
shapes
and
locations
rock
units
are
first
emplaced
either
by
deposition
onto
the
surface
or
intrude
into
the
overlying
rock
deposition
can
occur
when
sediments
settle
onto
the
surface
of
the
earth
and
later
lithify
into
sedimentary
rock
or
when
as
volcanic
material
such
as
volcanic
ash
or
lava
flows
blanket
the
surface
igneous
intrusions
such
as
batholiths
laccoliths
dikes
and
sills
push
upwards
into
the
overlying
rock
and
crystallize
as
they
intrude
after
the
initial
sequence
of
rocks
has
been
deposited
the
rock
units
can
be
deformed
and/or
metamorphosed
deformation
typically
occurs
as
a
result
of
horizontal
shortening
horizontal
extension
or
side-to-side
(strike-slip)
motion
these
structural
regimes
broadly
relate
to
convergent
boundaries
divergent
boundaries
and
transform
boundaries
respectively
between
tectonic
plates
earth
is
estimated
to
have
formed
454 billion
years
ago
from
the
solar
nebula
along
with
the
sun
and
other
planets
the
moon
formed
roughly
20 million
years
later
initially
molten
the
outer
layer
of
the
earth
cooled
resulting
in
the
solid
crust
outgassing
and
volcanic
activity
produced
the
primordial
atmosphere
condensing
water
vapor
most
or
all
of
which
came
from
ice
delivered
by
comets
produced
the
oceans
and
other
water
sources
the
highly
energetic
chemistry
is
believed
to
have
produced
a
self-replicating
molecule
around
4 billion
years
ago
continents
formed
then
broke
up
and
reformed
as
the
surface
of
earth
reshaped
over
hundreds
of
millions
of
years
occasionally
combining
to
make
a
supercontinent
roughly
750 million
years
ago
the
earliest
known
supercontinent
rodinia
began
to
break
apart
the
continents
later
recombined
to
form
pannotia
which
broke
apart
about
540 million
years
ago
then
finally
pangaea
which
broke
apart
about
180 million
years
ago
during
the
neoproterozoic
era
freezing
temperatures
covered
much
of
the
earth
in
glaciers
and
ice
sheets
this
hypothesis
has
been
termed
the
"snowball
earth"
and
it
is
of
particular
interest
as
it
precedes
the
cambrian
explosion
in
which
multicellular
life
forms
began
to
proliferate
about
530–540 million
years
ago
since
the
cambrian
explosion
there
have
been
five
distinctly
identifiable
mass
extinctions
the
last
mass
extinction
occurred
some
66
million
years
ago
when
a
meteorite
collision
probably
triggered
the
extinction
of
the
non-avian
dinosaurs
and
other
large
reptiles
but
spared
small
animals
such
as
mammals
over
the
past
66 million
years
mammalian
life
diversified
several
million
years
ago
a
species
of
small
african
ape
gained
the
ability
to
stand
upright
the
subsequent
advent
of
human
life
and
the
development
of
agriculture
and
further
civilization
allowed
humans
to
affect
the
earth
more
rapidly
than
any
previous
life
form
affecting
both
the
nature
and
quantity
of
other
organisms
as
well
as
global
climate
by
comparison
the
great
oxygenation
event
produced
by
the
proliferation
of
algae
during
the
siderian
period
required
about
300 million
years
to
culminate
the
present
era
is
classified
as
part
of
a
mass
extinction
event
the
holocene
extinction
event
the
fastest
ever
to
have
occurred
some
such
as
e
o
wilson
of
harvard
university
predict
that
human
destruction
of
the
biosphere
could
cause
the
extinction
of
one-half
of
all
species
in
the
next
100 years
the
extent
of
the
current
extinction
event
is
still
being
researched
debated
and
calculated
by
biologists
the
earth's
atmosphere
is
a
key
factor
in
sustaining
the
ecosystem
the
thin
layer
of
gases
that
envelops
the
earth
is
held
in
place
by
gravity
air
is
mostly
nitrogen
oxygen
water
vapor
with
much
smaller
amounts
of
carbon
dioxide
argon
etc
the
atmospheric
pressure
declines
steadily
with
altitude
the
ozone
layer
plays
an
important
role
in
depleting
the
amount
of
ultraviolet
(uv)
radiation
that
reaches
the
surface
as
dna
is
readily
damaged
by
uv
light
this
serves
to
protect
life
at
the
surface
the
atmosphere
also
retains
heat
during
the
night
thereby
reducing
the
daily
temperature
extremes
terrestrial
weather
occurs
almost
exclusively
in
the
lower
part
of
the
atmosphere
and
serves
as
a
convective
system
for
redistributing
heat
ocean
currents
are
another
important
factor
in
determining
climate
particularly
the
major
underwater
thermohaline
circulation
which
distributes
heat
energy
from
the
equatorial
oceans
to
the
polar
regions
these
currents
help
to
moderate
the
differences
in
temperature
between
winter
and
summer
in
the
temperate
zones
also
without
the
redistributions
of
heat
energy
by
the
ocean
currents
and
atmosphere
the
tropics
would
be
much
hotter
and
the
polar
regions
much
colder
weather
can
have
both
beneficial
and
harmful
effects
extremes
in
weather
such
as
tornadoes
or
hurricanes
and
cyclones
can
expend
large
amounts
of
energy
along
their
paths
and
produce
devastation
surface
vegetation
has
evolved
a
dependence
on
the
seasonal
variation
of
the
weather
and
sudden
changes
lasting
only
a
few
years
can
have
a
dramatic
effect
both
on
the
vegetation
and
on
the
animals
which
depend
on
its
growth
for
their
food
climate
is
a
measure
of
the
long-term
trends
in
the
weather
various
factors
are
known
to
influence
the
climate
including
ocean
currents
surface
albedo
greenhouse
gases
variations
in
the
solar
luminosity
and
changes
to
the
earth's
orbit
based
on
historical
records
the
earth
is
known
to
have
undergone
drastic
climate
changes
in
the
past
including
ice
ages
the
climate
of
a
region
depends
on
a
number
of
factors
especially
latitude
a
latitudinal
band
of
the
surface
with
similar
climatic
attributes
forms
a
climate
region
there
are
a
number
of
such
regions
ranging
from
the
tropical
climate
at
the
equator
to
the
polar
climate
in
the
northern
and
southern
extremes
weather
is
also
influenced
by
the
seasons
which
result
from
the
earth's
axis
being
tilted
relative
to
its
orbital
plane
thus
at
any
given
time
during
the
summer
or
winter
one
part
of
the
earth
is
more
directly
exposed
to
the
rays
of
the
sun
this
exposure
alternates
as
the
earth
revolves
in
its
orbit
at
any
given
time
regardless
of
season
the
northern
and
southern
hemispheres
experience
opposite
seasons
weather
is
a
chaotic
system
that
is
readily
modified
by
small
changes
to
the
environment
so
accurate
weather
forecasting
is
limited
to
only
a
few
days
overall
two
things
are
happening
worldwide:
(1)
temperature
is
increasing
on
the
average;
and
(2)
regional
climates
have
been
undergoing
noticeable
changes
water
is
a
chemical
substance
that
is
composed
of
hydrogen
and
oxygen
and
is
vital
for
all
known
forms
of
life
in
typical
usage
"water"
refers
only
to
its
liquid
form
or
state
but
the
substance
also
has
a
solid
state
ice
and
a
gaseous
state
water
vapor
or
steam
water
covers
71%
of
the
earth's
surface
on
earth
it
is
found
mostly
in
oceans
and
other
large
bodies
of
water
with
16%
of
water
below
ground
in
aquifers
and
0001%
in
the
air
as
vapor
clouds
and
precipitation
oceans
hold
97%
of
surface
water
glaciers
and
polar
ice
caps
24%
and
other
land
surface
water
such
as
rivers
lakes
and
ponds
06%
additionally
a
minute
amount
of
the
earth's
water
is
contained
within
biological
bodies
and
manufactured
products
an
ocean
is
a
major
body
of
saline
water
and
a
principal
component
of
the
hydrosphere
approximately
71%
of
the
earth's
surface
(an
area
of
some
361
million
square
kilometers)
is
covered
by
ocean
a
continuous
body
of
water
that
is
customarily
divided
into
several
principal
oceans
and
smaller
seas
more
than
half
of
this
area
is
over
deep
average
oceanic
salinity
is
around
35
parts
per
thousand
(ppt)
(35%)
and
nearly
all
seawater
has
a
salinity
in
the
range
of
30
to
38
ppt
though
generally
recognized
as
several
'separate'
oceans
these
waters
comprise
one
global
interconnected
body
of
salt
water
often
referred
to
as
the
world
ocean
or
global
ocean
this
concept
of
a
global
ocean
as
a
continuous
body
of
water
with
relatively
free
interchange
among
its
parts
is
of
fundamental
importance
to
oceanography
the
major
oceanic
divisions
are
defined
in
part
by
the
continents
various
archipelagos
and
other
criteria:
these
divisions
are
(in
descending
order
of
size)
the
pacific
ocean
the
atlantic
ocean
the
indian
ocean
the
southern
ocean
and
the
arctic
ocean
smaller
regions
of
the
oceans
are
called
seas
gulfs
bays
and
other
names
there
are
also
salt
lakes
which
are
smaller
bodies
of
landlocked
saltwater
that
are
not
interconnected
with
the
world
ocean
two
notable
examples
of
salt
lakes
are
the
aral
sea
and
the
great
salt
lake
a
lake
(from
latin
"lacus")
is
a
terrain
feature
(or
physical
feature)
a
body
of
liquid
on
the
surface
of
a
world
that
is
localized
to
the
bottom
of
basin
(another
type
of
landform
or
terrain
feature;
that
is
it
is
not
global)
and
moves
slowly
if
it
moves
at
all
on
earth
a
body
of
water
is
considered
a
lake
when
it
is
inland
not
part
of
the
ocean
is
larger
and
deeper
than
a
pond
and
is
fed
by
a
river
the
only
world
other
than
earth
known
to
harbor
lakes
is
titan
saturn's
largest
moon
which
has
lakes
of
ethane
most
likely
mixed
with
methane
it
is
not
known
if
titan's
lakes
are
fed
by
rivers
though
titan's
surface
is
carved
by
numerous
river
beds
natural
lakes
on
earth
are
generally
found
in
mountainous
areas
rift
zones
and
areas
with
ongoing
or
recent
glaciation
other
lakes
are
found
in
endorheic
basins
or
along
the
courses
of
mature
rivers
in
some
parts
of
the
world
there
are
many
lakes
because
of
chaotic
drainage
patterns
left
over
from
the
last
ice
age
all
lakes
are
temporary
over
geologic
time
scales
as
they
will
slowly
fill
in
with
sediments
or
spill
out
of
the
basin
containing
them
a
pond
is
a
body
of
standing
water
either
natural
or
man-made
that
is
usually
smaller
than
a
lake
a
wide
variety
of
man-made
bodies
of
water
are
classified
as
ponds
including
water
gardens
designed
for
aesthetic
ornamentation
fish
ponds
designed
for
commercial
fish
breeding
and
solar
ponds
designed
to
store
thermal
energy
ponds
and
lakes
are
distinguished
from
streams
via
current
speed
while
currents
in
streams
are
easily
observed
ponds
and
lakes
possess
thermally
driven
micro-currents
and
moderate
wind
driven
currents
these
features
distinguish
a
pond
from
many
other
aquatic
terrain
features
such
as
stream
pools
and
tide
pools
a
river
is
a
natural
watercourse
usually
freshwater
flowing
toward
an
ocean
a
lake
a
sea
or
another
river
in
a
few
cases
a
river
simply
flows
into
the
ground
or
dries
up
completely
before
reaching
another
body
of
water
small
rivers
may
also
be
called
by
several
other
names
including
stream
creek
brook
rivulet
and
rill;
there
is
no
general
rule
that
defines
what
can
be
called
a
river
many
names
for
small
rivers
are
specific
to
geographic
location;
one
example
is
"burn"
in
scotland
and
north-east
england
sometimes
a
river
is
said
to
be
larger
than
a
creek
but
this
is
not
always
the
case
due
to
vagueness
in
the
language
a
river
is
part
of
the
hydrological
cycle
water
within
a
river
is
generally
collected
from
precipitation
through
surface
runoff
groundwater
recharge
springs
and
the
release
of
stored
water
in
natural
ice
and
snowpacks
(ie
from
glaciers)
a
stream
is
a
flowing
body
of
water
with
a
current
confined
within
a
bed
and
stream
banks
in
the
united
states
a
stream
is
classified
as
a
watercourse
less
than
wide
streams
are
important
as
conduits
in
the
water
cycle
instruments
in
groundwater
recharge
and
they
serve
as
corridors
for
fish
and
wildlife
migration
the
biological
habitat
in
the
immediate
vicinity
of
a
stream
is
called
a
riparian
zone
given
the
status
of
the
ongoing
holocene
extinction
streams
play
an
important
corridor
role
in
connecting
fragmented
habitats
and
thus
in
conserving
biodiversity
the
study
of
streams
and
waterways
in
general
involves
many
branches
of
inter-disciplinary
natural
science
and
engineering
including
hydrology
fluvial
geomorphology
aquatic
ecology
fish
biology
riparian
ecology
and
others
ecosystems
are
composed
of
a
variety
of
abiotic
and
biotic
components
that
function
in
an
interrelated
way
the
structure
and
composition
is
determined
by
various
environmental
factors
that
are
interrelated
variations
of
these
factors
will
initiate
dynamic
modifications
to
the
ecosystem
some
of
the
more
important
components
are:
soil
atmosphere
radiation
from
the
sun
water
and
living
organisms
central
to
the
ecosystem
concept
is
the
idea
that
living
organisms
interact
with
every
other
element
in
their
local
environment
eugene
odum
a
founder
of
ecology
stated:
"any
unit
that
includes
all
of
the
organisms
(ie:
the
"community")
in
a
given
area
interacting
with
the
physical
environment
so
that
a
flow
of
energy
leads
to
clearly
defined
trophic
structure
biotic
diversity
and
material
cycles
(ie:
exchange
of
materials
between
living
and
nonliving
parts)
within
the
system
is
an
ecosystem"
within
the
ecosystem
species
are
connected
and
dependent
upon
one
another
in
the
food
chain
and
exchange
energy
and
matter
between
themselves
as
well
as
with
their
environment
the
human
ecosystem
concept
is
based
on
the
human/nature
dichotomy
and
the
idea
that
all
species
are
ecologically
dependent
on
each
other
as
well
as
with
the
abiotic
constituents
of
their
biotope
a
smaller
unit
of
size
is
called
a
microecosystem
for
example
a
microsystem
can
be
a
stone
and
all
the
life
under
it
a
"macroecosystem"
might
involve
a
whole
ecoregion
with
its
drainage
basin
wilderness
is
generally
defined
as
areas
that
have
not
been
significantly
modified
by
human
activity
wilderness
areas
can
be
found
in
preserves
estates
farms
conservation
preserves
ranches
national
parks
and
even
in
urban
areas
along
rivers
gulches
or
otherwise
undeveloped
areas
wilderness
areas
and
protected
parks
are
considered
important
for
the
survival
of
certain
species
ecological
studies
conservation
and
solitude
some
nature
writers
believe
wilderness
areas
are
vital
for
the
human
spirit
and
creativity
and
some
ecologists
consider
wilderness
areas
to
be
an
integral
part
of
the
earth's
self-sustaining
natural
ecosystem
(the
biosphere)
they
may
also
preserve
historic
genetic
traits
and
that
they
provide
habitat
for
wild
flora
and
fauna
that
may
be
difficult
or
impossible
to
recreate
in
zoos
arboretums
or
laboratories
although
there
is
no
universal
agreement
on
the
definition
of
life
scientists
generally
accept
that
the
biological
manifestation
of
life
is
characterized
by
organization
metabolism
growth
adaptation
response
to
stimuli
and
reproduction
life
may
also
be
said
to
be
simply
the
characteristic
state
of
organisms
properties
common
to
terrestrial
organisms
(plants
animals
fungi
protists
archaea
and
bacteria)
are
that
they
are
cellular
carbon-and-water-based
with
complex
organization
having
a
metabolism
a
capacity
to
grow
respond
to
stimuli
and
reproduce
an
entity
with
these
properties
is
generally
considered
life
however
not
every
definition
of
life
considers
all
of
these
properties
to
be
essential
human-made
analogs
of
life
may
also
be
considered
to
be
life
the
biosphere
is
the
part
of
earth's
outer
shell –
including
land
surface
rocks
water
air
and
the
atmosphere –
within
which
life
occurs
and
which
biotic
processes
in
turn
alter
or
transform
from
the
broadest
geophysiological
point
of
view
the
biosphere
is
the
global
ecological
system
integrating
all
living
beings
and
their
relationships
including
their
interaction
with
the
elements
of
the
lithosphere
(rocks)
hydrosphere
(water)
and
atmosphere
(air)
the
entire
earth
contains
over
75 billion
tons
(150
"trillion"
pounds
or
about
68×10 kilograms)
of
biomass
(life)
which
lives
within
various
environments
within
the
biosphere
over
nine-tenths
of
the
total
biomass
on
earth
is
plant
life
on
which
animal
life
depends
very
heavily
for
its
existence
more
than
2
million
species
of
plant
and
animal
life
have
been
identified
to
date
and
estimates
of
the
actual
number
of
existing
species
range
from
several
million
to
well
over
50 million
the
number
of
individual
species
of
life
is
constantly
in
some
degree
of
flux
with
new
species
appearing
and
others
ceasing
to
exist
on
a
continual
basis
the
total
number
of
species
is
in
rapid
decline
the
origin
of
life
on
earth
is
not
well
understood
but
it
is
known
to
have
occurred
at
least
35 billion
years
ago
during
the
hadean
or
archean
eons
on
a
primordial
earth
that
had
a
substantially
different
environment
than
is
found
at
present
these
life
forms
possessed
the
basic
traits
of
self-replication
and
inheritable
traits
once
life
had
appeared
the
process
of
evolution
by
natural
selection
resulted
in
the
development
of
ever-more
diverse
life
forms
species
that
were
unable
to
adapt
to
the
changing
environment
and
competition
from
other
life
forms
became
extinct
however
the
fossil
record
retains
evidence
of
many
of
these
older
species
current
fossil
and
dna
evidence
shows
that
all
existing
species
can
trace
a
continual
ancestry
back
to
the
first
primitive
life
forms
when
basic
forms
of
plant
life
developed
the
process
of
photosynthesis
the
sun's
energy
could
be
harvested
to
create
conditions
which
allowed
for
more
complex
life
forms
the
resultant
oxygen
accumulated
in
the
atmosphere
and
gave
rise
to
the
ozone
layer
the
incorporation
of
smaller
cells
within
larger
ones
resulted
in
the
development
of
yet
more
complex
cells
called
eukaryotes
cells
within
colonies
became
increasingly
specialized
resulting
in
true
multicellular
organisms
with
the
ozone
layer
absorbing
harmful
ultraviolet
radiation
life
colonized
the
surface
of
earth
the
first
form
of
life
to
develop
on
the
earth
were
microbes
and
they
remained
the
only
form
of
life
until
about
a
billion
years
ago
when
multi-cellular
organisms
began
to
appear
microorganisms
are
single-celled
organisms
that
are
generally
microscopic
and
smaller
than
the
human
eye
can
see
they
include
bacteria
fungi
archaea
and
protista
these
life
forms
are
found
in
almost
every
location
on
the
earth
where
there
is
liquid
water
including
in
the
earth's
interior
their
reproduction
is
both
rapid
and
profuse
the
combination
of
a
high
mutation
rate
and
a
horizontal
gene
transfer
ability
makes
them
highly
adaptable
and
able
to
survive
in
new
environments
including
outer
space
they
form
an
essential
part
of
the
planetary
ecosystem
however
some
microorganisms
are
pathogenic
and
can
post
health
risk
to
other
organisms
originally
aristotle
divided
all
living
things
between
plants
which
generally
do
not
move
fast
enough
for
humans
to
notice
and
animals
in
linnaeus'
system
these
became
the
kingdoms
vegetabilia
(later
plantae)
and
animalia
since
then
it
has
become
clear
that
the
plantae
as
originally
defined
included
several
unrelated
groups
and
the
fungi
and
several
groups
of
algae
were
removed
to
new
kingdoms
however
these
are
still
often
considered
plants
in
many
contexts
bacterial
life
is
sometimes
included
in
flora
and
some
classifications
use
the
term
"bacterial
flora"
separately
from
"plant
flora"
among
the
many
ways
of
classifying
plants
are
by
regional
floras
which
depending
on
the
purpose
of
study
can
also
include
"fossil
flora"
remnants
of
plant
life
from
a
previous
era
people
in
many
regions
and
countries
take
great
pride
in
their
individual
arrays
of
characteristic
flora
which
can
vary
widely
across
the
globe
due
to
differences
in
climate
and
terrain
regional
floras
commonly
are
divided
into
categories
such
as
"native
flora"
and
"agricultural
and
garden
flora"
the
lastly
mentioned
of
which
are
intentionally
grown
and
cultivated
some
types
of
"native
flora"
actually
have
been
introduced
centuries
ago
by
people
migrating
from
one
region
or
continent
to
another
and
become
an
integral
part
of
the
native
or
natural
flora
of
the
place
to
which
they
were
introduced
this
is
an
example
of
how
human
interaction
with
nature
can
blur
the
boundary
of
what
is
considered
nature
another
category
of
plant
has
historically
been
carved
out
for
"weeds"
though
the
term
has
fallen
into
disfavor
among
botanists
as
a
formal
way
to
categorize
"useless"
plants
the
informal
use
of
the
word
"weeds"
to
describe
those
plants
that
are
deemed
worthy
of
elimination
is
illustrative
of
the
general
tendency
of
people
and
societies
to
seek
to
alter
or
shape
the
course
of
nature
similarly
animals
are
often
categorized
in
ways
such
as
"domestic"
"farm
animals"
"wild
animals"
"pests"
etc
according
to
their
relationship
to
human
life
animals
as
a
category
have
several
characteristics
that
generally
set
them
apart
from
other
living
things
animals
are
eukaryotic
and
usually
multicellular
(although
see
myxozoa)
which
separates
them
from
bacteria
archaea
and
most
protists
they
are
heterotrophic
generally
digesting
food
in
an
internal
chamber
which
separates
them
from
plants
and
algae
they
are
also
distinguished
from
plants
algae
and
fungi
by
lacking
cell
walls
with
a
few
exceptions—most
notably
the
two
phyla
consisting
of
sponges
and
placozoans—animals
have
bodies
that
are
differentiated
into
tissues
these
include
muscles
which
are
able
to
contract
and
control
locomotion
and
a
nervous
system
which
sends
and
processes
signals
there
is
also
typically
an
internal
digestive
chamber
the
eukaryotic
cells
possessed
by
all
animals
are
surrounded
by
a
characteristic
extracellular
matrix
composed
of
collagen
and
elastic
glycoproteins
this
may
be
calcified
to
form
structures
like
shells
bones
and
spicules
a
framework
upon
which
cells
can
move
about
and
be
reorganized
during
development
and
maturation
and
which
supports
the
complex
anatomy
required
for
mobility
although
humans
comprise
only
a
minuscule
proportion
of
the
total
living
biomass
on
earth
the
human
effect
on
nature
is
disproportionately
large
because
of
the
extent
of
human
influence
the
boundaries
between
what
humans
regard
as
nature
and
"made
environments"
is
not
clear
cut
except
at
the
extremes
even
at
the
extremes
the
amount
of
natural
environment
that
is
free
of
discernible
human
influence
is
diminishing
at
an
increasingly
rapid
pace
the
development
of
technology
by
the
human
race
has
allowed
the
greater
exploitation
of
natural
resources
and
has
helped
to
alleviate
some
of
the
risk
from
natural
hazards
in
spite
of
this
progress
however
the
fate
of
human
civilization
remains
closely
linked
to
changes
in
the
environment
there
exists
a
highly
complex
feedback
loop
between
the
use
of
advanced
technology
and
changes
to
the
environment
that
are
only
slowly
becoming
understood
man-made
threats
to
the
earth's
natural
environment
include
pollution
deforestation
and
disasters
such
as
oil
spills
humans
have
contributed
to
the
extinction
of
many
plants
and
animals
humans
employ
nature
for
both
leisure
and
economic
activities
the
acquisition
of
natural
resources
for
industrial
use
remains
a
sizable
component
of
the
world's
economic
system
some
activities
such
as
hunting
and
fishing
are
used
for
both
sustenance
and
leisure
often
by
different
people
agriculture
was
first
adopted
around
the
9th
millennium
bce
ranging
from
food
production
to
energy
nature
influences
economic
wealth
although
early
humans
gathered
uncultivated
plant
materials
for
food
and
employed
the
medicinal
properties
of
vegetation
for
healing
most
modern
human
use
of
plants
is
through
agriculture
the
clearance
of
large
tracts
of
land
for
crop
growth
has
led
to
a
significant
reduction
in
the
amount
available
of
forestation
and
wetlands
resulting
in
the
loss
of
habitat
for
many
plant
and
animal
species
as
well
as
increased
erosion
beauty
in
nature
has
historically
been
a
prevalent
theme
in
art
and
books
filling
large
sections
of
libraries
and
bookstores
that
nature
has
been
depicted
and
celebrated
by
so
much
art
photography
poetry
and
other
literature
shows
the
strength
with
which
many
people
associate
nature
and
beauty
reasons
why
this
association
exists
and
what
the
association
consists
of
are
studied
by
the
branch
of
philosophy
called
aesthetics
beyond
certain
basic
characteristics
that
many
philosophers
agree
about
to
explain
what
is
seen
as
beautiful
the
opinions
are
virtually
endless
nature
and
wildness
have
been
important
subjects
in
various
eras
of
world
history
an
early
tradition
of
landscape
art
began
in
china
during
the
tang
dynasty
(618–907)
the
tradition
of
representing
nature
"as
it
is"
became
one
of
the
aims
of
chinese
painting
and
was
a
significant
influence
in
asian
art
although
natural
wonders
are
celebrated
in
the
psalms
and
the
book
of
job
wilderness
portrayals
in
art
became
more
prevalent
in
the
1800s
especially
in
the
works
of
the
romantic
movement
british
artists
john
constable
and
j
m
w
turner
turned
their
attention
to
capturing
the
beauty
of
the
natural
world
in
their
paintings
before
that
paintings
had
been
primarily
of
religious
scenes
or
of
human
beings
william
wordsworth's
poetry
described
the
wonder
of
the
natural
world
which
had
formerly
been
viewed
as
a
threatening
place
increasingly
the
valuing
of
nature
became
an
aspect
of
western
culture
this
artistic
movement
also
coincided
with
the
transcendentalist
movement
in
the
western
world
a
common
classical
idea
of
beautiful
art
involves
the
word
mimesis
the
imitation
of
nature
also
in
the
realm
of
ideas
about
beauty
in
nature
is
that
the
perfect
is
implied
through
perfect
mathematical
forms
and
more
generally
by
patterns
in
nature
as
david
rothenburg
writes
"the
beautiful
is
the
root
of
science
and
the
goal
of
art
the
highest
possibility
that
humanity
can
ever
hope
to
see"
some
fields
of
science
see
nature
as
matter
in
motion
obeying
certain
laws
of
nature
which
science
seeks
to
understand
for
this
reason
the
most
fundamental
science
is
generally
understood
to
be
"physics" –
the
name
for
which
is
still
recognizable
as
meaning
that
it
is
the
study
of
nature
matter
is
commonly
defined
as
the
substance
of
which
physical
objects
are
composed
it
constitutes
the
observable
universe
the
visible
components
of
the
universe
are
now
believed
to
compose
only
49
percent
of
the
total
mass
the
remainder
is
believed
to
consist
of
268
percent
cold
dark
matter
and
683
percent
dark
energy
the
exact
arrangement
of
these
components
is
still
unknown
and
is
under
intensive
investigation
by
physicists
the
behavior
of
matter
and
energy
throughout
the
observable
universe
appears
to
follow
well-defined
physical
laws
these
laws
have
been
employed
to
produce
cosmological
models
that
successfully
explain
the
structure
and
the
evolution
of
the
universe
we
can
observe
the
mathematical
expressions
of
the
laws
of
physics
employ
a
set
of
twenty
physical
constants
that
appear
to
be
static
across
the
observable
universe
the
values
of
these
constants
have
been
carefully
measured
but
the
reason
for
their
specific
values
remains
a
mystery
outer
space
also
simply
called
"space"
refers
to
the
relatively
empty
regions
of
the
universe
outside
the
atmospheres
of
celestial
bodies
"outer"
space
is
used
to
distinguish
it
from
airspace
(and
terrestrial
locations)
there
is
no
discrete
boundary
between
the
earth's
atmosphere
and
space
as
the
atmosphere
gradually
attenuates
with
increasing
altitude
outer
space
within
the
solar
system
is
called
interplanetary
space
which
passes
over
into
interstellar
space
at
what
is
known
as
the
heliopause
outer
space
is
sparsely
filled
with
several
dozen
types
of
organic
molecules
discovered
to
date
by
microwave
spectroscopy
blackbody
radiation
left
over
from
the
big
bang
and
the
origin
of
the
universe
and
cosmic
rays
which
include
ionized
atomic
nuclei
and
various
subatomic
particles
there
is
also
some
gas
plasma
and
dust
and
small
meteors
additionally
there
are
signs
of
human
life
in
outer
space
today
such
as
material
left
over
from
previous
manned
and
unmanned
launches
which
are
a
potential
hazard
to
spacecraft
some
of
this
debris
re-enters
the
atmosphere
periodically
although
the
earth
is
the
only
body
within
the
solar
system
known
to
support
life
evidence
suggests
that
in
the
distant
past
the
planet
mars
possessed
bodies
of
liquid
water
on
the
surface
for
a
brief
period
in
mars'
history
it
may
have
also
been
capable
of
forming
life
at
present
though
most
of
the
water
remaining
on
mars
is
frozen
if
life
exists
at
all
on
mars
it
is
most
likely
to
be
located
underground
where
liquid
water
can
still
exist
conditions
on
the
other
terrestrial
planets
mercury
and
venus
appear
to
be
too
harsh
to
support
life
as
we
know
it
but
it
has
been
conjectured
that
europa
the
fourth-largest
moon
of
jupiter
may
possess
a
sub-surface
ocean
of
liquid
water
and
could
potentially
host
life
astronomers
have
started
to
discover
extrasolar
earth
analogs
–
planets
that
lie
in
the
habitable
zone
of
space
surrounding
a
star
and
therefore
could
possibly
host
life
as
we
know
it
media:
organizations:
philosophy:
patterns
in
nature
patterns
in
nature
are
visible
regularities
of
form
found
in
the
natural
world
these
patterns
recur
in
different
contexts
and
can
sometimes
be
modelled
mathematically
natural
patterns
include
symmetries
trees
spirals
meanders
waves
foams
tessellations
cracks
and
stripes
early
greek
philosophers
studied
pattern
with
plato
pythagoras
and
empedocles
attempting
to
explain
order
in
nature
the
modern
understanding
of
visible
patterns
developed
gradually
over
time
in
the
19th
century
belgian
physicist
joseph
plateau
examined
soap
films
leading
him
to
formulate
the
concept
of
a
minimal
surface
german
biologist
and
artist
ernst
haeckel
painted
hundreds
of
marine
organisms
to
emphasise
their
symmetry
scottish
biologist
d'arcy
thompson
pioneered
the
study
of
growth
patterns
in
both
plants
and
animals
showing
that
simple
equations
could
explain
spiral
growth
in
the
20th
century
british
mathematician
alan
turing
predicted
mechanisms
of
morphogenesis
which
give
rise
to
patterns
of
spots
and
stripes
hungarian
biologist
aristid
lindenmayer
and
french
american
mathematician
benoît
mandelbrot
showed
how
the
mathematics
of
fractals
could
create
plant
growth
patterns
mathematics
physics
and
chemistry
can
explain
patterns
in
nature
at
different
levels
patterns
in
living
things
are
explained
by
the
biological
processes
of
natural
selection
and
sexual
selection
studies
of
pattern
formation
make
use
of
computer
models
to
simulate
a
wide
range
of
patterns
early
greek
philosophers
attempted
to
explain
order
in
nature
anticipating
modern
concepts
pythagoras
(c 570–c 495 bc)
explained
patterns
in
nature
like
the
harmonies
of
music
as
arising
from
number
which
he
took
to
be
the
basic
constituent
of
existence
empedocles
(c 494–c 434 bc)
to
an
extent
anticipated
darwin's
evolutionary
explanation
for
the
structures
of
organisms
plato
(c 427–c 347 bc)
argued
for
the
existence
of
natural
universals
he
considered
these
to
consist
of
ideal
forms
(
"eidos":
"form")
of
which
physical
objects
are
never
more
than
imperfect
copies
thus
a
flower
may
be
roughly
circular
but
it
is
never
a
perfect
circle
theophrastus
(c 372–c 287 bc)
noted
that
plants
"that
have
flat
leaves
have
them
in
a
regular
series";
pliny
the
elder
(23–79 ad)
noted
their
patterned
circular
arrangement
centuries
later
leonardo
da
vinci
(1452–1519)
noted
the
spiral
arrangement
of
leaf
patterns
johannes
kepler
(1571–1630)
pointed
out
the
presence
of
the
fibonacci
sequence
in
nature
using
it
to
explain
the
pentagonal
form
of
some
flowers
in
1754
charles
bonnet
observed
that
the
spiral
phyllotaxis
of
plants
were
frequently
expressed
in
both
clockwise
and
counter-clockwise
golden
ratio
series
mathematical
observations
of
phyllotaxis
followed
with
karl
friedric
schimper
and
his
friend
alexander
braun's
1830
and
1830
work
respectively;
auguste
bravais
and
his
brother
louis
connected
phyllotaxis
ratios
to
the
fibonacci
sequence
in
1837
also
noting
its
appearance
in
pinecones
and
pineapples
in
his
1854
book
german
psychologist
adolf
zeising
explored
the
golden
ratio
expressed
in
the
arrangement
of
plant
parts
the
skeletons
of
animals
and
the
branching
patterns
of
their
veins
and
nerves
as
well
as
in
crystals
a
h
church
studied
the
patterns
of
phyllotaxis
in
his
1904
book
in
1917
d'arcy
thompson
published
"on
growth
and
form";
his
description
of
phyllotaxis
and
the
fibonacci
sequence
the
mathematical
relationships
in
the
spiral
growth
patterns
of
plants
showed
that
simple
equations
could
describe
the
spiral
growth
patterns
of
animal
horns
and
mollusc
shells
in
1202
leonardo
fibonacci
introduced
the
fibonacci
sequence
to
the
western
world
with
his
book
"liber
abaci"
fibonacci
presented
a
thought
experiment
on
the
growth
of
an
idealized
rabbit
population
in
1658
the
english
physician
and
philosopher
sir
thomas
browne
discussed
"how
nature
geometrizeth"
in
"the
garden
of
cyrus"
citing
pythagorean
numerology
involving
the
number
5
and
the
platonic
form
of
the
quincunx
pattern
the
discourse's
central
chapter
features
examples
and
observations
of
the
quincunx
in
botany
the
belgian
physicist
joseph
plateau
(1801–1883)
formulated
the
mathematical
problem
of
the
existence
of
a
minimal
surface
with
a
given
boundary
which
is
now
named
after
him
he
studied
soap
films
intensively
formulating
plateau's
laws
which
describe
the
structures
formed
by
films
in
foams
ernst
haeckel
(1834–1919)
painted
beautiful
illustrations
of
marine
organisms
in
particular
radiolaria
emphasising
their
symmetry
to
support
his
faux-darwinian
theories
of
evolution
the
american
photographer
wilson
bentley
took
the
first
micrograph
of
a
snowflake
in
1885
in
1952
alan
turing
(1912–1954)
better
known
for
his
work
on
computing
and
codebreaking
wrote
"the
chemical
basis
of
morphogenesis"
an
analysis
of
the
mechanisms
that
would
be
needed
to
create
patterns
in
living
organisms
in
the
process
called
morphogenesis
he
predicted
oscillating
chemical
reactions
in
particular
the
belousov–zhabotinsky
reaction
these
activator-inhibitor
mechanisms
can
turing
suggested
generate
patterns
(dubbed
"turing
patterns")
of
stripes
and
spots
in
animals
and
contribute
to
the
spiral
patterns
seen
in
plant
phyllotaxis
in
1968
the
hungarian
theoretical
biologist
aristid
lindenmayer
(1925–1989)
developed
the
l-system
a
formal
grammar
which
can
be
used
to
model
plant
growth
patterns
in
the
style
of
fractals
l-systems
have
an
alphabet
of
symbols
that
can
be
combined
using
production
rules
to
build
larger
strings
of
symbols
and
a
mechanism
for
translating
the
generated
strings
into
geometric
structures
in
1975
after
centuries
of
slow
development
of
the
mathematics
of
patterns
by
gottfried
leibniz
georg
cantor
helge
von
koch
wacław
sierpiński
and
others
benoît
mandelbrot
wrote
a
famous
paper
"how
long
is
the
coast
of
britain?
statistical
self-similarity
and
fractional
dimension"
crystallising
mathematical
thought
into
the
concept
of
the
fractal
living
things
like
orchids
hummingbirds
and
the
peacock's
tail
have
abstract
designs
with
a
beauty
of
form
pattern
and
colour
that
artists
struggle
to
match
the
beauty
that
people
perceive
in
nature
has
causes
at
different
levels
notably
in
the
mathematics
that
governs
what
patterns
can
physically
form
and
among
living
things
in
the
effects
of
natural
selection
that
govern
how
patterns
evolve
mathematics
seeks
to
discover
and
explain
abstract
patterns
or
regularities
of
all
kinds
visual
patterns
in
nature
find
explanations
in
chaos
theory
fractals
logarithmic
spirals
topology
and
other
mathematical
patterns
for
example
l-systems
form
convincing
models
of
different
patterns
of
tree
growth
the
laws
of
physics
apply
the
abstractions
of
mathematics
to
the
real
world
often
as
if
it
were
perfect
for
example
a
crystal
is
perfect
when
it
has
no
structural
defects
such
as
dislocations
and
is
fully
symmetric
exact
mathematical
perfection
can
only
approximate
real
objects
visible
patterns
in
nature
are
governed
by
physical
laws;
for
example
meanders
can
be
explained
using
fluid
dynamics
in
biology
natural
selection
can
cause
the
development
of
patterns
in
living
things
for
several
reasons
including
camouflage
sexual
selection
and
different
kinds
of
signalling
including
mimicry
and
cleaning
symbiosis
in
plants
the
shapes
colours
and
patterns
of
insect-pollinated
flowers
like
the
lily
have
evolved
to
attract
insects
such
as
bees
radial
patterns
of
colours
and
stripes
some
visible
only
in
ultraviolet
light
serve
as
nectar
guides
that
can
be
seen
at
a
distance
symmetry
is
pervasive
in
living
things
animals
mainly
have
bilateral
or
mirror
symmetry
as
do
the
leaves
of
plants
and
some
flowers
such
as
orchids
plants
often
have
radial
or
rotational
symmetry
as
do
many
flowers
and
some
groups
of
animals
such
as
sea
anemones
fivefold
symmetry
is
found
in
the
echinoderms
the
group
that
includes
starfish
sea
urchins
and
sea
lilies
among
non-living
things
snowflakes
have
striking
sixfold
symmetry;
each
flake's
structure
forms
a
record
of
the
varying
conditions
during
its
crystallization
with
nearly
the
same
pattern
of
growth
on
each
of
its
six
arms
crystals
in
general
have
a
variety
of
symmetries
and
crystal
habits;
they
can
be
cubic
or
octahedral
but
true
crystals
cannot
have
fivefold
symmetry
(unlike
quasicrystals)
rotational
symmetry
is
found
at
different
scales
among
non-living
things
including
the
crown-shaped
splash
pattern
formed
when
a
drop
falls
into
a
pond
and
both
the
spheroidal
shape
and
rings
of
a
planet
like
saturn
symmetry
has
a
variety
of
causes
radial
symmetry
suits
organisms
like
sea
anemones
whose
adults
do
not
move:
food
and
threats
may
arrive
from
any
direction
but
animals
that
move
in
one
direction
necessarily
have
upper
and
lower
sides
head
and
tail
ends
and
therefore
a
left
and
a
right
the
head
becomes
specialised
with
a
mouth
and
sense
organs
(cephalisation)
and
the
body
becomes
bilaterally
symmetric
(though
internal
organs
need
not
be)
more
puzzling
is
the
reason
for
the
fivefold
(pentaradiate)
symmetry
of
the
echinoderms
early
echinoderms
were
bilaterally
symmetrical
as
their
larvae
still
are
sumrall
and
wray
argue
that
the
loss
of
the
old
symmetry
had
both
developmental
and
ecological
causes
fractals
are
infinitely
self-similar
iterated
mathematical
constructs
having
fractal
dimension
infinite
iteration
is
not
possible
in
nature
so
all
'fractal'
patterns
are
only
approximate
for
example
the
leaves
of
ferns
and
umbellifers
(apiaceae)
are
only
self-similar
(pinnate)
to
2
3
or
4
levels
fern-like
growth
patterns
occur
in
plants
and
in
animals
including
bryozoa
corals
hydrozoa
like
the
air
fern
"sertularia
argentea"
and
in
non-living
things
notably
electrical
discharges
lindenmayer
system
fractals
can
model
different
patterns
of
tree
growth
by
varying
a
small
number
of
parameters
including
branching
angle
distance
between
nodes
or
branch
points
(internode
length)
and
number
of
branches
per
branch
point
fractal-like
patterns
occur
widely
in
nature
in
phenomena
as
diverse
as
clouds
river
networks
geologic
fault
lines
mountains
coastlines
animal
coloration
snow
flakes
crystals
blood
vessel
branching
actin
cytoskeleton
and
ocean
waves
spirals
are
common
in
plants
and
in
some
animals
notably
molluscs
for
example
in
the
nautilus
a
cephalopod
mollusc
each
chamber
of
its
shell
is
an
approximate
copy
of
the
next
one
scaled
by
a
constant
factor
and
arranged
in
a
logarithmic
spiral
given
a
modern
understanding
of
fractals
a
growth
spiral
can
be
seen
as
a
special
case
of
self-similarity
plant
spirals
can
be
seen
in
phyllotaxis
the
arrangement
of
leaves
on
a
stem
and
in
the
arrangement
(parastichy)
of
other
parts
as
in
composite
flower
heads
and
seed
heads
like
the
sunflower
or
fruit
structures
like
the
pineapple
and
snake
fruit
as
well
as
in
the
pattern
of
scales
in
pine
cones
where
multiple
spirals
run
both
clockwise
and
anticlockwise
these
arrangements
have
explanations
at
different
levels
–
mathematics
physics
chemistry
biology
–
each
individually
correct
but
all
necessary
together
phyllotaxis
spirals
can
be
generated
mathematically
from
fibonacci
ratios:
the
fibonacci
sequence
runs
1
1
2
3
5
8
13
(each
subsequent
number
being
the
sum
of
the
two
preceding
ones)
for
example
when
leaves
alternate
up
a
stem
one
rotation
of
the
spiral
touches
two
leaves
so
the
pattern
or
ratio
is
1/2
in
hazel
the
ratio
is
1/3;
in
apricot
it
is
2/5;
in
pear
it
is
3/8;
in
almond
it
is
5/13
in
disc
phyllotaxis
as
in
the
sunflower
and
daisy
the
florets
are
arranged
in
fermat's
spiral
with
fibonacci
numbering
at
least
when
the
flowerhead
is
mature
so
all
the
elements
are
the
same
size
fibonacci
ratios
approximate
the
golden
angle
137508°
which
governs
the
curvature
of
fermat's
spiral
from
the
point
of
view
of
physics
spirals
are
lowest-energy
configurations
which
emerge
spontaneously
through
self-organizing
processes
in
dynamic
systems
from
the
point
of
view
of
chemistry
a
spiral
can
be
generated
by
a
reaction-diffusion
process
involving
both
activation
and
inhibition
phyllotaxis
is
controlled
by
proteins
that
manipulate
the
concentration
of
the
plant
hormone
auxin
which
activates
meristem
growth
alongside
other
mechanisms
to
control
the
relative
angle
of
buds
around
the
stem
from
a
biological
perspective
arranging
leaves
as
far
apart
as
possible
in
any
given
space
is
favoured
by
natural
selection
as
it
maximises
access
to
resources
especially
sunlight
for
photosynthesis
in
mathematics
a
dynamical
system
is
chaotic
if
it
is
(highly)
sensitive
to
initial
conditions
(the
so-called
"butterfly
effect")
which
requires
the
mathematical
properties
of
topological
mixing
and
dense
periodic
orbits
alongside
fractals
chaos
theory
ranks
as
an
essentially
universal
influence
on
patterns
in
nature
there
is
a
relationship
between
chaos
and
fractals—the
"strange
attractors"
in
chaotic
systems
have
a
fractal
dimension
some
cellular
automata
simple
sets
of
mathematical
rules
that
generate
patterns
have
chaotic
behaviour
notably
stephen
wolfram's
rule
30
vortex
streets
are
zigzagging
patterns
of
whirling
vortices
created
by
the
unsteady
separation
of
flow
of
a
fluid
most
often
air
or
water
over
obstructing
objects
smooth
(laminar)
flow
starts
to
break
up
when
the
size
of
the
obstruction
or
the
velocity
of
the
flow
become
large
enough
compared
to
the
viscosity
of
the
fluid
meanders
are
sinuous
bends
in
rivers
or
other
channels
which
form
as
a
fluid
most
often
water
flows
around
bends
as
soon
as
the
path
is
slightly
curved
the
size
and
curvature
of
each
loop
increases
as
helical
flow
drags
material
like
sand
and
gravel
across
the
river
to
the
inside
of
the
bend
the
outside
of
the
loop
is
left
clean
and
unprotected
so
erosion
accelerates
further
increasing
the
meandering
in
a
powerful
positive
feedback
loop
waves
are
disturbances
that
carry
energy
as
they
move
mechanical
waves
propagate
through
a
medium
–
air
or
water
making
it
oscillate
as
they
pass
by
wind
waves
are
sea
surface
waves
that
create
the
characteristic
chaotic
pattern
of
any
large
body
of
water
though
their
statistical
behaviour
can
be
predicted
with
wind
wave
models
as
waves
in
water
or
wind
pass
over
sand
they
create
patterns
of
ripples
when
winds
blow
over
large
bodies
of
sand
they
create
dunes
sometimes
in
extensive
dune
fields
as
in
the
taklamakan
desert
dunes
may
form
a
range
of
patterns
including
crescents
very
long
straight
lines
stars
domes
parabolas
and
longitudinal
or
seif
('sword')
shapes
barchans
or
crescent
dunes
are
produced
by
wind
acting
on
desert
sand;
the
two
horns
of
the
crescent
and
the
slip
face
point
downwind
sand
blows
over
the
upwind
face
which
stands
at
about
15
degrees
from
the
horizontal
and
falls
onto
the
slip
face
where
it
accumulates
up
to
the
angle
of
repose
of
the
sand
which
is
about
35
degrees
when
the
slip
face
exceeds
the
angle
of
repose
the
sand
avalanches
which
is
a
nonlinear
behaviour:
the
addition
of
many
small
amounts
of
sand
causes
nothing
much
to
happen
but
then
the
addition
of
a
further
small
amount
suddenly
causes
a
large
amount
to
avalanche
apart
from
this
nonlinearity
barchans
behave
rather
like
solitary
waves
a
soap
bubble
forms
a
sphere
a
surface
with
minimal
area
—
the
smallest
possible
surface
area
for
the
volume
enclosed
two
bubbles
together
form
a
more
complex
shape:
the
outer
surfaces
of
both
bubbles
are
spherical;
these
surfaces
are
joined
by
a
third
spherical
surface
as
the
smaller
bubble
bulges
slightly
into
the
larger
one
a
foam
is
a
mass
of
bubbles;
foams
of
different
materials
occur
in
nature
foams
composed
of
soap
films
obey
plateau's
laws
which
require
three
soap
films
to
meet
at
each
edge
at
120°
and
four
soap
edges
to
meet
at
each
vertex
at
the
tetrahedral
angle
of
about
1095°
plateau's
laws
further
require
films
to
be
smooth
and
continuous
and
to
have
a
constant
average
curvature
at
every
point
for
example
a
film
may
remain
nearly
flat
on
average
by
being
curved
up
in
one
direction
(say
left
to
right)
while
being
curved
downwards
in
another
direction
(say
front
to
back)
structures
with
minimal
surfaces
can
be
used
as
tents
lord
kelvin
identified
the
problem
of
the
most
efficient
way
to
pack
cells
of
equal
volume
as
a
foam
in
1887;
his
solution
uses
just
one
solid
the
bitruncated
cubic
honeycomb
with
very
slightly
curved
faces
to
meet
plateau's
laws
no
better
solution
was
found
until
1993
when
denis
weaire
and
robert
phelan
proposed
the
weaire–phelan
structure;
the
beijing
national
aquatics
center
adapted
the
structure
for
their
outer
wall
in
the
2008
summer
olympics
at
the
scale
of
living
cells
foam
patterns
are
common;
radiolarians
sponge
spicules
silicoflagellate
exoskeletons
and
the
calcite
skeleton
of
a
sea
urchin
"cidaris
rugosa"
all
resemble
mineral
casts
of
plateau
foam
boundaries
the
skeleton
of
the
radiolarian
"aulonia
hexagona"
a
beautiful
marine
form
drawn
by
ernst
haeckel
looks
as
if
it
is
a
sphere
composed
wholly
of
hexagons
but
this
is
mathematically
impossible
the
euler
characteristic
states
that
for
any
convex
polyhedron
the
number
of
faces
plus
the
number
of
vertices
(corners)
equals
the
number
of
edges
plus
two
a
result
of
this
formula
is
that
any
closed
polyhedron
of
hexagons
has
to
include
exactly
12
pentagons
like
a
soccer
ball
buckminster
fuller
geodesic
dome
or
fullerene
molecule
this
can
be
visualised
by
noting
that
a
mesh
of
hexagons
is
flat
like
a
sheet
of
chicken
wire
but
each
pentagon
that
is
added
forces
the
mesh
to
bend
(there
are
fewer
corners
so
the
mesh
is
pulled
in)
tessellations
are
patterns
formed
by
repeating
tiles
all
over
a
flat
surface
there
are
17
wallpaper
groups
of
tilings
while
common
in
art
and
design
exactly
repeating
tilings
are
less
easy
to
find
in
living
things
the
cells
in
the
paper
nests
of
social
wasps
and
the
wax
cells
in
honeycomb
built
by
honey
bees
are
well-known
examples
among
animals
bony
fish
reptiles
or
the
pangolin
or
fruits
like
the
salak
are
protected
by
overlapping
scales
or
osteoderms
these
form
more-or-less
exactly
repeating
units
though
often
the
scales
in
fact
vary
continuously
in
size
among
flowers
the
snake's
head
fritillary
"fritillaria
meleagris"
have
a
tessellated
chequerboard
pattern
on
their
petals
the
structures
of
minerals
provide
good
examples
of
regularly
repeating
three-dimensional
arrays
despite
the
hundreds
of
thousands
of
known
minerals
there
are
rather
few
possible
types
of
arrangement
of
atoms
in
a
crystal
defined
by
crystal
structure
crystal
system
and
point
group;
for
example
there
are
exactly
14
bravais
lattices
for
the
7
lattice
systems
in
three-dimensional
space
cracks
are
linear
openings
that
form
in
materials
to
relieve
stress
when
an
elastic
material
stretches
or
shrinks
uniformly
it
eventually
reaches
its
breaking
strength
and
then
fails
suddenly
in
all
directions
creating
cracks
with
120
degree
joints
so
three
cracks
meet
at
a
node
conversely
when
an
inelastic
material
fails
straight
cracks
form
to
relieve
the
stress
further
stress
in
the
same
direction
would
then
simply
open
the
existing
cracks;
stress
at
right
angles
can
create
new
cracks
at
90
degrees
to
the
old
ones
thus
the
pattern
of
cracks
indicates
whether
the
material
is
elastic
or
not
in
a
tough
fibrous
material
like
oak
tree
bark
cracks
form
to
relieve
stress
as
usual
but
they
do
not
grow
long
as
their
growth
is
interrupted
by
bundles
of
strong
elastic
fibres
since
each
species
of
tree
has
its
own
structure
at
the
levels
of
cell
and
of
molecules
each
has
its
own
pattern
of
splitting
in
its
bark
leopards
and
ladybirds
are
spotted;
angelfish
and
zebras
are
striped
these
patterns
have
an
evolutionary
explanation:
they
have
functions
which
increase
the
chances
that
the
offspring
of
the
patterned
animal
will
survive
to
reproduce
one
function
of
animal
patterns
is
camouflage;
for
instance
a
leopard
that
is
harder
to
see
catches
more
prey
another
function
is
signalling
—
for
instance
a
ladybird
is
less
likely
to
be
attacked
by
predatory
birds
that
hunt
by
sight
if
it
has
bold
warning
colours
and
is
also
distastefully
bitter
or
poisonous
or
mimics
other
distasteful
insects
a
young
bird
may
see
a
warning
patterned
insect
like
a
ladybird
and
try
to
eat
it
but
it
will
only
do
this
once;
very
soon
it
will
spit
out
the
bitter
insect;
the
other
ladybirds
in
the
area
will
remain
undisturbed
the
young
leopards
and
ladybirds
inheriting
genes
that
somehow
create
spottedness
survive
but
while
these
evolutionary
and
functional
arguments
explain
why
these
animals
need
their
patterns
they
do
not
explain
how
the
patterns
are
formed
alan
turing
and
later
the
mathematical
biologist
james
murray
described
a
mechanism
that
spontaneously
creates
spotted
or
striped
patterns:
a
reaction-diffusion
system
the
cells
of
a
young
organism
have
genes
that
can
be
switched
on
by
a
chemical
signal
a
morphogen
resulting
in
the
growth
of
a
certain
type
of
structure
say
a
darkly
pigmented
patch
of
skin
if
the
morphogen
is
present
everywhere
the
result
is
an
even
pigmentation
as
in
a
black
leopard
but
if
it
is
unevenly
distributed
spots
or
stripes
can
result
turing
suggested
that
there
could
be
feedback
control
of
the
production
of
the
morphogen
itself
this
could
cause
continuous
fluctuations
in
the
amount
of
morphogen
as
it
diffused
around
the
body
a
second
mechanism
is
needed
to
create
standing
wave
patterns
(to
result
in
spots
or
stripes):
an
inhibitor
chemical
that
switches
off
production
of
the
morphogen
and
that
itself
diffuses
through
the
body
more
quickly
than
the
morphogen
resulting
in
an
activator-inhibitor
scheme
the
belousov–zhabotinsky
reaction
is
a
non-biological
example
of
this
kind
of
scheme
a
chemical
oscillator
later
research
has
managed
to
create
convincing
models
of
patterns
as
diverse
as
zebra
stripes
giraffe
blotches
jaguar
spots
(medium-dark
patches
surrounded
by
dark
broken
rings)
and
ladybird
shell
patterns
(different
geometrical
layouts
of
spots
and
stripes
see
illustrations)
richard
prum's
activation-inhibition
models
developed
from
turing's
work
use
six
variables
to
account
for
the
observed
range
of
nine
basic
within-feather
pigmentation
patterns
from
the
simplest
a
central
pigment
patch
via
concentric
patches
bars
chevrons
eye
spot
pair
of
central
spots
rows
of
paired
spots
and
an
array
of
dots
more
elaborate
models
simulate
complex
feather
patterns
in
the
guineafowl
"numida
meleagris"
in
which
the
individual
feathers
feature
transitions
from
bars
at
the
base
to
an
array
of
dots
at
the
far
(distal)
end
these
require
an
oscillation
created
by
two
inhibiting
signals
with
interactions
in
both
space
and
time
patterns
can
form
for
other
reasons
in
the
vegetated
landscape
of
tiger
bush
and
fir
waves
tiger
bush
stripes
occur
on
arid
slopes
where
plant
growth
is
limited
by
rainfall
each
roughly
horizontal
stripe
of
vegetation
effectively
collects
the
rainwater
from
the
bare
zone
immediately
above
it
fir
waves
occur
in
forests
on
mountain
slopes
after
wind
disturbance
during
regeneration
when
trees
fall
the
trees
that
they
had
sheltered
become
exposed
and
are
in
turn
more
likely
to
be
damaged
so
gaps
tend
to
expand
downwind
meanwhile
on
the
windward
side
young
trees
grow
protected
by
the
wind
shadow
of
the
remaining
tall
trees
natural
patterns
are
sometimes
formed
by
animals
as
in
the
mima
mounds
of
the
northwestern
united
states
and
some
other
areas
which
appear
to
be
created
over
many
years
by
the
burrowing
activities
of
pocket
gophers
while
the
so-called
fairy
circles
of
namibia
appear
to
be
created
by
the
interaction
of
competing
groups
of
sand
termites
along
with
competition
for
water
among
the
desert
plants
in
permafrost
soils
with
an
active
upper
layer
subject
to
annual
freeze
and
thaw
patterned
ground
can
form
creating
circles
nets
ice
wedge
polygons
steps
and
stripes
thermal
contraction
causes
shrinkage
cracks
to
form;
in
a
thaw
water
fills
the
cracks
expanding
to
form
ice
when
next
frozen
and
widening
the
cracks
into
wedges
these
cracks
may
join
up
to
form
polygons
and
other
shapes
the
fissured
pattern
that
develops
on
vertebrate
brains
are
caused
by
a
physical
process
of
constrained
expansion
dependent
on
two
geometric
parameters:
relative
tangential
cortical
expansion
and
relative
thickness
of
the
cortex
similar
patterns
of
gyri
(peaks)
and
sulci
(troughs)
have
been
demonstrated
in
models
of
the
brain
starting
from
smooth
layered
gels
with
the
patterns
caused
by
compressive
mechanical
forces
resulting
from
the
expansion
of
the
outer
layer
(representing
the
cortex)
after
the
addition
of
a
solvent
numerical
models
in
computer
simulations
support
natural
and
experimental
observations
that
the
surface
folding
patterns
increase
in
larger
brains
footnotes
citations
pioneering
authors
general
books
patterns
from
nature
(as
art)
natural
landscape
a
natural
landscape
is
the
original
landscape
that
exists
before
it
is
acted
upon
by
human
culture
the
natural
landscape
and
the
cultural
landscape
are
separate
parts
of
the
landscape
however
in
the
twenty-first
century
landscapes
that
are
totally
untouched
by
human
activity
no
longer
exist
so
that
reference
is
sometimes
now
made
to
degrees
of
naturalness
within
a
landscape
in
"silent
spring"
(1962)
rachel
carson
describes
a
roadside
verge
as
it
used
to
look:
"along
the
roads
laurel
viburnum
and
alder
great
ferns
and
wildflowers
delighted
the
traveler’s
eye
through
much
of
the
year"
and
then
how
it
looks
now
following
the
use
of
herbicides:
"the
roadsides
once
so
attractive
were
now
lined
with
browned
and
withered
vegetation
as
though
swept
by
fire"
even
though
the
landscape
before
it
is
sprayed
is
biologically
degraded
and
may
well
contains
alien
species
the
concept
of
what
might
constitute
a
natural
landscape
can
still
be
deduced
from
the
context
the
phrase
"natural
landscape"
was
first
used
in
connection
with
landscape
painting
and
landscape
gardening
to
contrast
a
formal
style
with
a
more
natural
one
closer
to
nature
alexander
von
humboldt
(1769
–
1859)
was
to
further
conceptualize
this
into
the
idea
of
a
natural
landscape
"separate"
from
the
cultural
landscape
then
in
1908
geographer
otto
schlüter
developed
the
terms
original
landscape
("urlandschaft")
and
its
opposite
cultural
landscape
("kulturlandschaft")
in
an
attempt
to
give
the
science
of
geography
a
subject
matter
that
was
different
from
the
other
sciences
an
early
use
of
the
actual
phrase
"natural
landscape"
by
a
geographer
can
be
found
in
carl
o
sauer's
paper
"the
morphology
of
landscape"
(1925)
the
concept
of
a
natural
landscape
was
first
developed
in
connection
with
landscape
painting
though
the
actual
term
itself
was
first
used
in
relation
to
landscape
gardening
in
both
cases
it
was
used
to
contrast
a
formal
style
with
a
more
natural
one
that
is
closer
to
nature
chunglin
kwa
suggests
"that
a
seventeenth-century
or
early-eighteenth-century
person
could
experience
natural
scenery
‘just
like
on
a
painting’
and
so
with
or
without
the
use
of
the
word
itself
designate
it
as
a
landscape"
with
regard
to
landscape
gardening
john
aikin
commented
in
1794:
"whatever
therefore
there
be
of
"novelty"
in
the
singular
scenery
of
an
artificial
garden
it
is
soon
exhausted
whereas
the
infinite
diversity
of
a
natural
landscape
presents
an
inexhaustible
flore
of
new
forms"
writing
in
1844
the
prominent
american
landscape
gardener
andrew
jackson
downing
comments:
"straight
canals
round
or
oblong
pieces
of
water
and
all
the
regular
forms
of
the
geometric
mode
would
evidently
be
in
violent
opposition
to
the
whole
character
and
expression
of
natural
landscape"
in
his
extensive
travels
in
south
america
alexander
von
humboldt
became
the
first
to
conceptualize
a
natural
landscape
separate
from
the
cultural
landscape
though
he
does
not
actually
use
these
terms
andrew
jackson
downing
was
aware
of
and
sympathetic
to
humboldt's
ideas
which
therefore
influenced
american
landscape
gardening
subsequently
the
geographer
otto
schlüter
in
1908
argued
that
by
defining
geography
as
a
"landschaftskunde"
(landscape
science)
would
give
geography
a
logical
subject
matter
shared
by
no
other
discipline
he
defined
two
forms
of
landscape:
the
"urlandschaft"
(original
landscape)
or
landscape
that
existed
before
major
human
induced
changes
and
the
"kulturlandschaft"
(cultural
landscape)
a
landscape
created
by
human
culture
schlüter
argued
that
the
major
task
of
geography
was
to
trace
the
changes
in
these
two
landscapes
the
term
natural
landscape
is
sometimes
used
as
a
synonym
for
wilderness
but
for
geographers
natural
landscape
is
a
scientific
term
which
refers
to
the
biological
geological
climatological
and
other
aspects
of
a
landscape
not
the
cultural
values
that
are
implied
by
the
word
wilderness
matters
are
complicated
by
the
fact
that
the
words
nature
and
natural
have
more
than
one
meaning
on
the
one
hand
there
is
the
main
dictionary
meaning
for
nature:
"the
phenomena
of
the
physical
world
collectively
including
plants
animals
the
landscape
and
other
features
and
products
of
the
earth
as
opposed
to
humans
or
human
creations"
on
the
other
hand
there
is
the
growing
awareness
especially
since
charles
darwin
of
humanities
biological
affinity
with
nature
the
dualism
of
the
first
definition
has
its
roots
is
an
"ancient
concept"
because
early
people
viewed
"nature
or
the
nonhuman
world
[…]
as
a
divine
"other"
godlike
in
its
separation
from
humans"
in
the
west
christianity's
myth
of
the
fall
that
is
the
expulsion
of
humankind
from
the
garden
of
eden
where
all
creation
lived
in
harmony
into
an
imperfect
world
has
been
the
major
influence
cartesian
dualism
from
the
seventeenth
century
on
further
reinforced
this
dualistic
thinking
about
nature
with
this
dualism
goes
value
judgement
as
to
the
superiority
of
the
natural
over
the
artificial
modern
science
however
is
moving
towards
a
holistic
view
of
nature
what
is
meant
by
natural
within
the
american
conservation
movement
has
been
changing
over
the
last
century
and
a
half
in
the
mid-nineteenth
century
american
began
to
realize
that
the
land
was
becoming
more
and
more
domesticated
and
wildlife
was
disappearing
this
led
to
the
creation
of
american
national
parks
and
other
conservation
sites
initially
it
was
believed
that
all
that
was
needed
to
do
was
to
separate
what
was
seen
as
natural
landscape
and
"avoid
disturbances
such
as
logging
grazing
fire
and
insect
outbreaks"
this
and
subsequent
environmental
policy
until
recently
was
influenced
by
ideas
of
the
wilderness
however
this
policy
was
not
consistently
applied
and
in
yellowstone
park
to
take
one
example
the
existing
ecology
was
altered
firstly
by
the
exclusion
of
native
americans
and
later
with
the
virtual
extermination
of
the
wolf
population
a
century
later
in
the
mid-twentieth
century
it
began
to
be
believed
that
the
earlier
policy
of
"protection
from
disturbance
was
inadequate
to
preserve
park
values"
and
that
is
that
direct
human
intervention
was
necessary
to
restore
the
landscape
of
national
parks
to
its
‘’natural’’
condition
in
1963
the
leopold
report
argued
that
"a
national
park
should
represent
a
vignette
of
primitive
america"
this
policy
change
eventually
led
to
the
restoration
of
wolves
in
yellowstone
park
in
the
1990s
however
recent
research
in
various
disciplines
indicates
that
a
pristine
natural
or
"primitive"
landscape
is
a
myth
and
it
now
realised
that
people
have
been
changing
the
natural
into
a
cultural
landscape
for
a
long
while
and
that
there
are
few
places
untouched
in
some
way
from
human
influence
the
earlier
conservation
policies
were
now
seen
as
cultural
interventions
the
idea
of
what
is
natural
and
what
artificial
or
cultural
and
how
to
maintain
the
natural
elements
in
a
landscape
has
been
further
complicated
by
the
discovery
of
global
warming
and
how
it
is
changing
natural
landscapes
also
important
is
a
reaction
recently
amongst
scholars
against
dualistic
thinking
about
nature
and
culture
maria
kaika
comments:
"nowadays
we
are
beginning
to
see
nature
and
culture
as
intertwined
once
again
–
not
ontologically
separated
anymore
[…]what
i
used
to
perceive
as
a
compartmentalized
world
consisting
of
neatly
and
tightly
sealed
autonomous
‘space
envelopes’
(the
home
the
city
and
nature)
was
in
fact
a
messy
socio-spatial
continuum”
and
william
cronon
argues
against
the
idea
of
wilderness
because
it
"involves
a
dualistic
vision
in
which
the
human
is
entirely
outside
the
natural"
and
affirms
that
"wildness
(as
opposed
to
wilderness)
can
be
found
anywhere"
even
"in
the
cracks
of
a
manhattan
sidewalk"
according
to
cronon
we
have
to
"abandon
the
dualism
that
sees
the
tree
in
the
garden
as
artificial
[…]
and
the
tree
in
the
wilderness
as
natural
[…]
both
in
some
ultimate
sense
are
wild"
here
he
bends
somewhat
the
regular
dictionary
meaning
of
wild
to
emphasise
that
nothing
natural
even
in
a
garden
is
fully
under
human
control
the
landscape
of
europe
has
considerably
altered
by
people
and
even
in
an
area
like
the
cairngorm
mountains
of
scotland
with
a
low
population
density
only
"
the
high
summits
of
the
cairngorm
mountains
consist
entirely
of
natural
elements
these
"high
summits"
are
of
course
only
part
of
the
cairngorms
and
there
are
no
longer
wolves
bears
wild
boar
or
lynx
in
scotland's
wilderness
the
scots
pine
in
the
form
of
the
caledonian
forest
also
covered
much
more
of
the
scottish
landscape
than
today
the
swiss
national
park
however
represent
a
more
natural
landscape
it
was
founded
in
1914
and
is
one
of
the
earliest
national
parks
in
europe
visitors
are
not
allowed
to
leave
the
motor
road
or
paths
through
the
park
make
fire
or
camp
the
only
building
within
the
park
is
chamanna
cluozza
mountain
hut
it
is
also
forbidden
to
disturb
the
animals
or
the
plants
or
to
take
home
anything
found
in
the
park
dogs
are
not
allowed
due
to
these
strict
rules
the
swiss
national
park
is
the
only
park
in
the
alps
who
has
been
categorized
by
the
iucn
as
a
strict
nature
reserve
which
is
the
highest
protection
level
no
place
on
the
earth
is
unaffected
by
people
and
their
culture
people
are
part
of
biodiversity
but
human
activity
affects
biodiversity
and
this
alters
the
natural
landscape
mankind
have
altered
landscape
to
such
an
extent
that
few
places
on
earth
remain
pristine
but
once
free
of
human
influences
the
landscape
can
return
to
a
natural
or
near
natural
state
even
the
remote
yukon
and
alaskan
wilderness
the
bi-national
kluane-wrangell-st
elias-glacier
bay-tatshenshini-alsek
park
system
comprising
kluane
wrangell-st
elias
glacier
bay
and
tatshenshini-alsek
parks
a
unesco
world
heritage
site
is
not
free
from
human
influence
because
the
kluane
national
park
lies
within
the
traditional
territories
of
the
champagne
and
aishihik
first
nations
and
kluane
first
nation
who
have
a
long
history
of
living
in
this
region
through
their
respective
final
agreements
with
the
canadian
government
they
have
made
into
law
their
rights
to
harvest
in
this
region
cultural
forces
intentionally
or
unintentionally
have
an
influence
upon
the
landscape
cultural
landscapes
are
places
or
artifacts
created
and
maintained
by
people
examples
of
cultural
intrusions
into
a
landscape
are:
fences
roads
parking
lots
sand
pits
buildings
hiking
trails
management
of
plants
including
the
introduction
of
invasive
species
extraction
or
removal
of
plants
management
of
animals
mining
hunting
natural
landscaping
farming
and
forestry
pollution
areas
that
might
be
confused
with
a
natural
landscape
include
public
parks
farms
orchards
artificial
lakes
and
reservoirs
managed
forests
golf
courses
nature
center
trails
gardens
aesthetics
of
nature
aesthetics
of
nature
is
a
sub-field
of
philosophical
ethics
and
refers
to
the
study
of
natural
objects
from
their
aesthetical
perspective
aesthetics
of
nature
developed
as
a
sub-field
of
philosophical
ethics
in
the
18th
and
19th
century
the
aesthetics
of
nature
advanced
the
concepts
of
disinterestedness
the
pictures
and
the
introduction
of
the
idea
of
positive
aesthetics
the
first
major
developments
of
nature
occurred
in
the
18th
century
the
concept
of
disinterestedness
had
been
explained
by
many
thinkers
anthony
ashley-cooper
introduced
the
concept
as
a
way
of
characterizing
the
notion
of
the
aesthetic
later
magnified
by
francis
hutcheson
who
expanded
it
to
exclude
personal
and
utilitarianism
interests
and
associations
of
a
more
general
nature
from
aesthetic
experience
this
concept
was
further
developed
by
archibald
alison
who
referred
it
to
a
particular
state
of
mind
the
theory
of
disinterestedness
opened
doors
for
a
better
understanding
of
the
aesthetics
dimensions
of
nature
in
terms
of
three
conceptualizations:
objects
experienced
as
beautiful
tend
to
be
small
smooth
and
fair
in
color
in
contrast
objects
viewed
as
sublime
tend
to
be
powerful
intense
and
terrifying
picturesque
items
are
a
mixture
of
both
which
can
be
seen
as
varied
and
irregular
rich
and
forceful
and
even
vibrant
cognitive
and
non-cognitive
approaches
of
nature
have
directed
their
focus
from
natural
environments
to
the
consideration
of
human
and
human
influenced
environments
and
developed
aesthetic
investigations
of
everyday
life(carlson
and
lintott
2007;
parsons
2008a;
carlson
2010)
people
may
be
mistaken
by
the
art
object
analogy
for
instance
a
sandhill
crane
is
not
an
art
object;
an
art
object
is
not
a
sandhill
crane
in
fact
an
art
object
should
be
called
an
"artifact"
the
crane
is
wildlife
on
its
own
and
is
not
an
art
object
this
can
be
related
to
satio's
definition
of
the
cognitive
view
in
elaboration
the
crane
lives
through
various
ecosystems
such
as
yellowstone
nature
is
a
living
system
which
includes
animals
plants
and
eco-systems
in
contrast
an
art
object
has
no
regeneration
evolutionary
history
or
metabolism
an
individual
may
be
in
the
forest
and
perceive
it
as
beautiful
because
of
the
plethora
of
colors
such
as
red
green
and
yellow
this
is
a
result
of
the
chemicals
interacting
with
chlorophyll
an
individual's
aesthetic
experience
may
increase;
however
none
of
the
things
mentioned
have
anything
to
do
with
what
is
really
going
on
in
the
forest
the
chlorophyll
is
capturing
solar
energy
and
the
residual
chemicals
protect
the
trees
from
insect
grazing
any
color
perceived
by
human
visitors
for
a
few
hours
is
entirely
different
from
what
is
really
happening
according
to
leopold
the
three
features
of
ecosystems
that
generate
land
ethic
are
integrity
stability
and
beauty
none
of
the
mentioned
features
are
real
in
nature
ecosystems
are
not
stable:
they
are
dramatically
changing
and
they
have
little
integration;
ergo
beauty
is
in
the
eye
of
the
beholder
in
a
post-modern
approach
when
an
individual
engages
in
aesthetically
appreciating
a
natural
thing
we
give
meaning
to
the
thing
we
appreciate
and
in
that
meaning
we
express
and
develop
our
own
attitudes
values
and
beliefs
our
interest
in
natural
things
are
not
only
a
passive
reflection
of
our
inclinations
as
croce
describes
as
the
appreciation
of
nature
as
looking
in
a
mirror
or
what
we
might
call
our
inward
life;
but
may
instead
be
the
things
we
come
across
in
nature
that
engage
and
stimulate
our
imagination
as
a
result
we
are
challenged
to
think
differently
and
apply
thoughts
and
associations
to
in
new
situations
and
ways
as
a
characterization
of
the
appreciation
of
art
nature
aestheticists
argue
that
post
modernism
is
a
mistaken
view
because
we
do
not
have
a
case
of
anything
goesthe
aesthetics
appreciation
of
art
is
governed
by
some
normative
standards
in
the
world
of
art
criticism
may
take
place
when
people
come
together
and
discuss
books
and
films
or
critics
write
appraisals
for
publications
on
the
contrary
there
are
not
obvious
instances
of
debate
and
appraisals
where
different
judgments
about
the
aesthetics
of
character
of
nature
are
evaluated
physis
physis
(greek:
"phusis")
is
a
greek
theological
philosophical
and
scientific
term
usually
translated
into
english
as
"nature"
the
term
is
central
to
greek
philosophy
and
as
a
consequence
to
western
philosophy
as
a
whole
in
pre-socratic
usage
"phusis"
was
contrasted
with
"law
human
convention"
since
aristotle
however
the
"physical"
(the
subject
matter
of
"physics"
properly
"natural
things")
has
more
typically
been
juxtaposed
to
the
"metaphysical"
the
word
φύσις
is
a
verbal
noun
based
on
φύω
"to
grow
to
appear"
(cognate
with
english
"to
be")
in
homeric
greek
it
is
used
quite
literally
of
the
manner
of
growth
of
a
particular
species
of
plant
in
pre-socratic
philosophy
beginning
with
heraclitus
"phusis"
in
keeping
with
its
etymology
of
"growing
becoming"
is
always
used
in
the
sense
of
the
"natural"
"development"
although
the
focus
might
lie
either
with
the
origin
or
the
process
or
the
end
result
of
the
process
there
is
some
evidence
that
by
the
6th
century
bc
beginning
with
the
ionian
school
the
word
could
also
be
used
in
the
comprehensive
sense
as
referring
to
""all"
things"
as
it
were
"nature"
in
the
sense
of
"universe"
in
the
sophist
tradition
the
term
stood
in
opposition
to
"nomos"
()
"law"
or
"custom"
in
the
debate
on
which
parts
of
human
existence
are
natural
and
which
are
due
to
convention
the
contrast
of
"phisis"
vs
"nomos"
could
be
applied
to
any
subject
much
like
the
modern
contrast
of
"nature
vs
nurture"
in
book
10
of
"laws"
plato
criticizes
those
who
write
works
"peri
phuseōs"
the
criticism
is
that
such
authors
tend
to
focus
on
a
purely
"naturalistic"
explanation
of
the
world
ignoring
the
role
of
"intention"
or
"technē"
and
thus
becoming
prone
to
the
error
of
naive
atheism
plato
accuses
even
hesiod
of
this
for
the
reason
that
the
gods
in
hesiod
"grow"
out
of
primordial
entities
after
the
physical
universe
had
been
established
"because
those
who
use
the
term
mean
to
say
that
nature
is
the
first
creative
power;
but
if
the
soul
turns
out
to
be
the
primeval
element
and
not
fire
or
air
then
in
the
truest
sense
and
beyond
other
things
the
soul
may
be
said
to
exist
"by"
nature;
and
this
would
be
true
if
you
proved
that
the
soul
is
older
than
the
body
but
not
otherwise"
aristotle
sought
out
the
definition
of
"physis"
to
prove
that
there
was
more
than
one
definition
of
"physis"
and
more
than
one
way
to
interpret
nature
"though
aristotle
retains
the
ancient
sense
of
"physis"
as
growth
he
insists
that
an
adequate
definition
of
"physis"
requires
the
different
perspectives
of
the
four
causes
(aitia):
material
efficient
formal
and
final"
aristotle
believed
that
nature
itself
contained
its
own
source
of
matter
(material)
power/motion
(efficiency)
form
and
end
(final)
a
unique
feature
about
aristotle's
definition
of
"physis"
was
his
relationship
between
art
and
nature
aristotle
said
that
"physis"
(nature)
is
dependent
on
techne
(art)
"the
critical
distinction
between
art
and
nature
concerns
their
different
efficient
causes:
nature
is
its
own
source
of
motion
whereas
techne
always
requires
a
source
of
motion
outside
itself"
what
aristotle
was
trying
to
bring
to
light
was
that
art
does
not
contain
within
itself
its
form
or
source
of
motion
consider
the
process
of
an
acorn
becoming
an
oak
tree
this
is
a
natural
process
that
has
its
own
driving
force
behind
it
there
is
no
external
force
pushing
this
acorn
to
its
final
state
rather
it
is
progressively
developing
towards
one
specific
end
(telos)
though
φύσις
was
often
used
in
hellenistic
philosophy
it
is
used
only
14
times
in
the
new
testament
(10
of
those
in
the
writings
of
paul)
its
meaning
varies
throughout
paul's
writings
one
usage
refers
to
the
established
or
natural
order
of
things
as
in
"romans
2:14"
where
paul
writes
"for
when
gentiles
who
do
not
have
the
law
by
"nature"
do
what
the
law
requires
they
are
a
law
to
themselves
even
though
they
do
not
have
the
law"
another
use
of
φύσις
in
the
sense
of
"natural
order"
is
"romans
1:26"
where
he
writes
"the
men
likewise
gave
up
"natural"
relations
with
women
and
were
consumed
with
passion
for
one
another"
in
"1
corinthians
11:14"
paul
asks
"does
not
nature
itself
teach
you
that
if
a
man
wears
long
hair
it
is
a
disgrace
for
him?"
this
use
of
φύσις
as
referring
to
a
"natural
order"
in
"romans
1:26"
and
"1
corinthians
11:14"
may
have
been
influenced
by
stoicism
the
greek
philosophers
including
aristotle
and
the
stoics
are
credited
with
distinguishing
between
man-made
laws
and
a
natural
law
of
universal
validity
but
gerhard
kittel
states
that
the
stoic
philosophers
were
not
able
to
combine
the
concepts
of
νόμος
(law)
and
φύσις
(nature)
to
produce
the
concept
of
"natural
law"
in
the
sense
that
was
made
possible
by
judeo-christian
theology
as
part
of
the
pauline
theology
of
salvation
by
grace
paul
writes
in
"ephesians
2:3"
that
"we
all
once
lived
in
the
passions
of
our
flesh
carrying
out
the
desires
of
the
body
and
the
mind
and
were
by
"nature"
children
of
wrath
like
the
rest
of
mankind
in
the
next
verse
he
writes
"by
grace
you
have
been
saved"
theologians
of
the
early
christian
period
differed
in
the
usage
of
this
term
in
antiochene
circles
it
connoted
the
humanity
or
divinity
of
christ
conceived
as
a
concrete
set
of
characteristics
or
attributes
in
alexandrine
thinking
it
meant
a
concrete
individual
or
independent
existent
and
approximated
to
hypostasis
without
being
a
synonym
while
it
refers
to
much
the
same
thing
as
ousia
it
is
more
empirical
and
descriptive
focussing
on
function
while
ousia
is
metaphysical
and
focuses
more
on
reality
although
found
in
the
context
of
the
trinitarian
debate
it
is
chiefly
important
in
the
christology
of
cyril
of
alexandria
the
greek
adjective
"phusikos"
is
represented
in
various
forms
in
modern
english:
as
"physics"
"the
study
of
nature"
as
"physical"
(via
middle
latin
"physicalis")
referring
both
to
physics
(the
study
of
nature
the
material
universe)
and
to
the
human
body
the
term
physiology
("physiologia")
is
of
16th-century
coinage
(jean
fernel)
the
term
"physique"
for
"the
bodily
constitution
of
a
person"
is
a
19th-century
loan
from
french
in
medicine
the
suffix
"-physis"
occurs
in
such
compounds
as
"symphysis"
"epiphysis"
and
a
few
others
in
the
sense
of
"a
growth"
the
physis
also
refers
to
the
"growth
plate"
or
site
of
growth
at
the
end
of
long
bones
ecosystem
health
ecosystem
health
is
a
metaphor
used
to
describe
the
condition
of
an
ecosystem
ecosystem
condition
can
vary
as
a
result
of
fire
flooding
drought
extinctions
invasive
species
climate
change
mining
overexploitation
in
fishing
farming
or
logging
chemical
spills
and
a
host
of
other
reasons
there
is
no
universally
accepted
benchmark
for
a
healthy
ecosystem
rather
the
apparent
health
status
of
an
ecosystem
can
vary
depending
upon
which
health
metrics
are
employed
in
judging
it
and
which
societal
aspirations
are
driving
the
assessment
advocates
of
the
health
metaphor
argue
for
its
simplicity
as
a
communication
tool
"policy-makers
and
the
public
need
simple
understandable
concepts
like
health"
critics
worry
that
ecosystem
health
a
"value-laden
construct"
is
often
"passed
off
as
science
to
unsuspecting
policy
makers
and
the
public"
the
health
metaphor
applied
to
the
environment
has
been
in
use
at
least
since
the
early
1800s
and
the
great
american
conservationist
aldo
leopold
(1887–1948)
spoke
metaphorically
of
land
health
land
sickness
mutilation
and
violence
when
describing
land
use
practices
the
term
"ecosystem
management"
has
been
in
use
at
least
since
the
1950s
the
term
"ecosystem
health"
has
become
widespread
in
the
ecological
literature
as
a
general
metaphor
meaning
something
good
and
as
an
environmental
quality
goal
in
field
assessments
of
rivers
lakes
seas
and
forests
recently
however
this
metaphor
has
been
subject
of
quantitative
formulation
using
complex
systems
concepts
such
as
criticality
meaning
that
a
healthy
ecosystem
is
in
some
sort
of
balance
between
adaptability
(randomness)
and
robustness
(order)
nevertheless
the
universality
of
criticality
is
still
under
examination
and
is
known
as
the
criticality
hypothesis
which
states
that
systems
in
a
dynamic
regime
shifting
between
order
and
disorder
attain
the
highest
level
of
computational
capabilities
and
achieve
an
optimal
trade-off
between
robustness
and
flexibility
recent
results
in
cell
and
evolutionary
biology
neuroscience
and
computer
science
have
great
interest
in
the
criticality
hypothesis
emphasizing
its
role
as
a
viable
candidate
general
law
in
the
realm
of
adaptive
complex
systems
(see
and
references
therein)
the
term
ecosystem
health
has
been
employed
to
embrace
some
suite
of
environmental
goals
deemed
desirable
edward
grumbine's
highly
cited
paper
"what
is
ecosystem
management?"
surveyed
ecosystem
management
and
ecosystem
health
literature
and
summarized
frequently
encountered
goal
statements:
grumbine
describes
each
of
these
goals
as
a
"value
statement"
and
stresses
the
role
of
human
values
in
setting
ecosystem
management
goals
it
is
the
last
goal
mentioned
in
the
survey
accommodating
humans
that
is
most
contentious
"we
have
observed
that
when
groups
of
stakeholders
work
to
define
…
visions
this
leads
to
debate
over
whether
to
emphasize
ecosystem
health
or
human
well-being
…
whether
the
priority
is
ecosystems
or
people
greatly
influences
stakeholders'
assessment
of
desirable
ecological
and
social
states"
and
for
example
"for
some
wolves
are
critical
to
ecosystem
health
and
an
essential
part
of
nature
for
others
they
are
a
symbol
of
government
overreach
threatening
their
livelihoods
and
cultural
values"
measuring
ecosystem
health
requires
extensive
goal-driven
environmental
sampling
for
example
a
vision
for
ecosystem
health
of
lake
superior
was
developed
by
a
public
forum
and
a
series
of
objectives
were
prepared
for
protection
of
habitat
and
maintenance
of
populations
of
some
70
indigenous
fish
species
a
suite
of
80
lake
health
indicators
was
developed
for
the
great
lakes
basin
including
monitoring
native
fish
species
exotic
species
water
levels
phosphorus
levels
toxic
chemicals
phytoplankton
zooplankton
fish
tissue
contaminants
etc
some
authors
have
attempted
broad
definitions
of
ecosystem
health
such
as
benchmarking
as
healthy
the
historical
ecosystem
state
"prior
to
the
onset
of
anthropogenic
stress"
a
difficulty
is
that
the
historical
composition
of
many
human-altered
ecosystems
is
unknown
or
unknowable
also
fossil
and
pollen
records
indicate
that
the
species
that
occupy
an
ecosystem
reshuffle
through
time
so
it
is
difficult
to
identify
one
snapshot
in
time
as
optimum
or
"healthy"
a
commonly
cited
broad
definition
states
that
a
healthy
ecosystem
has
three
attributes:
while
this
captures
significant
ecosystem
properties
a
generalization
is
elusive
as
those
properties
do
not
necessarily
co-vary
in
nature
for
example
there
is
not
necessarily
a
clear
or
consistent
relationship
between
productivity
and
species
richness
similarly
the
relationship
between
resilience
and
diversity
is
complex
and
ecosystem
stability
may
depend
upon
one
or
a
few
species
rather
than
overall
diversity
and
some
undesirable
ecosystems
are
highly
productive
"resilience
is
not
desirable
per
se
there
can
be
highly
resilient
states
of
ecosystems
which
are
very
undesirable
from
some
human
perspectives
such
as
algal-dominated
coral
reefs"
ecological
resilience
is
a
"capacity"
that
varies
depending
upon
which
properties
of
the
ecosystem
are
to
be
studied
and
depending
upon
what
kinds
of
disturbances
are
considered
and
how
they
are
to
be
quantified
approaches
to
assessing
it
"face
high
uncertainties
and
still
require
a
considerable
amount
of
empirical
and
theoretical
research"
other
authors
have
sought
a
numerical
index
of
ecosystem
health
that
would
permit
quantitative
comparisons
among
ecosystems
and
within
ecosystems
over
time
one
such
system
employs
ratings
of
the
three
properties
mentioned
above:
health
=
system
vigor
x
system
organization
x
system
resilience
ecologist
glenn
suter
argues
that
such
indices
employ
"nonsense
units"
the
indices
have
"no
meaning;
they
cannot
be
predicted
so
they
are
not
applicable
to
most
regulatory
problems;
they
have
no
diagnostic
power;
effects
of
one
component
are
eclipsed
by
responses
of
other
components
and
the
reason
for
a
high
or
low
index
value
is
unknown"
health
metrics
are
determined
by
stakeholder
goals
which
drive
ecosystem
definition
an
ecosystem
is
an
abstraction
"ecosystems
cannot
be
identified
or
found
in
nature
instead
they
must
be
delimited
by
an
observer
this
can
be
done
in
many
different
ways
for
the
same
chunk
of
nature
depending
on
the
specific
perspectives
of
interest"
ecosystem
definition
determines
the
acceptable
range
of
variability
(reference
conditions)
and
determines
measurement
variables
the
latter
are
used
as
indicators
of
ecosystem
structure
and
function
and
can
be
used
as
indicators
of
"health"
an
indicator
is
a
variable
such
as
a
chemical
or
biological
property
that
when
measured
is
used
to
infer
trends
in
another
(unmeasured)
environmental
variable
or
cluster
of
unmeasured
variables
(the
indicandum)
for
example
rising
mortality
rate
of
canaries
in
a
coal
mine
is
an
indicator
of
rising
carbon
monoxide
levels
rising
chlorophyll-a
levels
in
a
lake
may
signal
eutrophication
ecosystem
assessments
employ
two
kinds
of
indicators
descriptive
indicators
and
normative
indicators
"indicators
can
be
used
descriptively
for
a
scientific
purpose
or
normatively
for
a
political
purpose"
used
descriptively
high
chlorophyll-a
is
an
indicator
of
eutrophication
but
it
may
also
be
used
as
an
ecosystem
health
indicator
when
used
as
a
normative
(health)
indicator
it
indicates
a
rank
on
a
health
scale
a
rank
that
can
vary
widely
depending
on
societal
preferences
as
to
what
is
desirable
a
high
chlorophyll-a
level
in
a
natural
successional
wetland
might
be
viewed
as
healthy
whereas
a
human-impacted
wetland
with
the
"same"
indicator
value
may
be
judged
unhealthy
estimation
of
ecosystem
health
has
been
criticized
for
intermingling
the
two
types
of
environmental
indicators
a
health
indicator
is
a
normative
indicator
and
if
conflated
with
descriptive
indicators
"implies
that
normative
values
can
be
measured
objectively
which
is
certainly
not
true
thus
implicit
values
are
insinuated
to
the
reader
a
situation
which
has
to
be
avoided"
it
can
be
argued
that
the
very
act
of
selecting
indicators
of
any
kind
is
biased
by
the
observer's
perspective
but
separation
of
goals
from
descriptions
has
been
advocated
as
a
step
toward
transparency:
"a
separation
of
descriptive
and
normative
indicators
is
essential
from
the
perspective
of
the
philosophy
of
science
…
goals
and
values
cannot
be
deduced
directly
from
descriptions
…
a
fact
that
is
emphasized
repeatedly
in
the
literature
of
environmental
ethics
…
hence
we
advise
always
specifying
the
definition
of
indicators
and
propose
clearly
distinguishing
ecological
indicators
in
science
from
policy
indicators
used
for
decision-making
processes"
and
integration
of
multiple
possibly
conflicting
normative
indicators
into
a
single
measure
of
"ecosystem
health"
is
problematic
using
56
indicators
"determining
environmental
status
and
assessing
marine
ecosystems
health
in
an
integrative
way
is
still
one
of
the
grand
challenges
in
marine
ecosystems
ecology
research
and
management"
another
issue
with
indicators
is
validity
good
indicators
must
have
an
independently
validated
high
predictive
value
that
is
high
sensitivity
(high
probability
of
indicating
a
significant
change
in
the
indicandum)
and
high
specificity
(low
probability
of
wrongly
indicating
a
change)
the
reliability
of
various
health
metrics
has
been
questioned
and
"what
combination
of
measurements
should
be
used
to
evaluate
ecosystems
is
a
matter
of
current
scientific
debate"
most
attempts
to
identify
ecological
indicators
have
been
correlative
rather
than
derived
from
prospective
testing
of
their
predictive
value
and
the
selection
process
for
many
indicators
has
been
based
upon
weak
evidence
or
has
been
lacking
in
evidence
in
some
cases
no
reliable
indicators
are
known:
"we
found
no
examples
of
invertebrates
successfully
used
in
[forest]
monitoring
programs
their
richness
and
abundance
ensure
that
they
play
significant
roles
in
ecosystem
function
but
thwart
focus
on
a
few
key
species"
and
"reviews
of
species-based
monitoring
approaches
reveal
that
no
single
species
nor
even
a
group
of
species
accurately
reflects
entire
communities
understanding
the
response
of
a
single
species
may
not
provide
reliable
predictions
about
a
group
of
species
even
when
the
group
is
a
few
very
similar
species"
a
trade-off
between
human
health
and
the
"health"
of
nature
has
been
termed
the
"health
paradox"
and
it
illuminates
how
human
values
drive
perceptions
of
ecosystem
health
human
health
has
benefited
by
sacrificing
the
"health"
of
wild
ecosystems
such
as
dismantling
and
damming
of
wild
valleys
destruction
of
mosquito-bearing
wetlands
diversion
of
water
for
irrigation
conversion
of
wilderness
to
farmland
timber
removal
and
extirpation
of
tigers
whales
ferrets
and
wolves
there
has
been
an
acrimonious
schism
among
conservationists
and
resource
managers
over
the
question
of
whether
to
"ratchet
back
human
domination
of
the
biosphere"
or
whether
to
embrace
it
these
two
perspectives
have
been
characterized
as
utilitarian
vs
protectionist
the
utilitarian
view
treats
human
health
and
well-being
as
criteria
of
ecosystem
health
for
example
destruction
of
wetlands
to
control
malaria
mosquitoes
"resulted
in
an
improvement
in
ecosystem
health"
the
protectionist
view
treats
humans
as
an
invasive
species:
"if
there
was
ever
a
species
that
qualified
as
an
invasive
pest
it
is
"homo
sapiens""
proponents
of
the
utilitarian
view
argue
that
"healthy
ecosystems
are
characterized
by
their
capability
to
sustain
healthy
human
populations"
and
"healthy
ecosystems
must
be
economically
viable"
as
it
is
"unhealthy"
ecosystems
that
are
likely
to
result
in
increases
in
contamination
infectious
diseases
fires
floods
crop
failures
and
fishery
collapse
protectionists
argue
that
privileging
of
human
health
is
a
conflict
of
interest
as
humans
have
demolished
massive
numbers
of
ecosystems
to
maintain
their
welfare
also
disease
and
parasitism
are
historically
normal
in
pre-industrial
nature
diseases
and
parasites
promote
ecosystem
functioning
driving
biodiversity
and
productivity
and
parasites
may
constitute
a
significant
fraction
of
ecosystem
biomass
the
very
choice
of
the
word
"health"
applied
to
ecology
has
been
questioned
as
lacking
in
neutrality
in
a
bioscience
article
on
responsible
use
of
scientific
language:
"some
conservationists
fear
that
these
terms
could
endorse
human
domination
of
the
planet
…
and
could
exacerbate
the
shifting
cognitive
baseline
whereby
humans
tend
to
become
accustomed
to
new
and
often
degraded
ecosystems
and
thus
forget
the
nature
of
the
past"
criticism
of
ecosystem
health
largely
targets
the
failure
of
proponents
to
explicitly
distinguish
the
normative
dimension
from
the
descriptive
dimension
and
has
included
the
following:
alternatives
have
been
proposed
for
the
term
ecosystem
health
including
more
neutral
language
such
as
ecosystem
status
ecosystem
prognosis
and
ecosystem
sustainability
another
alternative
to
the
use
of
a
health
metaphor
is
to
"express
exactly
and
clearly
the
public
policy
and
the
management
objective"
to
employ
habitat
descriptors
and
real
properties
of
ecosystems
an
example
of
a
policy
statement
is
"the
maintenance
of
viable
natural
populations
of
wildlife
and
ecological
functions
always
takes
precedence
over
any
human
use
of
wildlife"
an
example
of
a
goal
is
"maintain
viable
populations
of
all
native
species
in
situ"
an
example
of
a
management
objective
is
"maintain
self-sustaining
populations
of
lake
whitefish
within
the
range
of
abundance
observed
during
1990-99"
kurt
jax
presented
an
ecosystem
assessment
format
that
avoids
imposing
a
preconceived
notion
of
normality
that
avoids
the
muddling
of
normative
and
descriptive
and
that
gives
serious
attention
to
ecosystem
definition
(1)
societal
purposes
for
the
ecosystem
are
negotiated
by
stakeholders
(2)
a
functioning
ecosystem
is
defined
with
emphasis
on
phenomena
relevant
to
stakeholder
goals
(3)
benchmark
reference
conditions
and
permissible
variation
of
the
system
are
established
(4)
measurement
variables
are
chosen
for
use
as
indicators
and
(5)
the
time
scale
and
spatial
scale
of
assessment
are
decided
ecological
health
has
been
used
as
a
medical
term
in
reference
to
human
allergy
and
multiple
chemical
sensitivity
and
as
a
public
health
term
for
programs
to
modify
health
risks
(diabetes
obesity
smoking
etc)
human
health
itself
when
viewed
in
its
broadest
sense
is
viewed
as
having
ecological
foundations
it
is
also
an
urban
planning
term
in
reference
to
"green"
cities
(composting
recycling)
and
has
been
used
loosely
with
regard
to
various
environmental
issues
and
as
the
condition
of
human-disturbed
environmental
sites
ecosystem
integrity
implies
a
condition
of
an
ecosystem
exposed
to
a
minimum
of
human
influence
ecohealth
is
the
relationship
of
human
health
to
the
environment
including
the
effect
of
climate
change
wars
food
production
urbanization
and
ecosystem
structure
and
function
ecosystem
management
and
ecosystem-based
management
refer
to
the
sustainable
management
of
ecosystems
and
in
some
cases
may
employ
the
terms
ecosystem
health
or
ecosystem
integrity
as
a
goal
nature
religion
a
nature
religion
is
a
religious
movement
that
believes
nature
and
the
natural
world
is
an
embodiment
of
divinity
sacredness
or
spiritual
power
nature
religions
include
indigenous
religions
practiced
in
various
parts
of
the
world
by
cultures
who
consider
the
environment
to
be
imbued
with
spirits
and
other
sacred
entities
it
also
includes
contemporary
pagan
faiths
which
are
primarily
concentrated
in
europe
and
north
america
the
term
"nature
religion"
was
first
coined
by
the
american
religious
studies
scholar
catherine
albanese
who
used
it
in
her
work
"nature
religion
in
america:
from
the
algonkian
indians
to
the
new
age"
(1991)
and
later
went
on
to
use
it
in
other
studies
following
on
from
albanese's
development
of
the
term
it
has
since
been
used
by
other
academics
working
in
the
discipline
catherine
albanese
described
nature
religion
as
"a
symbolic
center
and
the
cluster
of
beliefs
behaviours
and
values
that
encircles
it"
deeming
it
to
be
useful
for
shining
a
light
on
aspects
of
history
that
are
rarely
viewed
as
religious
in
a
paper
of
his
on
the
subject
the
canadian
religious
studies
scholar
peter
beyer
described
"nature
religion"
as
a
"useful
analytical
abstraction"
to
refer
to
"any
religious
belief
or
practice
in
which
devotees
consider
nature
to
be
the
embodiment
of
divinity
sacredness
transcendence
spiritual
power
or
whatever
cognate
term
one
wishes
to
use"
he
went
on
to
note
that
in
this
way
nature
religion
was
not
an
"identifiable
religious
tradition"
such
as
buddhism
or
christianity
are
but
that
it
instead
covers
"a
range
of
religious
and
quasi-religious
movements
groups
and
social
networks
whose
participants
may
or
may
not
identify
with
one
of
the
many
constructed
religions
of
global
society
which
referred
to
many
other
nature
religion"
peter
beyer
noted
the
existence
of
a
series
of
common
characteristics
which
he
believed
were
shared
by
different
nature
religions
he
remarked
that
although
"one
must
be
careful
not
to
overgeneralise"
he
suspected
that
there
were
a
series
of
features
which
"occur
sufficiently
often"
in
those
nature
religions
known
to
recorded
scholarship
to
constitute
a
pattern
the
first
of
these
common
characteristics
was
nature
religion's
"comparative
resistance
to
institutionalisation
and
legitimisation
in
terms
of
identifiable
socio-religious
authorities
and
organisations"
meaning
that
nature
religionists
rarely
formed
their
religious
beliefs
into
large
visible
socio-political
structures
such
as
churches
furthermore
beyer
noted
nature
religionists
often
held
a
"concomitant
distrust
of
and
even
eschewing
of
politically
orientated
power"
instead
of
this
he
felt
that
among
nature
religious
communities
there
was
"a
valuing
of
community
as
non-hierarchical"
and
a
"conditional
optimism
with
regard
to
human
capacity
and
the
future"
in
the
sphere
of
the
environment
beyer
noted
that
nature
religionists
held
to
a
"holistic
conception
of
reality"
and
"a
valorisation
of
physical
place
as
vital
aspects
of
their
spiritualities"
similarly
beyer
noted
the
individualism
which
was
favoured
by
nature
religionists
he
remarked
that
those
adhering
to
such
beliefs
typically
had
respect
for
"charismatic
and
hence
purely
individual
authority"
and
place
a
"strong
emphasis
on
individual
paths"
which
led
them
to
believe
in
"the
equal
value
of
individuals
and
groups"
along
similar
lines
he
also
commented
on
the
"strong
experiential
basis"
to
nature
religionist
beliefs
"where
personal
experience
is
a
final
arbiter
of
truth
or
validity"
in
april
1996
the
university
of
lancaster
in
north
west
england
held
a
conference
on
contemporary
paganism
entitled
"nature
religion
today:
western
paganism
shamanism
and
esotericism
in
the
1990s"
and
ultimately
led
to
the
publication
of
an
academic
anthology
of
the
same
name
two
years
later
this
book
"nature
religion
today:
paganism
in
the
modern
world"
was
edited
by
members
of
the
university's
department
of
religious
studies
a
postgraduate
named
joanne
pearson
and
two
professors
richard
h
roberts
and
geoffrey
samuel
in
his
study
of
wicca
the
pagan
studies
scholar
ethan
doyle
white
expressed
the
view
that
the
category
of
"nature
religion"
was
problematic
from
a
"historical
perspective"
because
it
solely
emphasises
the
"commonalities
of
belief
and
attitude
to
the
natural
world"
that
are
found
between
different
religions
and
in
doing
so
divorces
these
different
belief
systems
from
their
distinctive
socio-cultural
and
historical
backgrounds
nature-based
solutions
nature-based
solutions
(nbs)
refers
to
the
sustainable
management
and
use
of
nature
for
tackling
socio-environmental
challenges
the
challenges
include
issues
such
as
climate
change
water
security
water
pollution
food
security
human
health
and
disaster
risk
management
a
definition
by
the
european
union
states
that
these
solutions
are
"inspired
and
supported
by
nature
which
are
cost-effective
simultaneously
provide
environmental
social
and
economic
benefits
and
help
build
resilience
the
nature-based
solutions
initiative
meanwhile
defines
them
as
"actions
that
work
with
and
enhance
nature
so
as
to
help
people
adapt
to
change
and
disasters"
such
solutions
bring
more
and
more
diverse
nature
and
natural
features
and
processes
into
cities
landscapes
and
seascapes
through
locally
adapted
resource-efficient
and
systemic
interventions"
with
nbs
healthy
resilient
and
diverse
ecosystems
(whether
natural
managed
or
newly
created)
can
provide
solutions
for
the
benefit
of
societies
and
overall
biodiversity
for
instance
the
restoration
or
protection
of
mangroves
along
coastlines
utilizes
a
nature-based
solution
to
accomplish
several
things
mangroves
moderate
the
impact
of
waves
and
wind
on
coastal
settlements
or
cities
and
sequester
co
they
also
provide
safe
nurseries
for
marine
life
that
can
be
the
basis
for
sustaining
populations
of
fish
that
local
populations
may
depend
on
additionally
the
mangrove
forests
can
help
control
coastal
erosion
resulting
from
sea
level
rise
similarly
in
cities
green
roofs
or
walls
are
nature-based
solutions
that
can
be
used
to
moderate
the
impact
of
high
temperatures
capture
storm
water
abate
pollution
and
act
as
carbon
sinks
while
enhancing
biodiversity
conservation
approaches
and
environment
management
initiatives
have
been
carried
out
for
decades
what
is
new
is
that
the
benefits
of
such
nature-based
solutions
to
human
well-being
have
been
articulated
well
more
recently
even
if
the
term
itself
is
still
being
framed
examples
of
nature-based
solutions
can
be
found
all
over
the
world
and
imitated
nature-based
solutions
are
on
their
way
to
being
mainstreamed
in
national
and
international
policies
and
programmes
(eg
climate
change
policy
law
infrastructure
investment
and
financing
mechanisms)
for
example
the
theme
for
world
water
day
2018
was
"nature
for
water"
and
by
un-water's
accompanying
un
world
water
development
report
had
the
title
"nature-based
solutions
for
water"
societies
increasingly
face
challenges
such
as
climate
change
urbanization
jeopardized
food
security
and
water
resource
provision
and
disaster
risk
one
approach
to
answer
these
challenges
is
to
singularly
rely
on
technological
strategies
an
alternative
approach
is
to
manage
the
(socio-)ecological
systems
in
a
comprehensive
way
in
order
to
sustain
and
potentially
increase
the
delivery
of
ecosystem
services
to
humans
in
this
context
nature-based
solutions
(nbs)
have
recently
been
put
forward
by
practitioners
and
quickly
thereafter
by
policymakers
these
solutions
stress
the
sustainable
use
of
nature
in
solving
coupled
environmental-social-economic
challenges
while
ecosystem
services
are
often
valued
in
terms
of
immediate
benefits
to
human
well-being
and
economy
nbs
focus
on
the
benefits
to
people
and
the
environment
itself
to
allow
for
sustainable
solutions
that
are
able
to
respond
to
environmental
change
and
hazards
in
the
long-term
nbs
go
beyond
the
traditional
biodiversity
conservation
and
management
principles
by
"re-focusing"
the
debate
on
humans
and
specifically
integrating
societal
factors
such
as
human
well-being
and
poverty
reduction
socio-economic
development
and
governance
principles
with
respect
to
water
issues
nbs
can
achieve
the
following
according
to
the
world
water
development
report
2018
by
un-water:
in
2015
the
european
network
biodiversa
highlighted
how
nbs
relate
to
concepts
like
ecosystem
approaches
and
ecological
engineering
nbs
are
strongly
connected
to
ideas
such
as
natural
systems
agriculture
natural
solutions
ecosystem-based
approaches
adaptation
services
natural
infrastructure
green
infrastructure
and
ecological
engineering
for
instance
ecosystem-based
approaches
are
increasingly
promoted
for
climate
change
adaptation
and
mitigation
by
organisations
like
united
nations
environment
programme
and
non-governmental
organisations
such
as
the
nature
conservancy
these
organisations
refer
to
"policies
and
measures
that
take
into
account
the
role
of
ecosystem
services
in
reducing
the
vulnerability
of
society
to
climate
change
in
a
multi-sectoral
and
multi-scale
approach"
likewise
natural
infrastructure
is
defined
as
a
"strategically
planned
and
managed
network
of
natural
lands
such
as
forests
and
wetlands
working
landscapes
and
other
open
spaces
that
conserves
or
enhances
ecosystem
values
and
functions
and
provides
associated
benefits
to
human
populations";
and
green
infrastructure
refers
to
an
"interconnected
network
of
green
spaces
that
conserves
natural
systems
and
provides
assorted
benefits
to
human
populations"
similarly
the
concept
of
ecological
engineering
generally
refers
to
"protecting
restoring
(ie
ecosystem
restoration)
or
modifying
ecological
systems
to
increase
the
quantity
quality
and
sustainability
of
particular
services
they
provide
or
to
build
new
ecological
systems
that
provide
services
that
would
otherwise
be
provided
through
more
conventional
engineering
based
on
non-renewable
resources"
the
international
union
for
the
conservation
of
nature
(iucn)
defines
nbs
as
actions
to
protect
sustainably
manage
and
restore
natural
or
modified
ecosystems
that
address
societal
challenges
effectively
and
adaptively
simultaneously
providing
human
well-being
and
biodiversity
benefits
with
climate
change
food
security
disaster
risks
water
security
social
and
economic
development
as
well
as
human
health
being
the
common
societal
challenges
iucn
proposes
to
consider
nbs
as
an
umbrella
concept
categories
and
examples
of
nbs
approaches
according
to
iucn
include:
the
general
objective
of
nbs
is
clear
namely
the
sustainable
management
and
use
of
nature
for
tackling
societal
challenges
however
different
stakeholders
view
nbs
from
other
perspectives
for
instance
iucn
defines
nbs
as
"actions
to
protect
sustainably
manage
and
restore
natural
or
modified
ecosystems
which
address
societal
challenges
effectively
and
adaptively
while
simultaneously
providing
human
well-being
and
biodiversity
benefits"
this
framing
puts
the
need
for
well-managed
and
restored
ecosystems
at
the
heart
of
nbs
with
the
overarching
goal
of
"supporting
the
achievement
of
society's
development
goals
and
safeguard
human
well-being
in
ways
that
reflect
cultural
and
societal
values
and
enhance
the
resilience
of
ecosystems
their
capacity
for
renewal
and
the
provision
of
services"
in
the
context
of
the
ongoing
political
debate
on
jobs
and
growth
(main
drivers
of
the
current
eu
policy
agenda)
the
european
commission
underlines
that
nbs
can
transform
environmental
and
societal
challenges
into
innovation
opportunities
by
turning
natural
capital
into
a
source
for
green
growth
and
sustainable
development
in
their
view
nbs
to
societal
challenges
are
"solutions
that
are
inspired
and
supported
by
nature
which
are
cost-effective
simultaneously
provide
environmental
social
and
economic
benefits
and
help
build
resilience
such
solutions
bring
more
and
more
diverse
nature
and
natural
features
and
processes
into
cities
landscapes
and
seascapes
through
locally
adapted
resource-efficient
and
systemic
interventions"
this
framing
is
somewhat
broader
and
puts
economy
and
social
assets
at
the
heart
of
nbs
as
importantly
as
sustaining
environmental
conditions
it
shares
similarities
with
the
definition
proposed
by
maes
and
jacobs
(2015)
defining
nbs
as
"any
transition
to
a
use
of
es
with
decreased
input
of
non-renewable
natural
capital
and
increased
investment
in
renewable
natural
processes"
in
their
view
development
and
evaluation
of
nbs
spans
three
basic
requirements:
(1)
decrease
of
fossil
fuel
input
per
produced
unit;
(2)
lowering
of
systemic
trade-offs
and
increasing
synergies
between
es;
and
(3)
increasing
labor
input
and
jobs
here
nature
is
seen
as
a
tool
to
inspire
more
systemic
solutions
to
societal
problems
whatever
definition
used
promoting
sustainability
and
the
increased
role
of
natural
self-sustained
processes
relying
on
biodiversity
are
inherent
to
nbs
they
constitute
actions
easily
seen
as
positive
for
a
wide
range
of
stakeholders
as
they
bring
about
benefits
at
environmental
economic
and
social
levels
as
a
consequence
the
concept
of
nbs
is
gaining
acceptance
outside
the
conservation
community
(eg
urban
planning)
and
is
now
on
its
way
to
be
mainstreamed
into
policies
and
programmes
(climate
change
policy
law
infrastructure
investment
and
financing
mechanisms)
in
2014-2015
the
european
network
biodiversa
mobilized
a
range
of
scientists
research
donors
and
stakeholders
and
proposed
a
typology
characterizing
nbs
along
two
gradients
1
"how
much
engineering
of
biodiversity
and
ecosystems
is
involved
in
nbs"
and
2
"how
many
ecosystem
services
and
stakeholder
groups
are
targeted
by
a
given
nbs"
the
typology
highlights
that
nbs
can
involve
very
different
actions
on
ecosystems
(from
protection
to
management
and
even
creation
of
new
ecosystems)
and
is
based
on
the
assumption
that
the
higher
the
number
of
services
and
stakeholder
groups
targeted
the
lower
the
capacity
to
maximize
the
delivery
of
each
service
and
simultaneously
fulfil
the
specific
needs
of
all
stakeholder
groups
as
such
three
types
of
nbs
are
distinguished
(figure
2):
type
1
nbs
consists
of
no
or
minimal
intervention
in
ecosystems
with
the
objectives
of
maintaining
or
improving
the
delivery
of
a
range
of
es
both
inside
and
outside
of
these
conserved
ecosystems
examples
include
the
protection
of
mangroves
in
coastal
areas
to
limit
risks
associated
to
extreme
weather
conditions
and
provide
benefits
and
opportunities
to
local
populations;
and
the
establishment
of
marine
protected
areas
to
conserve
biodiversity
within
these
areas
while
exporting
biomass
into
fishing
grounds
this
type
of
nbs
is
connected
to
for
example
the
concept
of
biosphere
reserves
which
incorporates
core
protected
areas
for
nature
conservation
and
buffer
zones
and
transition
areas
where
people
live
and
work
in
a
sustainable
way
type
2
nbs
corresponds
to
management
approaches
that
develop
sustainable
and
multifunctional
ecosystems
and
landscapes
(extensively
or
intensively
managed)
these
types
improve
the
delivery
of
selected
es
compared
to
what
would
be
obtained
with
a
more
conventional
intervention
examples
include
innovative
planning
of
agricultural
landscapes
to
increase
their
multi-functionality;
and
approaches
for
enhancing
tree
species
and
genetic
diversity
to
increase
forest
resilience
to
extreme
events
this
type
of
nbs
is
strongly
connected
to
concepts
like
natural
systems
agriculture
agro-ecology
and
evolutionary-orientated
forestry
type
3
nbs
consists
of
managing
ecosystems
in
very
extensive
ways
or
even
creating
new
ecosystems
(eg
artificial
ecosystems
with
new
assemblages
of
organisms
for
green
roofs
and
walls
to
mitigate
city
warming
and
clean
polluted
air)
type
3
is
linked
to
concepts
like
green
and
blue
infrastructures
and
objectives
like
restoration
of
heavily
degraded
or
polluted
areas
and
greening
cities
type
1
and
2
would
typically
fall
within
the
iucn
nbs
framework
whereas
type
2
and
moreover
type
3
are
often
exemplified
by
ec
for
turning
natural
capital
into
a
source
for
green
growth
and
sustainable
development
hybrid
solutions
exist
along
this
gradient
both
in
space
and
time
for
instance
at
landscape
scale
mixing
protected
and
managed
areas
could
be
needed
to
fulfil
multi-functionality
and
sustainability
goals
similarly
a
constructed
wetland
can
be
developed
as
a
type
3
but
when
well
established
may
subsequently
be
preserved
and
surveyed
as
a
type
1
demonstrating
the
benefits
of
nature
and
healthy
ecosystems
and
showcasing
the
return
on
investment
they
can
offer
is
necessary
in
order
to
increase
awareness
but
also
to
provide
support
and
guidance
on
how
to
implement
nbs
a
large
number
of
initiatives
around
the
world
already
highlight
the
effectiveness
of
nbs
approaches
to
address
a
wide
range
of
societal
challenges
the
following
table
shows
examples
from
around
the
world:
in
2018
the
hindu
reported
that
the
east
kolkata
wetlands
the
world's
largest
organic
sewage
treatment
facility
had
been
used
to
clean
the
sewage
of
kolkata
in
an
organic
manner
by
using
algae
for
several
decades
in
use
since
the
1930s
the
natural
system
was
discovered
by
dhrubajyoti
ghosh
an
ecologist
and
a
municipal
engineer
in
the
1970s
while
working
in
the
region
ghosh
worked
for
decades
to
protect
the
wetlands
it
had
been
a
practice
in
kolkata
one
of
the
five
largest
cities
in
india
for
the
municipal
authorities
to
pump
sewage
into
shallow
ponds
("bheris")
under
the
heat
of
the
tropical
sun
algae
proliferated
in
them
converting
the
sewage
into
clean
water
which
in
turn
was
used
by
villagers
to
grow
paddy
and
vegetables
this
system
has
been
in
use
in
the
region
since
the
1930s
and
treats
750
million
litres
of
wastewater
per
day
giving
livelihood
to
100000
people
in
the
vicinity
for
his
work
ghosh
was
included
in
the
un
global
500
roll
of
honour
in
1990
and
received
the
luc
hoffmann
award
in
2016
there
is
currently
no
accepted
basis
on
which
a
government
agency
municipality
or
private
company
can
systematically
assess
the
efficiency
effectiveness
and
sustainability
of
a
particular
nature-based
solution
however
a
series
of
principles
are
proposed
to
guide
effective
and
appropriate
implementation
and
thus
to
upscale
nbs
in
practice
for
example
nbs
embrace
and
are
not
meant
to
replace
nature
conservation
norms
also
nbs
are
determined
by
site-specific
natural
and
cultural
contexts
that
include
traditional
local
and
scientific
knowledge
nbs
are
an
integral
part
of
the
overall
design
of
policies
and
measure
or
actions
to
address
a
specific
challenges
finally
nbs
can
be
implemented
alone
or
in
an
integrated
manner
with
other
solutions
to
societal
challenges
(eg
technological
and
engineering
solutions)
and
they
are
applied
at
the
landscape
scale
implementing
nbs
requires
political
economic
and
scientific
challenges
to
be
tackled
first
and
foremost
private
sector
investment
is
needed
not
to
replace
but
to
supplement
traditional
sources
of
capital
such
as
public
funding
or
philanthropy
the
challenge
is
therefore
to
provide
a
robust
evidence
base
for
the
contribution
of
nature
to
economic
growth
and
jobs
and
to
demonstrate
the
economic
viability
of
these
solutions
–
compared
to
technological
ones
–
on
a
timescale
compatible
with
that
of
global
change
furthermore
it
requires
measures
like
adaptation
of
economic
subsidy
schemes
and
the
creation
of
opportunities
for
conservation
finance
to
name
a
few
indeed
such
measures
will
be
needed
to
scale
up
nbs
interventions
and
strengthen
their
impact
in
mitigating
the
world's
most
pressing
challenges
since
2016
the
eu
is
supporting
a
multi-stakeholder
dialogue
platform
(called
thinknature)
to
promote
the
co-design
testing
and
deployment
of
improved
and
innovative
nbs
in
an
integrated
way
creation
of
such
science-policy-business-society
interfaces
could
promote
the
market
uptake
of
nbs
the
project
is
part
of
the
eu’s
horizon
2020
–
research
and
innovation
programme
and
will
last
for
3
years
there
are
a
total
of
17
international
partners
involved
including
the
technical
university
of
crete
(project
leader)
the
university
of
helsinki
and
biodiversa
in
2017
as
part
of
the
presidency
of
the
estonian
republic
of
the
council
of
the
european
union
a
conference
called
“nature-based
solutions:
from
innovation
to
common-use”
was
organized
by
the
ministry
of
the
environment
of
estonia
and
the
university
of
tallinn
this
conference
aimed
to
strengthen
synergies
among
various
recent
initiatives
and
programs
related
to
nbs
launched
by
the
european
commission
and
by
the
eu
member
states
focusing
on
policy
and
governance
of
nbs
and
on
research
and
innovation
in
recognition
of
the
importance
of
natural
ecosystems
for
mitigation
and
adaptation
the
paris
agreement
calls
on
all
parties
to
acknowledge
“the
importance
of
the
conservation
and
enhancement
as
appropriate
of
sinks
and
reservoirs
of
the
greenhouse
gases”
and
to
“note
the
importance
of
ensuring
the
integrity
of
all
ecosystems
including
oceans
and
the
protection
of
biodiversity
recognized
by
some
cultures
as
mother
earth”
it
then
includes
in
its
articles
several
references
to
nature-based
solutions
for
example
article
52
encourages
parties
to
adopt
“…policy
approaches
and
positive
incentives
for
activities
relating
to
reducing
emissions
from
deforestation
and
forest
degradation
and
the
role
of
conservation
and
sustainable
management
of
forests
and
enhancement
of
forest
carbon
stocks
in
developing
countries;
and
alternative
policy
approaches
such
as
joint
mitigation
and
adaptation
approaches
for
the
integral
and
sustainable
management
of
forests
while
reaffirming
the
importance
of
incentivizing
as
appropriate
non-carbon
benefits
associated
with
such
approaches”
article
71
further
encourages
parties
to
build
the
resilience
of
socioeconomic
and
ecological
systems
including
through
economic
diversification
and
sustainable
management
of
natural
resources
in
total
the
agreement
refers
to
nature
(ecosystems
natural
resources
forests)
in
13
distinct
places
an
in-depth
analysis
of
all
nationally
determined
contributions
submitted
to
unfccc
revealed
that
around
130
ndcs
or
65%
of
signatories
commit
to
nature-based
solutions
in
their
climate
pledges
suggesting
broad
consensus
for
the
role
of
nature
in
helping
meet
climate
change
goals
however
high-level
commitments
rarely
translate
into
robust
measurable
actions
on-the-ground
the
term
nbs
was
put
forward
by
practitioners
in
the
late
2000s
(in
particular
the
international
union
for
the
conservation
of
nature
and
the
world
bank)
and
thereafter
by
policymakers
in
europe
(most
notably
the
european
commission)
the
term
"nature-based
solutions"
was
first
used
in
the
late
2000s
it
was
used
in
the
context
of
finding
new
solutions
to
mitigate
and
adapt
to
climate
change
effects
whilst
simultaneously
protecting
biodiversity
and
improving
sustainable
livelihoods
the
iucn
referred
to
nbs
in
a
position
paper
for
the
united
nations
framework
convention
on
climate
change
the
term
was
also
adopted
by
european
policymakers
in
particular
by
the
european
commission
in
a
report
stressing
that
nbs
can
offer
innovative
means
to
create
jobs
and
growth
as
part
of
a
green
economy
the
term
started
to
make
appearances
in
the
mainstream
media
around
the
time
of
the
global
climate
action
summit
in
california
in
september
2018
natural
environment
the
natural
environment
encompasses
all
living
and
non-living
things
occurring
naturally
meaning
in
this
case
not
artificial
the
term
is
most
often
applied
to
the
earth
or
some
parts
of
earth
this
environment
encompasses
the
interaction
of
all
living
species
climate
weather
and
natural
resources
that
affect
human
survival
and
economic
activity
the
concept
of
the
"natural
environment"
can
be
distinguished
as
components:
in
contrast
to
the
natural
environment
is
the
built
environment
in
such
areas
where
man
has
fundamentally
transformed
landscapes
such
as
urban
settings
and
agricultural
land
conversion
the
natural
environment
is
greatly
modified
into
a
simplified
human
environment
even
acts
which
seem
less
extreme
such
as
building
a
mud
hut
or
a
photovoltaic
system
in
the
desert
the
modified
environment
becomes
an
artificial
one
though
many
animals
build
things
to
provide
a
better
environment
for
themselves
they
are
not
human
hence
beaver
dams
and
the
works
of
mound-building
termites
are
thought
of
as
natural
people
seldom
find
"absolutely
natural"
environments
on
earth
and
naturalness
usually
varies
in
a
continuum
from
100%
natural
in
one
extreme
to
0%
natural
in
the
other
more
precisely
we
can
consider
the
different
aspects
or
components
of
an
environment
and
see
that
their
degree
of
naturalness
is
not
uniform
if
for
instance
in
an
agricultural
field
the
mineralogic
composition
and
the
structure
of
its
soil
are
similar
to
those
of
an
undisturbed
forest
soil
but
the
structure
is
quite
different
"natural
environment"
is
often
used
as
a
synonym
for
habitat
for
instance
when
we
say
that
the
natural
environment
of
giraffes
is
the
savanna
earth
science
generally
recognizes
4
spheres
the
lithosphere
the
hydrosphere
the
atmosphere
and
the
biosphere
as
correspondent
to
rocks
water
air
and
life
respectively
some
scientists
include
as
part
of
the
spheres
of
the
earth
the
cryosphere
(corresponding
to
ice)
as
a
distinct
portion
of
the
hydrosphere
as
well
as
the
pedosphere
(corresponding
to
soil)
as
an
active
and
intermixed
sphere
earth
science
(also
known
as
geoscience
the
geographical
sciences
or
the
earth
sciences)
is
an
all-embracing
term
for
the
sciences
related
to
the
planet
earth
there
are
four
major
disciplines
in
earth
sciences
namely
geography
geology
geophysics
and
geodesy
these
major
disciplines
use
physics
chemistry
biology
chronology
and
mathematics
to
build
a
qualitative
and
quantitative
understanding
of
the
principal
areas
or
"spheres"
of
earth
the
earth's
crust
or
lithosphere
is
the
outermost
solid
surface
of
the
planet
and
is
chemically
and
mechanically
different
from
underlying
mantle
it
has
been
generated
greatly
by
igneous
processes
in
which
magma
cools
and
solidifies
to
form
solid
rock
beneath
the
lithosphere
lies
the
mantle
which
is
heated
by
the
decay
of
radioactive
elements
the
mantle
though
solid
is
in
a
state
of
rheic
convection
this
convection
process
causes
the
lithospheric
plates
to
move
albeit
slowly
the
resulting
process
is
known
as
plate
tectonics
volcanoes
result
primarily
from
the
melting
of
subducted
crust
material
or
of
rising
mantle
at
mid-ocean
ridges
and
mantle
plumes
most
water
is
found
in
one
or
another
natural
kind
of
body
of
water
an
ocean
is
a
major
body
of
saline
water
and
a
component
of
the
hydrosphere
approximately
71%
of
the
earth's
surface
(an
area
of
some
362
million
square
kilometers)
is
covered
by
ocean
a
continuous
body
of
water
that
is
customarily
divided
into
several
principal
oceans
and
smaller
seas
more
than
half
of
this
area
is
over
3000
meters
(9800 ft)
deep
average
oceanic
salinity
is
around
35
parts
per
thousand
(ppt)
(35%)
and
nearly
all
seawater
has
a
salinity
in
the
range
of
30
to
38
ppt
though
generally
recognized
as
several
'separate'
oceans
these
waters
comprise
one
global
interconnected
body
of
salt
water
often
referred
to
as
the
world
ocean
or
global
ocean
the
deep
seabeds
are
more
than
half
the
earth's
surface
and
are
among
the
least-modified
natural
environments
the
major
oceanic
divisions
are
defined
in
part
by
the
continents
various
archipelagos
and
other
criteria:
these
divisions
are
(in
descending
order
of
size)
the
pacific
ocean
the
atlantic
ocean
the
indian
ocean
the
southern
ocean
and
the
arctic
ocean
a
river
is
a
natural
watercourse
usually
freshwater
flowing
toward
an
ocean
a
lake
a
sea
or
another
river
a
few
rivers
simply
flow
into
the
ground
and
dry
up
completely
before
reaching
another
body
of
water
the
water
in
a
river
is
usually
in
a
channel
made
up
of
a
stream
bed
between
banks
in
larger
rivers
there
is
also
a
wider
floodplain
shaped
by
waters
over-topping
the
channel
flood
plains
may
be
very
wide
in
relation
to
the
size
of
the
river
channel
rivers
are
a
part
of
the
hydrological
cycle
water
within
a
river
is
generally
collected
from
precipitation
through
surface
runoff
groundwater
recharge
springs
and
the
release
of
water
stored
in
glaciers
and
snowpacks
small
rivers
may
also
be
termed
by
several
other
names
including
stream
creek
and
brook
their
current
is
confined
within
a
bed
and
stream
banks
streams
play
an
important
corridor
role
in
connecting
fragmented
habitats
and
thus
in
conserving
biodiversity
the
study
of
streams
and
waterways
in
general
is
known
as
"surface
hydrology"
a
lake
(from
latin
"lacus")
is
a
terrain
feature
a
body
of
water
that
is
localized
to
the
bottom
of
basin
a
body
of
water
is
considered
a
lake
when
it
is
inland
is
not
part
of
an
ocean
and
is
larger
and
deeper
than
a
pond
natural
lakes
on
earth
are
generally
found
in
mountainous
areas
rift
zones
and
areas
with
ongoing
or
recent
glaciation
other
lakes
are
found
in
endorheic
basins
or
along
the
courses
of
mature
rivers
in
some
parts
of
the
world
there
are
many
lakes
because
of
chaotic
drainage
patterns
left
over
from
the
last
ice
age
all
lakes
are
temporary
over
geologic
time
scales
as
they
will
slowly
fill
in
with
sediments
or
spill
out
of
the
basin
containing
them
a
pond
is
a
body
of
standing
water
either
natural
or
man-made
that
is
usually
smaller
than
a
lake
a
wide
variety
of
man-made
bodies
of
water
are
classified
as
ponds
including
water
gardens
designed
for
aesthetic
ornamentation
fish
ponds
designed
for
commercial
fish
breeding
and
solar
ponds
designed
to
store
thermal
energy
ponds
and
lakes
are
distinguished
from
streams
by
their
current
speed
while
currents
in
streams
are
easily
observed
ponds
and
lakes
possess
thermally
driven
micro-currents
and
moderate
wind
driven
currents
these
features
distinguish
a
pond
from
many
other
aquatic
terrain
features
such
as
stream
pools
and
tide
pools
humans
impact
the
water
in
different
ways
such
as
modifying
rivers
(through
dams
and
stream
channelization)
urbanization
and
deforestation
these
impact
lake
levels
groundwater
conditions
water
pollution
thermal
pollution
and
marine
pollution
humans
modify
rivers
by
using
direct
channel
manipulation
they
are
building
dams
and
reservoirs
and
manipulating
the
direction
of
the
rivers
and
water
path
dams
are
good
for
humans
some
communities
need
the
reservoirs
to
survive
however
reservoirs
and
dams
may
negatively
impact
the
environment
and
wildlife
dams
stops
fish
migration
and
the
moving
of
organisms
down
stream
urbanization
effects
the
environment
because
of
deforestation
and
changing
lake
levels
groundwater
conditions
etc
deforestation
and
urbanization
go
hand
in
hand
deforestation
may
cause
flooding
declining
stream
flow
and
changes
in
riverside
vegetation
the
changing
vegetation
occurs
because
when
trees
cannot
get
adequate
water
they
start
to
deteriorate
leading
to
a
decreased
food
supply
for
the
wildlife
in
an
area
the
atmosphere
of
the
earth
serves
as
a
key
factor
in
sustaining
the
planetary
ecosystem
the
thin
layer
of
gases
that
envelops
the
earth
is
held
in
place
by
the
planet's
gravity
dry
air
consists
of
78%
nitrogen
21%
oxygen
1%
argon
and
other
inert
gases
such
as
carbon
dioxide
the
remaining
gases
are
often
referred
to
as
trace
gases
among
which
are
the
greenhouse
gases
such
as
water
vapor
carbon
dioxide
methane
nitrous
oxide
and
ozone
filtered
air
includes
trace
amounts
of
many
other
chemical
compounds
air
also
contains
a
variable
amount
of
water
vapor
and
suspensions
of
water
droplets
and
ice
crystals
seen
as
clouds
many
natural
substances
may
be
present
in
tiny
amounts
in
an
unfiltered
air
sample
including
dust
pollen
and
spores
sea
spray
volcanic
ash
and
meteoroids
various
industrial
pollutants
also
may
be
present
such
as
chlorine
(elementary
or
in
compounds)
fluorine
compounds
elemental
mercury
and
sulphur
compounds
such
as
sulphur
dioxide
[so]
the
ozone
layer
of
the
earth's
atmosphere
plays
an
important
role
in
depleting
the
amount
of
ultraviolet
(uv)
radiation
that
reaches
the
surface
as
dna
is
readily
damaged
by
uv
light
this
serves
to
protect
life
at
the
surface
the
atmosphere
also
retains
heat
during
the
night
thereby
reducing
the
daily
temperature
extremes
earth's
atmosphere
can
be
divided
into
five
main
layers
these
layers
are
mainly
determined
by
whether
temperature
increases
or
decreases
with
altitude
from
highest
to
lowest
these
layers
are:
within
the
five
principal
layers
determined
by
temperature
there
are
several
layers
determined
by
other
properties
the
dangers
of
global
warming
are
being
increasingly
studied
by
a
wide
global
consortium
of
scientists
these
scientists
are
increasingly
concerned
about
the
potential
long-term
effects
of
global
warming
on
our
natural
environment
and
on
the
planet
of
particular
concern
is
how
climate
change
and
global
warming
caused
by
anthropogenic
or
human-made
releases
of
greenhouse
gases
most
notably
carbon
dioxide
can
act
interactively
and
have
adverse
effects
upon
the
planet
its
natural
environment
and
humans'
existence
it
is
clear
the
planet
is
warming
and
warming
rapidly
this
is
due
to
the
greenhouse
effect
which
is
caused
by
greenhouse
gases
which
trap
heat
inside
the
earth's
atmosphere
because
of
their
more
complex
molecular
structure
which
allows
them
to
vibrate
and
in
turn
trap
heat
and
release
it
back
towards
the
earth
this
warming
is
also
responsible
for
the
extinction
of
natural
habitats
which
in
turn
leads
to
a
reduction
in
wildlife
populationthe
most
recent
report
from
the
intergovernmental
panel
on
climate
change
(the
group
of
the
leading
climate
scientists
in
the
world)
concluded
that
the
earth
will
warm
anywhere
from
27
to
almost
11
degrees
fahrenheit
(15
to
6
degrees
celsius)
between
1990
and
2100
efforts
have
been
increasingly
focused
on
the
mitigation
of
greenhouse
gases
that
are
causing
climatic
changes
on
developing
adaptative
strategies
to
global
warming
to
assist
humans
other
animal
and
plant
species
ecosystems
regions
and
nations
in
adjusting
to
the
effects
of
global
warming
some
examples
of
recent
collaboration
to
address
climate
change
and
global
warming
include:
a
significantly
profound
challenge
is
to
identify
the
natural
environmental
dynamics
in
contrast
to
environmental
changes
not
within
natural
variances
a
common
solution
is
to
adapt
a
static
view
neglecting
natural
variances
to
exist
methodologically
this
view
could
be
defended
when
looking
at
processes
which
change
slowly
and
short
time
series
while
the
problem
arrives
when
fast
processes
turns
essential
in
the
object
of
the
study
climate
looks
at
the
statistics
of
temperature
humidity
atmospheric
pressure
wind
rainfall
atmospheric
particle
count
and
other
meteorological
elements
in
a
given
region
over
long
periods
of
time
weather
on
the
other
hand
is
the
present
condition
of
these
same
elements
over
periods
up
to
two
weeks
climates
can
be
classified
according
to
the
average
and
typical
ranges
of
different
variables
most
commonly
temperature
and
precipitation
the
most
commonly
used
classification
scheme
is
the
one
originally
developed
by
wladimir
köppen
the
thornthwaite
system
in
use
since
1948
uses
evapotranspiration
as
well
as
temperature
and
precipitation
information
to
study
animal
species
diversity
and
the
potential
impacts
of
climate
changes
weather
is
a
set
of
all
the
phenomena
occurring
in
a
given
atmospheric
area
at
a
given
time
most
weather
phenomena
occur
in
the
troposphere
just
below
the
stratosphere
weather
refers
generally
to
day-to-day
temperature
and
precipitation
activity
whereas
climate
is
the
term
for
the
average
atmospheric
conditions
over
longer
periods
of
time
when
used
without
qualification
"weather"
is
understood
to
be
the
weather
of
earth
weather
occurs
due
to
density
(temperature
and
moisture)
differences
between
one
place
and
another
these
differences
can
occur
due
to
the
sun
angle
at
any
particular
spot
which
varies
by
latitude
from
the
tropics
the
strong
temperature
contrast
between
polar
and
tropical
air
gives
rise
to
the
jet
stream
weather
systems
in
the
mid-latitudes
such
as
extratropical
cyclones
are
caused
by
instabilities
of
the
jet
stream
flow
because
the
earth's
axis
is
tilted
relative
to
its
orbital
plane
sunlight
is
incident
at
different
angles
at
different
times
of
the
year
on
the
earth's
surface
temperatures
usually
range
±40 °c
(100 °f
to
−40 °f)
annually
over
thousands
of
years
changes
in
the
earth's
orbit
have
affected
the
amount
and
distribution
of
solar
energy
received
by
the
earth
and
influence
long-term
climate
surface
temperature
differences
in
turn
cause
pressure
differences
higher
altitudes
are
cooler
than
lower
altitudes
due
to
differences
in
compressional
heating
weather
forecasting
is
the
application
of
science
and
technology
to
predict
the
state
of
the
atmosphere
for
a
future
time
and
a
given
location
the
atmosphere
is
a
chaotic
system
and
small
changes
to
one
part
of
the
system
can
grow
to
have
large
effects
on
the
system
as
a
whole
human
attempts
to
control
the
weather
have
occurred
throughout
human
history
and
there
is
evidence
that
civilized
human
activity
such
as
agriculture
and
industry
has
inadvertently
modified
weather
patterns
evidence
suggests
that
life
on
earth
has
existed
for
about
37
billion
years
all
known
life
forms
share
fundamental
molecular
mechanisms
and
based
on
these
observations
theories
on
the
origin
of
life
attempt
to
find
a
mechanism
explaining
the
formation
of
a
primordial
single
cell
organism
from
which
all
life
originates
there
are
many
different
hypotheses
regarding
the
path
that
might
have
been
taken
from
simple
organic
molecules
via
pre-cellular
life
to
protocells
and
metabolism
although
there
is
no
universal
agreement
on
the
definition
of
life
scientists
generally
accept
that
the
biological
manifestation
of
life
is
characterized
by
organization
metabolism
growth
adaptation
response
to
stimuli
and
reproduction
life
may
also
be
said
to
be
simply
the
characteristic
state
of
organisms
in
biology
the
science
of
living
organisms
"life"
is
the
condition
which
distinguishes
active
organisms
from
inorganic
matter
including
the
capacity
for
growth
functional
activity
and
the
continual
change
preceding
death
a
diverse
variety
of
living
organisms
(life
forms)
can
be
found
in
the
biosphere
on
earth
and
properties
common
to
these
organisms—plants
animals
fungi
protists
archaea
and
bacteria—are
a
carbon-
and
water-based
cellular
form
with
complex
organization
and
heritable
genetic
information
living
organisms
undergo
metabolism
maintain
homeostasis
possess
a
capacity
to
grow
respond
to
stimuli
reproduce
and
through
natural
selection
adapt
to
their
environment
in
successive
generations
more
complex
living
organisms
can
communicate
through
various
means
an
ecosystem
(also
called
as
environment)
is
a
natural
unit
consisting
of
all
plants
animals
and
micro-organisms
(biotic
factors)
in
an
area
functioning
together
with
all
of
the
non-living
physical
(abiotic)
factors
of
the
environment
central
to
the
ecosystem
concept
is
the
idea
that
living
organisms
are
continually
engaged
in
a
highly
interrelated
set
of
relationships
with
every
other
element
constituting
the
environment
in
which
they
exist
eugene
odum
one
of
the
founders
of
the
science
of
ecology
stated:
"any
unit
that
includes
all
of
the
organisms
(ie:
the
"community")
in
a
given
area
interacting
with
the
physical
environment
so
that
a
flow
of
energy
leads
to
clearly
defined
trophic
structure
biotic
diversity
and
material
cycles
(ie:
exchange
of
materials
between
living
and
nonliving
parts)
within
the
system
is
an
ecosystem"
a
greater
number
or
variety
of
species
or
biological
diversity
of
an
ecosystem
may
contribute
to
greater
resilience
of
an
ecosystem
because
there
are
more
species
present
at
a
location
to
respond
to
change
and
thus
"absorb"
or
reduce
its
effects
this
reduces
the
effect
before
the
ecosystem's
structure
is
fundamentally
changed
to
a
different
state
this
is
not
universally
the
case
and
there
is
no
proven
relationship
between
the
species
diversity
of
an
ecosystem
and
its
ability
to
provide
goods
and
services
on
a
sustainable
level
the
term
ecosystem
can
also
pertain
to
human-made
environments
such
as
human
ecosystems
and
human-influenced
ecosystems
and
can
describe
any
situation
where
there
is
relationship
between
living
organisms
and
their
environment
fewer
areas
on
the
surface
of
the
earth
today
exist
free
from
human
contact
although
some
genuine
wilderness
areas
continue
to
exist
without
any
forms
of
human
intervention
biomes
are
terminologically
similar
to
the
concept
of
ecosystems
and
are
climatically
and
geographically
defined
areas
of
ecologically
similar
climatic
conditions
on
the
earth
such
as
communities
of
plants
animals
and
soil
organisms
often
referred
to
"as"
ecosystems
biomes
are
defined
on
the
basis
of
factors
such
as
plant
structures
(such
as
trees
shrubs
and
grasses)
leaf
types
(such
as
broadleaf
and
needleleaf)
plant
spacing
(forest
woodland
savanna)
and
climate
unlike
ecozones
biomes
are
not
defined
by
genetic
taxonomic
or
historical
similarities
biomes
are
often
identified
with
particular
patterns
of
ecological
succession
and
climax
vegetation
global
biogeochemical
cycles
are
critical
to
life
most
notably
those
of
water
oxygen
carbon
nitrogen
and
phosphorus
wilderness
is
generally
defined
as
a
natural
environment
on
earth
that
has
not
been
significantly
modified
by
human
activity
the
wild
foundation
goes
into
more
detail
defining
wilderness
as:
"the
most
intact
undisturbed
wild
natural
areas
left
on
our
planet
-
those
last
truly
wild
places
that
humans
do
not
control
and
have
not
developed
with
roads
pipelines
or
other
industrial
infrastructure"
wilderness
areas
and
protected
parks
are
considered
important
for
the
survival
of
certain
species
ecological
studies
conservation
solitude
and
recreation
wilderness
is
deeply
valued
for
cultural
spiritual
moral
and
aesthetic
reasons
some
nature
writers
believe
wilderness
areas
are
vital
for
the
human
spirit
and
creativity
the
word
"wilderness"
derives
from
the
notion
of
wildness;
in
other
words
that
which
is
not
controllable
by
humans
the
word's
etymology
is
from
the
old
english
"wildeornes"
which
in
turn
derives
from
"wildeor"
meaning
"wild
beast"
(wild
+
deor
=
beast
deer)
from
this
point
of
view
it
is
the
wildness
of
a
place
that
makes
it
a
wilderness
the
mere
presence
or
activity
of
people
does
not
disqualify
an
area
from
being
"wilderness"
many
ecosystems
that
are
or
have
been
inhabited
or
influenced
by
activities
of
people
may
still
be
considered
"wild"
this
way
of
looking
at
wilderness
includes
areas
within
which
natural
processes
operate
without
very
noticeable
human
interference
wildlife
includes
all
non-domesticated
plants
animals
and
other
organisms
domesticating
wild
plant
and
animal
species
for
human
benefit
has
occurred
many
times
all
over
the
planet
and
has
a
major
impact
on
the
environment
both
positive
and
negative
wildlife
can
be
found
in
all
ecosystems
deserts
rain
forests
plains
and
other
areas—including
the
most
developed
urban
sites—all
have
distinct
forms
of
wildlife
while
the
term
in
popular
culture
usually
refers
to
animals
that
are
untouched
by
civilized
human
factors
most
scientists
agree
that
wildlife
around
the
world
is
(now)
impacted
by
human
activities
it
is
the
common
understanding
of
"natural
environment"
that
underlies
environmentalism
—
a
broad
political
social
and
philosophical
movement
that
advocates
various
actions
and
policies
in
the
interest
of
protecting
what
nature
remains
in
the
natural
environment
or
restoring
or
expanding
the
role
of
nature
in
this
environment
while
true
wilderness
is
increasingly
rare
"wild"
nature
(eg
unmanaged
forests
uncultivated
grasslands
wildlife
wildflowers)
can
be
found
in
many
locations
previously
inhabited
by
humans
goals
for
the
benefit
of
people
and
natural
systems
commonly
expressed
by
environmental
scientists
and
environmentalists
include:
in
some
cultures
the
term
environment
is
meaningless
because
there
is
no
separation
between
people
and
what
they
view
as
the
natural
world
or
their
surroundings
specifically
in
the
united
states
many
native
cultures
do
not
recognize
the
"environment"
or
see
themselves
as
environmentalists
final
straw:
food
earth
happiness
final
straw:
food
earth
happiness
is
a
documentary/art
film
released
in
june
2015
that
takes
audiences
through
farms
and
urban
landscapes
in
japan
south
korea
and
the
united
states
interviewing
leading
practitioners
in
the
natural
farming
movement
the
film
began
when
an
environmental
artist
(patrick
m
lydon)
and
an
environmental
book
editor
(suhee
kang)
had
a
chance
meeting
in
seoul
south
korea
and
began
conducting
short
interviews
together
with
leaders
in
the
ecology
and
social
justice
movements
upon
meeting
korean
farmer
seong
hyun
choi
however
the
two
were
so
impressed
by
his
ecological
mindset
and
way
of
working
that
they
set
out
to
produce
a
feature
film
about
the
movement
lydon
and
kang
ended
up
quitting
their
jobs
giving
away
most
of
their
possessions
and
becoming
voluntarily
homeless
for
four
years
in
order
to
afford
producing
the
film
the
film
is
split
into
three
sections
1)
modern
life
2)
foundations
and
mindset
of
natural
farming
and
3)
natural
farming
in
practice
and
life
according
to
the
filmmakers
as
they
began
to
understand
more
about
how
natural
farming
itself
was
not
rooted
in
methods
but
in
a
way
of
thinking
they
chose
to
explore
the
life
philosophies
and
ways
of
thinking
of
natural
farming
practitioners
in
a
more
free-flowing
and
artistic
way
rather
than
an
instructive
one;
the
result
is
an
unconventional
documentary
that
features
slow
paced
musical
interludes
alongside
interviews
reviewers
have
called
both
"meditative
and
mindful"
and
"an
inspiring
call
to
action"
author
and
musician
alicia
bay
laurel
called
the
film
"both
art
and
documentary"
lydon
and
kang
spent
what
they
call
a
"meager"
life
savings
to
make
the
film
along
with
the
volunteer
efforts
of
farmers
translators
writers
musicians
they
had
met
during
their
journey
although
the
film
was
filmed
written
and
edited
entirely
by
the
two
directors
they
readily
admit
that
the
process
of
making
the
film
was
co-operative
effort
with
more
than
200
volunteers
directly
involved
in
the
process
in
some
way
the
soundtrack
was
recorded
with
professional
musicians
from
each
of
the
three
countries
where
filming
took
place
all
of
whom
donated
their
time
to
contribute
to
the
film
project
with
the
continued
help
of
international
volunteers
the
film
is
available
in
four
languages
(english
korean
japanese
vietnamese)
and
three
more
(chinese
portuguese
french)
are
in
progress
frustrated
by
the
lack
of
distribution
and
film
festival
options
for
low-
and
no-budget
films
the
filmmakers
made
the
decision
to
manage
distribution
and
touring
in
the
same
way
they
went
about
filming
through
co-operative
effort
with
the
help
of
volunteers
independent
theater
owners
and
community
organizers
they
launched
an
extensive
tour
throughout
japan
and
south
korea
from
2015-2016
eventually
screening
the
film
at
over
130
venues
rather
than
simply
screening
the
film
the
filmmakers
decided
to
transition
their
existing
media
production
organization
"sociecity"
into
a
vehicle
for
art
and
community
engagement
they
made
a
point
of
hosting
interactive
events
along
with
their
screenings
and
in
several
cases
stayed
in
communities
for
up
to
three
months
at
a
time
to
build
natural
gardens
and
host
a
project
they
call
realtimefood
a
grown-to-order
restaurant
which
connects
the
ideas
from
the
film
with
real-world
practices
in
farming
food
and
crafts
in
most
cases
these
efforts
were
funded
by
grants
from
local
philanthropic
organizations
and/or
supported
by
the
communities
themselves
interested
in
the
unconventional
way
the
film
was
being
made
and
toured
multiple
magazines
and
newspapers
in
japan
and
korea
followed
the
directors
during
several
parts
of
their
journey
notably
essen
bar
and
dining
and
road
magazines
and
shikoku
shinbun
and
huffington
post
newspapers
during
the
tour
the
film
was
eventually
picked
up
by
festivals
including
tassie
eco
film
festival
and
belleville
doc
fest
balance
of
nature
the
balance
of
nature
is
a
theory
that
proposes
that
ecological
systems
are
usually
in
a
stable
equilibrium
or
homeostasis
which
is
to
say
that
a
small
change
in
some
particular
parameter
(the
size
of
a
particular
population
for
example)
will
be
corrected
by
some
negative
feedback
that
will
bring
the
parameter
back
to
its
original
"point
of
balance"
with
the
rest
of
the
system
it
may
apply
where
populations
depend
on
each
other
for
example
in
predator/prey
systems
or
relationships
between
herbivores
and
their
food
source
it
is
also
sometimes
applied
to
the
relationship
between
the
earth's
ecosystem
the
composition
of
the
atmosphere
and
the
world's
weather
the
gaia
hypothesis
is
a
balance
of
nature-based
theory
that
suggests
that
the
earth
and
its
ecology
may
act
as
co-ordinated
systems
in
order
to
maintain
the
balance
of
nature
the
theory
that
nature
is
permanently
in
balance
has
been
largely
discredited
by
scientists
working
in
ecology
as
it
has
been
found
that
chaotic
changes
in
population
levels
are
common
but
nevertheless
the
idea
continues
to
be
popular
in
the
general
public
during
the
later
half
of
the
twentieth
century
the
theory
was
superseded
by
catastrophe
theory
and
chaos
theory
the
concept
that
nature
maintains
its
condition
is
of
ancient
provenance;
herodotus
commented
on
the
wonderful
relationship
between
predator
and
prey
species
which
remained
in
a
steady
proportion
to
one
another
with
predators
never
excessively
consuming
their
prey
populations
the
"balance
of
nature"
concept
once
ruled
ecological
research
as
well
as
once
governing
the
management
of
natural
resources
this
led
to
a
doctrine
popular
among
some
conservationists
that
nature
was
best
left
to
its
own
devices
and
that
human
intervention
into
it
was
by
definition
unacceptable
the
validity
of
a
"balance
of
nature"
was
already
questioned
in
the
early
1900s
but
the
general
abandonment
of
the
theory
by
scientists
working
in
ecology
only
happened
in
the
last
quarter
of
that
century
when
studies
showed
that
it
did
not
match
what
could
be
observed
among
plant
and
animal
populations
predator-prey
populations
tend
to
show
chaotic
behavior
within
limits
where
the
sizes
of
populations
change
in
a
way
that
may
appear
random
but
is
in
fact
obeying
deterministic
laws
based
only
on
the
relationship
between
a
population
and
its
food
source
illustrated
by
the
lotka–volterra
equation
an
experimental
example
of
this
was
shown
in
an
eight-year
study
on
small
baltic
sea
creatures
such
as
plankton
which
were
isolated
from
the
rest
of
the
ocean
each
member
of
the
food
web
was
shown
to
take
turns
multiplying
and
declining
even
though
the
scientists
kept
the
outside
conditions
constant
an
article
in
the
journal
"nature"
stated;
"advanced
mathematical
techniques
proved
the
indisputable
presence
of
chaos
in
this
food
web
short-term
prediction
is
possible
but
long-term
prediction
is
not"
although
some
conservationist
organizations
argue
that
human
activity
is
incompatible
with
a
balanced
ecosystem
there
are
numerous
examples
in
history
showing
that
several
modern
day
habitats
originate
from
human
activity:
some
of
latin
america's
rain
forests
owe
their
existence
to
humans
planting
and
transplanting
them
while
the
abundance
of
grazing
animals
in
the
serengeti
plain
of
africa
is
thought
by
some
ecologists
to
be
partly
due
to
human-set
fires
that
created
savanna
habitats
possibly
one
of
the
best
examples
of
an
ecosystem
fundamentally
modified
by
human
activity
can
be
observed
as
a
consequence
of
the
australian
aboriginal
practice
of
"fire-stick
farming"
the
legacy
of
this
practice
over
long
periods
has
resulted
in
forests
being
converted
to
grasslands
capable
of
sustaining
larger
populations
of
faunal
prey
particularly
in
the
northern
and
western
regions
of
the
continent
so
thorough
has
been
the
effect
of
these
deliberate
regular
burnings
that
many
plant
and
tree
species
from
affected
regions
have
now
completely
adapted
to
the
annual
fire
regime
in
that
they
require
the
passage
of
a
fire
before
their
seeds
will
even
germinate
one
school
in
los
angeles
states
"
“we
have
let
our
kids
go
to
the
forest
area
of
the
playground
however
five
years
later
we
found
that
none
of
the
flowers
were
growing
the
natural
damp
soil
had
been
hardened
and
all
of
the
beautiful
grass
had
been
plucked”
despite
being
discredited
among
ecologists
the
theory
is
widely
held
to
be
true
by
the
general
public
with
one
authority
calling
it
an
"enduring
myth"
at
least
in
midwestern
america
the
"balance
of
nature"
idea
was
shown
to
be
widely
held
by
both
science
majors
and
the
general
student
population
in
a
study
at
the
university
of
patras
educational
sciences
students
were
asked
to
reason
about
the
future
of
ecosystems
which
suffered
human-driven
disturbances
subjects
agreed
that
it
was
very
likely
for
the
ecosystems
to
fully
recover
their
initial
state
referring
to
either
a
'recovery
process'
which
restores
the
initial
'balance'
or
specific
'recovery
mechanisms'
as
an
ecosystem's
inherent
characteristic
in
a
2017
study
ampatzidis
and
ergazaki
discuss
the
learning
objectives
and
design
criteria
that
a
learning
environment
for
non-biology
major
students
should
meet
to
support
them
challenge
the
"balance
of
nature"
idea
earth
earth
is
the
third
planet
from
the
sun
and
the
only
astronomical
object
known
to
harbor
life
according
to
radiometric
dating
and
other
sources
of
evidence
earth
formed
over
45
billion
years
ago
earth's
gravity
interacts
with
other
objects
in
space
especially
the
sun
and
the
moon
earth's
only
natural
satellite
earth
revolves
around
the
sun
in
36526
days
a
period
known
as
an
earth
year
during
this
time
earth
rotates
about
its
axis
about
36626
times
earth's
axis
of
rotation
is
tilted
with
respect
to
its
orbital
plane
producing
seasons
on
earth
the
gravitational
interaction
between
earth
and
the
moon
causes
ocean
tides
stabilizes
earth's
orientation
on
its
axis
and
gradually
slows
its
rotation
earth
is
the
densest
planet
in
the
solar
system
and
the
largest
of
the
four
terrestrial
planets
earth's
lithosphere
is
divided
into
several
rigid
tectonic
plates
that
migrate
across
the
surface
over
periods
of
many
millions
of
years
about
71%
of
earth's
surface
is
covered
with
water
mostly
by
oceans
the
remaining
29%
is
land
consisting
of
continents
and
islands
that
together
have
many
lakes
rivers
and
other
sources
of
water
that
contribute
to
the
hydrosphere
the
majority
of
earth's
polar
regions
are
covered
in
ice
including
the
antarctic
ice
sheet
and
the
sea
ice
of
the
arctic
ice
pack
earth's
interior
remains
active
with
a
solid
iron
inner
core
a
liquid
outer
core
that
generates
the
earth's
magnetic
field
and
a
convecting
mantle
that
drives
plate
tectonics
within
the
first
billion
years
of
earth's
history
life
appeared
in
the
oceans
and
began
to
affect
the
earth's
atmosphere
and
surface
leading
to
the
proliferation
of
aerobic
and
anaerobic
organisms
some
geological
evidence
indicates
that
life
may
have
arisen
as
much
as
41 billion
years
ago
since
then
the
combination
of
earth's
distance
from
the
sun
physical
properties
and
geological
history
have
allowed
life
to
evolve
and
thrive
in
the
history
of
the
earth
biodiversity
has
gone
through
long
periods
of
expansion
occasionally
punctuated
by
mass
extinction
events
over
99%
of
all
species
that
ever
lived
on
earth
are
extinct
estimates
of
the
number
of
species
on
earth
today
vary
widely;
most
species
have
not
been
described
over
76 billion
humans
live
on
earth
and
depend
on
its
biosphere
and
natural
resources
for
their
survival
humans
have
developed
diverse
societies
and
cultures;
politically
the
world
has
about
200
sovereign
states
the
modern
english
word
"earth"
developed
from
a
wide
variety
of
middle
english
forms
which
derived
from
an
old
english
noun
most
often
spelled
'
it
has
cognates
in
every
germanic
language
and
their
proto-germanic
root
has
been
reconstructed
as
*"erþō"
in
its
earliest
appearances
"eorðe"
was
already
being
used
to
translate
the
many
senses
of
latin
'
and
greek
("gē"):
the
ground
its
soil
dry
land
the
human
world
the
surface
of
the
world
(including
the
sea)
and
the
globe
itself
as
with
terra
and
gaia
earth
was
a
personified
goddess
in
germanic
paganism:
the
angles
were
listed
by
tacitus
as
among
the
devotees
of
nerthus
and
later
norse
mythology
included
jörð
a
giantess
often
given
as
the
mother
of
thor
originally
"earth"
was
written
in
lowercase
and
from
early
middle
english
its
definite
sense
as
"the
globe"
was
expressed
as
"the
earth"
by
early
modern
english
many
nouns
were
capitalized
and
"the
earth"
became
(and
often
remained)
"the
earth"
particularly
when
referenced
along
with
other
heavenly
bodies
more
recently
the
name
is
sometimes
simply
given
as
"earth"
by
analogy
with
the
names
of
the
other
planets
house
styles
now
vary:
oxford
spelling
recognizes
the
lowercase
form
as
the
most
common
with
the
capitalized
form
an
acceptable
variant
another
convention
capitalizes
"earth"
when
appearing
as
a
name
(eg
"earth's
atmosphere")
but
writes
it
in
lowercase
when
preceded
by
"the"
(eg
"the
atmosphere
of
the
earth")
it
almost
always
appears
in
lowercase
in
colloquial
expressions
such
as
"what
on
earth
are
you
doing?"
the
oldest
material
found
in
the
solar
system
is
dated
to
(bya)
by
the
primordial
earth
had
formed
the
bodies
in
the
solar
system
formed
and
evolved
with
the
sun
in
theory
a
solar
nebula
partitions
a
volume
out
of
a
molecular
cloud
by
gravitational
collapse
which
begins
to
spin
and
flatten
into
a
circumstellar
disk
and
then
the
planets
grow
out
of
that
disk
with
the
sun
a
nebula
contains
gas
ice
grains
and
dust
(including
primordial
nuclides)
according
to
nebular
theory
planetesimals
formed
by
accretion
with
the
primordial
earth
taking
10–
(mys)
to
form
a
subject
of
research
is
the
formation
of
the
moon
some
453
bya
a
leading
hypothesis
is
that
it
was
formed
by
accretion
from
material
loosed
from
earth
after
a
mars-sized
object
named
theia
hit
earth
in
this
view
the
mass
of
theia
was
approximately
10
percent
of
earth
it
hit
earth
with
a
glancing
blow
and
some
of
its
mass
merged
with
earth
between
approximately
41
and
numerous
asteroid
impacts
during
the
late
heavy
bombardment
caused
significant
changes
to
the
greater
surface
environment
of
the
moon
and
by
inference
to
that
of
earth
earth's
atmosphere
and
oceans
were
formed
by
volcanic
activity
and
outgassing
water
vapor
from
these
sources
condensed
into
the
oceans
augmented
by
water
and
ice
from
asteroids
protoplanets
and
comets
in
this
model
atmospheric
"greenhouse
gases"
kept
the
oceans
from
freezing
when
the
newly
forming
sun
had
only
70%
of
its
current
luminosity
by
earth's
magnetic
field
was
established
which
helped
prevent
the
atmosphere
from
being
stripped
away
by
the
solar
wind
a
crust
formed
when
the
molten
outer
layer
of
earth
cooled
to
form
a
solid
the
two
models
that
explain
land
mass
propose
either
a
steady
growth
to
the
present-day
forms
or
more
likely
a
rapid
growth
early
in
earth
history
followed
by
a
long-term
steady
continental
area
continents
formed
by
plate
tectonics
a
process
ultimately
driven
by
the
continuous
loss
of
heat
from
earth's
interior
over
the
period
of
hundreds
of
millions
of
years
the
supercontinents
have
assembled
and
broken
apart
roughly
(mya)
one
of
the
earliest
known
supercontinents
rodinia
began
to
break
apart
the
continents
later
recombined
to
form
pannotia
then
finally
pangaea
which
also
broke
apart
the
present
pattern
of
ice
ages
began
about
and
then
intensified
during
the
pleistocene
about
high-latitude
regions
have
since
undergone
repeated
cycles
of
glaciation
and
thaw
repeating
about
every
the
last
continental
glaciation
ended
ago
chemical
reactions
led
to
the
first
self-replicating
molecules
about
four
billion
years
ago
a
half
billion
years
later
the
last
common
ancestor
of
all
current
life
arose
the
evolution
of
photosynthesis
allowed
the
sun's
energy
to
be
harvested
directly
by
life
forms
the
resultant
molecular
oxygen
()
accumulated
in
the
atmosphere
and
due
to
interaction
with
ultraviolet
solar
radiation
formed
a
protective
ozone
layer
()
in
the
upper
atmosphere
the
incorporation
of
smaller
cells
within
larger
ones
resulted
in
the
development
of
complex
cells
called
eukaryotes
true
multicellular
organisms
formed
as
cells
within
colonies
became
increasingly
specialized
aided
by
the
absorption
of
harmful
ultraviolet
radiation
by
the
ozone
layer
life
colonized
earth's
surface
among
the
earliest
fossil
evidence
for
life
is
microbial
mat
fossils
found
in
348 billion-year-old
sandstone
in
western
australia
biogenic
graphite
found
in
37 billion-year-old
metasedimentary
rocks
in
western
greenland
and
remains
of
biotic
material
found
in
41 billion-year-old
rocks
in
western
australia
the
earliest
direct
evidence
of
life
on
earth
is
contained
in
345
billion-year-old
australian
rocks
showing
fossils
of
microorganisms
during
the
neoproterozoic
much
of
earth
might
have
been
covered
in
ice
this
hypothesis
has
been
termed
"snowball
earth"
and
it
is
of
particular
interest
because
it
preceded
the
cambrian
explosion
when
multicellular
life
forms
significantly
increased
in
complexity
following
the
cambrian
explosion
there
have
been
five
mass
extinctions
the
most
recent
such
event
was
when
an
asteroid
impact
triggered
the
extinction
of
the
non-avian
dinosaurs
and
other
large
reptiles
but
spared
some
small
animals
such
as
mammals
which
at
the
time
resembled
shrews
mammalian
life
has
diversified
over
the
past
and
several
million
years
ago
an
african
ape-like
animal
such
as
"orrorin
tugenensis"
gained
the
ability
to
stand
upright
this
facilitated
tool
use
and
encouraged
communication
that
provided
the
nutrition
and
stimulation
needed
for
a
larger
brain
which
led
to
the
evolution
of
humans
the
development
of
agriculture
and
then
civilization
led
to
humans
having
an
influence
on
earth
and
the
nature
and
quantity
of
other
life
forms
that
continues
to
this
day
earth's
expected
long-term
future
is
tied
to
that
of
the
sun
over
the
next
solar
luminosity
will
increase
by
10%
and
over
the
next
by
40%
the
earth's
increasing
surface
temperature
will
accelerate
the
inorganic
carbon
cycle
reducing
concentration
to
levels
lethally
low
for
plants
(
for
c4
photosynthesis)
in
approximately
the
lack
of
vegetation
will
result
in
the
loss
of
oxygen
in
the
atmosphere
making
animal
life
impossible
after
another
billion
years
all
surface
water
will
have
disappeared
and
the
mean
global
temperature
will
reach
from
that
point
the
earth
is
expected
to
be
habitable
for
another
possibly
up
to
if
nitrogen
is
removed
from
the
atmosphere
even
if
the
sun
were
eternal
and
stable
27%
of
the
water
in
the
modern
oceans
will
descend
to
the
mantle
in
one
billion
years
due
to
reduced
steam
venting
from
mid-ocean
ridges
the
sun
will
evolve
to
become
a
red
giant
in
about
models
predict
that
the
sun
will
expand
to
roughly
about
250
times
its
present
radius
earth's
fate
is
less
clear
as
a
red
giant
the
sun
will
lose
roughly
30%
of
its
mass
so
without
tidal
effects
earth
will
move
to
an
orbit
from
the
sun
when
the
star
reaches
its
maximum
radius
most
if
not
all
remaining
life
will
be
destroyed
by
the
sun's
increased
luminosity
(peaking
at
about
5000
times
its
present
level)
a
2008
simulation
indicates
that
earth's
orbit
will
eventually
decay
due
to
tidal
effects
and
drag
causing
it
to
enter
the
sun's
atmosphere
and
be
vaporized
the
shape
of
earth
is
approximately
oblate
spheroidal
due
to
rotation
the
earth
is
flattened
at
the
poles
and
bulging
around
the
equator
the
diameter
of
the
earth
at
the
equator
is
larger
than
the
pole-to-pole
diameter
thus
the
point
on
the
surface
farthest
from
earth's
center
of
mass
is
the
summit
of
the
equatorial
chimborazo
volcano
in
ecuador
()
the
average
diameter
of
the
reference
spheroid
is
local
topography
deviates
from
this
idealized
spheroid
although
on
a
global
scale
these
deviations
are
small
compared
to
earth's
radius:
the
maximum
deviation
of
only
017%
is
at
the
mariana
trench
(
below
local
sea
level)
whereas
mount
everest
(
above
local
sea
level)
represents
a
deviation
of
014%
in
geodesy
the
exact
shape
that
earth's
oceans
would
adopt
in
the
absence
of
land
and
perturbations
such
as
tides
and
winds
is
called
the
geoid
more
precisely
the
geoid
is
the
surface
of
gravitational
equipotential
at
mean
sea
level
earth's
mass
is
approximately
(5970
yg)
it
is
composed
mostly
of
iron
(321%)
oxygen
(301%)
silicon
(151%)
magnesium
(139%)
sulfur
(29%)
nickel
(18%)
calcium
(15%)
and
aluminium
(14%)
with
the
remaining
12%
consisting
of
trace
amounts
of
other
elements
due
to
mass
segregation
the
core
region
is
estimated
to
be
primarily
composed
of
iron
(888%)
with
smaller
amounts
of
nickel
(58%)
sulfur
(45%)
and
less
than
1%
trace
elements
the
most
common
rock
constituents
of
the
crust
are
nearly
all
oxides:
chlorine
sulfur
and
fluorine
are
the
important
exceptions
to
this
and
their
total
amount
in
any
rock
is
usually
much
less
than
1%
over
99%
of
the
crust
is
composed
of
11
oxides
principally
silica
alumina
iron
oxides
lime
magnesia
potash
and
soda
earth's
interior
like
that
of
the
other
terrestrial
planets
is
divided
into
layers
by
their
chemical
or
physical
(rheological)
properties
the
outer
layer
is
a
chemically
distinct
silicate
solid
crust
which
is
underlain
by
a
highly
viscous
solid
mantle
the
crust
is
separated
from
the
mantle
by
the
mohorovičić
discontinuity
the
thickness
of
the
crust
varies
from
about
under
the
oceans
to
for
the
continents
the
crust
and
the
cold
rigid
top
of
the
upper
mantle
are
collectively
known
as
the
lithosphere
and
it
is
of
the
lithosphere
that
the
tectonic
plates
are
composed
beneath
the
lithosphere
is
the
asthenosphere
a
relatively
low-viscosity
layer
on
which
the
lithosphere
rides
important
changes
in
crystal
structure
within
the
mantle
occur
at
below
the
surface
spanning
a
transition
zone
that
separates
the
upper
and
lower
mantle
beneath
the
mantle
an
extremely
low
viscosity
liquid
outer
core
lies
above
a
solid
inner
core
the
earth's
inner
core
might
rotate
at
a
slightly
higher
angular
velocity
than
the
remainder
of
the
planet
advancing
by
01–05°
per
year
the
radius
of
the
inner
core
is
about
one
fifth
of
that
of
earth
earth's
internal
heat
comes
from
a
combination
of
residual
heat
from
planetary
accretion
(about
20%)
and
heat
produced
through
radioactive
decay
(80%)
the
major
heat-producing
isotopes
within
earth
are
potassium-40
uranium-238
and
thorium-232
at
the
center
the
temperature
may
be
up
to
and
the
pressure
could
reach
because
much
of
the
heat
is
provided
by
radioactive
decay
scientists
postulate
that
early
in
earth's
history
before
isotopes
with
short
half-lives
were
depleted
earth's
heat
production
was
much
higher
at
approximately
twice
the
present-day
heat
would
have
been
produced
increasing
the
rates
of
mantle
convection
and
plate
tectonics
and
allowing
the
production
of
uncommon
igneous
rocks
such
as
komatiites
that
are
rarely
formed
today
the
mean
heat
loss
from
earth
is
for
a
global
heat
loss
of
a
portion
of
the
core's
thermal
energy
is
transported
toward
the
crust
by
mantle
plumes
a
form
of
convection
consisting
of
upwellings
of
higher-temperature
rock
these
plumes
can
produce
hotspots
and
flood
basalts
more
of
the
heat
in
earth
is
lost
through
plate
tectonics
by
mantle
upwelling
associated
with
mid-ocean
ridges
the
final
major
mode
of
heat
loss
is
through
conduction
through
the
lithosphere
the
majority
of
which
occurs
under
the
oceans
because
the
crust
there
is
much
thinner
than
that
of
the
continents
earth's
mechanically
rigid
outer
layer
the
lithosphere
is
divided
into
tectonic
plates
these
plates
are
rigid
segments
that
move
relative
to
each
other
at
one
of
three
boundaries
types:
at
convergent
boundaries
two
plates
come
together;
at
divergent
boundaries
two
plates
are
pulled
apart;
and
at
transform
boundaries
two
plates
slide
past
one
another
laterally
along
these
plate
boundaries
earthquakes
volcanic
activity
mountain-building
and
oceanic
trench
formation
can
occur
the
tectonic
plates
ride
on
top
of
the
asthenosphere
the
solid
but
less-viscous
part
of
the
upper
mantle
that
can
flow
and
move
along
with
the
plates
as
the
tectonic
plates
migrate
oceanic
crust
is
subducted
under
the
leading
edges
of
the
plates
at
convergent
boundaries
at
the
same
time
the
upwelling
of
mantle
material
at
divergent
boundaries
creates
mid-ocean
ridges
the
combination
of
these
processes
recycles
the
oceanic
crust
back
into
the
mantle
due
to
this
recycling
most
of
the
ocean
floor
is
less
than
old
the
oldest
oceanic
crust
is
located
in
the
western
pacific
and
is
estimated
to
be
old
by
comparison
the
oldest
dated
continental
crust
is
the
seven
major
plates
are
the
pacific
north
american
eurasian
african
antarctic
indo-australian
and
south
american
other
notable
plates
include
the
arabian
plate
the
caribbean
plate
the
nazca
plate
off
the
west
coast
of
south
america
and
the
scotia
plate
in
the
southern
atlantic
ocean
the
australian
plate
fused
with
the
indian
plate
between
the
fastest-moving
plates
are
the
oceanic
plates
with
the
cocos
plate
advancing
at
a
rate
of
and
the
pacific
plate
moving
at
the
other
extreme
the
slowest-moving
plate
is
the
eurasian
plate
progressing
at
a
typical
rate
of
the
total
surface
area
of
earth
is
about
of
this
708%
or
is
below
sea
level
and
covered
by
ocean
water
below
the
ocean's
surface
are
much
of
the
continental
shelf
mountains
volcanoes
oceanic
trenches
submarine
canyons
oceanic
plateaus
abyssal
plains
and
a
globe-spanning
mid-ocean
ridge
system
the
remaining
292%
or
not
covered
by
water
has
terrain
that
varies
greatly
from
place
to
place
and
consists
of
mountains
deserts
plains
plateaus
and
other
landforms
tectonics
and
erosion
volcanic
eruptions
flooding
weathering
glaciation
the
growth
of
coral
reefs
and
meteorite
impacts
are
among
the
processes
that
constantly
reshape
the
earth's
surface
over
geological
time
the
continental
crust
consists
of
lower
density
material
such
as
the
igneous
rocks
granite
and
andesite
less
common
is
basalt
a
denser
volcanic
rock
that
is
the
primary
constituent
of
the
ocean
floors
sedimentary
rock
is
formed
from
the
accumulation
of
sediment
that
becomes
buried
and
compacted
together
nearly
75%
of
the
continental
surfaces
are
covered
by
sedimentary
rocks
although
they
form
about
5%
of
the
crust
the
third
form
of
rock
material
found
on
earth
is
metamorphic
rock
which
is
created
from
the
transformation
of
pre-existing
rock
types
through
high
pressures
high
temperatures
or
both
the
most
abundant
silicate
minerals
on
earth's
surface
include
quartz
feldspars
amphibole
mica
pyroxene
and
olivine
common
carbonate
minerals
include
calcite
(found
in
limestone)
and
dolomite
the
elevation
of
the
land
surface
varies
from
the
low
point
of
at
the
dead
sea
to
a
maximum
altitude
of
at
the
top
of
mount
everest
the
mean
height
of
land
above
sea
level
is
about
the
pedosphere
is
the
outermost
layer
of
earth's
continental
surface
and
is
composed
of
soil
and
subject
to
soil
formation
processes
the
total
arable
land
is
109%
of
the
land
surface
with
13%
being
permanent
cropland
close
to
40%
of
earth's
land
surface
is
used
for
agriculture
or
an
estimated
of
cropland
and
of
pastureland
the
abundance
of
water
on
earth's
surface
is
a
unique
feature
that
distinguishes
the
"blue
planet"
from
other
planets
in
the
solar
system
earth's
hydrosphere
consists
chiefly
of
the
oceans
but
technically
includes
all
water
surfaces
in
the
world
including
inland
seas
lakes
rivers
and
underground
waters
down
to
a
depth
of
the
deepest
underwater
location
is
challenger
deep
of
the
mariana
trench
in
the
pacific
ocean
with
a
depth
of
the
mass
of
the
oceans
is
approximately
135 metric
tons
or
about
1/4400
of
earth's
total
mass
the
oceans
cover
an
area
of
with
a
mean
depth
of
resulting
in
an
estimated
volume
of
if
all
of
earth's
crustal
surface
were
at
the
same
elevation
as
a
smooth
sphere
the
depth
of
the
resulting
world
ocean
would
be
about
975%
of
the
water
is
saline;
the
remaining
25%
is
fresh
water
most
fresh
water
about
687%
is
present
as
ice
in
ice
caps
and
glaciers
the
average
salinity
of
earth's
oceans
is
about
35 grams
of
salt
per
kilogram
of
sea
water
(35%
salt)
most
of
this
salt
was
released
from
volcanic
activity
or
extracted
from
cool
igneous
rocks
the
oceans
are
also
a
reservoir
of
dissolved
atmospheric
gases
which
are
essential
for
the
survival
of
many
aquatic
life
forms
sea
water
has
an
important
influence
on
the
world's
climate
with
the
oceans
acting
as
a
large
heat
reservoir
shifts
in
the
oceanic
temperature
distribution
can
cause
significant
weather
shifts
such
as
the
el
niño–southern
oscillation
the
atmospheric
pressure
at
earth's
sea
level
averages
with
a
scale
height
of
about
a
dry
atmosphere
is
composed
of
78084%
nitrogen
20946%
oxygen
0934%
argon
and
trace
amounts
of
carbon
dioxide
and
other
gaseous
molecules
water
vapor
content
varies
between
001%
and
4%
but
averages
about
1%
the
height
of
the
troposphere
varies
with
latitude
ranging
between
at
the
poles
to
at
the
equator
with
some
variation
resulting
from
weather
and
seasonal
factors
earth's
biosphere
has
significantly
altered
its
atmosphere
oxygenic
photosynthesis
evolved
forming
the
primarily
nitrogen–oxygen
atmosphere
of
today
this
change
enabled
the
proliferation
of
aerobic
organisms
and
indirectly
the
formation
of
the
ozone
layer
due
to
the
subsequent
conversion
of
atmospheric
into
the
ozone
layer
blocks
ultraviolet
solar
radiation
permitting
life
on
land
other
atmospheric
functions
important
to
life
include
transporting
water
vapor
providing
useful
gases
causing
small
meteors
to
burn
up
before
they
strike
the
surface
and
moderating
temperature
this
last
phenomenon
is
known
as
the
greenhouse
effect:
trace
molecules
within
the
atmosphere
serve
to
capture
thermal
energy
emitted
from
the
ground
thereby
raising
the
average
temperature
water
vapor
carbon
dioxide
methane
nitrous
oxide
and
ozone
are
the
primary
greenhouse
gases
in
the
atmosphere
without
this
heat-retention
effect
the
average
surface
temperature
would
be
in
contrast
to
the
current
and
life
on
earth
probably
would
not
exist
in
its
current
form
in
may
2017
glints
of
light
seen
as
twinkling
from
an
orbiting
satellite
a
million
miles
away
were
found
to
be
reflected
light
from
ice
crystals
in
the
atmosphere
earth's
atmosphere
has
no
definite
boundary
slowly
becoming
thinner
and
fading
into
outer
space
three-quarters
of
the
atmosphere's
mass
is
contained
within
the
first
of
the
surface
this
lowest
layer
is
called
the
troposphere
energy
from
the
sun
heats
this
layer
and
the
surface
below
causing
expansion
of
the
air
this
lower-density
air
then
rises
and
is
replaced
by
cooler
higher-density
air
the
result
is
atmospheric
circulation
that
drives
the
weather
and
climate
through
redistribution
of
thermal
energy
the
primary
atmospheric
circulation
bands
consist
of
the
trade
winds
in
the
equatorial
region
below
30°
latitude
and
the
westerlies
in
the
mid-latitudes
between
30°
and
60°
ocean
currents
are
also
important
factors
in
determining
climate
particularly
the
thermohaline
circulation
that
distributes
thermal
energy
from
the
equatorial
oceans
to
the
polar
regions
water
vapor
generated
through
surface
evaporation
is
transported
by
circulatory
patterns
in
the
atmosphere
when
atmospheric
conditions
permit
an
uplift
of
warm
humid
air
this
water
condenses
and
falls
to
the
surface
as
precipitation
most
of
the
water
is
then
transported
to
lower
elevations
by
river
systems
and
usually
returned
to
the
oceans
or
deposited
into
lakes
this
water
cycle
is
a
vital
mechanism
for
supporting
life
on
land
and
is
a
primary
factor
in
the
erosion
of
surface
features
over
geological
periods
precipitation
patterns
vary
widely
ranging
from
several
meters
of
water
per
year
to
less
than
a
millimeter
atmospheric
circulation
topographic
features
and
temperature
differences
determine
the
average
precipitation
that
falls
in
each
region
the
amount
of
solar
energy
reaching
earth's
surface
decreases
with
increasing
latitude
at
higher
latitudes
the
sunlight
reaches
the
surface
at
lower
angles
and
it
must
pass
through
thicker
columns
of
the
atmosphere
as
a
result
the
mean
annual
air
temperature
at
sea
level
decreases
by
about
per
degree
of
latitude
from
the
equator
earth's
surface
can
be
subdivided
into
specific
latitudinal
belts
of
approximately
homogeneous
climate
ranging
from
the
equator
to
the
polar
regions
these
are
the
tropical
(or
equatorial)
subtropical
temperate
and
polar
climates
this
latitudinal
rule
has
several
anomalies:
the
commonly
used
köppen
climate
classification
system
has
five
broad
groups
(humid
tropics
arid
humid
middle
latitudes
continental
and
cold
polar)
which
are
further
divided
into
more
specific
subtypes
the
köppen
system
rates
regions
of
terrain
based
on
observed
temperature
and
precipitation
the
highest
air
temperature
ever
measured
on
earth
was
in
furnace
creek
california
in
death
valley
in
1913
the
lowest
air
temperature
ever
directly
measured
on
earth
was
at
vostok
station
in
1983
but
satellites
have
used
remote
sensing
to
measure
temperatures
as
low
as
in
east
antarctica
these
temperature
records
are
only
measurements
made
with
modern
instruments
from
the
20th
century
onwards
and
likely
do
not
reflect
the
full
range
of
temperature
on
earth
above
the
troposphere
the
atmosphere
is
usually
divided
into
the
stratosphere
mesosphere
and
thermosphere
each
layer
has
a
different
lapse
rate
defining
the
rate
of
change
in
temperature
with
height
beyond
these
the
exosphere
thins
out
into
the
magnetosphere
where
the
geomagnetic
fields
interact
with
the
solar
wind
within
the
stratosphere
is
the
ozone
layer
a
component
that
partially
shields
the
surface
from
ultraviolet
light
and
thus
is
important
for
life
on
earth
the
kármán
line
defined
as
100 km
above
earth's
surface
is
a
working
definition
for
the
boundary
between
the
atmosphere
and
outer
space
thermal
energy
causes
some
of
the
molecules
at
the
outer
edge
of
the
atmosphere
to
increase
their
velocity
to
the
point
where
they
can
escape
from
earth's
gravity
this
causes
a
slow
but
steady
loss
of
the
atmosphere
into
space
because
unfixed
hydrogen
has
a
low
molecular
mass
it
can
achieve
escape
velocity
more
readily
and
it
leaks
into
outer
space
at
a
greater
rate
than
other
gases
the
leakage
of
hydrogen
into
space
contributes
to
the
shifting
of
earth's
atmosphere
and
surface
from
an
initially
reducing
state
to
its
current
oxidizing
one
photosynthesis
provided
a
source
of
free
oxygen
but
the
loss
of
reducing
agents
such
as
hydrogen
is
thought
to
have
been
a
necessary
precondition
for
the
widespread
accumulation
of
oxygen
in
the
atmosphere
hence
the
ability
of
hydrogen
to
escape
from
the
atmosphere
may
have
influenced
the
nature
of
life
that
developed
on
earth
in
the
current
oxygen-rich
atmosphere
most
hydrogen
is
converted
into
water
before
it
has
an
opportunity
to
escape
instead
most
of
the
hydrogen
loss
comes
from
the
destruction
of
methane
in
the
upper
atmosphere
the
gravity
of
earth
is
the
acceleration
that
is
imparted
to
objects
due
to
the
distribution
of
mass
within
the
earth
near
the
earth's
surface
gravitational
acceleration
is
approximately
local
differences
in
topography
geology
and
deeper
tectonic
structure
cause
local
and
broad
regional
differences
in
the
earth's
gravitational
field
known
as
gravity
anomalies
the
main
part
of
earth's
magnetic
field
is
generated
in
the
core
the
site
of
a
dynamo
process
that
converts
the
kinetic
energy
of
thermally
and
compositionally
driven
convection
into
electrical
and
magnetic
field
energy
the
field
extends
outwards
from
the
core
through
the
mantle
and
up
to
earth's
surface
where
it
is
approximately
a
dipole
the
poles
of
the
dipole
are
located
close
to
earth's
geographic
poles
at
the
equator
of
the
magnetic
field
the
magnetic-field
strength
at
the
surface
is
with
global
magnetic
dipole
moment
of
the
convection
movements
in
the
core
are
chaotic;
the
magnetic
poles
drift
and
periodically
change
alignment
this
causes
secular
variation
of
the
main
field
and
field
reversals
at
irregular
intervals
averaging
a
few
times
every
million
years
the
most
recent
reversal
occurred
approximately
700000
years
ago
the
extent
of
earth's
magnetic
field
in
space
defines
the
magnetosphere
ions
and
electrons
of
the
solar
wind
are
deflected
by
the
magnetosphere;
solar
wind
pressure
compresses
the
dayside
of
the
magnetosphere
to
about
10
earth
radii
and
extends
the
nightside
magnetosphere
into
a
long
tail
because
the
velocity
of
the
solar
wind
is
greater
than
the
speed
at
which
waves
propagate
through
the
solar
wind
a
supersonic
bowshock
precedes
the
dayside
magnetosphere
within
the
solar
wind
charged
particles
are
contained
within
the
magnetosphere;
the
plasmasphere
is
defined
by
low-energy
particles
that
essentially
follow
magnetic
field
lines
as
earth
rotates;
the
ring
current
is
defined
by
medium-energy
particles
that
drift
relative
to
the
geomagnetic
field
but
with
paths
that
are
still
dominated
by
the
magnetic
field
and
the
van
allen
radiation
belt
are
formed
by
high-energy
particles
whose
motion
is
essentially
random
but
otherwise
contained
by
the
magnetosphere
during
magnetic
storms
and
substorms
charged
particles
can
be
deflected
from
the
outer
magnetosphere
and
especially
the
magnetotail
directed
along
field
lines
into
earth's
ionosphere
where
atmospheric
atoms
can
be
excited
and
ionized
causing
the
aurora
earth's
rotation
period
relative
to
the
sun—its
mean
solar
day—is
of
mean
solar
time
()
because
earth's
solar
day
is
now
slightly
longer
than
it
was
during
the
19th
century
due
to
tidal
deceleration
each
day
varies
between
longer
earth's
rotation
period
relative
to
the
fixed
stars
called
its
"stellar
day"
by
the
international
earth
rotation
and
reference
systems
service
(iers)
is
of
mean
solar
time
(ut1)
or
earth's
rotation
period
relative
to
the
precessing
or
moving
mean
vernal
equinox
misnamed
its
"sidereal
day"
is
of
mean
solar
time
(ut1)
thus
the
sidereal
day
is
shorter
than
the
stellar
day
by
about
84 ms
the
length
of
the
mean
solar
day
in
si
seconds
is
available
from
the
iers
for
the
periods
1623–2005
and
1962–2005
apart
from
meteors
within
the
atmosphere
and
low-orbiting
satellites
the
main
apparent
motion
of
celestial
bodies
in
earth's
sky
is
to
the
west
at
a
rate
of
15°/h
=
15'/min
for
bodies
near
the
celestial
equator
this
is
equivalent
to
an
apparent
diameter
of
the
sun
or
the
moon
every
two
minutes;
from
earth's
surface
the
apparent
sizes
of
the
sun
and
the
moon
are
approximately
the
same
earth
orbits
the
sun
at
an
average
distance
of
about
every
3652564
mean
solar
days
or
one
sidereal
year
this
gives
an
apparent
movement
of
the
sun
eastward
with
respect
to
the
stars
at
a
rate
of
about
1°/day
which
is
one
apparent
sun
or
moon
diameter
every
12 hours
due
to
this
motion
on
average
it
takes
24 hours—a
solar
day—for
earth
to
complete
a
full
rotation
about
its
axis
so
that
the
sun
returns
to
the
meridian
the
orbital
speed
of
earth
averages
about
which
is
fast
enough
to
travel
a
distance
equal
to
earth's
diameter
about
in
seven
minutes
and
the
distance
to
the
moon
in
about
35
hours
the
moon
and
earth
orbit
a
common
barycenter
every
2732 days
relative
to
the
background
stars
when
combined
with
the
earth–moon
system's
common
orbit
around
the
sun
the
period
of
the
synodic
month
from
new
moon
to
new
moon
is
2953 days
viewed
from
the
celestial
north
pole
the
motion
of
earth
the
moon
and
their
axial
rotations
are
all
counterclockwise
viewed
from
a
vantage
point
above
the
north
poles
of
both
the
sun
and
earth
earth
orbits
in
a
counterclockwise
direction
about
the
sun
the
orbital
and
axial
planes
are
not
precisely
aligned:
earth's
axis
is
tilted
some
2344 degrees
from
the
perpendicular
to
the
earth–sun
plane
(the
ecliptic)
and
the
earth–moon
plane
is
tilted
up
to
±51 degrees
against
the
earth–sun
plane
without
this
tilt
there
would
be
an
eclipse
every
two
weeks
alternating
between
lunar
eclipses
and
solar
eclipses
the
hill
sphere
or
the
sphere
of
gravitational
influence
of
the
earth
is
about
in
radius
this
is
the
maximum
distance
at
which
the
earth's
gravitational
influence
is
stronger
than
the
more
distant
sun
and
planets
objects
must
orbit
the
earth
within
this
radius
or
they
can
become
unbound
by
the
gravitational
perturbation
of
the
sun
earth
along
with
the
solar
system
is
situated
in
the
milky
way
and
orbits
about
28000 light-years
from
its
center
it
is
about
20 light-years
above
the
galactic
plane
in
the
orion
arm
the
axial
tilt
of
the
earth
is
approximately
23439281°
with
the
axis
of
its
orbit
plane
always
pointing
towards
the
celestial
poles
due
to
earth's
axial
tilt
the
amount
of
sunlight
reaching
any
given
point
on
the
surface
varies
over
the
course
of
the
year
this
causes
the
seasonal
change
in
climate
with
summer
in
the
northern
hemisphere
occurring
when
the
tropic
of
cancer
is
facing
the
sun
and
winter
taking
place
when
the
tropic
of
capricorn
in
the
southern
hemisphere
faces
the
sun
during
the
summer
the
day
lasts
longer
and
the
sun
climbs
higher
in
the
sky
in
winter
the
climate
becomes
cooler
and
the
days
shorter
in
northern
temperate
latitudes
the
sun
rises
north
of
true
east
during
the
summer
solstice
and
sets
north
of
true
west
reversing
in
the
winter
the
sun
rises
south
of
true
east
in
the
summer
for
the
southern
temperate
zone
and
sets
south
of
true
west
above
the
arctic
circle
an
extreme
case
is
reached
where
there
is
no
daylight
at
all
for
part
of
the
year
up
to
six
months
at
the
north
pole
itself
a
polar
night
in
the
southern
hemisphere
the
situation
is
exactly
reversed
with
the
south
pole
oriented
opposite
the
direction
of
the
north
pole
six
months
later
this
pole
will
experience
a
midnight
sun
a
day
of
24
hours
again
reversing
with
the
south
pole
by
astronomical
convention
the
four
seasons
can
be
determined
by
the
solstices—the
points
in
the
orbit
of
maximum
axial
tilt
toward
or
away
from
the
sun—and
the
equinoxes
when
the
direction
of
the
tilt
and
the
direction
to
the
sun
are
perpendicular
in
the
northern
hemisphere
winter
solstice
currently
occurs
around
21
december;
summer
solstice
is
near
21
june
spring
equinox
is
around
20
march
and
autumnal
equinox
is
about
22
or
23
september
in
the
southern
hemisphere
the
situation
is
reversed
with
the
summer
and
winter
solstices
exchanged
and
the
spring
and
autumnal
equinox
dates
swapped
the
angle
of
earth's
axial
tilt
is
relatively
stable
over
long
periods
of
time
its
axial
tilt
does
undergo
nutation;
a
slight
irregular
motion
with
a
main
period
of
186 years
the
orientation
(rather
than
the
angle)
of
earth's
axis
also
changes
over
time
precessing
around
in
a
complete
circle
over
each
25800 year
cycle;
this
precession
is
the
reason
for
the
difference
between
a
sidereal
year
and
a
tropical
year
both
of
these
motions
are
caused
by
the
varying
attraction
of
the
sun
and
the
moon
on
earth's
equatorial
bulge
the
poles
also
migrate
a
few
meters
across
earth's
surface
this
polar
motion
has
multiple
cyclical
components
which
collectively
are
termed
quasiperiodic
motion
in
addition
to
an
annual
component
to
this
motion
there
is
a
14-month
cycle
called
the
chandler
wobble
earth's
rotational
velocity
also
varies
in
a
phenomenon
known
as
length-of-day
variation
in
modern
times
earth's
perihelion
occurs
around
3
january
and
its
aphelion
around
4
july
these
dates
change
over
time
due
to
precession
and
other
orbital
factors
which
follow
cyclical
patterns
known
as
milankovitch
cycles
the
changing
earth–sun
distance
causes
an
increase
of
about
69%
in
solar
energy
reaching
earth
at
perihelion
relative
to
aphelion
because
the
southern
hemisphere
is
tilted
toward
the
sun
at
about
the
same
time
that
earth
reaches
the
closest
approach
to
the
sun
the
southern
hemisphere
receives
slightly
more
energy
from
the
sun
than
does
the
northern
over
the
course
of
a
year
this
effect
is
much
less
significant
than
the
total
energy
change
due
to
the
axial
tilt
and
most
of
the
excess
energy
is
absorbed
by
the
higher
proportion
of
water
in
the
southern
hemisphere
a
study
from
2016
suggested
that
planet
nine
tilted
all
solar
system
planets
including
earth's
by
about
six
degrees
a
planet
that
can
sustain
life
is
termed
habitable
even
if
life
did
not
originate
there
earth
provides
liquid
water—an
environment
where
complex
organic
molecules
can
assemble
and
interact
and
sufficient
energy
to
sustain
metabolism
the
distance
of
earth
from
the
sun
as
well
as
its
orbital
eccentricity
rate
of
rotation
axial
tilt
geological
history
sustaining
atmosphere
and
magnetic
field
all
contribute
to
the
current
climatic
conditions
at
the
surface
a
planet's
life
forms
inhabit
ecosystems
whose
total
is
sometimes
said
to
form
a
"biosphere"
earth's
biosphere
is
thought
to
have
begun
evolving
about
the
biosphere
is
divided
into
a
number
of
biomes
inhabited
by
broadly
similar
plants
and
animals
on
land
biomes
are
separated
primarily
by
differences
in
latitude
height
above
sea
level
and
humidity
terrestrial
biomes
lying
within
the
arctic
or
antarctic
circles
at
high
altitudes
or
in
extremely
arid
areas
are
relatively
barren
of
plant
and
animal
life;
species
diversity
reaches
a
peak
in
humid
lowlands
at
equatorial
latitudes
in
july
2016
scientists
reported
identifying
a
set
of
355
genes
from
the
last
universal
common
ancestor
(luca)
of
all
organisms
living
on
earth
earth
has
resources
that
have
been
exploited
by
humans
those
termed
non-renewable
resources
such
as
fossil
fuels
only
renew
over
geological
timescales
large
deposits
of
fossil
fuels
are
obtained
from
earth's
crust
consisting
of
coal
petroleum
and
natural
gas
these
deposits
are
used
by
humans
both
for
energy
production
and
as
feedstock
for
chemical
production
mineral
ore
bodies
have
also
been
formed
within
the
crust
through
a
process
of
ore
genesis
resulting
from
actions
of
magmatism
erosion
and
plate
tectonics
these
bodies
form
concentrated
sources
for
many
metals
and
other
useful
elements
earth's
biosphere
produces
many
useful
biological
products
for
humans
including
food
wood
pharmaceuticals
oxygen
and
the
recycling
of
many
organic
wastes
the
land-based
ecosystem
depends
upon
topsoil
and
fresh
water
and
the
oceanic
ecosystem
depends
upon
dissolved
nutrients
washed
down
from
the
land
in
1980
of
earth's
land
surface
consisted
of
forest
and
woodlands
was
grasslands
and
pasture
and
was
cultivated
as
croplands
the
estimated
amount
of
irrigated
land
in
1993
was
humans
also
live
on
the
land
by
using
building
materials
to
construct
shelters
large
areas
of
earth's
surface
are
subject
to
extreme
weather
such
as
tropical
cyclones
hurricanes
or
typhoons
that
dominate
life
in
those
areas
from
1980
to
2000
these
events
caused
an
average
of
11800
human
deaths
per
year
many
places
are
subject
to
earthquakes
landslides
tsunamis
volcanic
eruptions
tornadoes
sinkholes
blizzards
floods
droughts
wildfires
and
other
calamities
and
disasters
many
localized
areas
are
subject
to
human-made
pollution
of
the
air
and
water
acid
rain
and
toxic
substances
loss
of
vegetation
(overgrazing
deforestation
desertification)
loss
of
wildlife
species
extinction
soil
degradation
soil
depletion
and
erosion
there
is
a
scientific
consensus
linking
human
activities
to
global
warming
due
to
industrial
carbon
dioxide
emissions
this
is
predicted
to
produce
changes
such
as
the
melting
of
glaciers
and
ice
sheets
more
extreme
temperature
ranges
significant
changes
in
weather
and
a
global
rise
in
average
sea
levels
cartography
the
study
and
practice
of
map-making
and
geography
the
study
of
the
lands
features
inhabitants
and
phenomena
on
earth
have
historically
been
the
disciplines
devoted
to
depicting
earth
surveying
the
determination
of
locations
and
distances
and
to
a
lesser
extent
navigation
the
determination
of
position
and
direction
have
developed
alongside
cartography
and
geography
providing
and
suitably
quantifying
the
requisite
information
earth's
human
population
reached
approximately
seven
billion
on
31
october
2011
projections
indicate
that
the
world's
human
population
will
reach
92 billion
in
2050
most
of
the
growth
is
expected
to
take
place
in
developing
nations
human
population
density
varies
widely
around
the
world
but
a
majority
live
in
asia
by
2020
60%
of
the
world's
population
is
expected
to
be
living
in
urban
rather
than
rural
areas
68%
of
the
land
mass
of
the
world
is
in
the
northern
hemisphere
partly
due
to
the
predominance
of
land
mass
90%
of
humans
live
in
the
northern
hemisphere
it
is
estimated
that
one-eighth
of
earth's
surface
is
suitable
for
humans
to
live
on –
three-quarters
of
earth's
surface
is
covered
by
oceans
leaving
one-quarter
as
land
half
of
that
land
area
is
desert
(14%)
high
mountains
(27%)
or
other
unsuitable
terrains
the
northernmost
permanent
settlement
in
the
world
is
alert
on
ellesmere
island
in
nunavut
canada
(82°28′n)
the
southernmost
is
the
amundsen–scott
south
pole
station
in
antarctica
almost
exactly
at
the
south
pole
(90°s)
independent
sovereign
nations
claim
the
planet's
entire
land
surface
except
for
some
parts
of
antarctica
a
few
land
parcels
along
the
danube
river's
western
bank
and
the
unclaimed
area
of
bir
tawil
between
egypt
and
sudan
there
are
193
sovereign
states
that
are
member
states
of
the
united
nations
plus
two
observer
states
and
72
dependent
territories
and
states
with
limited
recognition
earth
has
never
had
a
sovereign
government
with
authority
over
the
entire
globe
although
some
nation-states
have
striven
for
world
domination
and
failed
the
united
nations
is
a
worldwide
intergovernmental
organization
that
was
created
with
the
goal
of
intervening
in
the
disputes
between
nations
thereby
avoiding
armed
conflict
the
un
serves
primarily
as
a
forum
for
international
diplomacy
and
international
law
when
the
consensus
of
the
membership
permits
it
provides
a
mechanism
for
armed
intervention
the
first
human
to
orbit
earth
was
yuri
gagarin
on
12
april
1961
in
total
about
487
people
have
visited
outer
space
and
reached
orbit
and
of
these
twelve
have
walked
on
the
moon
normally
the
only
humans
in
space
are
those
on
the
international
space
station
the
station's
crew
made
up
of
six
people
is
usually
replaced
every
six
months
the
farthest
that
humans
have
traveled
from
earth
is
achieved
during
the
apollo
13
mission
in
1970
the
moon
is
a
relatively
large
terrestrial
planet-like
natural
satellite
with
a
diameter
about
one-quarter
of
earth's
it
is
the
largest
moon
in
the
solar
system
relative
to
the
size
of
its
planet
although
charon
is
larger
relative
to
the
dwarf
planet
pluto
the
natural
satellites
of
other
planets
are
also
referred
to
as
"moons"
after
earth's
the
gravitational
attraction
between
earth
and
the
moon
causes
tides
on
earth
the
same
effect
on
the
moon
has
led
to
its
tidal
locking:
its
rotation
period
is
the
same
as
the
time
it
takes
to
orbit
earth
as
a
result
it
always
presents
the
same
face
to
the
planet
as
the
moon
orbits
earth
different
parts
of
its
face
are
illuminated
by
the
sun
leading
to
the
lunar
phases;
the
dark
part
of
the
face
is
separated
from
the
light
part
by
the
solar
terminator
due
to
their
tidal
interaction
the
moon
recedes
from
earth
at
the
rate
of
approximately
over
millions
of
years
these
tiny
modifications—and
the
lengthening
of
earth's
day
by
about
23 µs/yr—add
up
to
significant
changes
during
the
devonian
period
for
example
(approximately
)
there
were
400
days
in
a
year
with
each
day
lasting
218
hours
the
moon
may
have
dramatically
affected
the
development
of
life
by
moderating
the
planet's
climate
paleontological
evidence
and
computer
simulations
show
that
earth's
axial
tilt
is
stabilized
by
tidal
interactions
with
the
moon
some
theorists
think
that
without
this
stabilization
against
the
torques
applied
by
the
sun
and
planets
to
earth's
equatorial
bulge
the
rotational
axis
might
be
chaotically
unstable
exhibiting
chaotic
changes
over
millions
of
years
as
appears
to
be
the
case
for
mars
viewed
from
earth
the
moon
is
just
far
enough
away
to
have
almost
the
same
apparent-sized
disk
as
the
sun
the
angular
size
(or
solid
angle)
of
these
two
bodies
match
because
although
the
sun's
diameter
is
about
400
times
as
large
as
the
moon's
it
is
also
400
times
more
distant
this
allows
total
and
annular
solar
eclipses
to
occur
on
earth
the
most
widely
accepted
theory
of
the
moon's
origin
the
giant-impact
hypothesis
states
that
it
formed
from
the
collision
of
a
mars-size
protoplanet
called
theia
with
the
early
earth
this
hypothesis
explains
(among
other
things)
the
moon's
relative
lack
of
iron
and
volatile
elements
and
the
fact
that
its
composition
is
nearly
identical
to
that
of
earth's
crust
earth
has
at
least
five
co-orbital
asteroids
including
3753
cruithne
and
a
trojan
asteroid
companion
is
librating
around
the
leading
lagrange
triangular
point
l4
in
the
earth's
orbit
around
the
sun
the
tiny
near-earth
asteroid
makes
close
approaches
to
the
earth–moon
system
roughly
every
twenty
years
during
these
approaches
it
can
orbit
earth
for
brief
periods
of
time
there
are
1886
operational
human-made
satellites
orbiting
earth
there
are
also
inoperative
satellites
including
vanguard
1
the
oldest
satellite
currently
in
orbit
and
over
16000
pieces
of
tracked
space
debris
earth's
largest
artificial
satellite
is
the
international
space
station
the
standard
astronomical
symbol
of
earth
consists
of
a
cross
circumscribed
by
a
circle
representing
the
four
corners
of
the
world
human
cultures
have
developed
many
views
of
the
planet
earth
is
sometimes
personified
as
a
deity
in
many
cultures
it
is
a
mother
goddess
that
is
also
the
primary
fertility
deity
and
by
the
mid-20th
century
the
gaia
principle
compared
earth's
environments
and
life
as
a
single
self-regulating
organism
leading
to
broad
stabilization
of
the
conditions
of
habitability
creation
myths
in
many
religions
involve
the
creation
of
earth
by
a
supernatural
deity
or
deities
scientific
investigation
has
resulted
in
several
culturally
transformative
shifts
in
people's
view
of
the
planet
initial
belief
in
a
flat
earth
was
gradually
displaced
in
the
greek
colonies
of
southern
italy
during
the
late
6th
century
bc
by
the
idea
of
spherical
earth
which
was
attributed
to
both
the
philosophers
pythagoras
and
parmenides
by
the
end
of
the
5th
century
bc
the
sphericity
of
earth
was
universally
accepted
among
greek
intellectuals
earth
was
generally
believed
to
be
the
center
of
the
universe
until
the
16th
century
when
scientists
first
conclusively
demonstrated
that
it
was
a
moving
object
comparable
to
the
other
planets
in
the
solar
system
due
to
the
efforts
of
influential
christian
scholars
and
clerics
such
as
james
ussher
who
sought
to
determine
the
age
of
earth
through
analysis
of
genealogies
in
scripture
westerners
before
the
19th
century
generally
believed
earth
to
be
a
few
thousand
years
old
at
most
it
was
only
during
the
19th
century
that
geologists
realized
earth's
age
was
at
least
many
millions
of
years
lord
kelvin
used
thermodynamics
to
estimate
the
age
of
earth
to
be
between
20
million
and
400 million
years
in
1864
sparking
a
vigorous
debate
on
the
subject;
it
was
only
when
radioactivity
and
radioactive
dating
were
discovered
in
the
late
19th
and
early
20th
centuries
that
a
reliable
mechanism
for
determining
earth's
age
was
established
proving
the
planet
to
be
billions
of
years
old
the
perception
of
earth
shifted
again
in
the
20th
century
when
humans
first
viewed
it
from
orbit
and
especially
with
photographs
of
earth
returned
by
the
apollo
program
selvaggio
blu
(sardinia)
the
selvaggio
blu
(wild
blue)
is
a
trekking
route
in
the
territory
of
the
district
of
baunei
(sardinia)
it
was
conceived
in
1987
by
mario
verin
(photographer
and
alpinist)
and
peppino
cicalò
(architect)
president
of
the
nuoro
section
of
the
italian
alpine
club
the
itinerary
extends
for
over
40
kilometers
(approximately
25
miles)
from
the
touristic
port
of
santa
maria
navarrese
(baunei)
to
the
beach
of
cala
sisine
(baunei)
it
takes
on
average
4
days
to
complete
the
selvaggio
blu
is
considered
one
of
the
last
wild
trekking
routes
of
the
mediterranean
because
for
the
major
part
of
the
itinerary
it
can
only
be
accessed
by
boat
or
by
following
the
path
along
the
coast
of
the
gulf
of
orosei
verin
and
cicalò
used
the
name
'selvaggio
blu'
to
reflect
the
main
characteristics
of
the
journey:
"selvaggio"
to
reflect
the
wildness
and
pureness
of
the
experience
and
"blu"
because
the
trek
goes
along
the
coast
where
the
color
of
the
sea
and
the
sky
is
predominant
the
selvaggio
blu
is
located
entirely
in
the
territory
of
the
district
of
baunei
which
extends
for
2119
km2
on
the
east
coast
of
sardinia
in
the
province
of
ogliastra
the
baunei
area
is
considered
one
of
the
wildest
in
sardinia
going
from
the
coastal
town
of
santa
maria
navarrese
and
traversing
limestone
plateaux
and
coastal
scenery
to
the
beach
of
cala
luna
it
contains
all
the
main
centers
on
the
route
including:
santa
maria
navarrese
pedra
longa
portu
pedrosu
cala
goloritzè
su
feilau
cala
sisine
and
also
all
the
centers
included
in
selvaggio
blu
variations:
cala
mariolu
cala
biriala;
s'istrada
longa
grotta
del
fico
and
the
plateau
of
golgo
selvaggio
blu
has
a
strategic
location
to
see
the
geological
history
of
sardinia
as
it
is
located
40 km
along
the
coast
there
are
several
important
geology
observations
on
selvaggio
blu's
hiking
route
which
can
be
reached
both
by
land
and
sea
in
the
northern
part
of
santa
maria
navarrese
hikers
have
the
possibility
to
move
along
a
section
with
fractured
granite
from
the
palazoic
age
where
rocks
are
several
meters
thick
this
area
contains
some
of
sardinia's
oldest
rocks
covered
with
layers
of
cambrian-ordovician
metasandstones
phyllites
and
quartzites
in
pedra
longa
the
section
with
granite
ends
and
there
is
a
transition
to
the
limestones
which
are
a
common
feature
of
the
gulf
of
orosei
coast
because
of
climate
changes
the
limestone
has
been
affected
by
rising
and
falling
of
sea
level
changes
caused
by
the
mixing
of
fresh
water
from
the
limestone
and
salt
sea
water
include
larger
limestones
cavities
and
fluctuations
in
different
colours
grotta
del
fico
is
a
karst
cave
located
on
the
selvaggio
blu
between
santa
maria
navarrese
and
cala
luna
grotta
del
fico
is
accessible
by
boat
or
walking
it
was
discovered
by
fishermen
in
the
early
20th
century and
opened
to
the
public
with
guided
tours
in
2003
inside
grotta
del
fico
there
is
a
lake
with
clear
water
which
reflects
inside
the
cave
the
lake
is
created
by
karst
water
that
flows
in
the
main
part
of
the
cave
and
fills
up
the
lake
selvaggio
blu
is
a
40
kilometers
trek
that
has
an
estimated
travel
time
of
4
days
but
many
people
take
about
6
–
7
days
depending
on
experience
and
fitness
the
selvaggio
blu
starts
at
pedra
longa
(40°1'38"n
9°42'25"e)
and
goes
to
portu
pedrosu
(40°4'5"n
9°44'2"e)
it
has
an
estimated
travel
time
of
9
hours
to
cover
its
12 km
of
length
in
this
stage
it
is
reached
the
maximum
height
of
all
the
selvaggio
blu
which
is
770
m
this
section
gets
a
climbing
grade
of
eea
on
the
uaii
scale
which
makes
this
stage
the
third
hardest
in
technical
difficulty
of
the
entire
journey
the
path
crosses
a
small
valley
into
the
woods
until
it
reaches
a
point
where
it
is
possible
to
see
the
sea
the
path
continues
into
another
small
valley
with
holm
oaks
walking
on
very
sharp
limestone
flakes
the
path
then
crosses
two
gates
after
which
it
descends
toward
the
sea
following
the
cliffs
then
passing
on
the
top
of
the
grotta
dei
colombi
the
path
once
again
entersa
small
valley
descending
this
valley
on
the
left
there
is
a
section
equipped
with
juniper
trunks
that
allow
climbers
to
descend
into
bacu
tenadili
the
path
at
this
point
challenges
the
orientation
skills
of
the
climbers
because
of
the
lack
of
a
gps
signal
after
this
section
there
is
a
cracked
limestone
area
where
the
shepherds
are
used
to
collecting
water;
after
this
the
path
becomes
less
clear
following
a
steep
zigzag
the
path
leads
to
the
mooring
of
portu
pedrosu
were
the
first
stage
ends
walking
for
10
minutes
more
to
portu
cuau(40°5'11"n
9°43'55"e)
there
is
a
large
space
for
camping
the
itinerary
of
the
second
day
starts
at
portu
pedrosu
and
ends
in
cala
goloritzè
(40°6'29"n
9°41'23"e)
the
maximum
height
reached
during
this
day
is
about
495
m
this
stage
is
95 km
and
the
estimated
travel
time
is
of
6
hours
it
is
not
as
difficult
as
the
other
paths;
its
difficulty
has
been
ranked
as
ee
on
the
uaii
scale
which
makes
this
the
easiest
technical
section
of
the
selvaggio
blu
from
portu
pedrosu
the
path
is
well
defined
for
a
short
section
due
to
a
good
muletrack
surrounded
by
vegetation
passing
a
small
valley
the
path
climbs
up
to
a
rocky
plateau
where
the
track
becomes
less
well
defined
making
a
large
curve
towards
the
north
which
is
partially
covered
by
the
vegetation
the
path
follows
the
edge
of
a
small
valley
continuing
until
it
reaches
a
balcony
overlooking
the
sea
(2 km
from
portu
cuau)
keeping
the
sea
behind
the
path
proceeds
through
small
rocks
and
holm
oak
sections
until
it
reaches
the
ovile
of
kenos
trainos
one
of
the
most
important
along
the
selvaggio
blu
from
this
point
the
path
becomes
less
clear
because
of
thick
vegetation
and
a
small
rocky
step
reaching
su
runcu'e
su
press
it
is
possible
to
escape
from
the
selvaggio
blu
and
in
15
minutes
reach
a
clearing
which
is
accessible
by
a
suv
the
itinerary
of
the
third
day
is
74 km
long
with
a
maximum
height
of
485
m
it
is
the
most
difficult
stage
of
the
whole
route;
its
difficulty
has
been
ranked
iv+
of
the
uiaa
scale
some
people
decide
to
stop
during
this
stage
because
of
all
the
obstacles
on
the
path
to
bacu
su
feilau
this
section
requires
the
capability
to
travel
on
all
types
of
terrain
with
difficulties
including
the
exposure
difficult
vegetation
and
the
lack
of
gps
signal
during
this
stage
which
starts
from
cala
golorizè
and
ends
in
bacu
su
feilau
(40°3'59"n
9°34'52"e)
the
itinerary
climbs
two
rock
walls
the
first
one
of
20
meters
and
the
second
one
of
4
meters
these
represent
the
hardest
technical
climbing
difficulties
on
the
selvaggio
blu
this
section
includes
several
caves
and
woods
and
2
abseils
of
20
meters
each
bacu
su
feliau
which
is
a
big
hole
through
the
rocky
spur
that
overlooks
bacu
padente
was
originally
a
bivouac
shelter
on
selvaggio
blu
but
for
large
groups
it
is
recommended
to
descend
it
and
climb
up
towards
ololbizzi
a
charcoal
burners'
circle
in
the
upper
part
of
the
bacu
the
last
day
starts
from
bacu
su
feliau
and
ends
7 km
later
in
cala
sisine(40°10'45"n
9°38'1"e)
it
offers
a
large
variety
of
scenery
the
highest
altitude
reached
during
this
stage
is
480
m
the
estimated
travel
time
is
65
hours
and
the
technical
difficulty
is
ranked
iv
on
the
uiaa
scale
this
makes
this
stage
the
second
most
difficult
of
the
selvaggio
blu
at
the
start
of
this
stage
it
is
necessary
to
climb
a
juniper
trunk
with
giant
moss-covered
oaks
the
route
then
traverses
a
gully
and
is
then
marked
with
blue
waymarks
painted
on
rocks
these
mark
the
way
to
exit
the
woods
utilizing
ancient
mule
tracks
and
gives
a
panoramic
view
of
the
sea
before
descending
there
is
the
possibility
to
climb
a
cliff
named
"rottura
delle
altezze"
(which
means
"breaking
the
heights")
from
which
walkers
can
look
at
the
sea
200
m
below
one
of
the
paths
leads
to
the
ovule
piddi
meaning
mandragora
or
mandrake
in
sards;
this
poisonous
plant
grows
all
over
the
supramonte
helped
by
the
locals
verin
and
cicalò
built
a
network
of
mule
tracks
that
with
a
series
of
bends
pass
through
the
most
impervious
gullies
and
lead
to
the
sea
which
can
be
seen
only
in
cala
sisine;
here
there
are
docks
built
to
transport
charcoal
and
woods
from
cala
sisine
it
is
possible
to
return
to
the
starting
point
by
the
sea
with
an
inflatable
boat
or
by
land
with
a
suv
there
are
several
different
versions
of
the
selvaggio
blu:
there
are
other
variations
of
the
selvaggio
blu
as
daily
excursions:
the
guides
that
conceived
the
trek
are:
the
official
guides
of
the
trek
are:
in
the
first
stage
of
selvaggio
blu
oleanders
find
their
ideal
habitat
and
a
limestone
plateau
characterise
all
the
area
true
natural
monuments
as
holm
oaks
can
be
easily
found
the
path
is
coloured
by
the
bushy
euphorbias
(euphorbia
dendroides)
snakes
are
integral
parts
of
the
fauna
as
for
example
biacco
(coluber
viridiflavus)
in
the
second
stage
the
cistus
(rockrose)
can
be
found
in
numerous
pink
and
white
varieties
ferula
is
a
plant
belonging
to
the
apiaceae
family
that
are
in
the
third
stage
of
selvaggio
blu
during
the
fourth
part
of
the
path
the
possibility
to
find
shepherds
is
very
high
because
of
the
great
number
of
ovili
goats
are
basic
animals
of
the
fauna
sardinian
mouflon
can
also
be
seen
many
books
and
guides
about
selvaggio
blu
these
include:
selvaggio
blu
is
not
the
only
trekking
route
that
you
can
find
in
sardinia
there
are
4
more
trekking
routes
called:
