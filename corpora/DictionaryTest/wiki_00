<doc id="17062218" url="https://en.wikipedia.org/wiki?curid=17062218" title="Product teardown">
Product teardown

A product teardown, or simply teardown, is the act of disassembling a product, such that it helps to identify its component parts, chip & system functionality, and component costing information. For products having 'secret' technology, such as the Mikoyan-Gurevich MiG-25, the process may be secret. For others, including consumer electronics, the results are typically disseminated through photographs and component lists so that others can make use of the information without having to disassemble the product themselves. This information is important to designers of semiconductors, displays, batteries, packaging companies, integrated design firms, and semiconductor fabs, and the systems they operate within.

This information can be of interest to hobbyists, but can also be used commercially by the technical community to find out, for example, what semiconductor components are being utilized in consumer electronic products, such as the Wii video game console or Apple's iPhone. Such knowledge can aid understanding of how the product works, including innovative design features, and can facilitate estimating the bill of materials (BOM). The financial community therefore has an interest in teardowns, as knowing how a company's products are built can help guide a stock valuation. Manufacturers are often not allowed to announce what components are present in a product due to non-disclosure agreements (NDA). Teardowns can also play a part in evidence of use in court and litigation proceedings where a companies parts may have been used without their permission, counterfeited, or to show where intellectual property or patents might be infringed by another firms part or system.

Identifying semiconductor components in systems has become more difficult over the past years. The most notable change started with Apple's 8GB iPod nano, which were repackaged with Apple branding. This makes it more difficult to identify the actual device manufacturer and function of the component without performing a 'decap' – removing the outer packaging to analyze the die within it. Typically there are markings on the die inside the package that can lead experienced engineers to see who actually created the device and what functionality it performs in the system.



</doc>
<doc id="13549505" url="https://en.wikipedia.org/wiki?curid=13549505" title="Technology fusion">
Technology fusion

Technology fusion involves a transformation of core technologies through a combination process facilitated by technological advances such as the phone and the Internet, which ensure that labs are no longer isolated. This results in profitable advances that can be made cheaply by combining knowledge from different fields, companies, industries, and geographies. The technological fusion is distinguished from the so-called breakthrough approach, which is the linear technological development that replaces an older generation of technology through its focus on combining existing technologies into hybrid products that can revolutionize markets. 

The fusion of technologies goes beyond mere combination. Fusion is more than complementarism, because it creates a new market and new growth opportunities for each participant in the innovation. It blends incremental improvements from several (often previously separate) fields to create a product.

An example is the fusion of mechanical and electronic engineering to create mechatronics. There is also the case of fusing chemical and electronics technology to produce the Liquid Crystal display (LCD) technology.



</doc>
<doc id="74007" url="https://en.wikipedia.org/wiki?curid=74007" title="Technology assessment">
Technology assessment

Technology assessment (TA, German: , French: ) is a scientific, interactive, and communicative process that aims to contribute to the formation of public and political opinion on societal aspects of science and technology.

TA is the study and evaluation of new technologies. It is based on the conviction that new developments within, and discoveries by, the scientific community are relevant for the world at large rather than just for the scientific experts themselves, and that technological progress can never be free of ethical implications. Also, technology assessment recognizes the fact that scientists normally are not trained ethicists themselves and accordingly ought to be very careful when passing ethical judgement on their own, or their colleagues, new findings, projects, or work in progress.

Technology assessment assumes a global perspective and is future-oriented, not anti-technological. TA considers its task as an interdisciplinary approach to solving already existing problems and preventing potential damage caused by the uncritical application and the commercialization of new technologies.

Therefore, any results of technology assessment studies must be published, and particular consideration must be given to communication with political decision-makers.

An important problem concerning technology assessment is the so-called Collingridge dilemma: on the one hand, impacts of new technologies cannot be easily predicted until the technology is extensively developed and widely used; on the other hand, control or change of a technology is difficult as soon as it is widely used.

Technology assessments, which are a form of cost–benefit analysis, are difficult if not impossible to carry out in an objective manner since subjective decisions and value judgments have to be made regarding a number of complex issues such as (a) the boundaries of the analysis (i.e., what costs are internalized and externalized), (b) the selection of appropriate indicators of potential positive and negative consequences of the new technology, (c) the monetization of non-market values, and (d) a wide range of ethical perspectives. Consequently, most technology assessments are neither objective nor value-neutral exercises but instead are greatly influenced and biased by the values of the most powerful stakeholders, which are in many cases the developers and proponents (i.e., corporations and governments) of new technologies under consideration. In the most extreme view, as expressed by Ian Barbour in '’Technology, Environment, and Human Values'’, technology assessment is "a one-sided apology for contemporary technology by people with a stake in its continuation."

Some of the major fields of TA are: information technology, hydrogen technologies, nuclear technology, molecular nanotechnology, pharmacology, organ transplants, gene technology, artificial intelligence, the Internet and many more. Health technology assessment is related, but profoundly different, despite the similarity in the name.

The following types of concepts of TA are those that are most visible and practiced. There are, however, a number of further TA forms that are only proposed as concepts in the literature or are the label used by a particular TA institution.

Many TA institutions are members of the European Parliamentary Technology Assessment (EPTA) network, some are working for the STOA panel of the European Parliament and formed the European Technology Assessment Group (ETAG).




</doc>
<doc id="29816" url="https://en.wikipedia.org/wiki?curid=29816" title="Technology">
Technology

Technology ("science of craft", from Greek , "techne", "art, skill, cunning of hand"; and , "-logia") is the collection of techniques, skills, methods, and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation. Technology can be the knowledge of techniques, processes, and the like, or it can be embedded in machines to allow for operation without detailed knowledge of their workings. Systems (e. g. machines) applying technology by taking an input, changing it according to the system's use, and then producing an outcome are referred to as technology systems or technological systems.

The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times, including the printing press, the telephone, and the Internet, have lessened physical barriers to communication and allowed humans to interact freely on a global scale.

Technology has many effects. It has helped develop more advanced economies (including today's global economy) and has allowed the rise of a leisure class. Many technological processes produce unwanted by-products known as pollution and deplete natural resources to the detriment of Earth's environment. Innovations have always influenced the values of a society and raised new questions of the ethics of technology. Examples include the rise of the notion of efficiency in terms of human productivity, and the challenges of bioethics.

Philosophical debates have arisen over the use of technology, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticize the pervasiveness of technology, arguing that it harms the environment and alienates people; proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition.

The use of the term "technology" has changed significantly over the last 200 years. Before the 20th century, the term was uncommon in English, and it was used either to refer to the description or study of the useful arts or to allude to technical education, as in the Massachusetts Institute of Technology (chartered in 1861).

The term "technology" rose to prominence in the 20th century in connection with the Second Industrial Revolution. The term's meanings changed in the early 20th century when American social scientists, beginning with Thorstein Veblen, translated ideas from the German concept of "" into "technology." In German and other European languages, a distinction exists between "technik" and "technologie" that is absent in English, which usually translates both terms as "technology." By the 1930s, "technology" referred not only to the study of the industrial arts but to the industrial arts themselves.

In 1937, the American sociologist Read Bain wrote that "technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them." Bain's definition remains common among scholars today, especially social scientists. Scientists and engineers usually prefer to define technology as applied science, rather than as the things that people make and use. More recently, scholars have borrowed from European philosophers of "technique" to extend the meaning of technology to various forms of instrumental reason, as in Foucault's work on technologies of the self ("techniques de soi").

Dictionaries and scholars have offered a variety of definitions. The "Merriam-Webster Learner's Dictionary" offers a definition of the term: "the use of science in industry, engineering, etc., to invent useful things or to solve problems" and "a machine, piece of equipment, method, etc., that is created by technology." Ursula Franklin, in her 1989 "Real World of Technology" lecture, gave another definition of the concept; it is "practice, the way we do things around here." The term is often used to imply a specific field of technology, or to refer to high technology or just consumer electronics, rather than technology as a whole. Bernard Stiegler, in "Technics and Time, 1", defines technology in two ways: as "the pursuit of life by means other than life," and as "organized inorganic matter."

Technology can be most broadly defined as the entities, both material and immaterial, created by the application of mental and physical effort in order to achieve some value. In this usage, technology refers to tools and machines that may be used to solve real-world problems. It is a far-reaching term that may include simple tools, such as a crowbar or wooden spoon, or more complex machines, such as a space station or particle accelerator. Tools and machines need not be material; virtual technology, such as computer software and business methods, fall under this definition of technology. W. Brian Arthur defines technology in a similarly broad way as "a means to fulfill a human purpose."

The word "technology" can also be used to refer to a collection of techniques. In this context, it is the current state of humanity's knowledge of how to combine resources to produce desired products, to solve problems, fulfill needs, or satisfy wants; it includes technical methods, skills, processes, techniques, tools and raw materials. When combined with another term, such as "medical technology" or "space technology," it refers to the state of the respective field's knowledge and tools. "State-of-the-art technology" refers to the high technology available to humanity in any field.
Technology can be viewed as an activity that forms or changes culture. Additionally, technology is the application of math, science, and the arts for the benefit of life as it is known. A modern example is the rise of communication technology, which has lessened barriers to human interaction and as a result has helped spawn new subcultures; the rise of cyberculture has at its basis the development of the Internet and the computer. Not all technology enhances culture in a creative way; technology can also help facilitate political oppression and war via tools such as guns. As a cultural activity, technology predates both science and engineering, each of which formalize some aspects of technological endeavor.

The distinction between science, engineering, and technology is not always clear. Science is systematic knowledge of the physical or material world gained through observation and experimentation. Technologies are not usually exclusively products of science, because they have to satisfy requirements such as utility, usability, and safety.

Engineering is the goal-oriented process of designing and making tools and systems to exploit natural phenomena for practical human means, often (but not always) using results and techniques from science. The development of technology may draw upon many fields of knowledge, including scientific, engineering, mathematical, linguistic, and historical knowledge, to achieve some practical result.

Technology is often a consequence of science and engineering, although technology as a human activity precedes the two fields. For example, science might study the flow of electrons in electrical conductors by using already-existing tools and knowledge. This new-found knowledge may then be used by engineers to create new tools and machines such as semiconductors, computers, and other forms of advanced technology. In this sense, scientists and engineers may both be considered technologists; the three fields are often considered as one for the purposes of research and reference.

The exact relations between science and technology in particular have been debated by scientists, historians, and policymakers in the late 20th century, in part because the debate can inform the funding of basic and applied science. In the immediate wake of World War II, for example, it was widely considered in the United States that technology was simply "applied science" and that to fund basic science was to reap technological results in due time. An articulation of this philosophy could be found explicitly in Vannevar Bush's treatise on postwar science policy, "Science – The Endless Frontier": "New products, new industries, and more jobs require continuous additions to knowledge of the laws of nature ... This essential new knowledge can be obtained only through basic scientific research." In the late-1960s, however, this view came under direct attack, leading towards initiatives to fund science for specific tasks (initiatives resisted by the scientific community). The issue remains contentious, though most analysts resist the model that technology simply is a result of scientific research.

The use of tools by early humans was partly a process of discovery and of evolution. Early humans evolved from a species of foraging hominids which were already bipedal, with a brain mass approximately one third of modern humans. Tool use remained relatively unchanged for most of early human history. Approximately 50,000 years ago, the use of tools and complex set of behaviors emerged, believed by many archaeologists to be connected to the emergence of fully modern language.

Hominids started using primitive stone tools millions of years ago. The earliest stone tools were little more than a fractured rock, but approximately 75,000 years ago, pressure flaking provided a way to make much finer work.

The discovery and utilization of fire, a simple energy source with many profound uses, was a turning point in the technological evolution of humankind. The exact date of its discovery is not known; evidence of burnt animal bones at the Cradle of Humankind suggests that the domestication of fire occurred before 1 Ma; scholarly consensus indicates that "Homo erectus" had controlled fire by between 500 and 400 ka. Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.

Other technological advances made during the Paleolithic era were clothing and shelter; the adoption of both technologies cannot be dated exactly, but they were a key to humanity's progress. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 ka, humans were constructing temporary wood huts. Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate
out of Africa by 200 ka and into other continents such as Eurasia.

Human's technological ascent began in earnest in what is known as the Neolithic Period ("New Stone Age"). The invention of polished stone axes was a major advance that allowed forest clearance on a large scale to create farms. This use of polished stone axes increased greatly in the Neolithic, but were originally used in the preceding Mesolithic in some areas such as Ireland. Agriculture fed larger populations, and the transition to sedentism allowed simultaneously raising more children, as infants no longer needed to be carried, as nomadic ones must. Additionally, children could contribute labor to the raising of crops more readily than they could to the hunter-gatherer economy.

With this increase in population and availability of labor came an increase in labor specialization. What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war amongst adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.

Continuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge of gold, copper, silver, and lead native metals found in relatively pure form in nature. The advantages of copper tools over stone, bone, and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 ka). Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4000 BCE). The first uses of iron alloys such as steel dates to around 1800 BCE.

Meanwhile, humans were learning to harness other forms of energy. The earliest known use of wind power is the sailing ship; the earliest record of a ship under sail is that of a Nile boat dating to the 8th millennium BCE. From prehistoric times, Egyptians probably used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and "catch" basins. The ancient Sumerians in Mesopotamia used a complex system of canals and levees to divert water from the Tigris and Euphrates rivers for irrigation.

According to archaeologists, the wheel was invented around 4000 BCE probably independently and nearly simultaneously in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture) and Central Europe. Estimates on when this may have occurred range from 5500 to 3000 BCE with most experts putting it closer to 4000 BCE. The oldest artifacts with drawings depicting wheeled carts date from about 3500 BCE; however, the wheel may have been in use for millennia before these drawings were made. More recently, the oldest-known wooden wheel in the world was found in the Ljubljana marshes of Slovenia.

The invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. The ancient Sumerians used the potter's wheel and may have invented it. A stone pottery wheel found in the city-state of Ur dates to around 3429 BCE, and even older fragments of wheel-thrown pottery have been found in the same area. Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois and were first used in Mesopotamia and Iran in around 3000 BCE.

The oldest known constructed roadways are the stone-paved streets of the city-state of Ur, dating to circa 4000 BCE and timber roads leading through the swamps of Glastonbury, England, dating to around the same time period. The first long-distance road, which came into use around 3500 BCE, spanned 1,500 miles from the Persian Gulf to the Mediterranean Sea, but was not paved and was only partially maintained. In around 2000 BCE, the Minoans on the Greek island of Crete built a fifty-kilometer (thirty-mile) road leading from the palace of Gortyn on the south side of the island, through the mountains, to the palace of Knossos on the north side of the island. Unlike the earlier road, the Minoan road was completely paved.

Ancient Minoan private homes had running water. A bathtub virtually identical to modern ones was unearthed at the Palace of Knossos. Several Minoan private homes also had toilets, which could be flushed by pouring water down the drain. The ancient Romans had many public flush toilets, which emptied into an extensive sewage system. The primary sewer in Rome was the Cloaca Maxima; construction began on it in the sixth century BCE and it is still in use today.

The ancient Romans also had a complex system of aqueducts, which were used to transport water across long distances. The first Roman aqueduct was built in 312 BCE. The eleventh and final ancient Roman aqueduct was built in 226 CE. Put together, the Roman aqueducts extended over 450 kilometers, but less than seventy kilometers of this was above ground and supported by arches.

Innovations continued through the Middle Ages with innovations such as silk, the horse collar and horseshoes in the first few hundred years after the fall of the Roman Empire. Medieval technology saw the use of simple machines (such as the lever, the screw, and the pulley) being combined to form more complicated tools, such as the wheelbarrow, windmills and clocks. The Renaissance brought forth many of these innovations, including the printing press (which facilitated the greater communication of knowledge), and technology became increasingly associated with science, beginning a cycle of mutual advancement. The advancements in technology in this era allowed a more steady supply of food, followed by the wider availability of consumer goods.
Starting in the United Kingdom in the 18th century, the Industrial Revolution was a period of great technological discovery, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, driven by the discovery of steam power. Technology took another step in a second industrial revolution with the harnessing of electricity to create such innovations as the electric motor, light bulb, and countless others. Scientific advancement and the discovery of new concepts later allowed for powered flight and advancements in medicine, chemistry, physics, and engineering. The rise in technology has led to skyscrapers and broad urban areas whose inhabitants rely on motors to transport them and their food supply. Communication was also greatly improved with the invention of the telegraph, telephone, radio and television. The late 19th and early 20th centuries saw a revolution in transportation with the invention of the airplane and automobile.
The 20th century brought a host of innovations. In physics, the discovery of nuclear fission has led to both nuclear weapons and nuclear power. Computers were also invented and later miniaturized utilizing transistors and integrated circuits. Information technology subsequently led to the creation of the Internet, which ushered in the current Information Age. Humans have also been able to explore space with satellites (later used for telecommunication) and in manned missions going all the way to the moon. In medicine, this era brought innovations such as open-heart surgery and later stem cell therapy along with new medications and treatments.

Complex manufacturing and construction techniques and organizations are needed to make and maintain these new technologies, and entire industries have arisen to support and develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have been created to support them, including engineering, medicine, and computer science, and other fields have been made more complex, such as construction, transportation, and architecture.

Generally, technicism is the belief in the utility of technology for improving human societies. Taken to an extreme, technicism "reflects a fundamental attitude which seeks to control reality, to resolve all problems with the use of scientific–technological methods and tools." In other words, human beings will someday be able to master all problems and possibly even control the future using technology. Some, such as Stephen V. Monsma, connect these ideas to the abdication of religion as a higher moral authority.

Optimistic assumptions are made by proponents of ideologies such as transhumanism and singularitarianism, which view technological development as generally having beneficial effects for the society and the human condition. In these ideologies, technological development is morally good.

Transhumanists generally believe that the point of technology is to overcome barriers, and that what we commonly refer to as the human condition is just another barrier to be surpassed.

Singularitarians believe in some sort of "accelerating change"; that the rate of technological progress accelerates as we obtain more technology, and that this will culminate in a "Singularity" after artificial general intelligence is invented in which progress is nearly infinite; hence the term. Estimates for the date of this Singularity vary, but prominent futurist Ray Kurzweil estimates the Singularity will occur in 2045.

Kurzweil is also known for his history of the universe in six epochs: (1) the physical/chemical epoch, (2) the life epoch, (3) the human/brain epoch, (4) the technology epoch, (5) the artificial intelligence epoch, and (6) the universal colonization epoch. Going from one epoch to the next is a Singularity in its own right, and a period of speeding up precedes it. Each epoch takes a shorter time, which means the whole history of the universe is one giant Singularity event.

Some critics see these ideologies as examples of scientism and techno-utopianism and fear the notion of human enhancement and technological singularity which they support. Some have described Karl Marx as a techno-optimist.

On the somewhat skeptical side are certain philosophers like Herbert Marcuse and John Zerzan, who believe that technological societies are inherently flawed. They suggest that the inevitable result of such a society is to become evermore technological at the cost of freedom and psychological health.

Many, such as the Luddites and prominent philosopher Martin Heidegger, hold serious, although not entirely, deterministic reservations about technology (see "The Question Concerning Technology"). According to Heidegger scholars Hubert Dreyfus and Charles Spinosa, "Heidegger does not oppose technology. He hopes to reveal the essence of technology in a way that 'in no way confines us to a stultified compulsion to push on blindly with technology or, what comes to the same thing, to rebel helplessly against it.' Indeed, he promises that 'when we once open ourselves expressly to the essence of technology, we find ourselves unexpectedly taken into a freeing claim.' What this entails is a more complex relationship to technology than either techno-optimists or techno-pessimists tend to allow."

Some of the most poignant criticisms of technology are found in what are now considered to be dystopian literary classics such as Aldous Huxley's "Brave New World", Anthony Burgess's "A Clockwork Orange", and George Orwell's "Nineteen Eighty-Four". In Goethe's "Faust", Faust selling his soul to the devil in return for power over the physical world is also often interpreted as a metaphor for the adoption of industrial technology. More recently, modern works of science fiction such as those by Philip K. Dick and William Gibson and films such as "Blade Runner" and "Ghost in the Shell" project highly ambivalent or cautionary attitudes toward technology's impact on human society and identity.

The late cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called "technopolies," societies that are dominated by the ideology of technological and scientific progress to the exclusion or harm of other cultural practices, values, and world-views.

Darin Barney has written about technology's impact on practices of citizenship and democratic culture, suggesting that technology can be construed as (1) an object of political debate, (2) a means or medium of discussion, and (3) a setting for democratic deliberation and citizenship. As a setting for democratic culture, Barney suggests that technology tends to make ethical questions, including the question of what a good life consists in, nearly impossible because they already give an answer to the question: a good life is one that includes the use of more and more technology.

Nikolas Kompridis has also written about the dangers of new technology, such as genetic engineering, nanotechnology, synthetic biology, and robotics. He warns that these technologies introduce unprecedented new challenges to human beings, including the possibility of the permanent alteration of our biological nature. These concerns are shared by other philosophers, scientists and public intellectuals who have written about similar issues (e.g. Francis Fukuyama, Jürgen Habermas, William Joy, and Michael Sandel).

Another prominent critic of technology is Hubert Dreyfus, who has published books such as "On the Internet" and "What Computers Still Can't Do".

A more infamous anti-technological treatise is "", written by the Unabomber Ted Kaczynski and printed in several major newspapers (and later books) as part of an effort to end his bombing campaign of the techno-industrial infrastructure. There are also subcultures that disapprove of some or most technology, such as self-identified off-gridders.

The notion of appropriate technology was developed in the 20th century by thinkers such as E.F. Schumacher and Jacques Ellul to describe situations where it was not desirable to use very new technologies or those that required access to some centralized infrastructure or parts or skills imported from elsewhere. The ecovillage movement emerged in part due to this concern.

"This section mainly focuses on American concerns even if it can reasonably be generalized to other Western countries. "

In his article, Jared Bernstein, a Senior Fellow at the Center on Budget and Policy Priorities, questions the widespread idea that automation, and more broadly, technological advances, have mainly contributed to this growing labor market problem.
His thesis appears to be a third way between optimism and skepticism. Essentially, he stands for a neutral approach of the linkage between technology and American issues concerning unemployment and declining wages.

He uses two main arguments to defend his point.
First, because of recent technological advances, an increasing number of workers are losing their jobs. Yet, scientific evidence fails to clearly demonstrate that technology has displaced so many workers that it has created more problems than it has solved. Indeed, automation threatens repetitive jobs but higher-end jobs are still necessary because they complement technology and manual jobs that "requires flexibility judgment and common sense" remain hard to replace with machines. Second, studies have not shown clear links between recent technology advances and the wage trends of the last decades.

Therefore, according to Bernstein, instead of focusing on technology and its hypothetical influences on current American increasing unemployment and declining wages, one needs to worry more about "bad policy that fails to offset the imbalances in demand, trade, income, and opportunity."

For people who use both the Internet and mobile devices in excessive quantities it is likely for them to experience fatigue and over exhaustion as a result of disruptions in their sleeping patterns. Continuous studies have shown that increased BMI and weight gain are associated with people who spend long hours online and not exercising frequently. Heavy Internet use is also displayed in the school lower grades of those who use it in excessive amounts. It has also been noted that the use of mobile phones whilst driving has increased the occurrence of road accidents — particularly amongst teen drivers. Statistically, teens reportedly have fourfold the number of road traffic incidents as those who are 20 years or older, and a very high percentage of adolescents write (81%) and read (92%) texts while driving. In this context, mass media and technology have a negative impact on people, on both their mental and physical health.

Thomas P. Hughes stated that because technology has been considered as a key way to solve problems, we need to be aware of its complex and varied characters to use it more efficiently. What is the difference between a wheel or a compass and cooking machines such as an oven or a gas stove? Can we consider all of them, only a part of them, or none of them as technologies?

Technology is often considered too narrowly; according to Hughes, "Technology is a creative process involving human ingenuity". This definition's emphasis on creativity avoids unbounded definitions that may mistakenly include cooking "technologies," but it also highlights the prominent role of humans and therefore their responsibilities for the use of complex technological systems.

Yet, because technology is everywhere and has dramatically changed landscapes and societies, Hughes argues that engineers, scientists, and managers have often believed that they can use technology to shape the world as they want. They have often supposed that technology is easily controllable and this assumption has to be thoroughly questioned. For instance, Evgeny Morozov particularly challenges two concepts: "Internet-centrism" and "solutionism." Internet-centrism refers to the idea that our society is convinced that the Internet is one of the most stable and coherent forces. Solutionism is the ideology that every social issue can be solved thanks to technology and especially thanks to the internet. In fact, technology intrinsically contains uncertainties and limitations. According to Alexis Madrigal's review of Morozov's theory, to ignore it will lead to "unexpected consequences that could eventually cause more damage than the problems they seek to address." Benjamin R. Cohen and Gwen Ottinger also discussed the multivalent effects of technology.

Therefore, recognition of the limitations of technology, and more broadly, scientific knowledge, is needed – especially in cases dealing with environmental justice and health issues. Ottinger continues this reasoning and argues that the ongoing recognition of the limitations of scientific knowledge goes hand in hand with scientists and engineers’ new comprehension of their role. Such an approach of technology and science "[require] technical professionals to conceive of their roles in the process differently. [They have to consider themselves as] collaborators in research and problem solving rather than simply providers of information and technical solutions."

Technology is properly defined as any application of science to accomplish a function. The science can be leading edge or well established and the function can have high visibility or be significantly more mundane, but it is all technology, and its exploitation is the foundation of all competitive advantage.

Technology-based planning is what was used to build the US industrial giants before WWII (e.g., Dow, DuPont, GM) and it is what was used to transform the US into a superpower. It was not economic-based planning.

The use of basic technology is also a feature of other animal species apart from humans. These include primates such as chimpanzees, some dolphin communities, and crows. Considering a more generic perspective of technology as ethology of active environmental conditioning and control, we can also refer to animal examples such as beavers and their dams, or bees and their honeycombs.

The ability to make and use tools was once considered a defining characteristic of the genus Homo. However, the discovery of tool construction among chimpanzees and related primates has discarded the notion of the use of technology as unique to humans. For example, researchers have observed wild chimpanzees utilising tools for foraging: some of the tools used include leaf sponges, termite fishing probes, pestles and levers. West African chimpanzees also use stone hammers and anvils for cracking nuts, as do capuchin monkeys of Boa Vista, Brazil.

Theories of technology often attempt to predict the future of technology based on the high technology and science of the time. As with all predictions of the future, however, technology's is uncertain.

In 2005, futurist Ray Kurzweil predicted that the future of technology would mainly consist of an overlapping "GNR Revolution" of genetics, nanotechnology and robotics, with robotics being the most important of the three.



</doc>
<doc id="6504692" url="https://en.wikipedia.org/wiki?curid=6504692" title="Outline of technology">
Outline of technology

The following outline is provided as an overview of and topical guide to technology:

Technology – collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technologies. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.




History of technology






Potential technology of the future includes:

Hypothetical technology – 

Philosophy of technology – 











Fictional technology – 









</doc>
<doc id="58306941" url="https://en.wikipedia.org/wiki?curid=58306941" title="Technology readiness level">
Technology readiness level

Technology readiness levels (TRL) are a method of estimating technology maturity of Critical Technology Elements (CTE) of a program during the acquisition process. They are determined during a Technology Readiness Assessment (TRA) that examines program concepts, technology requirements, and demonstrated technology capabilities. TRL are based on a scale from 1 to 9 with 9 being the most mature technology. The use of TRLs enables consistent, uniform discussions of technical maturity across different types of technology. TRL has been in widespread use at NASA since the 1980s where it was originally invented. In 1999 the US Department of Defense was advised by GAO to use the scale for procurement which it did from the early 2000s. By 2008 the scale was also in use at the European Space Agency (ESA) as it is evidenced by their handbook. The European Commission advised EU-funded research and innovation projects to adopt the scale in 2010 which they did from 2014 in its Horizon 2020 program. In 2013 TRL was further canonized by the ISO 16290:2013 standard. A comprehensive approach and discussion about TRLs has been published by the European Association of Research and Technology Organisations (EARTO). Extensive criticism of the adoption of TRL scale by the European Union was published in The Innovation Journal, in that "concreteness and sophistication of the TRL scale gradually diminished as its usage spread outside its original context (space programs)".

Technology Readiness Levels were originally conceived at NASA in 1974 and formally defined in 1989. The original definition included seven levels, but in the 1990s NASA adopted the current nine-level scale that subsequently gained widespread acceptance.

Original NASA TRL Definitions (1989)

The TRL methodology was originated by Stan Sadin at NASA Headquarters in 1974. At that time, Ray Chase was the JPL Propulsion Division representative on the Jupiter Orbiter design team. At the suggestion of Stan Sadin, Mr Chase used this methodology to assess the technology readiness of the proposed JPL Jupiter Orbiter spacecraft design. Later Mr Chase spent a year at NASA Headquarters helping Mr Sadin institutionalize the TRL methodology. Mr Chase joined ANSER in 1978, where he used the TRL methodology to evaluate the technology readiness of proposed Air Force development programs. He published several articles during the 1980s and 90s on reusable launch vehicles utilizing the TRL methodology. These documented an expanded version of the methodology that included design tools, test facilities, and manufacturing readiness on the Air Force Have Not program. The Have Not program manager, Greg Jenkins, and Ray Chase published the expanded version of the TRL methodology, which included design and manufacturing. Leon McKinney and Mr Chase used the expanded version to assess the technology readiness of the ANSER team's Highly Reusable Space Transportation ("HRST") concept. ANSER also created an adapted version of the TRL methodology for proposed Homeland Security Agency programs.

The United States Air Force adopted the use of Technology Readiness Levels in the 1990s.

In 1995, John C. Mankins, NASA, wrote a paper that discussed NASA's use of TRL, extended the scale, and proposed expanded descriptions for each TRL. In 1999, the United States General Accounting Office produced an influential report that examined the differences in technology transition between the DOD and private industry. It concluded that the DOD takes greater risks and attempts to transition emerging technologies at lesser degrees of maturity than does private industry. The GAO concluded that use of immature technology increased overall program risk. The GAO recommended that the DOD make wider use of Technology Readiness Levels as a means of assessing technology maturity prior to transition. In 2001, the Deputy Under Secretary of Defense for Science and Technology issued a memorandum that endorsed use of TRLs in new major programs. Guidance for assessing technology maturity was incorporated into the "Defense Acquisition Guidebook". Subsequently, the DOD developed detailed guidance for using TRLs in the 2003 DOD Technology Readiness Assessment Deskbook.

The European Space Agency adopted the TRL scale in the mid-2000s. Its handbook closely follows the NASA definition of TRLs. The universal usage of TRL in EU policy was proposed in the final report of the first High Level Expert Group on Key Enabling Technologies, and it was indeed implemented in the subsequent EU framework program, called H2020, running from 2013 to 2020. This means not only space and weapons programs, but everything from nanotechnology to informatics and communication technology.

A "Technology Readiness Level Calculator" was developed by the United States Air Force. This tool is a standard set of questions implemented in Microsoft Excel that produces a graphical display of the TRLs achieved. This tool is intended to provide a snapshot of technology maturity at a given point in time.

The "Technology Program Management Model" was developed by the United States Army. The TPMM is a TRL-gated high-fidelity activity model that provides a flexible management tool to assist Technology Managers in planning, managing, and assessing their technologies for successful technology transition. The model provides a core set of activities including systems engineering and program management tasks that are tailored to the technology development and management goals. This approach is comprehensive, yet it consolidates the complex activities that are relevant to the development and transition of a specific technology program into one integrated model.

The primary purpose of using technology readiness levels is to help management in making decisions concerning the development and transitioning of technology. It should be viewed as one of several tools that are needed to manage the progress of research and development activity within an organization.

Among the advantages of TRLs:


Some of the characteristics of TRLs that limit their utility:


Current TRL models tend to disregard negative and obsolescence factors. There have been suggestions made for incorporating such factors into assessments.

For complex technologies that incorporate various development stages, a more detailed scheme called the Technology Readiness Pathway Matrix has been developed going from basic units to applications in society. This tool aims to show that a readiness level of a technology is based on a less linear process but on a more complex pathway through its application in society.. 





</doc>
<doc id="59091880" url="https://en.wikipedia.org/wiki?curid=59091880" title="Multimodal anthropology">
Multimodal anthropology

Multimodal anthropology is an emerging subfield of social cultural anthropology that encompasses anthropological research and knowledge production across multiple traditional and new media platforms and practices including film, video, photography, theatre, design, podcast, mobile apps, interactive games, web-based social networking, immersive 360 video and augmented reality. As characterized in American Anthropologist"," multimodal anthropology is an "anthropology that works across multiple media, but one that also engages in public anthropology and collaborative anthropology through a field of differentially linked media platforms" (Collins, Durington & Gill). A multimodal approach also encourages anthropologist to reconsider the ways in which they conduct their research, to pay close attention to the role various media technologies and digital devices plays in the lives of their interlocutors, and how they these technologies redefine what fieldwork looks like.

Multimodal anthropology is not a new concept. It has been a fundamental part of anthropological research and fieldwork from the early days of the disciple. Anthropologists have been experimenting with different forms media technologies throughout the twentieth century whenever confronted with the limitation of text-based ethnography. Multimodal is a term that has readily been used since the 1970s in varied disciplines as psychotherapy, phonetics, genetics, literature and medicine to characterize different approaches to carrying out scientific research that involves to a certain degree, thinking outside of the box. In the early 1990s, semioticians used the terms to discuss different forms of communication across different media, eventually including digital media.

Technological advances in the later part of the twentieth century, the accessibility to photography, film cameras and audio recorders led to the emergence of visual anthropology as a sub discipline dedicated to the study and production of ethnographic photography, film and media. Building in this legacy, multimodal anthropology seeks to expand the boundaries of visual anthropology to incorporate emerging technologies of twenty-first century including mobile networking, social media, geo-mapping, virtual reality, podcasting, interactive design, along with other traditional forms of learning and knowledge production like art and drawing that were often sidelined within visual anthropology, such as interactive gaming, theatre, performance, graphic novels, ethnofiction and experimental ethnography. As Samuel Collins, Matthew Durington and Harjant Gill note in their introductory essay on title "Multimodality: An Invitation," published in American Anthropologist, "multimodal anthropologies does not attempt – or desire – to supplant visual anthropology. Rather it seeks to include traditional forms of visual anthropology while simultaneously broadening the purview of the discipline to engage in variety of media forms that exist today."



</doc>
<doc id="58592450" url="https://en.wikipedia.org/wiki?curid=58592450" title="Enchroma">
Enchroma

Enchroma lenses are glasses designed to improve and modify some aspects of color vision deficiency for color blind people. The glasses were invented by Dr. Donald McPherson in 2002. Wearing the glasses results in subtle differences when color blind people look longer and more carefully.

Glass scientist Dr. Donald McPherson invented Enchroma glasses by accident. He originally invented this type of lens to protect surgeons during laser operations. In 2002 at the Ultimate Frisbee tournament in Santa Cruz, California McPherson lent a pair to a friend who was color blind. His friend saw colors he had never seen before. McPherson started studying color blindness, and with Andrew Schmeder founded the company EnChroma Inc. in 2010 to sell glasses that compensate for color vision deficiency. Enchroma glasses target people with difficulties in distinguishing reds and greens. The first pair of commercial glasses were released in 2012.

The optical filter used in Enchroma lenses can improve or modify aspects of color vision deficiency (CVD). Enchroma lenses focus on the most common color vision deficiency which is caused by the red and green retinal cone cells that, when responding to light, coincide. The most common form of color blindness is known as deuteranomaly, it is genetic and an X-linked trait that affects up to one in every 12 men (8%) and one in 200 women (0.5%). To eliminate the overlapping of the wavelengths of light, there is an optical material called a 'multi-notch' filter, is capable of removing the exact wavelengths of light in the location where it overlaps, getting a simplified differentiation of colors. The glasses block specific wavelengths to create a clearer separation of different color signals so that they can be better calculated by the brain. The separation of signals allows most people with CVD to distinguish colors, but the glasses will have little to no effect on the 20% of color blind people who have severe color impairment.

The filters designed by the method have a spectral transmittance that can be essentially described as a multi-band filter, made up by a plurality of passbands interleaved with stopbands. This technology makes it possible to remove the overlapping of colours.

The use of glass in lens manufacturing has reduced due to the availability of new lens materials offering different characteristics. Lens research is aimed at finding the optimum materials for the most common vision deficiencies. Enchroma technology is one of the few materials to compenste for CVD. Trivex eyeglass lenses were first developed by PPG industries in the United States. The lenses can be progressive or photochromic. They are significant in allowing better vision sharpness and eye protection, and are also comfortable to wear since the weight of the material is minimized. The tinted plastic lens used to make enchroma glasses is coated in approximtely 100 layers of dielectric material. A coating can be applied on both sides of the lens to eliminate light reflection and to protect the lens itself for scratches. A predecessor of the Trivex lens with similar characteristics is made from polycarbonate.

Enchroma lenses are used in various types of glasses, including sunglasses, indoor glasses, infant, industrial safety and sports glasses.

Sunglasses were the first product to use Enchroma lenses. They can be used in various lighting conditions, even in bright indoor environments, and assist people affected by deuteranopia and protanopia. They protect those who are affected by color blindness from solar ultraviolet and blue light and do not let solar radiation with a wavelength shorter than 450 nanometers pass through. They block just one or two colors (usually green and blue), permitting people to see colors such as dull brown or yellow. This is due to a specific coat that increases the brightness of colors and provides protection from UV rays. 

Different models of sunglasses exist, according to the color deficiency the person suffers from.

This version of the glasses for people with CVD can be used indoors, but they work best in bright lighting and have applications for people who spend a lot of time facing a computer monitor. 
Students with color blindness can have a benefit from the glasses, because they can understand particular color-coded information better at their school or at university.

There are also particular glasses for athletic people, for children and for industrial safety use which are made of particular materials such as polycarbonate. This makes the glasses stronger and less breakable. These kind of lenses are able to correct severe vision deficiency but they do not need additional lens thickness to achieve this. Trivex eyeglass lenses offer the same characteristics as those that are made for athletic people or children and provide an alternative material to polycarbonate.

The glasses are available in single vision lenses, bifocals, and progressive lens. The sunglasses are also used by some senior citizens to help with sunlight sensitivity or due to medical conditions such as diabetes, glaucoma, macular degeneration, cataracts, multiple sclerosis, and eye cancer.

The Qassim University and the Pacific University conducted joint experiments.

The Qassim University experiment involved 25 males aged from 20 to 25 years. Two of those 25 were excluded because of a vision disease that may affect the CVD deficiency.

The Pacific University College of Optometry's experiment tested whether the glasses helped people with particular deficiencies improve the way they see colors. The study involved ten individuals with hereditary deficiencies (nine males and one female from age 19 to 52).

The Enchroma Cx-14 filters did not significantly influence the vision of CVD subjects but, for two of them, the error score was improved.

In 2018, researchers from the University of Granada studied Enchroma lenses and proved that they merely helped color blind people to see the same colors in a different way since the colored filter altered the way colors appeared in their eyes.

The research involved 200 volunteers, including 48 who were color blind people. Out of the 48, four were women and 44 were men, aged between 14 and 64. These participants were already aware of their own condition of CVD. The study concluded that, although an improvement in color vision has been accomplished since the colors are enhanced by the effect of the glasses, the technology could not be considered completely effective because the color vision it offers ws not identical to the non-colour blind individuals.

In order to evaluate the effectiveness of the glasses, the color vision of the participants was evaluated without the Enchroma glasses using the Ishihara test, Farnsworth-Munsell 100 hue test, and a color-naming test (with 21 colors from an X-Rite GretagMacbeth Chart). The tests were repeated under the same conditions while wearing Enchroma glasses. In order to measure the full effect of Enchroma lenses, an adaptation time of 30 minutes was given to the participants. Between the two sections of the tests, more than 2 weeks passed in order to reduce the likelihood of participants memorizing the color patterns.

The researchers claimed that the effect of using Enchroma glasses is similar to glasses where the use of color filters changes the user’s perception and increases the contrast among the colors, such as those used for shooting or hunting. The research showed that Enchroma glasses did not reveal any improvement in the Ishihara test and Farnsworth Munsell 100 hue test.

In other research on the effects of wearing Enchroma glasses on red-green CVD, R. Mastey recruited 27 males: eight deuteranomalous, ten deuteranopic and nine protanopic, while E. J. Patterson recruited fifteen males: seven deuteranopic, six deuteranomalous, one protanomalous and one protanopic. For the research, the researchers used the Color Assessment and Diagnosis (CAD) test that provides chromatic discrimination thresholds.

In 2015 Dr. Blake Porter of the University of Otago conducted an experiment with 406 participants, of whom 42 already used Enchroma glasses. The participants were asked questions about their color blindness, their personal experience with the lens and the effects of those on their life and on the way they saw the colors.

90% of the participants said they would recommend them to other people, and more than 50% of those people stated that enchroma glasses improved their life. 10% of the remaining participants reported that the glasses did not have any impact on their life. 40% of the participants identified green as the color range that changed the most.

In some subjects, improvements due to the technology do not happen immediately because the brain requires time to rewire and create new links, neuroplasticity. Neuroplasticity depends on the age of the individual. Younger people reported that they perceived little changes from the first time they wore glasses and a second wearing in which they have had time to adapt to the technology. Middle age people reported that the colors continued to change from the first time they wore the lens, and that color brightness and enhancement got better with time. Most participants related that the biggest effects of the glasses are not perceived initially but after a few days, confirming that the neuroplasticity takes time.

The experiment demonstrated that the glasses have a positive effect on the everyday life of those subject to color blindness. The glasses achieve better results with some colors than others. The biggest improvements are achieved with greens, followed by purples, pinks, and reds. Subtle differences emerged when users looked longer and more carefully.



</doc>
<doc id="59844022" url="https://en.wikipedia.org/wiki?curid=59844022" title="Shivani Siroya">
Shivani Siroya

Shivani Siroya is the founder and Chief Executive Officer of Tala, a smartphone lending app. She founded the app in 2011 to offer instant credit scores to people in underrepresented markets such as Kenya, Tanzania and the Philippines. The app also acts as a lender and has granted more than $225M in microloans as of 2018. In April 2018, the app secured $65 million in funding from Female Founders Fund, Lowercase Capital and Revolution Capital, among others. Its total funding is over $105 million. 

Prior to founding Tala, Siroya worked for the UN Population Fund. She also worked at Citi, UBS and Credit Suisse in microfinance jobs. 

Siroya has a M.P.H. in Health Economics and Policy from Columbia University and a B.A. from Wesleyan University in International Relations. 

Melinda Gates nominated Siroya in 2018 as a Wired Icon. 


</doc>
<doc id="26700" url="https://en.wikipedia.org/wiki?curid=26700" title="Science">
Science

Science (from the Latin word "scientia", meaning "knowledge") is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

The earliest roots of science can be traced to Ancient Egypt and Mesopotamia in around 3500 to 3000 BCE. Their contributions to mathematics, astronomy, and medicine entered and shaped Greek natural philosophy of classical antiquity, whereby formal attempts were made to explain events of the physical world based on natural causes. After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages but was preserved in the Muslim world during the Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived natural philosophy, which was later transformed by the Scientific Revolution that began in the 16th century as new ideas and discoveries departed from previous Greek conceptions and traditions. The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape.

Modern science is typically divided into three major branches that consist of the natural sciences (e.g., biology, chemistry, and physics), which study nature in the broadest sense; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies; and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which study abstract concepts. There is disagreement, however, on whether the formal sciences actually constitute a science as they do not rely on empirical evidence. Disciplines that use existing scientific knowledge for practical purposes, such as engineering and medicine, are described as applied sciences.

Science is based on research, which is commonly conducted in academic and research institutions as well as in government agencies and companies. The practical impact of scientific research has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the development of commercial products, armaments, health care, and environmental protection.

Science in a broad sense existed before the modern era and in many historical civilizations. Modern science is distinct in its approach and successful in its results, so it now defines what science is in the strictest sense of the term. Science in its original sense was a word for a type of knowledge, rather than a specialized word for the pursuit of such knowledge. In particular, it was the type of knowledge which people can communicate to each other and share. For example, knowledge about the working of natural things was gathered long before recorded history and led to the development of complex abstract thought. This is shown by the construction of complex calendars, techniques for making poisonous plants edible, public works at national scale, such as those which harnessed the floodplain of the Yangtse with reservoirs, dams, and dikes, and buildings such as the Pyramids. However, no consistent conscious distinction was made between knowledge of such things, which are true in every community, and other types of communal knowledge, such as mythologies and legal systems. Metallurgy was known in prehistory, and the Vinča culture was the earliest known producer of bronze-like alloys. It is thought that early experimentation with heating and mixing of substances over time developed into alchemy.

Neither the words nor the concepts "science" and "nature" were part of the conceptual landscape in the ancient near east. The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing; they also studied animal physiology, anatomy, and behavior for divinatory purposes and made extensive records of the movements of astronomical objects for their study of astrology. The Mesopotamians had intense interest in medicine and the earliest medical prescriptions appear in Sumerian during the Third Dynasty of Ur ( 2112 BCE – 2004 BCE). Nonetheless, the Mesopotamians seem to have had little interest in gathering information about the natural world for the mere sake of gathering information and mainly only studied scientific subjects which had obvious practical applications or immediate relevance to their religious system.

In the classical world, there is no real ancient analog of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time. Before the invention or discovery of the concept of "nature" (ancient Greek "phusis") by the Pre-Socratic philosophers, the same words tend to be used to describe the "natural" "way" in which a plant grows, and the "way" in which, for example, one tribe worships a particular god. For this reason, it is claimed these men were the first philosophers in the strict sense, and also the first people to clearly distinguish "nature" and "convention." Natural philosophy, the precursor of natural science, was thereby distinguished as the knowledge of nature and things which are true for every community, and the name of the specialized pursuit of such knowledge was "philosophy" – the realm of the first philosopher-physicists. They were mainly speculators or theorists, particularly interested in astronomy. In contrast, trying to use knowledge of nature to imitate nature (artifice or technology, Greek "technē") was seen by classical scientists as a more appropriate interest for lower class artisans.

The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural. The Pythagoreans developed a complex number philosophy and contributed significantly to the development of mathematical science. The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus. The Greek doctor Hippocrates established the tradition of systematic medical science and is known as "The Father of Medicine".

A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. This was a reaction to the Sophist emphasis on rhetoric. The Socratic method searches for general, commonly held truths that shape beliefs and scrutinizes them to determine their consistency with other beliefs. Socrates criticized the older type of study of physics as too purely speculative and lacking in self-criticism. Socrates was later, in the words of his "Apology", accused of corrupting the youth of Athens because he did "not believe in the gods the state believes in, but in other new spiritual beings". Socrates refuted these claims, but was sentenced to death.

Aristotle later created a systematic programme of teleological philosophy: Motion and change is described as the actualization of potentials already in things, according to what types of things they are. In his physics, the Sun goes around the Earth, and many things have it as part of their nature that they are for humans. Each thing has a formal cause, a final cause, and a role in a cosmic order with an unmoved mover. The Socratics also insisted that philosophy should be used to consider the practical question of the best way to live for a human being (a study Aristotle divided into ethics and political philosophy). Aristotle maintained that man knows a thing scientifically "when he possesses a conviction arrived at in a certain way, and when the first principles on which that conviction rests are known to him with certainty".

The Greek astronomer Aristarchus of Samos (310–230 BCE) was the first to propose a heliocentric model of the universe, with the Sun at the center and all the planets orbiting it. Aristarchus's model was widely rejected because it was believed to violate the laws of physics. The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus and has sometimes been credited as its inventor, although his proto-calculus lacked several defining features. Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopedia "Natural History", dealing with history, geography, medicine, astronomy, earth science, botany, and zoology.
Other scientists or proto-scientists in Antiquity were Theophrastus, Euclid, Herophilos, Hipparchus, Ptolemy, and Galen.

During late antiquity, in the Byzantine empire many Greek classical texts were preserved. Many Syriac translations were done by groups such as the Nestorians and Monophysites. They played a role when they translated Greek classical texts into Arabic under the Caliphate, during which many types of classical learning were preserved and in some cases improved upon. In addition, the neighboring Sassanid Empire established the medical Academy of Gondeshapur where Greek, Syriac and Persian physicians established the most important medical center of the ancient world during the 6th and 7th centuries.

Because of the collapse of the Western Roman Empire due to the Migration Period an intellectual decline took place in the western part of Europe in the 400s. In contrast, the Byzantine Empire resisted the attacks from the barbarians, and preserved and improved upon the learning. John Philoponus, a Byzantine scholar in the 500s, was the first scholar ever to question Aristotle's teaching of physics and to note its flaws. John Philoponus' criticism of Aristotelian principles of physics served as an inspiration to medieval scholars as well as to Galileo Galilei who ten centuries later, during the Scientific Revolution, extensively cited Philoponus in his works while making the case as to why Aristotelian physics was flawed.

During late antiquity and the early Middle Ages, the Aristotelian approach to inquiries on natural phenomena was used. Aristotle's four causes prescribed that four "why" questions should be answered in order to explain things scientifically. Some ancient knowledge was lost, or in some cases kept in obscurity, during the fall of the Western Roman Empire and periodic political struggles. However, the general fields of science (or "natural philosophy" as it was called) and much of the general knowledge from the ancient world remained preserved through the works of the early Latin encyclopedists like Isidore of Seville. However, Aristotle's original texts were eventually lost in Western Europe, and only one text by Plato was widely known, the "Timaeus", which was the only Platonic dialogue, and one of the few original works of classical natural philosophy, available to Latin readers in the early Middle Ages. Another original work that gained influence in this period was Ptolemy's "Almagest", which contains a geocentric description of the solar system.

In the Byzantine empire, many Greek classical texts were preserved. Many Syriac translations were done by groups such as the Nestorians and Monophysites. They played a role when they translated Greek classical texts into Arabic under the Caliphate, during which many types of classical learning were preserved and in some cases improved upon.

The House of Wisdom was established in Abbasid-era Baghdad, Iraq,
where the Islamic study of Aristotelianism flourished. Al-Kindi (801–873) was the first of the Muslim Peripatetic philosophers, and is known for his efforts to introduce Greek and Hellenistic philosophy to the Arab world. The Islamic Golden Age flourished from this time until the Mongol invasions of the 13th century. Ibn al-Haytham (Alhazen), as well as his predecessor Ibn Sahl, was familiar with Ptolemy's "Optics", and used experiments as a means to gain knowledge. Furthermore, doctors and alchemists such as the Persians Avicenna and Al-Razi also greatly developed the science of Medicine with the former writing the Canon of Medicine, a medical encyclopedia used until the 18th century and the latter discovering multiple compounds like alcohol. Avicenna's canon is considered to be one of the most important publications in medicine and they both contributed significantly to the practice of experimental medicine, using clinical trials and experiments to back their claims.

In Classical antiquity, Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in Middle Ages it changed: medical teachers and students at Bologna began to open human bodies, and Mondino de Luzzi (c. 1275–1326) produced the ﬁrst known anatomy textbook based on human dissection.

By the eleventh century most of Europe had become Christian; stronger monarchies emerged; borders were restored; technological developments and agricultural innovations were made which increased the food supply and population. In addition, classical Greek texts started to be translated from Arabic and Greek into Latin, giving a higher level of scientific discussion in Western Europe.

By 1088, the first university in Europe (the University of Bologna) had emerged from its clerical beginnings. Demand for Latin translations grew (for example, from the Toledo School of Translators); western Europeans began collecting texts written not only in Latin, but also Latin translations from Greek, Arabic, and Hebrew. Manuscript copies of Alhazen's "Book of Optics" also propagated across Europe before 1240, as evidenced by its incorporation into Vitello's "Perspectiva". Avicenna's Canon was translated into Latin. In particular, the texts of Aristotle, Ptolemy, and Euclid, preserved in the Houses of Wisdom and also in the Byzantine Empire, were sought amongst Catholic scholars. The influx of ancient texts caused the Renaissance of the 12th century and the flourishing of a synthesis of Catholicism and Aristotelianism known as Scholasticism in western Europe, which became a new geographic center of science. An "experiment" in this period would be understood as a careful process of observing, describing, and classifying. One prominent scientist in this era was Roger Bacon. Scholasticism had a strong focus on revelation and dialectic reasoning, and gradually fell out of favour over the next centuries, as alchemy's focus on experiments that include direct observation and meticulous documentation slowly increased in importance.

Alhazen disproved Ptolemy's theory of vision, but did not make any corresponding changes to Aristotle's metaphysics. The scientific revolution ran concurrently to a process where elements of Aristotle's metaphysics such as ethics, teleology and formal causality slowly fell out of favour. Scholars slowly came to realize that the universe itself might well be devoid of both purpose and ethical imperatives. The development from a physics infused with goals, ethics, and spirit, toward a physics where these elements do not play an integral role, took centuries. This development was enhanced by the Condemnations of 1277, where Aristotle's books were banned by the Catholic church. This allowed the theoretical possibility of vacuum and motion in a vacuum. A direct result was the emergence of the science of dynamics.

New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. Before what we now know as the Renaissance started, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle. A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.

In the sixteenth century, Copernicus formulated a heliocentric model of the solar system unlike the geocentric model of Ptolemy's "Almagest". This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.

Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light. Kepler modelled the eye as a water-filled glass sphere with an aperture in front of it to model the entrance pupil. He found that all the light from a single point of the scene was imaged at a single point at the back of the glass sphere. The optical chain ends on the retina at the back of the eye. Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics, and described his work as a search for the Harmony of the Spheres.
Galileo made innovative use of experiment and mathematics. However, he became persecuted after Pope Urban VIII blessed Galileo to write about the Copernican system. Galileo had used arguments from the Pope and put them in the voice of the simpleton in the work "Dialogue Concerning the Two Chief World Systems", which greatly offended Urban VIII.

In Northern Europe, the new technology of the printing press was widely used to publish many arguments, including some that disagreed widely with contemporary ideas of nature. René Descartes and Francis Bacon published philosophical arguments in favor of a new type of non-Aristotelian science. Descartes emphasized individual thought and argued that mathematics rather than geometry should be used in order to study nature. Bacon emphasized the importance of experiment over contemplation. Bacon further questioned the Aristotelian concepts of formal cause and final cause, and promoted the idea that science should study the laws of "simple" natures, such as heat, rather than assuming that there is any specific nature, or "formal cause", of each complex type of thing. This new science began to see itself as describing "laws of nature". This updated approach to studies in nature was seen as mechanistic. Bacon also argued that science should aim for the first time at practical inventions for the improvement of all human life.

As a precursor to the Age of Enlightenment, Isaac Newton and Gottfried Wilhelm Leibniz succeeded in developing a new physics, now referred to as classical mechanics, which could be confirmed by experiment and explained using mathematics. Leibniz also incorporated terms from Aristotelian physics, but now being used in a new non-teleological way, for example, "energy" and "potential" (modern versions of Aristotelian ""energeia" and "potentia""). This implied a shift in the view of objects: Where Aristotle had noted that objects have certain innate goals that can be actualized, objects were now regarded as devoid of innate goals. In the style of Francis Bacon, Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes for each type of thing. It is during this period that the word "science" gradually became more commonly used to refer to a "type of pursuit" of a type of knowledge, especially knowledge of nature – coming close in meaning to the old term "natural philosophy."

During this time, the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, "the real and legitimate goal of sciences is the endowment of human life with new inventions and riches", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond "the fume of subtle, sublime, or pleasing speculation".

Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the "Encyclopédie" and the popularization of Newtonianism by Voltaire as well as by Émilie du Châtelet, the French translator of Newton's "Principia".

Some historians have marked the 18th century as a drab period in the history of science; however, the century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.

Enlightenment philosophers chose a short history of scientific predecessors – Galileo, Boyle, and Newton principally – as the guides and guarantors of their applications of the singular concept of nature and natural law to every physical and social field of the day. In this respect, the lessons of history and the social structures built upon it could be discarded.

The nineteenth century is a particularly important period in the history of science since during this era many distinguishing characteristics of contemporary modern science began to take shape such as: transformation of the life and physical sciences, frequent use of precision instruments, emergence of terms like "biologist", "physicist", "scientist"; slowly moving away from antiquated labels like "natural philosophy" and "natural history", increased professionalization of those studying nature lead to reduction in amateur naturalists, scientists gained cultural authority over many dimensions of society, economic expansion and industrialization of numerous countries, thriving of popular science writings and emergence of science journals.

Early in the 19th century, John Dalton suggested the modern atomic theory, based on Democritus's original idea of individible particles called "atoms".

Both John Herschel and William Whewell systematized methodology: the latter coined the term scientist. When Charles Darwin published "On the Origin of Species" he established evolution as the prevailing explanation of biological complexity. His theory of natural selection provided a natural explanation of how species originated, but this only gained wide acceptance a century later.

The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. With the advent of the steam engine and the industrial revolution, there was, however, an increased understanding that all forms of energy as defined by Newton were not equally useful; they did not have the same energy quality. This realization led to the development of the laws of thermodynamics, in which the cumulative energy quality of the universe is seen as constantly declining: the entropy of the universe increases over time.

The electromagnetic theory was also established in the 19th century, and raised new questions which could not easily be answered using Newton's framework. The phenomena that would allow the deconstruction of the atom were discovered in the last decade of the 19th century: the discovery of X-rays inspired the discovery of radioactivity. In the next year came the discovery of the first subatomic particle, the electron.

Einstein's theory of relativity and the development of quantum mechanics led to the replacement of classical mechanics with a new physics which contains two parts that describe different types of events in nature.

In the first half of the century, the development of antibiotics and artificial fertilizer made global human population growth possible. At the same time, the structure of the atom and its nucleus was discovered, leading to the release of "atomic energy" (nuclear power). In addition, the extensive use of technological innovation stimulated by the wars of this century led to revolutions in transportation (automobiles and aircraft), the development of ICBMs, a space race, and a nuclear arms race.

The molecular structure of DNA was discovered in 1953. The discovery of the cosmic microwave background radiation in 1964 led to a rejection of the Steady State theory of the universe in favour of the Big Bang theory of Georges Lemaître.

The development of spaceflight in the second half of the century allowed the first astronomical measurements done on or near other objects in space, including manned landings on the Moon. Space telescopes lead to numerous discoveries in astronomy and cosmology.

Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematization of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling, which are partly based on the Aristotelian paradigm.

Harmful environmental issues such as ozone depletion, acidification, eutrophication and climate change came to the public's attention in the same period, and caused the onset of environmental science and environmental technology. In a 1967 article, Lynn Townsend White Jr. blamed the ecological crisis on the historical decline of the notion of spirit in nature.

With the discovery of the Higgs boson in 2012, the last particle predicted by the Standard Model of particle physics was found. In 2015, gravitational waves, predicted by general relativity a century before, were first observed.

The Human Genome Project was completed in 2003, determining the sequence of nucleotide base pairs that make up human DNA, and identifying and mapping all of the genes of the human genome. Induced pluripotent stem cells were developed in 2006, a technology allowing adult cells to be transformed into stem cells capable of giving rise to any cell type found in the body, potentially of huge importance to the field of regenerative medicine.

Modern science is commonly divided into three major branches that consist of the natural sciences, social sciences, and formal sciences. Each of these branches comprise various specialized yet overlapping scientific disciplines that often possess their own nomenclature and expertise. Both natural and social sciences are empirical sciences as their knowledge are based on empirical observations and are capable of being tested for its validity by other researchers working under the same conditions.

There are also closely related disciplines that use science, such as engineering and medicine.

Natural science is concerned with the description, prediction, and understanding of natural phenomena based on empirical evidence from observation and experimentation. It can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These two branches may be further divided into more specialized disciplines. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, "natural history" suggests observational descriptions aimed at popular audiences.

Social science is concerned with society and the relationships among individuals within a society. It has many branches that include, but are not limited to, anthropology, archaeology, communication studies, economics, history, human geography, jurisprudence, linguistics, political science, psychology, public health, and sociology. Social scientists may adopt various philosophical theories to study individuals and society. For example, positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term "social research" has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.

Formal science is involved in the study of formal systems. It includes mathematics, systems theory, robotics, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts. The formal sciences are therefore "a priori" disciplines and because of this, there is disagreement on whether they actually constitute a science. Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics. Natural and social sciences that rely heavily on mathematical applications include mathematical physics, mathematical chemistry, mathematical biology, mathematical finance, and mathematical economics.

Scientific research can be labeled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Although some scientific research is applied research into specific problems, a great deal of our understanding comes from the curiosity-driven undertaking of basic research. This leads to options for technological advance that were not planned or sometimes even imaginable. This point was made by Michael Faraday when allegedly in response to the question "what is the "use" of basic research?" he responded: "Sir, what is the use of a new-born child?". For example, research into the effects of red light on the human eye's rod cells did not seem to have any practical purpose; eventually, the discovery that our night vision is not troubled by red light would lead search and rescue teams (among others) to adopt red light in the cockpits of jets and helicopters. Finally, even basic research can take unexpected turns, and there is some sense in which the scientific method is built to harness luck.

Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way. An explanatory thought experiment or hypothesis is put forward as explanation using principles such as parsimony (also known as "Occam's Razor") and are generally expected to seek consilience – fitting well with other accepted facts related to the phenomena. This new explanation is used to make falsifiable predictions that are testable by experiment or observation. The predictions are to be posted before a confirming experiment or observation is sought, as proof that no tampering has occurred. Disproof of a prediction is evidence of progress. This is done partly through observation of natural phenomena, but also through experimentation that tries to simulate natural events under controlled conditions as appropriate to the discipline (in the observational sciences, such as astronomy or geology, a predicted observation might take the place of a controlled experiment). Experimentation is especially important in science to help establish causal relationships (to avoid the correlation fallacy).

When a hypothesis proves unsatisfactory, it is either modified or discarded. If the hypothesis survived testing, it may become adopted into the framework of a scientific theory, a logically reasoned, self-consistent model or framework for describing the behavior of certain natural phenomena. A theory typically describes the behavior of much broader sets of phenomena than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. In addition to testing hypotheses, scientists may also generate a model, an attempt to describe or depict the phenomenon in terms of a logical, physical or mathematical representation and to generate new hypotheses that can be tested, based on observable phenomena.

While performing experiments to test hypotheses, scientists may have a preference for one outcome over another, and so it is important to ensure that science as a whole can eliminate this bias. This can be achieved by careful experimental design, transparency, and a thorough peer review process of the experimental results as well as any conclusions. After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be. Taken in its entirety, the scientific method allows for highly creative problem solving while minimizing any effects of subjective bias on the part of its users (especially the confirmation bias).

Mathematics is essential in the formation of hypotheses, theories, and laws in the natural and social sciences. For example, it is used in quantitative scientific modeling, which can generate new hypotheses and predictions to be tested. It is also used extensively in observing and collecting measurements. Statistics, a branch of mathematics, is used to summarize and analyze data, which allow scientists to assess the reliability and variability of their experimental results.

Computational science applies computing power to simulate real-world situations, enabling a better understanding of scientific problems than formal mathematics alone can achieve. According to the Society for Industrial and Applied Mathematics, computation is now as important as theory and experiment in advancing scientific knowledge.

John Ziman points out that intersubjective verifiability is fundamental to the creation of all scientific knowledge. Ziman shows how scientists can identify patterns to each other across centuries; he refers to this ability as "perceptual consensibility." He then makes consensibility, leading to consensus, the touchstone of reliable knowledge.

Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: (1) that there is an objective reality shared by all rational observers; (2) that this objective reality is governed by natural laws; (3) that these laws can be discovered by means of systematic observation and experimentation. Philosophy of science seeks a deep understanding of what these underlying assumptions mean and whether they are valid.

The belief that scientific theories should and do represent metaphysical reality is known as realism. It can be contrasted with anti-realism, the view that the success of science does not depend on it being accurate about unobservable entities such as electrons. One form of anti-realism is idealism, the belief that the mind or consciousness is the most basic essence, and that each mind generates its own reality. In an idealistic world view, what is true for one mind need not be true for other minds.

There are different schools of thought in philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation and that scientific theories are the result of generalizations from such observations. Empiricism generally encompasses inductivism, a position that tries to explain the way general theories can be justified by the finite number of observations humans can make and hence the finite amount of empirical evidence available to confirm scientific theories. This is necessary because the number of predictions those theories make is infinite, which means that they cannot be known from the finite amount of evidence using deductive logic only. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method.
Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation. Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories and that the only way a theory can be affected by observation is when it comes in conflict with it. Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories and replacing induction with falsification as the empirical method. Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error. It covers all products of the human mind, including science, mathematics, philosophy, and art.

Another approach, instrumentalism, colloquially termed "shut up and multiply," emphasizes the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be something that should simply be ignored and that scientists shouldn't make a fuss about (see interpretations of quantum mechanics). Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.

Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent "portrait" of the world that is consistent with observations made from its framing. He characterized "normal science" as the process of observation and "puzzle solving" which takes place within a paradigm, whereas "revolutionary science" occurs when one paradigm overtakes another in a paradigm shift. Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more "portraits" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism.

Finally, another approach often cited in debates of scientific skepticism against controversial movements like "creation science" is methodological naturalism. Its main point is that a difference between natural and supernatural explanations should be made and that science should be restricted methodologically to natural explanations. That the restriction is merely methodological (rather than ontological) means that science should not consider supernatural explanations itself, but should not claim them to be wrong either. Instead, supernatural explanations should be left a matter of personal belief outside the scope of science. Methodological naturalism maintains that proper science requires strict adherence to empirical study and independent verification as a process for properly developing and evaluating explanations for observable phenomena. The absence of these standards, arguments from authority, biased observational studies and other common fallacies are frequently cited by supporters of methodological naturalism as characteristic of the non-science they criticize.

A scientific theory is empirical and is always open to falsification if new evidence is presented. That is, no theory is ever considered strictly certain as science accepts the concept of fallibilism. The philosopher of science Karl Popper sharply distinguished truth from certainty. He wrote that scientific knowledge "consists in the search for truth," but it "is not the search for certainty ... All human knowledge is fallible and therefore uncertain."

New scientific knowledge rarely results in vast changes in our understanding. According to psychologist Keith Stanovich, it may be the media's overuse of words like "breakthrough" that leads the public to imagine that science is constantly proving everything it thought was true to be false. While there are such famous cases as the theory of relativity that required a complete reconceptualization, these are extreme exceptions. Knowledge in science is gained by a gradual synthesis of information from different experiments by various researchers across different branches of science; it is more like a climb than a leap. Theories vary in the extent to which they have been tested and verified, as well as their acceptance in the scientific community. For example, heliocentric theory, the theory of evolution, relativity theory, and germ theory still bear the name "theory" even though, in practice, they are considered factual.
Philosopher Barry Stroud adds that, although the best definition for "knowledge" is contested, being skeptical and entertaining the "possibility" that one is incorrect is compatible with being correct. Therefore, scientists adhering to proper scientific approaches will doubt themselves even once they possess the truth. The fallibilist C. S. Peirce argued that inquiry is the struggle to resolve actual doubt and that merely quarrelsome, verbal, or hyperbolic doubt is fruitless – but also that the inquirer should try to attain genuine doubt rather than resting uncritically on common sense. He held that the successful sciences trust not to any single chain of inference (no stronger than its weakest link) but to the cable of multiple and various arguments intimately connected.

Stanovich also asserts that science avoids searching for a "magic bullet"; it avoids the single-cause fallacy. This means a scientist would not ask merely "What is "the" cause of ...", but rather "What "are" the most significant "causes" of ...". This is especially the case in the more macroscopic fields of science (e.g. psychology, physical cosmology). Research often analyzes few factors at once, but these are always added to the long list of factors that are most important to consider. For example, knowing the details of only a person's genetics, or their history and upbringing, or the current situation may not explain a behavior, but a deep understanding of all these variables combined can be very predictive.

An area of study or speculation that masquerades as science in an attempt to claim a legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science. Physicist Richard Feynman coined the term "cargo cult science" for cases in which researchers believe they are doing science because their activities have the outward appearance of science but actually lack the "kind of utter honesty" that allows their results to be rigorously evaluated. Various types of commercial advertising, ranging from hype to fraud, may fall into these categories.

There can also be an element of political or ideological bias on all sides of scientific debates. Sometimes, research may be characterized as "bad science," research that may be well-intended but is actually incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term "scientific misconduct" refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.

Scientific research is published in an enormous range of scientific literature. Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, "Journal des Sçavans" followed by the "Philosophical Transactions", began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500. The United States National Library of Medicine currently indexes 5,516 journals that contain articles on topics related to the life sciences. Although the journals are in 39 languages, 91 percent of the indexed articles are published in English.

Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is generally considered necessary to communicate the achievements, news, and ambitions of scientists to a wider populace.

Science magazines such as "New Scientist", "Science & Vie", and "Scientific American" cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research. Science books engage the interest of many more people. Tangentially, the science fiction genre, primarily fantastic in nature, engages the public imagination and transmits the ideas, if not the methods, of science.

Recent efforts to intensify or develop links between science and non-scientific disciplines such as literature or more specifically, poetry, include the "Creative Writing Science" resource developed through the Royal Literary Fund.

Discoveries in fundamental science can be world-changing. For example:

The scientific community is a group of all interacting scientists, along with their respective societies and institutions.

Scientists are individuals who conduct scientific research to advance knowledge in an area of interest. The term "scientist" was coined by William Whewell in 1833. In modern times, many professional scientists are trained in an academic setting and upon completion, attain an academic degree, with the highest degree being a doctorate such as a Doctor of Philosophy (PhD), Doctor of Medicine (MD), or Doctor of Engineering (DEng). Many scientists pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit environments.

Scientists exhibit a strong curiosity about reality, with some scientists having a desire to apply scientific knowledge for the benefit of health, nations, environment, or industries. Other motivations include recognition by their peers and prestige. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, chemistry, and economics.

Science has historically been a male-dominated field, with some notable exceptions. Women faced considerable discrimination in science, much as they did in other areas of male-dominated societies, such as frequently being passed over for job opportunities and denied credit for their work. For example, Christine Ladd (1847–1930) was able to enter a PhD program as "C. Ladd"; Christine "Kitty" Ladd completed the requirements in 1882, but was awarded her degree only in 1926, after a career which spanned the algebra of logic (see truth table), color vision, and psychology. Her work preceded notable researchers like Ludwig Wittgenstein and Charles Sanders Peirce. The achievements of women in science have been attributed to their defiance of their traditional role as laborers within the domestic sphere.

In the late 20th century, active recruitment of women and elimination of institutional discrimination on the basis of sex greatly increased the number of women scientists, but large gender disparities remain in some fields; in the early 21st century over half of new biologists were female, while 80% of PhDs in physics are given to men. In the early part of the 21st century, women in the United States earned 50.3% of bachelor's degrees, 45.6% of master's degrees, and 40.7% of PhDs in science and engineering fields. They earned more than half of the degrees in psychology (about 70%), social sciences (about 50%), and biology (about 50-60%) but earned less than half the degrees in the physical sciences, earth sciences, mathematics, engineering, and computer science. Lifestyle choice also plays a major role in female engagement in science; women with young children are 28% less likely to take tenure-track positions due to work-life balance issues, and female graduate students' interest in careers in research declines dramatically over the course of graduate school, whereas that of their male colleagues remains unchanged.

Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance. Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines. Membership may be open to all, may require possession of some qualifications, or may be an honor conferred by election. Membership often requires possession of some scientific credentials, or may be an honor conferred by election. Most scientific societies are non-profit organizations, and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some also act as professional bodies, regulating the activities of their members in the public interest or the collective interest of the membership. Scholars in the sociology of science argue that learned societies are of key importance and their formation assists in the emergence and development of new disciplines or professions.

The professionalization of science, begun in the 19th century, was partly enabled by the creation of distinguished academy of sciences in a number of countries such as the Italian in 1603, the British Royal Society in 1660, the French in 1666, the American National Academy of Sciences in 1863, the German Kaiser Wilhelm Institute in 1911, and the Chinese Academy of Sciences in 1928. International scientific organizations, such as the International Council for Science, have since been formed to promote cooperation between the scientific communities of different nations.

Science policy is an area of public policy concerned with the policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care and environmental monitoring. Science policy also refers to the act of applying scientific knowledge and consensus to the development of public policies. Science policy thus deals with the entire domain of issues that involve the natural sciences. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.

State policy has influenced the funding of public works and science for thousands of years, particularly within civilizations with highly organized governments such as imperial China and the Roman Empire. Prominent historical examples include the Great Wall of China, completed over the course of two millennia through the state support of several dynasties, and the Grand Canal of the Yangtze River, an immense feat of hydraulic engineering begun by Sunshu Ao (孫叔敖 7th c. BCE), Ximen Bao (西門豹 5th c.BCE), and Shi Chi (4th c. BCE). This construction dates from the 6th century BCE under the Sui Dynasty and is still in use today. In China, such state-supported infrastructure and scientific research projects date at least from the time of the Mohists, who inspired the study of logic during the period of the Hundred Schools of Thought and the study of defensive fortifications like the Great Wall of China during the Warring States period.

Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organizations that fund research. Vannevar Bush, director of the Office of Scientific Research and Development for the United States government, the forerunner of the National Science Foundation, wrote in July 1945 that "Science is a proper concern of government."

Scientific research is often funded through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP. In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10% respectively by universities and government. The government funding proportion in certain industries is higher, and it dominates research in social science and humanities. Similarly, with some exceptions (e.g. biotechnology) government provides the bulk of the funds for basic scientific research. Many governments have dedicated agencies to support scientific research. Prominent scientific organizations include the National Science Foundation in the United States, the National Scientific and Technical Research Council in Argentina, Commonwealth Scientific and Industrial Research Organisation (CSIRO) in Australia, in France, the Max Planck Society and in Germany, and CSIC in Spain. In commercial research and development, all but the most research-oriented corporations focus more heavily on near-term commercialisation possibilities rather than "blue-sky" ideas or technologies (such as nuclear fusion).

The public awareness of science relates to the attitudes, behaviors, opinions, and activities that make up the relations between science and the general public. it integrates various themes and activities such as science communication, science museums, science festivals, science fairs, citizen science, and science in popular culture. Social scientists have devised various metrics to measure the public understanding of science such as factual knowledge, self-reported knowledge, and structural knowledge.

The mass media face a number of pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter. Few journalists have real scientific knowledge, and even beat reporters who know a great deal about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.

Politicization of science occurs when government, business, or advocacy groups use legal or economic pressure to influence the findings of scientific research or the way it is disseminated, reported, or interpreted. Many factors can act as facets of the politicization of science such as populist anti-intellectualism, perceived threats to religious beliefs, postmodernist subjectivism, and fear for business interests. Politicization of science is usually accomplished when scientific information is presented in a way that emphasizes the uncertainty associated with the scientific evidence. Tactics such as shifting conversation, failing to acknowledge facts, and capitalizing on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence. Examples of issues that have involved the politicization of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.




Publications

Resources


</doc>
<doc id="9145213" url="https://en.wikipedia.org/wiki?curid=9145213" title="Outline of science">
Outline of science

The following outline is provided as a topical overview of science:

Science – the systematic effort of acquiring knowledge—through observation and experimentation coupled with logic and reasoning to find out what can be proved or not proved—and the knowledge thus acquired. The word "science" comes from the Latin word "scientia" meaning knowledge. A practitioner of science is called a "scientist". Modern science respects objective logical reasoning, and follows a set of core procedures or rules in order to determine the nature and underlying natural laws of the universe and everything in it. Some scientists do not know of the rules themselves, but follow them through research policies. These procedures are known as the scientific method.


Scientific method   (outline) – body of techniques for investigating phenomena and acquiring new knowledge, as well as for correcting and integrating previous knowledge. It is based on observable, empirical, measurable evidence, and subject to laws of reasoning, both deductive and inductive.

Branches of science – divisions within science with respect to the entity or system concerned, which typically embodies its own terminology and nomenclature.

Natural science   (outline) – major branch of science, that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: biology, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.

Formal science – branches of knowledge that are concerned with formal systems, such as those under the branches of: logic, mathematics, computer science, statistics, and some aspects of linguistics. Unlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the real world, but instead with the properties of formal systems based on definitions and rules.



Social science – study of the social world constructed between humans. The social sciences usually limit themselves to an anthropomorphically centric view of these interactions with minimal emphasis on the inadvertent impact of social human behavior on the external environment (physical, biological, ecological, etc.). 'Social' is the concept of exchange/influence of ideas, thoughts, and relationship interactions (resulting in harmony, peace, self enrichment, favoritism, maliciousness, justice seeking, etc.) between humans. The scientific method is utilized in many social sciences, albeit adapted to the needs of the social construct being studied.

Applied science – branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements.







See – 






The scientific fields mentioned below are generally described by the science they study.




Science education




</doc>
<doc id="11983318" url="https://en.wikipedia.org/wiki?curid=11983318" title="Branches of science">
Branches of science

The branches of science, also referred to as sciences, "scientific fields", or "scientific disciplines," are commonly divided into three major groups:


Natural and social sciences are empirical sciences, meaning that the knowledge must be based on observable phenomena and must be capable of being verified by other researchers working under the same conditions.

Natural, social, and formal science make up the fundamental sciences, which form the basis of interdisciplinary and applied sciences such as engineering and medicine. Specialized scientific disciplines that exist in multiple categories may include parts of other scientific disciplines but often possess their own terminologies and expertises.

Natural science is a branch of science that seeks to elucidate the rules that govern the natural world by applying an empirical and scientific method to the study of the universe. The term natural sciences is used to distinguish it from the social sciences, which apply the scientific method to study human behavior and social patterns; the humanities, which use a critical, or analytical approach to the study of the human condition; and the formal sciences.

Physical science is an encompassing term for the branches of natural science and science that study non-living systems, in contrast to the life sciences. However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena. There is a difference between physical science and physics.

"Physics" (from ) is a natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.

Physics is one of the oldest academic disciplines, perhaps the oldest through its inclusion of astronomy. Over the last two millennia, physics was a part of natural philosophy along with chemistry, certain branches of mathematics, and biology, but during the Scientific Revolution in the 16th century, the natural sciences emerged as unique research programs in their own right. Certain research areas are interdisciplinary, such as biophysics and quantum chemistry, which means that the boundaries of physics are not rigidly defined. In the nineteenth and twentieth centuries physicalism emerged as a major unifying feature of the philosophy of science as physics provides fundamental explanations for every observed natural phenomenon. New ideas in physics often explain the fundamental mechanisms of other sciences, while opening to new research areas in mathematics and philosophy.

"Chemistry" (the etymology of the word has been much disputed) is the science of matter and the changes it undergoes. The science of matter is also addressed by physics, but while physics takes a more general and fundamental approach, chemistry is more specialized, being concerned by the composition, behavior (or reaction), structure, and properties of matter, as well as the changes it undergoes during chemical reactions. It is a physical science which studies various substances, atoms, molecules, and matter (especially carbon based); biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system (see subdisciplines).

"Earth science" (also known as "geoscience", "the geosciences" or "the Earth sciences") is an all-embracing term for the sciences related to the planet Earth. It is arguably a special case in planetary science, the Earth being the only known life-bearing planet. There are both reductionist and holistic approaches to Earth sciences. The formal discipline of Earth sciences may include the study of the atmosphere, hydrosphere, oceans and biosphere, as well as the solid earth. Typically Earth scientists will use tools from physics, chemistry, biology, geography, chronology and mathematics to build a quantitative understanding of how the Earth system works, and how it evolved to its current state.

Ecology (from Greek: οἶκος, "house"; -λογία, "study of") is the scientific study of the relationships that living organisms have with each other and with their abiotic environment. Topics of interest to ecologists include the composition, distribution, amount (biomass), number, and changing states of organisms within and among ecosystems.

Oceanography, or marine biology, is the branch of Earth science that studies ocean. It covers a wide range of topics, including marine organisms and ecosystem dynamics; ocean currents, waves, and geophysical fluid dynamics; plate tectonics and the geology of the sea floor; and fluxes of various chemical substances and physical properties within the ocean and across its boundaries. These diverse topics reflect multiple disciplines that oceanographers blend to further knowledge of the world ocean and understanding of processes within it: biology, chemistry, geology, meteorology, and physics as well as geography.

Geology (from the Greek γῆ, gê, "earth" and λόγος, logos, "study") is the science comprising the study of solid Earth, the rocks of which it is composed, and the processes by which they change.

Meteorology is the interdisciplinary scientific study of the atmosphere. Studies in the field stretch back millennia, though significant progress in meteorology did not occur until the 17th century. The 19th century saw breakthroughs occur after observing networks developed across several countries. After the development of the computer in the latter half of the 20th century, breakthroughs in weather forecasting were achieved.

Space science or Astronomy is the study of everything in outer space. This has sometimes been called astronomy, but recently astronomy has come to be regarded as a division of broader space science, which has grown to include other related fields, such as studying issues related to space travel and space exploration (including space medicine), space archaeology and science performed in outer space (see space research).

"The science of living things comprises the branches of science that involve the scientific study of living organisms, like plants, animals, and human beings. However, the study of behavior of organisms, such as practiced in ethology and psychology, is only included in as much as it involves a clearly biological aspect. While biology remains the centerpiece of the science of living things, technological advances in molecular biology and biotechnology have led to a burgeoning of specializations and new, often interdisciplinary, fields.

"Biology" is the branch of natural science concerned with the study of life and living organisms, including their structure, function, growth, origin, evolution, distribution, and taxonomy. Biology is a vast subject containing many subdivisions, topics, and disciplines.

Zoology, occasionally spelled zoölogy, is the branch of science that relates to the animal kingdom, including the structure, embryology, evolution, classification, habits, and distribution of all animals, both living and extinct. The term is derived from Ancient Greek ζῷον (zōon, "animal") + λόγος (logos, "knowledge"). Some branches of zoology include: anthrozoology, arachnology, archaeozoology, cetology, embryology, entomology, helminthology, herpetology, histology, ichthyology, malacology, mammalogy, morphology, nematology, ornithology, palaeozoology, pathology, primatology, protozoology, taxonomy, and zoogeography.

Human biology is an interdisciplinary academic field of biology, biological anthropology, nutrition and medicine which focuses on humans; it is closely related to primate biology, and a number of other fields.

Some branches of biology include: microbiology, anatomy, neurology and neuroscience, immunology, genetics, physiology, pathology, biophysics, biolinguistics, and ophthalmology.

Botany, plant science, or plant biology is a branch of biology that involves the scientific study of plant life. Botany covers a wide range of scientific disciplines including structure, growth, reproduction, metabolism, development, diseases, chemical properties, and evolutionary relationships among taxonomic groups. Botany began with early human efforts to identify edible, medicinal and poisonous plants, making it one of the oldest sciences. Today botanists study over 550,000 species of living organisms.
The term "botany" comes from Greek βοτάνη, meaning "pasture, grass, fodder", perhaps via the idea of a livestock keeper needing to know which plants are safe for livestock to eat.

The "social sciences" are the fields of scholarship that study society. "Social science" is commonly used as an umbrella term to refer to a plurality of fields outside of the natural sciences. These include: anthropology, archaeology, business administration, communication, criminology, economics, education, government, linguistics, international relations, political science, some branches of psychology (results of which can not be replicated or validated easily - e.g. social psychology), public health, theology, sociology and, in some contexts, geography, history and law.

The "formal sciences" are the branches of science that are concerned with formal systems, such as logic, mathematics, theoretical computer science, information theory, systems theory, decision theory, statistics, and theoretical linguistics.

Unlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the real world (empirical knowledge), but rather with the properties of formal systems based on definitions and rules. Methods of the formal sciences are, however, essential to the construction and testing of scientific models dealing with observable reality, and major advances in formal sciences have often enabled major advances in the empirical sciences.

"Decision theory" in economics, psychology, philosophy, mathematics, and statistics is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision. It is very closely related to the field of game law.

"Logic" (from the Greek "λογική" logikē) is the formal systematic study of the principles of valid inference and correct reasoning. Logic is used in most intellectual activities, but is studied primarily in the disciplines of philosophy, mathematics, semantics, and computer science. Logic examines general forms which arguments may take, which forms are valid, and which are fallacies. In philosophy, the study of logic figures in most major areas: epistemology, ethics, metaphysics. In mathematics and computer science, it is the study of valid inferences within some formal language.
Logic is also studied in argumentation theory.

"Mathematics", first of all known as The Science of numbers which is classified in Arithmetic and Algebra, is classified as a formal science, has both similarities and differences with the empirical sciences (the natural and social sciences). It is similar to empirical sciences in that it involves an objective, careful and systematic study of an area of knowledge; it is different because of its method of verifying its knowledge, using "a priori" rather than empirical methods.

"Statistics" is the study of the collection, organization, and interpretation of data. It deals with all aspects of this, including the planning of data collection in terms of the design of surveys and experiments.

A statistician is someone who is particularly well versed in the ways of thinking necessary for the successful application of statistical analysis. Such people have often gained this experience through working in any of a wide number of fields. There is also a discipline called "mathematical statistics", which is concerned with the theoretical basis of the subject.

The word "statistics", when referring to the scientific discipline, is singular, as in "Statistics is an art." This should not be confused with the word "statistic", referring to a quantity (such as mean or median) calculated from a set of data, whose plural is "statistics" ("this statistic seems wrong" or "these statistics are misleading").

"Systems theory" is the interdisciplinary study of systems in general, with the goal of elucidating principles that can be applied to all types of systems in all fields of research. The term does not yet have a well-established, precise meaning, but systems theory can reasonably be considered a specialization of systems thinking and a generalization of systems science. The term originates from Ludwig von Bertalanffy's General System Theory (GS) and is used in later efforts in other fields, such as the action theory of Alcott Parsons and the system-theory of Nickolas McLuhan.

In this context the word "systems" is used to refer specifically to self-regulating systems, i.e. that are self-correcting through feedback. Self-regulating systems are found in nature, including the physiological systems of our body, in local and global ecosystems, and in climate.

"Theoretical computer science" (TCS) is a division or subset of general computer science and focuses on more abstract or mathematical aspects of computing.

These divisions and subsets include analysis of algorithms and formal semantics of programming languages. Technically, there are hundreds of divisions and subsets besides these two. Each of the multiple parts have their own individual personal leaders (of popularity) and there are many associations and professional social groups and publications of distinction.

"Applied science" is the application of scientific knowledge transferred into a physical environment. Examples include testing a theoretical model through the use of formal science or solving a practical problem through the use of natural science.

Applied science differs from fundamental science, which seeks to describe the most basic objects and forces, having less emphasis on practical applications. Applied science can be like biological science and physical science.

Example fields of applied science include

Fields of engineering are closely related to applied sciences. Applied science is important for technology development. Its use in industrial settings is usually referred to as research and development (R&D).



</doc>
<doc id="59890985" url="https://en.wikipedia.org/wiki?curid=59890985" title="Schmidt Science Fellows">
Schmidt Science Fellows

Schmidt Science Fellows is a post-doctoral fellowship set up in 2018 and funded by former Google chairman Eric Schmidt and his wife Wendy Schmidt and run in partnership with the Rhodes Trust: a non-profit organization who administer the Rhodes Scholarship. The program is intended to foster interdisciplinary research and teach leadership skills in a select group of post-doctoral scientists. 

The fellowship is supported by Schmidt Futures, the philanthropic initiative of Eric Schmidt and Wendy Schmidt who pledged $25 million for the first three years as part of a broader $100 million drive to fund scientific research. The Rhodes Trust acts as a central partner and coordinates the selection process, mediates with partner universities, and assists in the post-doctoral placements of fellows. 

The program consists of an 11-month lab-based postdoctoral research study at a leading scientific research university, followed by four global meetings. 

In its inaugural year fellows were selected from a pool of 220 international applicants from natural sciences, engineering, mathematics, and computer sciences backgrounds; then whittled down to 31 finalists. Successful applicants are given $100,000 to study in a field outside of their usual area of research and undertake a series of advanced courses and networking programs. In its first year 14 fellows were selected (eight men and six women) from twelve different universities. 

The fellowship came about due to a necessity to fund interdisciplinary research, which is often lacking grant money. Spending 11 or more months outside of their core areas of research is believed to give future scientific leaders more interdisciplinary skills. 

At least half of the first 14 Schmidt Science Fellows, will be pursuing projects which rely on computation, data, AI or machine learning: technologies which in which Schmidt has shown an interest.


</doc>
<doc id="22939" url="https://en.wikipedia.org/wiki?curid=22939" title="Physics">
Physics

Physics (from , from "phýsis" "nature") is the natural science that studies matter and its motion and behavior through space and time and that studies the related entities of energy and force. Physics is one of the most fundamental scientific disciplines, and its main goal is to understand how the universe behaves.

Physics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest. Over the last two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the scientific revolution in the 17th century, these natural sciences emerged as unique research endeavors in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in academic disciplines such as mathematics and philosophy.

Advances in physics often enable advances in new technologies. For example, advances in the understanding of electromagnetism and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.

Astronomy is one of the oldest natural sciences. Early civilizations dating back to beyond 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, had a predictive knowledge and a basic understanding of the motions of the Sun, Moon, and stars. The stars and planets were often worshipped, believed to represent gods. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which however did not explain the positions of the planets.

According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his "Iliad" and "Odyssey"; later Greek astronomers provided names, which are still used today, for most constellations visible from the northern hemisphere.

Natural philosophy has its origins in Greece during the Archaic period, (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus.

The Western Roman Empire fell in the fifth century, and this resulted in a decline in intellectual pursuits in the western part of Europe. By contrast, the Eastern Roman Empire (also known as the Byzantine Empire) resisted the attacks from the barbarians, and continued to advance various fields of learning, including physics.

In the sixth century Isidore of Miletus created an important compilation of Archimedes' works that are copied in the Archimedes Palimpsest.

In sixth century Europe John Philoponus, a Byzantine scholar, questioned Aristotle's teaching of physics and noting its flaws. He introduced the theory of impetus. Aristotle's physics was not scrutinized until John Philoponus appeared, and unlike Aristotle who based his physics on verbal argument, Philoponus relied on observation. On Aristotle's physics John Philoponus wrote:

“"But this is completely erroneous, and our view may be corroborated by actual observation more effectively than by any sort of verbal argument. For if you let fall from the same height two weights of which one is many times as heavy as the other, you will see that the ratio of the times required for the motion does not depend on the ratio of the weights, but that the difference in time is a very small one. And so, if the difference in the weights is not considerable, that is, of one is, let us say, double the other, there will be no difference, or else an imperceptible difference, in time, though the difference in weight is by no means negligible, with one body weighing twice as much as the other"”

John Philoponus' criticism of Aristotelian principles of physics served as an inspiration for Galileo Galilei ten centuries later, during the Scientific Revolution. Galileo cited Philoponus substantially in his works when arguing that Aristotelian physics was flawed. In the 1300s Jean Buridan, a teacher in the faculty of arts at the University of Paris, developed the concept of impetus. It was a step toward the modern ideas of inertia and momentum.

Islamic scholarship inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further, especially placing emphasis on observation and "a priori" reasoning, developing early forms of the scientific method.

The most notable innovations were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. The most notable work was "The Book of Optics" (also known as Kitāb al-Manāẓir), written by Ibn al-Haytham, in which he conclusively disproved the ancient Greek idea about vision, but also came up with a new theory. In the book, he presented a study of the phenomenon of the camera obscura (his thousand-year-old version of the pinhole camera) and delved further into the way the eye itself works. Using dissections and the knowledge of previous scholars, he was able to begin to explain how light enters the eye. He asserted that the light ray is focused, but the actual explanation of how light projected to the back of the eye had to wait until 1604. His "Treatise on Light" explained the camera obscura, hundreds of years before the modern development of photography.
The seven-volume "Book of Optics" ("Kitab al-Manathir") hugely influenced thinking across disciplines from the theory of visual perception to the nature of perspective in medieval art, in both the East and the West, for more than 600 years. Many later European scholars and fellow polymaths, from Robert Grosseteste and Leonardo da Vinci to René Descartes, Johannes Kepler and Isaac Newton, were in his debt. Indeed, the influence of Ibn al-Haytham's Optics ranks alongside that of Newton's work of the same title, published 700 years later.

The translation of "The Book of Optics" had a huge impact on Europe. From it, later European scholars were able to build devices that replicated those Ibn al-Haytham had built, and understand the way light works. From this, such important things as eyeglasses, magnifying glasses, telescopes, and cameras were developed.

Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics.

Major developments in this period include the replacement of the geocentric model of the solar system with the heliocentric Copernican model, the laws governing the motion of planetary bodies determined by Johannes Kepler between 1609 and 1619, pioneering work on telescopes and observational astronomy by Galileo Galilei in the 16th and 17th Centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation that would come to bear his name. Newton also developed calculus, the mathematical study of change, which provided new mathematical methods for solving physical problems.

The discovery of new laws in thermodynamics, chemistry, and electromagnetics resulted from greater research efforts during the Industrial Revolution as energy needs increased. The laws comprising classical physics remain very widely used for objects on everyday scales travelling at non-relativistic speeds, since they provide a very close approximation in such situations, and theories such as quantum mechanics and the theory of relativity simplify to their classical equivalents at such scales. However, inaccuracies in classical mechanics for very small objects and very high velocities led to the development of modern physics in the 20th century.

Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted a varying speed of light, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism; this discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency; this, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics taking over from classical physics at very small scales.

Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrödinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups.

In many ways, physics stems from ancient Greek philosophy. From Thales' first attempt to characterise matter, to Democritus' deduction that matter ought to reduce to an invariant state, the Ptolemaic astronomy of a crystalline firmament, and Aristotle's book "Physics" (an early book on physics, which attempted to analyze and define motion from a philosophical point of view), various Greek philosophers advanced their own theories of nature. Physics was known as natural philosophy until the late 18th century.

By the 19th century, physics was realised as a discipline distinct from philosophy and the other sciences. Physics, as with the rest of science, relies on philosophy of science and its "scientific method" to advance our knowledge of the physical world. The scientific method employs "a priori reasoning" as well as "a posteriori" reasoning and the use of Bayesian inference to measure the validity of a given theory.

The development of physics has answered many questions of early philosophers, but has also raised new questions. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism and realism.

Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schrödinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose had been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, "The Road to Reality". Hawking referred to himself as an "unashamed reductionist" and took issue with Penrose's views.

Though physics deals with a wide variety of systems, certain theories are used by all physicists. Each of these theories were experimentally tested numerous times and found to be an adequate approximation of nature. For instance, the theory of classical mechanics accurately describes the motion of objects, provided they are much larger than atoms and moving at much less than the speed of light. These theories continue to be areas of active research today. Chaos theory, a remarkable aspect of classical mechanics was discovered in the 20th century, three centuries after the original formulation of classical mechanics by Isaac Newton (1642–1727).

These central theories are important tools for research into more specialised topics, and any physicist, regardless of their specialisation, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.

Classical physics includes the traditional branches and topics that were recognised and well-developed before the beginning of the 20th century—classical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.

Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.

Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics studies matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsense notions of space, time, matter, and energy are no longer valid.

The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with motion in the absence of gravitational fields and the general theory of relativity with motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in all areas of modern physics.

While physics aims to discover universal laws, its theories lie in explicit domains of applicability. Loosely speaking, the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Albert Einstein contributed the framework of special relativity, which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light. Max Planck, Erwin Schrödinger, and others introduced quantum mechanics, a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales. Later, quantum field theory unified quantum mechanics and special relativity. General relativity allowed for a dynamical, curved spacetime, with which highly massive systems and the large-scale structure of the universe can be well-described. General relativity has not yet been unified with the other fundamental descriptions; several candidate theories of quantum gravity are being developed.

Mathematics provides a compact and exact language used to describe the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton.

Physics uses mathematics to organise and formulate experimental results. From those results, precise or estimated solutions are obtained, quantitative results from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical data, with their units of measure and estimates of the errors in the measurements. Technologies based on mathematics, like computation have made computational physics an active area of research.

Ontology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data.

The distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a "mathematical model of a physical situation" (system) and a "mathematical description of a physical law" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for.

Physics is a branch of fundamental science, not practical science. Physics is also called "the fundamental science" because the subject of study of all branches of natural science like chemistry, astronomy, geology, and biology are constrained by laws of physics, similar to how chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge.

Physics is applied in industries like engineering and medicine.

Applied physics is a general term for physics research which is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem.

The approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics.

Physics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations.

With the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the earth, one can reasonably model earth's mass, temperature, and rate of rotation, as a function of time allowing one to extrapolate forward or backward in time and so predict future or prior events. It also allows for simulations in engineering which drastically speed up the development of a new technology.

But there is also considerable interdisciplinarity in the physicist's methods, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics).

Physicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of the theory.

A scientific law is a concise verbal or mathematical statement of a relation which expresses a fundamental principle of some theory, such as Newton's law of universal gravitation.

Theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they are strongly dependent upon each other. Progress in physics frequently comes about when experimentalists make a discovery that existing theories cannot explain, or when new theories generate experimentally testable predictions, which inspire new experiments.

Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory.

Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories. They then explore the consequences of these ideas and work toward making testable predictions.

Experimental physics expands, and is expanded by, engineering and technology. Experimental physicists involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas which are not well-explored by theorists.

Physics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the "fundamental science". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together.

For example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of "unifying" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (Theory of Everything) for why nature is as it is (see section "Current research" below for more information).

Contemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach.

Since the 20th century, the individual fields of physics have become increasingly specialised, and today most physicists work in a single field for their entire careers. "Universalists" such as Albert Einstein (1879–1955) and Lev Landau (1908–1968), who worked in multiple fields of physics, are now very rare.

The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table.

Particle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high energy accelerators, detectors, and computer programs necessary for this research. The field is also called "high-energy physics" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles.

Currently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of a Higgs mechanism.

Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.

Atomic, molecular, and optical physics (AMO) is the study of matter–matter and light–matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view).

Atomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see, e.g., hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of nuclear physics.

Molecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm.

Condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the "condensed" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong.

The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices.

Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term "condensed matter physics" was apparently coined by Philip Anderson when he renamed his research group—previously "solid-state theory"—in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering.

Astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.

The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy.

Physical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang.

The Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the ΛCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter.

Numerous possibilities and discoveries are anticipated to emerge from new data from the Fermi Gamma-ray Space Telescope over the upcoming decade and vastly revise or clarify existing models of the universe. In particular, the potential for a tremendous discovery surrounding dark matter is possible over the next several years. Fermi will search for evidence that dark matter is composed of weakly interacting massive particles, complementing similar experiments with the Large Hadron Collider and other underground detectors.

IBEX is already yielding new astrophysical discoveries: "No one knows what is creating the ENA (energetic neutral atoms) ribbon" along the termination shock of the solar wind, "but everyone agrees that it means the textbook picture of the heliosphere—in which the Solar System's enveloping pocket filled with the solar wind's charged particles is plowing through the onrushing 'galactic wind' of the interstellar medium in the shape of a comet—is wrong."

Research in physics is continually progressing on a large number of fronts.

In condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers.

In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. The Large Hadron Collider has already found the Higgs Boson, but future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. Research on the nature of the major mysteries of dark matter and dark energy is also currently ongoing.

Theoretical attempts to unify quantum mechanics and general relativity into a single theory of quantum gravity, a program ongoing for over half a century, have not yet been decisively resolved. The current leading candidates are M-theory, superstring theory and loop quantum gravity.

Many astronomical and cosmological phenomena have yet to be satisfactorily explained, including the origin of ultra-high energy cosmic rays, the baryon asymmetry, the acceleration of the universe and the anomalous rotation rates of galaxies.

Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections.

These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 "Annual Review of Fluid Mechanics", Horace Lamb said:






General


Organizations

Online course learning resources


</doc>
<doc id="28481" url="https://en.wikipedia.org/wiki?curid=28481" title="Statistical mechanics">
Statistical mechanics

Statistical mechanics is one of the pillars of modern physics. It is necessary for the fundamental study of any physical system that has a large number of degrees of freedom. The approach is based on statistical methods, probability theory and the microscopic physical laws.

It can be used to explain the thermodynamic behaviour of large systems. This branch of statistical mechanics, which treats and extends classical thermodynamics, is known as statistical thermodynamics or equilibrium statistical mechanics.

Statistical mechanics shows how the concepts from macroscopic observations (such as temperature and pressure) are related to the description of microscopic state that fluctuates around an average state. It connects thermodynamic quantities (such as heat capacity) to microscopic behaviour, whereas, in classical thermodynamics, the only available option would be to just measure and tabulate such quantities for various materials.

Statistical mechanics can also be used to study systems that are out of equilibrium. An important subbranch known as non-equilibrium statistical mechanics deals with the issue of microscopically modelling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions or flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

In physics, there are two types of mechanics usually examined: classical mechanics and quantum mechanics. For both types of mechanics, the standard mathematical approach is to consider two concepts:
Using these two concepts, the state at any other time, past or future, can in principle be calculated.
There is however a disconnection between these laws and everyday life experiences, as we do not find it necessary (nor even theoretically possible) to know exactly at a microscopic level the simultaneous positions and velocities of each molecule while carrying out processes at the human scale (for example, when performing a chemical reaction). Statistical mechanics fills this disconnection between the laws of mechanics and the practical experience of incomplete knowledge, by adding some uncertainty about which state the system is in.

Whereas ordinary mechanics only considers the behaviour of a single state, statistical mechanics introduces the statistical ensemble, which is a large collection of virtual, independent copies of the system in various states. The statistical ensemble is a probability distribution over all possible states of the system. In classical statistical mechanics, the ensemble is a probability distribution over phase points (as opposed to a single phase point in ordinary mechanics), usually represented as a distribution in a phase space with canonical coordinates. In quantum statistical mechanics, the ensemble is a probability distribution over pure states, and can be compactly summarized as a density matrix.

As is usual for probabilities, the ensemble can be interpreted in different ways:
These two meanings are equivalent for many purposes, and will be used interchangeably in this article.

However the probability is interpreted, each state in the ensemble evolves over time according to the equation of motion. Thus, the ensemble itself (the probability distribution over states) also evolves, as the virtual systems in the ensemble continually leave one state and enter another. The ensemble evolution is given by the Liouville equation (classical mechanics) or the von Neumann equation (quantum mechanics). These equations are simply derived by the application of the mechanical equation of motion separately to each virtual system contained in the ensemble, with the probability of the virtual system being conserved over time as it evolves from state to state.

One special class of ensemble is those ensembles that do not evolve over time. These ensembles are known as "equilibrium ensembles" and their condition is known as "statistical equilibrium". Statistical equilibrium occurs if, for each state in the ensemble, the ensemble also contains all of its future and past states with probabilities equal to the probability of being in that state. The study of equilibrium ensembles of isolated systems is the focus of statistical thermodynamics. Non-equilibrium statistical mechanics addresses the more general case of ensembles that change over time, and/or ensembles of non-isolated systems.

The primary goal of statistical thermodynamics (also known as equilibrium statistical mechanics) is to derive the classical thermodynamics of materials in terms of the properties of their constituent particles and the interactions between them. In other words, statistical thermodynamics provides a connection between the macroscopic properties of materials in thermodynamic equilibrium, and the microscopic behaviours and motions occurring inside the material.

Whereas statistical mechanics proper involves dynamics, here the attention is focussed on "statistical equilibrium" (steady state). Statistical equilibrium does not mean that the particles have stopped moving (mechanical equilibrium), rather, only that the ensemble is not evolving.

A sufficient (but not necessary) condition for statistical equilibrium with an isolated system is that the probability distribution is a function only of conserved properties (total energy, total particle numbers, etc.).
There are many different equilibrium ensembles that can be considered, and only some of them correspond to thermodynamics. Additional postulates are necessary to motivate why the ensemble for a given system should have one form or another.

A common approach found in many textbooks is to take the "equal a priori probability postulate". This postulate states that
The equal a priori probability postulate therefore provides a motivation for the microcanonical ensemble described below. There are various arguments in favour of the equal a priori probability postulate:
Other fundamental postulates for statistical mechanics have also been proposed.

There are three equilibrium ensembles with a simple form that can be defined for any isolated system bounded inside a finite volume. These are the most often discussed ensembles in statistical thermodynamics. In the macroscopic limit (defined below) they all correspond to classical thermodynamics.

For systems containing many particles (the thermodynamic limit), all three of the ensembles listed above tend to give identical behaviour. It is then simply a matter of mathematical convenience which ensemble is used. The Gibbs theorem about equivalence of ensembles was developed into the theory of concentration of measure phenomenon, which has applications in many areas of science, from functional analysis to methods of artificial intelligence and big data technology.

Important cases where the thermodynamic ensembles "do not" give identical results include:
In these cases the correct thermodynamic ensemble must be chosen as there are observable differences between these ensembles not just in the size of fluctuations, but also in average quantities such as the distribution of particles. The correct ensemble is that which corresponds to the way the system has been prepared and characterized—in other words, the ensemble that reflects the knowledge about that system.

Once the characteristic state function for an ensemble has been calculated for a given system, that system is 'solved' (macroscopic observables can be extracted from the characteristic state function). Calculating the characteristic state function of a thermodynamic ensemble is not necessarily a simple task, however, since it involves considering every possible state of the system. While some hypothetical systems have been exactly solved, the most general (and realistic) case is too complex for an exact solution. Various approaches exist to approximate the true ensemble and allow calculation of average quantities.

There are some cases which allow exact solutions.


One approximate approach that is particularly well suited to computers is the Monte Carlo method, which examines just a few of the possible states of the system, with the states chosen randomly (with a fair weight). As long as these states form a representative sample of the whole set of states of the system, the approximate characteristic function is obtained. As more and more random samples are included, the errors are reduced to an arbitrarily low level.



There are many physical phenomena of interest that involve quasi-thermodynamic processes out of equilibrium, for example:
All of these processes occur over time with characteristic rates, and these rates are of importance for engineering. The field of non-equilibrium statistical mechanics is concerned with understanding these non-equilibrium processes at the microscopic level. (Statistical thermodynamics can only be used to calculate the final result, after the external imbalances have been removed and the ensemble has settled back down to equilibrium.)

In principle, non-equilibrium statistical mechanics could be mathematically exact: ensembles for an isolated system evolve over time according to deterministic equations such as Liouville's equation or its quantum equivalent, the von Neumann equation. These equations are the result of applying the mechanical equations of motion independently to each state in the ensemble. Unfortunately, these ensemble evolution equations inherit much of the complexity of the underlying mechanical motion, and so exact solutions are very difficult to obtain. Moreover, the ensemble evolution equations are fully reversible and do not destroy information (the ensemble's Gibbs entropy is preserved). In order to make headway in modelling irreversible processes, it is necessary to consider additional factors besides probability and reversible mechanics.

Non-equilibrium mechanics is therefore an active area of theoretical research as the range of validity of these additional assumptions continues to be explored. A few approaches are described in the following subsections.

One approach to non-equilibrium statistical mechanics is to incorporate stochastic (random) behaviour into the system. Stochastic behaviour destroys information contained in the ensemble. While this is technically inaccurate (aside from hypothetical situations involving black holes, a system cannot in itself cause loss of information), the randomness is added to reflect that information of interest becomes converted over time into subtle correlations within the system, or to correlations between the system and environment. These correlations appear as chaotic or pseudorandom influences on the variables of interest. By replacing these correlations with randomness proper, the calculations can be made much easier.

Another important class of non-equilibrium statistical mechanical models deals with systems that are only very slightly perturbed from equilibrium. With very small perturbations, the response can be analysed in linear response theory. A remarkable result, as formalized by the fluctuation-dissipation theorem, is that the response of a system when near equilibrium is precisely related to the fluctuations that occur when the system is in total equilibrium. Essentially, a system that is slightly away from equilibrium—whether put there by external forces or by fluctuations—relaxes towards equilibrium in the same way, since the system cannot tell the difference or "know" how it came to be away from equilibrium.

This provides an indirect avenue for obtaining numbers such as ohmic conductivity and thermal conductivity by extracting results from equilibrium statistical mechanics. Since equilibrium statistical mechanics is mathematically well defined and (in some cases) more amenable for calculations, the fluctuation-dissipation connection can be a convenient shortcut for calculations in near-equilibrium statistical mechanics.

A few of the theoretical tools used to make this connection include:

An advanced approach uses a combination of stochastic methods and linear response theory. As an example, one approach to compute quantum coherence effects (weak localization, conductance fluctuations) in the conductance of an electronic system is the use of the Green-Kubo relations, with the inclusion of stochastic dephasing by interactions between various electrons by use of the Keldysh method.

The ensemble formalism also can be used to analyze general mechanical systems with uncertainty in knowledge about the state of a system. Ensembles are also used in:

In 1738, Swiss physicist and mathematician Daniel Bernoulli published "Hydrodynamica" which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.

In 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics. Maxwell also gave the first mechanical argument that molecular collisions entail an equalization of temperatures and hence a tendency towards equilibrium. Five years later, in 1864, Ludwig Boltzmann, a young student in Vienna, came across Maxwell's paper and spent much of his life developing the subject further.

Statistical mechanics proper was initiated in the 1870s with the work of Boltzmann, much of which was collectively published in his 1896 "Lectures on Gas Theory". Boltzmann's original papers on the statistical interpretation of thermodynamics, the H-theorem, transport theory, thermal equilibrium, the equation of state of gases, and similar subjects, occupy about 2,000 pages in the proceedings of the Vienna Academy and other societies. Boltzmann introduced the concept of an equilibrium statistical ensemble and also investigated for the first time non-equilibrium statistical mechanics, with his "H"-theorem.

The term "statistical mechanics" was coined by the American mathematical physicist J. Willard Gibbs in 1884. "Probabilistic mechanics" might today seem a more appropriate term, but "statistical mechanics" is firmly entrenched. Shortly before his death, Gibbs published in 1902 "Elementary Principles in Statistical Mechanics", a book which formalized statistical mechanics as a fully general approach to address all mechanical systems—macroscopic or microscopic, gaseous or non-gaseous. Gibbs' methods were initially derived in the framework classical mechanics, however they were of such generality that they were found to adapt easily to the later quantum mechanics, and still form the foundation of statistical mechanics to this day.




</doc>
<doc id="3445246" url="https://en.wikipedia.org/wiki?curid=3445246" title="Glossary of classical physics">
Glossary of classical physics

This article is a glossary of classical physics. It is some of the most common terms in classical physics and how they are used.




























</doc>
<doc id="48249441" url="https://en.wikipedia.org/wiki?curid=48249441" title="Phase stretch transform">
Phase stretch transform

Phase stretch transform (PST) is a computational approach to signal and image processing. One of its utilities is for feature detection and classification. PST is related to time stretch dispersive Fourier transform. It transforms the image by emulating propagation through a diffractive medium with engineered 3D dispersive property (refractive index). The operation relies on symmetry of the dispersion profile and can be understood in terms of dispersive eigenfunctions or stretch modes. PST performs similar functionality as phase-contrast microscopy, but on digital images. PST can be applied to digital images and temporal (time series) data.

Here the principle is described in the context of feature enhancement in digital images. The image is first filtered with a spatial kernel followed by application of a nonlinear frequency-dependent phase. The output of the transform is the phase in the spatial domain. The main step is the 2-D phase function which is typically applied in the frequency domain. The amount of phase applied to the image is frequency dependent, with higher amount of phase applied to higher frequency features of the image. Since sharp transitions, such as edges and corners, contain higher frequencies, PST emphasizes the edge information. Features can be further enhanced by applying thresholding and morphological operations. PST is related to warped or anamorphic stretch transform, another computational algorithm inspired by optical physics that performs nonuniform sampling and sparse coding.

Photonic time stretch technique can be understood by considering the propagation of an optical pulse through a dispersive fiber. By disregarding the loss and non-linearity in fiber, the non-linear Schrodinger equation governing the optical pulse propagation in fiber upon integration reduces to: 
<br>

formula_1 (1)

where formula_2 =GVD parameter, z is propagation distance, formula_3 is the reshaped output pulse at distance z and time t. The response of this dispersive element in the time-stretch system can be approximated as a phase propagator as presented in 
formula_4 (2)
<br>
Therefore, Eq. 1 can be written as following for a pulse that propagates through the time-stretch system and is reshaped into a temporal signal with a complex envelope given by 
<br>
formula_5 (3)
<br>
The time stretch operation is formulated as generalized phase and amplitude operations,
<br>
formula_6 (4)
<br>

where formula_7 is the phase filter and formula_8is the amplitude filter. Next the operator is converted to discrete domain, 
<br>
formula_9 (5)
<br>
where formula_10 is the discrete frequency, formula_11 is the phase filter, formula_12 is the amplitude filter and FFT is Fast Fourier Transform. 

The Stretch operator formula_13 for a digital image is then 
<br>
formula_14 (6)
<br>

In the above equations, formula_15 is the input image, formula_16 and formula_17 are the spatial variables, formula_18 is the two dimensional Fast Fourier Transform, and formula_19 and formula_20 are spatial frequency variables. The function formula_21 is the warped phase kernel and the function formula_22 is a localization kernel implemented in frequency domain. PST operator is defined as the phase of the Warped Stretch Transform output as follows 
<br>
<br>
formula_23 (7)
<br>
<br>
where formula_24is the angle operator. 
PST has been used for edge detection in biological and biomedical images as well as synthetic-aperture radar (SAR) image processing. PST has also been applied to improve the point spread function for single molecule imaging in order to achieve super-resolution. The transform exhibits intrinsic superior properties compared to conventional edge detectors for feature detection in low contrast visually impaired images.

The PST function can also be performed on 1-D temporal waveforms in the analog domain to reveal transitions and anomalies in real time.

On February 9, 2016, a UCLA Engineering research group has made public the computer code for PST algorithm that helps computers process images at high speeds and "see" them in ways that human eyes cannot. The researchers say the code could eventually be used in face, fingerprint, and iris recognition systems for high-tech security, as well as in self-driving cars' navigation systems or for inspecting industrial products.
Matlab and Python implementation for PST is available for free download from our Github Repository. The Matlab implementation for PST can also be downloaded from Matlab Files Exchange. However, it is provided for research purposes only, and a license must be obtained for any commercial applications. The software is protected under a US patent.


</doc>
<doc id="48583145" url="https://en.wikipedia.org/wiki?curid=48583145" title="Northwest Nuclear Consortium">
Northwest Nuclear Consortium

The Northwest Nuclear Consortium is an organization based in Washington state which uses a research grade ion collider to teach a class of high school students nuclear engineering principles based on the Department of Energy curriculum. They won the 1st Place at WSU Imagine Tomorrow in 2012. They also won the 1st place at the Washington State Science Fair, and the 2nd place worldwide at ISEF in 2013. In 2014 they won two 2nd place at the Central Sound Regional Science Fair at Bellevue College and they won 1st place twice in category at the Washington State Science & Engineering Fair at Bremerton. In 2015, they won 14 1st-place trophies at the Washington State Science and Engineering Fair, over $250,000 in scholarships at two different colleges and 3 of the 5 available trips to ISEF, where they won 4th place in the world against 72 countries.


</doc>
<doc id="24293838" url="https://en.wikipedia.org/wiki?curid=24293838" title="Wigner rotation">
Wigner rotation

In theoretical physics, the composition of two non-collinear Lorentz boosts results in a Lorentz transformation that is not a pure boost but is the composition of a boost and a rotation. This rotation is called Thomas rotation, Thomas–Wigner rotation or Wigner rotation. The rotation was discovered by Llewellyn Thomas in 1926, and derived by Wigner in 1939. If a sequence of non-collinear boosts returns an object to its initial velocity, then the sequence of Wigner rotations can combine to produce a net rotation called the Thomas precession.

There are still ongoing discussions about the correct form of equations for the Thomas rotation in different reference systems with contradicting results. Goldstein:
Einstein's principle of velocity reciprocity (EPVR) reads
With less careful interpretation, the EPVR is seemingly violated in some models. There is, of course, no true paradox present.

When studying the Thomas rotation at the fundamental level, one typically uses a setup with three coordinate frames, . Frame has velocity relative to frame , and frame has velocity relative to frame .

The axes are, by construction, oriented as follows. Viewed from , the axes of and are parallel (the same holds true for the pair of frames when viewed from .) Also viewed from , the spatial axes of and are parallel (and the same holds true for the pair of frames when viewed from .) This is an application of EVPR: If is the velocity of relative to , then is the velocity of relative to . The velocity makes the "same" angles with respect to coordinate axes in both the primed and unprimed systems. This does "not" represent a snapshot taken in any of the two frames of the combined system at any particular time, as should be clear from the detailed description below.

This is possible, since a boost in, say, the positive , preserves orthogonality of the coordinate axes. A general boost can be expressed as , where is a rotation taking the into the direction of and is a boost in the new . Each rotation retains the property that the spatial coordinate axes are orthogonal. The boost will stretch the (intermediate) by a factor , while leaving the and in place. The fact that coordinate axes are non-parallel in this construction after "two" consecutive non-collinear boosts is a precise expression of the phenomenon of Thomas precession.

The velocity of as seen in is denoted , where ⊕ refers to the relativistic addition of velocity (and not ordinary vector addition), given by

\frac{1}{c^2}\frac{\gamma_\mathbf{u}}{1+\gamma_\mathbf{u}} \mathbf u \cdot \mathbf v \right)\mathbf u +

and

which still completely defines the direction of the axis without loss of information.

The rotation is simply a "static" rotation and there is no relative rotational motion between the frames, there is relative translational motion in the boost. However, if the frames accelerate, then the rotated frame rotates with an angular velocity. This effect is known as the Thomas precession, and arises purely from the kinematics of successive Lorentz boosts.

In principle, it is pretty easy. Since every Lorentz transformation is a product of a boost and a rotation, the consecutive application of two pure boosts is a pure boost, either followed by or preceded by a pure rotation. Thus suppose

The task is to glean from this equation the boost velocity and the rotation from the matrix entries of . The coordinates of events are related by

Inverting this relation yields

or

Set Then will record the spacetime position of the origin of the primed system,

or

But

Multiplying this matrix with a pure rotation will not affect the zeroth columns and rows, and

which could have been anticipated from the formula for a simple boost in the -direction, and for the relative velocity vector

Thus given with , one obtains and by little more than inspection of . (Of course, can also be found using velocity addition per above.) From , construct . The solution for is then

With the ansatz

one finds by the same means

Finding a formal solution in terms of velocity parameters and involves first "formally" multiplying , formally inverting, then reading off form the result, "formally" building from the result, and, finally, formally multiplying . It should be clear that this is a daunting task, and it is difficult to interpret/identify the result as a rotation, though it is clear a priori that it is. It is these difficulties that the Goldstein quote at the top refers to. The problem has been thoroughly studied under simplifying assumptions over the years.

Another way to explain the origin of the rotation is by looking at the generators of the Lorentz group.

The passage from a velocity to a boost is obtained as follows. An arbitrary boost is given by

where is a triple of real numbers serving as coordinates on the boost subspace of the Lie algebra spanned by the matrices

The vector

is called the "boost parameter" or "boost vector", its norm is the rapidity. Here is the "velocity parameter", the magnitude of the vector . While for one has the is confined by , and hence . Thus

The set of velocities satisfying is an open ball in and is called the space of admissible velocities in the literature. It is endowed with a hyperbolic geometry described in the linked article.

The generators of boosts, in different directions do not commute. This has the effect that two consecutive boosts is not a pure boost in general, but a rotation preceding a boost.

Consider a succession of boosts in the x direction, then the y direction, expanding each boost to first order

then

and the group commutator is

Three of the commutation relations of the Lorentz generators are

where the bracket is a binary operation known as the "commutator", and the other relations can be found by taking cyclic permutations of x, y, z components (i.e. change x to y, y to z, and z to x, repeat).

Returning to the group commutator, the commutation relations of the boost generators imply for a boost along the x then y directions, there will be a rotation about the z axis. In terms of the rapidities, the rotation angle is given by

The familiar notion of vector addition for velocities in the Euclidean plane can be done in a triangular formation, or since vector addition is commutative, the vectors in both orderings geometrically form a parallelogram (see "parallelogram law"). This does not hold for relativistic velocity addition; instead a hyperbolic triangle arises whose edges are related to the rapidities of the boosts. Changing the order of the boost velocities, one does not find the resultant boost velocities to coincide.





</doc>
<doc id="33327002" url="https://en.wikipedia.org/wiki?curid=33327002" title="Cabbeling">
Cabbeling

Cabbeling is when two separate water parcels mix to form a third which sinks below both parents. The combined water parcel is denser than the original two water parcels.

The two parent water parcels may have the same density, but they have different properties; for instance, different salinities and temperatures. Seawater almost always gets more dense if it gets either slightly colder or slightly saltier. But medium-warm, medium-salty water can be denser than both fresher, colder water and saltier, warmer water; in other words, the equation of state for seawater is monotonic, but non-linear. See diagram.

Cabbeling may also occur in fresh water, since pure water is densest at about 4 °C (39 °F). A mixture of 1 °C water and 6 °C water, for instance, might have a temperature of 4 °C, making it denser than either parent. Ice is also less dense than water, so although ice floats in warm water, meltwater sinks in warm water.

The densification of the new mixed water parcel is a result of a slight contraction upon mixing; a decrease in volume of the combined water parcel. A new water parcel that has the same mass, but is lower in volume, will be denser. Denser water sinks or downwells in the otherwise neutral surface of the water body, where the two initial water parcels originated.

The importance of this process in oceanography was first pointed out by Witte, in a 1902 publication ().

The German origin of the term has caused some etymological confusion and disagreements as to the correct spelling of the term; for details, see the Wiktionary entry on cabelling. Oceanographers generally follow Stommel and refer to the process as "cabbeling".

Cabbeling may occur in high incidence in high latitude waters. Polar region waters are a place where cold and fresh water melting from sea ice meets warmer, saltier water. Ocean currents are responsible for bringing this warmer, saltier water to higher latitudes, especially on the eastern shores of Northern Hemisphere continents, and on the western shores of Southern Hemisphere continents. The phenomenon of cabbeling has been particularly noted in the Weddell Sea and the Greenland Sea.


</doc>
<doc id="49226157" url="https://en.wikipedia.org/wiki?curid=49226157" title="Weak gravity conjecture">
Weak gravity conjecture

The weak gravity conjecture (WGC) is a conjecture regarding the strength gravity can have in a theory of quantum gravity relative to the gauge forces in that theory. It roughly states that gravity should be the weakest force in any consistent theory of quantum gravity.



</doc>
<doc id="46545942" url="https://en.wikipedia.org/wiki?curid=46545942" title="Solid light">
Solid light

Solid light is a hypothetical material, made of light in a solidified state. Theoretically, it is possible to make such a material, and there are claims this material was already made, including claims from MIT and Harvard.

In theory, photons, the particles that make light, could be attracted in a nonlinear medium. In test, a laser beam fired into an extremely cold cloud of rubidium slowed down the photons and made them act as a single entity.

Solid light appears in many video game franchises, including "Halo", "Portal", "Mass Effect" and "Overwatch". It is also portrayed in "The Lightbringer" series by fantasy author Brent Weeks.
It also appears in Dr. Strange. The Gems, a fictional alien race featured in "Steven Universe", are gemstones that project physical bodies made of solid light.


</doc>
<doc id="42906061" url="https://en.wikipedia.org/wiki?curid=42906061" title="Total position spread">
Total position spread

In physics, the total position-spread (TPS) tensor is a quantity originally introduced in the modern theory of electrical conductivity. In the case of molecular systems, this tensor measures the fluctuation of the electrons around their mean positions, which corresponds to the delocalization of the electronic charge within a molecular system. The TPS can discriminate between metals and insulators taking information from the ground state wave function. This quantity can be very useful as an indicator to characterize Intervalence charge transfer processes, the bond nature of molecules (covalent, ionic or weakly bonded), and Metal–insulator transition.

The Localization Tensor (LT) is a "per electron" quantity proposed in the context of the theory of Kohn to characterize electrical conductivity properties. In 1964, Kohn realized that electrical conductivity is more related to the proper delocalization of the wave function than a simple band gap. In fact, he proposed that a qualitative difference between insulators and conductors also manifests as a different organization of the electrons in their ground state where one has that: the wave function is strongly localized in insulators and very delocalized in conductors.

The interesting outcome of this theory is: "i)" it relates the classical idea of localized electrons as a cause of insulating state; "ii)" the needed information can be recovered from the ground state wave function because in the insulated regime the wave function breaks down as a sum of disconnected terms.

It is until 1999 that Resta and coworkers found a way to define the Kohn delocalization by proposing the already mentioned Localization Tensor. The LT is defined as a second order moment cumulant of the position operator divided by the number of electrons in the system. The key property of the LT is that: it diverges for metals while it takes finite values for insulators in the Thermodynamic limit.

Recently, the global quantity (the LT not divided by the number of electrons) has been introduced to study molecules and named Total Position-Spread tensor.

The total position spread Λ is defined as the second moment cumulant of the total electron position operator, and its units are in length square (e.g. bohr²). In order to compute this quantity, one has to take into account the position operator and its tensorial square. For a system of "n" electrons, the position operator and its Cartesian components are defined as:

Where the "i" index runs over the number of electrons. Each component of the position operator is a one-electron operator, they can be represented in second quantization as follows:

where "i","j" run over orbitals. The expectation values of the position components are the first moments of the electrons' position.

Now we consider the tensorial square (second moment). In this sense, there are two types of them:



The second moment of the position becomes then the sum of the one- and two-electron operators already defined:

Given a "n"-electron wave function formula_10, one wants to compute the "second moment cumulant" of it. A cumulant is a linear combination of moments so we have:

The position operator can be partitioned according to spin components.

From the one-particle operator it is possible to define the total spin-partitioned position operator as:

Therefore, the total position operator formula_14 can be expressed by the sum of the two spin parts formula_15 and formula_16:

and the square of the total position operator decomposes as: 
Thus, there are four joint second moment cumulant of the spin-partitioned position operator: 

The Hubbard model is a very simple and approximate model employed in Condensed matter physics to describe the transition of materials from metals to insulators. It takes into account only two parameters: "i)" the kinetic energy or hopping integral denoted by "-t"; and "ii)" the on-site repulsion between electrons represented by "U" (see the ).

In Figure 1, there are two limit cases to consider: larger values of "-t/U" representing a strong charge fluctuation (electrons free to move) whereas for small values of "-t/U" the electrons are completely localized. The SS-TPS is very sensitive to these changes, because it increases faster than linearly when electrons start to present mobility (0.0 to 0.5 range of "-t/U").

The TPS is a powerful tool to monitor the wave function. In Figure 3 is shown the longitudinal SS-TPS (Λ) computed at full configuration interaction level for the H diatomic molecule. The Λ in the high repulsive region shows a value that is lower than in the asymptotic limit. This is a consequence of nuclei being near to each other's causing and enhancement of the effective nuclear charge that makes electrons to be more localized. When stretching the bond, the TPS starts growing until it reaches a maximum (strong delocalization of the wave function) before the bond is broken. Once the bond is broken, the wave function becomes a sum of disconnected localized regions and the tensor decreases until it reaches twice the value of the atomic limit (1 bohr² for each hydrogen atom).

When the total position-spread tensor is partitioned according to spin (SP-TPS), it becomes a powerful tool to describe spin delocalization in the insulating regime. In Figure 4 is shown the longitudinal SP-TPS (Λ) computed at full configuration interaction level for the H diatomic molecule. The horizontal line at 0 bohr divides the same spin (positive values) and different spin (negative values) contributions of the spin partitioned TPS. Unlike the SS-TPS that saturates to the atomic value for R>5, the SP-TPS diverges as R indicating that there is a strong spin delocalization. The SP-TPS can also be seen as a measure of how strong the electron correlation is.

The TPS is a cumulant and thus it possesses the following properties:



</doc>
<doc id="49885288" url="https://en.wikipedia.org/wiki?curid=49885288" title="Dirac membrane">
Dirac membrane

A model of a charged membrane introduced by Paul Dirac in 1962. Dirac's original motivation was to explain the mass of the muon as an excitation of the ground state corresponding to an electron. Anticipating the birth of string theory by almost a decade, he was the first to introduce what is now called a type of Nambu–Goto action for membranes.

In the Dirac membrane model the repulsive electromagnetic forces on the membrane are balanced by the contracting ones coming from the positive tension. In the case of the spherical membrane, classical equations of motion imply that the balance is met for the radius formula_1, where formula_2 is the classical electron radius. Using Bohr–Sommerfeld quantisation condition for the Hamiltonian of the spherically symmetric membrane, Dirac finds the approximation of the mass corresponding to the first excitation as formula_3, where formula_4 is the mass of the electron, which is about a quarter of the observed muon mass.

Dirac chose a non-standard way to formulate the action principle for the membrane. Because closed membranes in formula_5 provide a natural split of space into the interior and the exterior there exists a special curvilinear system of coordinates formula_6 in spacetime and a function formula_7 such that 
- formula_8 defines a membrane

- formula_9, formula_10 describe a region outside or inside the membrane
Choosing formula_11 and the following gauge formula_12, formula_13, formula_14 
where formula_15, ( formula_16) is the internal parametrization of the membrane word-volume, the membrane action proposed by Dirac is

where the induced metric and the factors J and M are given by

In the above formula_21 are rectilinear and orthogonal. The space-time signature used is (+,-,-,-). Note that formula_22 is just a usual action for the electromagnetic field in a curvilinear system while formula_23is the integral over the membrane world-volume i.e. precisely the type of the action used later in string theory.

There are 3 equations of motion following from the variation with respect to formula_24 and formula_25. They are:
- variation w.r.t. formula_24 for formula_27 - this results in sourceless Maxwell equations 
- variation w.r.t. formula_21 for formula_27 - this gives a consequence of Maxwell equations
- variation w.r.t. formula_21 for formula_31

The last equation has a geometric interpretation: the r.h.s. is proportional to the curvature of the membrane. For the spherically symmetric case we get

Therefore, the balance condition formula_34 implies formula_35 where formula_36 is the radius of the balanced membrane. The total energy for the spherical membrane with radius formula_37 is
and it is minimal in the equilibrium for formula_39, hence formula_40. On the other hand, the total energy in the equilibrium should be formula_4 (in formula_42 units)
and so we obtain formula_43.

Small oscillations about the equilibrium in the spherically symmetric case imply frequencies - formula_44. Therefore, going to quantum theory, the energy of one quantum would be formula_45.
This is much more than the muon mass but the frequencies are by no means small so this approximation may not work properly. To get a better quantum theory one needs to work out the Hamiltonian of the system and solve the corresponding Schroedinger equation.

For the Hamiltonian formulation Dirac introduces generalised momenta

- for formula_46: formula_47 and formula_48 - momenta conjugate to formula_24 and formula_50 respectively (formula_51, coordinate choice formula_52)

- for formula_53: formula_54 - momenta conjugate to formula_50

Then one notices the following constraints

- for the Maxwell field 

- for membrane momenta

where formula_58 - reciprocal of formula_59, formula_60.

These constraints need to be included when calculating the Hamiltonian, using the Dirac bracket method. 
The result of this calculation is the Hamiltonian of the form
where formula_63 is the Hamiltonian for the electromagnetic field written in the curvilinear system.

For spherically symmetric motion the Hamiltonian is 

however the direct quantisation is not clear due to the square-root of the differential operator. To get any further Dirac considers the Bohr - Sommerfeld method:
and finds formula_66 for formula_67.


P. A. M. Dirac, An Extensible Model of the Electron, Proc. Roy. Soc. A268, (1962) 57–67.


</doc>
<doc id="50013014" url="https://en.wikipedia.org/wiki?curid=50013014" title="Hopkinson effect">
Hopkinson effect

The Hopkinson effect is a feature of ferromagnetic or ferrimagnetic materials, in which an increase in magnetic susceptibility is observed at temperatures between the blocking temperature and the Curie temperature of the material. The Hopkinson effect can be observed as a peak in thermomagnetic curves that immediately precedes the susceptibility drop associated with the Curie temperature. It was first observed by John Hopkinson in 1889 in a study on iron.

In single domain particles, a large Hopkinson peak results from a transient superparamagnetic particle domain state.


</doc>
<doc id="676502" url="https://en.wikipedia.org/wiki?curid=676502" title="Rogue wave">
Rogue wave

Rogue waves (also known as freak waves, monster waves, episodic waves, killer waves, extreme waves, and abnormal waves) are large, unexpected and suddenly appearing surface waves that can be extremely dangerous, even to large ships such as ocean liners.

Rogue waves present considerable danger for several reasons: they are rare, unpredictable, may appear suddenly or without warning, and can impact with tremendous force. A wave in the usual "linear" wave model would have a breaking pressure of . Although modern ships are designed to tolerate a breaking wave of , a rogue wave can dwarf both of these figures with a breaking pressure of .

In oceanography, rogue waves are more precisely defined as waves whose height is more than twice the significant wave height ("H" or SWH), which is itself defined as the mean of the largest third of waves in a wave record. Therefore, rogue waves are not necessarily the biggest waves found on the water; they are, rather, unusually large waves for a given sea state. Rogue waves seem not to have a single distinct cause, but occur where physical factors such as high winds and strong currents cause waves to merge to create a single exceptionally large wave.

Rogue waves can occur in media other than water. They appear to be ubiquitous in nature and have also been reported in liquid helium, in nonlinear optics and in microwave cavities. Recent research has focused on optical rogue waves which facilitate the study of the phenomenon in the laboratory. A 2015 paper studied the wave behavior around a rogue wave, including optical, and the Draupner wave, and concluded that "rogue events do not necessarily appear without a warning, but are often preceded by a short phase of relative order". The physical origin of a rogue wave can be defined by the rogue wave theorem.

Rogue waves are an open water phenomenon, in which winds, currents, non-linear phenomena such as solitons, and other circumstances cause a wave to briefly form that is far larger than the "average" large occurring wave (the significant wave height or 'SWH') of that time and place. The basic underlying physics that makes phenomena such as rogue waves possible is that different waves can travel at different speeds, and so they can "pile up" in certain circumstances – known as "constructive interference". (In deep ocean the speed of a gravity wave is proportional to the square root of its wavelength—the distance peak-to-peak.) However other situations can also give rise to rogue waves, particularly situations where non-linear effects or instability effects can cause energy to move between waves and be concentrated in one or very few extremely large waves before returning to "normal" conditions.

Once considered mythical and lacking hard evidence for their existence, rogue waves are now proven to exist and known to be a natural ocean phenomenon. Eyewitness accounts from mariners and damage inflicted on ships have long suggested they occurred. The first scientific evidence of the existence of rogue waves came with the recording of a rogue wave by the Gorm platform in the central North Sea in 1984. A stand-out wave was detected with a wave height of in a relatively low sea state. However, the wave that caught the attention of the scientific community was the digital measurement of the "Draupner wave", a rogue wave at the Draupner platform in the North Sea on January 1, 1995, with a maximum wave height of (peak elevation of ) . During that event, minor damage was also inflicted on the platform, far above sea level, confirming that the reading was valid.

Their existence has also since been confirmed by video and photographs, and satellite imagery and radar of the ocean surface, by stereo wave imaging systems, by pressure transducers on the sea-floor and notably by oceanographic research vessels. In February 2000, a British oceanographic research vessel, the RRS "Discovery", sailing in the Rockall Trough west of Scotland encountered the largest waves ever recorded by scientific instruments in the open ocean, with a SWH of and individual waves up to . "In 2004 scientists using three weeks of radar images from European Space Agency satellites found ten rogue waves, each or higher."

A rogue wave is a natural ocean phenomenon that is not caused by land movement, only lasts briefly, occurs in a limited location, and most often happens far out at sea. Rogue waves are considered rare but potentially very dangerous, since they can involve the spontaneous formation of massive waves far beyond the usual expectations of ship designers, and can overwhelm the usual capabilities of ocean-going vessels which are not designed for such encounters. Rogue waves are therefore distinct from tsunamis. Tsunamis are caused by massive displacement of water, often resulting from sudden movement of the ocean floor, after which they propagate at high speed over a wide area. They are nearly unnoticeable in deep water and only become dangerous as they approach the shoreline and the ocean floor becomes shallower; therefore tsunamis do not present a threat to shipping at sea (the only ships lost in the 2004 Asian tsunami were in port). They are also distinct from megatsunamis, which are single massive waves caused by sudden impact, such as meteor impact or landslides within enclosed or limited bodies of water. They are also different from the waves described as "hundred-year waves", which is a purely statistical prediction of the highest wave likely to occur in a hundred-year period in a particular body of water.

Rogue waves have now been proven to be the cause of the sudden loss of some ocean-going vessels. Well documented instances include the freighter MS "München", lost in 1978 and the MV Derbyshire lost in 1980, the largest British ship ever lost at sea. A rogue wave has been implicated in the loss of other vessels including the Ocean Ranger which was a semi-submersible mobile offshore drilling unit that sank in Canadian waters on 15 February 1982. In 2007 the US National Oceanic and Atmospheric Administration compiled a catalogue of more than 50 historical incidents probably associated with rogue waves.

In 1826, French scientist and naval officer Captain Jules Dumont d'Urville reported waves as high as in the Indian Ocean with three colleagues as witnesses, yet he was publicly ridiculed by fellow scientist François Arago. In that era it was widely held that no wave could exceed . Author Susan Casey wrote that much of that disbelief came because there were very few people who had seen a rogue wave, and until the advent of steel double-hulled ships of the 20th century "people who encountered 100-foot rogue waves generally weren't coming back to tell people about it."

For almost 100 years, oceanographers, meteorologists, engineers and ship designers have used a mathematical system commonly called the Gaussian function (or Gaussian Sea or standard linear model) to predict wave height. This model assumes that waves vary in a regular way around the average (so-called 'significant') wave height. In a storm sea with a significant wave height of , the model suggests there will hardly ever be a wave higher than . One of could indeed happen – but only once in ten thousand years (of wave height of ). This basic assumption was well accepted (and acknowledged to be an approximation). The use of a Gaussian form to model waves has been the sole basis of virtually every text on that topic for the past 100 years.

The first known scientific article on "Freak waves" was written by Professor Laurence Draper in 1964. In that paper which has been described as a 'seminal article' he documented the efforts of the National Institute of Oceanography in the early 1960s to record wave height and the highest wave recorded at that time which was about . Draper also described "freak wave holes".

In 1995, strong scientific evidence for the existence of rogue waves came with the recording of what has become known as the Draupner wave. The Draupner E is one structure in a gas pipeline support complex operated by Statoil about offshore and west by southwest from the southern tip of Norway. The Draupner E platform is the first major oil platform of the jacket-type attached to the seabed with a bucket foundation instead of pilings and a suction anchoring system. As a precaution, the operator (Statoil) fitted the platform with an extensive array of instrumentation. The instruments continuously check the platform's movements in particular any movement of the foundations during storm events. The state-of-the-art instrumentation fitted to the platform was able to continuously measure seven key parameters:

The rig was built to withstand a calculated 1 in 10,000 years wave with a predicted height of and was also fitted with state-of-the-art laser wave recorder on the platform’s underside. At 3 p.m. on 1 January 1995 it recorded an rogue wave (i.e. taller than the predicted 10,000 year wave) that hit the rig at . This was the first confirmed measurement of a freak wave, more than twice as tall and steep as its neighbors with characteristics that fell outside any known wave model. The wave was recorded by all of the sensors fitted to the platform and it caused enormous interest in the scientific community.

Statoil researchers presented a paper in 2000 which collated evidence that freak waves were not the rare realizations of a typical or slightly non-gaussian sea surface population ("Classical" extreme waves) but rather they were the typical realizations of a rare and strongly non-gaussian sea surface population of waves ("Freak" extreme waves). A workshop of leading researchers in the world attended the first Rogue Waves 2000 workshop held in Brest in November 2000.

In 2000 the British oceanographic vessel RRS "Discovery" recorded a wave off the coast of Scotland near Rockall. This was a scientific research vessel and was fitted with high quality instruments. The subsequent analysis determined that under severe gale force conditions with wind speeds averaging a ship-borne wave recorder measured individual waves up to from crest to trough, and a maximum significant wave height of . These were some of the largest waves recorded by scientific instruments up to that time. The authors noted that modern wave prediction models are "known" to significantly under-predict extreme sea states for waves with a 'significant' height (H) above . The analysis of this event took a number of years, and noted that "none of the state-of-the-art weather forecasts and wave models—the information upon which all ships, oil rigs, fisheries, and passenger boats rely—had predicted these behemoths." Put simply, a scientific model (and also ship design method) to describe the waves encountered did not exist. This finding was widely reported in the press which reported that "according to all of the theoretical models at the time under this particular set of weather conditions waves of this size should not have existed".

Most popular texts on oceanography up until the mid 1990s such as that by Pirie made no mention of rogue or freak waves. The popular text on Oceanography by Gross (1996) only gave rogue waves a mention and stated that "Under extraordinary circumstances unusually large waves called rogue waves can form" without providing any further detail. From about 1997 most leading authors acknowledged the existence of rogue waves with the caveat that wave models had been unable to replicate rogue waves. The first scientific research which comprehensively proved that waves exist that are clearly outside the range of Gaussian waves was published in 1997. Some research confirms that observed wave height distribution in general follows well the Rayleigh distribution, but in shallow waters during high energy events, extremely high waves are more rare than this particular model predicts.

It is now proven via satellite radar studies that waves with crest to trough heights of to , occur far more frequently than previously thought. It is now known that rogue waves occur in all of the world's oceans many times each day. In 2004 the ESA MaxWave project identified more than ten individual giant waves above in height during a short survey period of three weeks in a limited area of the South Atlantic. The ESA's ERS satellites have helped to establish the widespread existence of these 'rogue' waves.

Thus acknowledgement of the existence of rogue waves (despite the fact that they cannot plausibly be explained by even state-of-the-art wave statistics) is a very modern scientific paradigm. It is now well accepted that rogue waves are a common phenomenon. Professor Akhmediev of the Australian National University, one of the world's leading researchers in this field, has stated that there are about 10 rogue waves in the world's oceans at any moment. Some researchers have speculated that approximately three of every 10,000 waves on the oceans achieve rogue status, yet in certain spots—like coastal inlets and river mouths—these extreme waves can make up three out of every 1,000 waves because wave energy can be focused.

Rogue waves may also occur in lakes. A phenomenon known as the "Three Sisters" is said to occur in Lake Superior when a series of three large waves forms. The second wave hits the ship's deck before the first wave clears. The third incoming wave adds to the two accumulated backwashes and suddenly overloads the ship deck with tons of water. The phenomenon is one of various theories as to the cause of the sinking of the SS Edmund Fitzgerald on Lake Superior in November 1975.

Serious studies of the phenomenon of rogue waves only started about 20–30 years ago and have intensified since about 2005. One of the remarkable features of the rogue waves is that they always appear from nowhere and quickly disappear without a trace. Recent research has suggested that there could also be 'super-rogue waves' which are up to five times the average sea-state. Rogue waves has now become a near universal term given by scientists to describe isolated large amplitude waves, that occur more frequently than expected for normal, Gaussian distributed, statistical events. Rogue waves appear to be ubiquitous in nature and are not limited to the oceans. They appear in other contexts and have recently been reported in liquid helium, in nonlinear optics and in microwave cavities. It is now universally accepted by marine researchers that these waves belong to a specific kind of sea wave, not taken into account by conventional models for sea wind waves.

Researchers at the Australian National University have also recently (2012) proven the existence of "rogue wave holes", an inverted profile of a rogue wave. In maritime folk-lore, stories of rogue holes are as common as stories of rogue waves. They follow from theoretical analysis but had never been proven experimentally. In 2012 the ANU published research confirming the existence of rogue wave holes on the water surface observed in a water wave tank.

On a smaller scale, kayakers call unpredictable 'exploding waves' caused by wave interaction "clapotis".

There are a number of research programmes currently underway focussed on rogue waves including:


Because the phenomenon of rogue waves is still a matter of active research, it is premature to state clearly what the most common causes are or whether they vary from place to place. The areas of highest predictable risk appear to be where a strong current runs counter to the primary direction of travel of the waves; the area near Cape Agulhas off the southern tip of Africa is one such area; the warm Agulhas Current runs to the southwest, while the dominant winds are westerlies. However, since this thesis does not explain the existence of all waves that have been detected, several different mechanisms are likely, with localized variation. Suggested mechanisms for freak waves include the following:






The spatio-temporal focusing seen in the NLS equation can also occur when the nonlinearity is removed. In this case, focusing is primarily due to different waves coming into phase, rather than any energy transfer processes. Further analysis of rogue waves using a fully nonlinear model by R. H. Gibbs (2005) brings this mode into question, as it is shown that a typical wavegroup focuses in such a way as to produce a significant wall of water, at the cost of a reduced height.

A rogue wave, and the deep trough commonly seen before and after it, may last only for some minutes before either breaking, or reducing in size again. Apart from one single rogue wave, the rogue wave may be part of a wave packet consisting of a few rogue waves. Such rogue wave groups have been observed in nature.

There are three categories of freak waves:

The possibility of the artificial stimulation of rogue wave phenomena has attracted research funding from DARPA, an agency of the United States Department of Defense. Bahram Jalali and other researchers at UCLA studied microstructured optical fibers near the threshold of soliton supercontinuum generation and observed rogue wave phenomena. After modelling the effect, the researchers announced that they had successfully characterized the proper initial conditions for generating rogue waves in any medium. Additional works carried out in optics have pointed out the role played by a nonlinear structure called Peregrine soliton that may explain those waves that appear and disappear without leaving a trace.

It should be noted that many of these encounters are only reported in the media, and are not examples of open ocean rogue waves. Often, in popular culture, an endangering huge wave is loosely denoted as a "rogue wave", while it has not been (and most often cannot be) established that the reported event is a rogue wave in the scientific sense — "i.e." of a very different nature in characteristics as the surrounding waves in that sea state and with very low probability of occurrence (according to a Gaussian process description as valid for linear wave theory).

This section lists a limited selection of notable incidents.




The loss of the MS München in 1978 provided some of the first physical evidence of the existence of rogue waves. The MS München was a state-of-the-art cargo ship with multiple water-tight compartments, an expert crew and was considered unsinkable. She was lost with all crew and the wreck has never been found. The only evidence found was the starboard lifeboat which was recovered from floating wreckage some time later. The lifeboats hung from forward and aft blocks above the waterline. The pins had been bent back from forward to aft, indicating the lifeboat hanging below it had been struck by a wave that had run from fore to aft of the ship which had torn the lifeboat from the ship. To exert such force the wave must have been considerably higher than . At the time of the inquiry, the existence of rogue waves was considered so statistically unlikely as to be near impossible. Consequently, the Maritime Court investigation concluded that the severe weather had somehow created an 'unusual event' that had led to the sinking of the München.

The 1980 loss of the MV "Derbyshire" during Typhoon Orchid south of Japan with the loss of all crew marked a turning point for ship design. The "Derbyshire" was an ore-bulk-oil combination carrier built in 1976. At 91,655 gross register tons, she was—and remains—the largest British ship ever to have been lost at sea. The wreck was found in June 1994. The survey team deployed a remotely operated vehicle to photograph the wreck. A private report was published in 1998 which prompted the British government to reopen a formal investigation into the sinking. The British government investigation included a comprehensive survey by the Woods Hole Oceanographic Institution which took 135,774 pictures of the wreck during two surveys. The formal forensic investigation concluded that the ship sank because of structural failure and absolved the crew of any responsibility. Most notably, the report determined the detailed sequence of events that led to the structural failure of the vessel. A third comprehensive analysis was subsequently done by Douglas Faulkner, professor of marine architecture and ocean engineering at the University of Glasgow. His highly analytical and scientific report published in 2001 examined and linked the loss of the MV "Derbyshire" with what he called the emerging body of scientific evidence regarding the mechanics of freak waves. Professor Faulkner concluded that it was almost certain that "Derbyshire" would have encountered a wave of sufficient size to destroy her. Faulkner's conclusions have not been refuted in the more than 15 years since they were first presented (as of 2016). Indeed, subsequent analysis by others has corroborated his findings. Faulkner's finding that the "Derbyshire" was lost because of a rogue wave has had widespread implications on ship design. Faulkner has subsequently proposed the need for a paradigm shift in thinking for the design of ships and offshore installations to include what he calls a Survival Design approach additional to current design requirements. There is however no evidence that his recommendations have yet been adopted (as of 2016).
In 2004 an extreme wave was recorded impacting the Admiralty Breakwater, Alderney in the Channel Islands. This breakwater is exposed to the Atlantic Ocean. The peak pressure recorded by a shore-mounted transducer was 745 kPa which corresponds to a pressure of 74.5 tonnes/m2 or 74.5 Mt/m (metric tonnes per square metre). This pressure far exceeds almost any design criteria for modern ships and this wave would have destroyed almost any merchant vessel.

More recent work by Smith in 2007 confirmed prior forensic work by Faulkner in 1998 and determined that the MV "Derbyshire" was exposed to a hydrostatic pressure of a "static head" of water of about with a resultant static pressure of 201 kN/m. This is in effect 20 metres of green water (possibly a "super rogue wave") flowing over the vessel. The deck cargo hatches on the "Derbyshire" were determined to be the key point of failure when the rogue wave washed over the ship. The design of the hatches only allowed for a static pressure of less than two metres of water or 17.1 kN/m, in other words the typhoon load on the hatches was more than ten (10) times the design load. The forensic structural analysis of the wreck of the "Derbyshire" is now widely regarded as irrefutable.

In addition fast moving waves are now known to "also" exert extremely high dynamic pressure. It is known that plunging or breaking waves can cause short-lived impulse pressure spikes called Gifle peaks. These can reach pressures of 200 kN/m (or more) for milliseconds which is sufficient pressure to lead to brittle fracture of mild steel. Evidence of failure by this mechanism was also found on the "Derbyshire". Smith has documented scenarios where hydrodynamic pressure of up to 5,650 kN/m or over 500 metric tonnes per square metre could occur.

Very few ship-wrecks have ever been fully investigated. The most recent bulk-carrier loss on the open seas to have been subjected to thorough investigation (as at March 2011) was the UK-owned M.V. "Derbyshire", which sank in 1980. Its entire crew of forty-four, all British citizens, perished. It took 14 years of pressure from the British public and a privately funded expedition to locate the wreck before a formal remote-camera search and investigation was done by the British government. At least a couple of hundred bulk carriers have been lost since 1980 and none have been properly investigated. A survey of 125 bulk carriers that sank between 1963 and 1996 found that seventy-six "probably" flooded, another four because of hatch-cover failure, the rest from "unidentified" causes. Nine other vessels broke completely in two. Causes of the remaining forty losses are unknown.
Montgomery-Swan has outlined the generic mechanism of ship failure when encountering a rogue wave:
The scenario is very simple: the weight of the ship accelerates her down the back slope of the previous wave, the bow sticks into the lower part of the front of the giant incoming wave, and thousands of tons of green water fall onto the fore part of the ship. What happens next depends on the structure of the vessel.
Professor Faulkner who did the forensic independent analysis of the loss of the M.V. "Derbyshire" explains why this is such a problem for bulk carriers. He states that "It is quite possible that some of the many unexplained heavy weather losses (of bulk carriers) may have been caused by hatch cover or coaming failures because fore end plunging due to flooding of large holds can be rapid." He noted in his report that "because of their high inertias and natural pitch periods, these large ships do not rise to the waves, as appropriately experienced masters have confirmed. They tend to bury into them." Faulkner concluded that "beyond any reasonable doubt, the direct cause of the loss of the M.V. "Derbyshire" was the quite inadequate strength of her cargo hatch covers to withstand the forces of Typhoon Orchid." He also noted that "It is not possible to say which of the eighteen covers failed first, or from which direction the waves came; but evidence and other arguments suggest that the no. 1 hatch covers were probably the first to yield, probably from waves over the bow with the ship hove-to."

In November 1997 the International Maritime Organization (IMO) adopted new rules covering survivability and structural requirements for bulk carriers of and upwards. The bulkhead and double bottom must be strong enough to allow the ship to survive flooding in hold one unless loading is restricted.

It is now widely held that rogue waves present considerable danger for several reasons: they are rare, unpredictable, may appear suddenly or without warning, and can impact with tremendous force. A wave in the usual "linear" model would have a breaking force of . Although modern ships are designed to (typically) tolerate a breaking wave of 15 MT/m, a rogue wave can dwarf both of these figures with a breaking force far exceeding 100 MT/m. Smith has presented calculations using the International Association of Classification Societies (IACS) Common Structural Rules (CSR) for a typical bulk carrier which are consistent.

Peter Challenor, a leading scientist in this field from the National Oceanography Centre in the United Kingdom was quoted in Casey's book in 2010 that "We don’t have that random messy theory for nonlinear waves. At all", he says. "People have been working actively on this for the past 50 years at least. We don’t even have the start of a theory".

In 2006 Smith proposed that the International Association of Classification Societies (IACS) recommendation 34 pertaining to standard wave data be modified so that the minimum design wave height be increased to . He presented analysis that there was sufficient evidence to conclude that high waves can be experienced in the 25-year lifetime of oceangoing vessels, and that high waves are less likely, but not out of the question. Therefore, a design criterion based on high waves seems inadequate when the risk of losing crew and cargo is considered. Smith has also proposed that the dynamic force of wave impacts should be included in the structural analysis.

It is notable that the Norwegian offshore standards now take into account extreme severe wave conditions and require that a 10,000-year wave does not endanger the ships integrity. Rosenthal notes that as at 2005 rogue waves were not explicitly accounted for in Classification Societies’ Rules for ships’ design. As an example, DNV GL, one of the world's largest international certification body and classification society with main expertise in technical assessment, advisory, and risk management publishes their Structure Design Load Principles which remain largely based on the 'Significant Wave height' and as at January 2016 still has not included any allowance for rogue waves.

The U.S. Navy historically took the design position that the largest wave likely to be encountered was 21.4 m (70 ft). Smith observed in 2007 that the navy now believes that larger waves can occur and the possibility of extreme waves that are steeper (i.e. do not have longer wavelengths) is now recognized. The navy has not had to make any fundamental changes in ship design as a consequence of new knowledge of waves greater than 21.4 m (70 ft) because they build to higher standards.

A characteristic of the shipping industry is that there are no uniform codes or international standards. There are more than 50 classification societies worldwide, each has different rules. Ship design has historically largely been led by the ship insurers who inspected, classified and insured vessels. Hence the widespread adoption of new rules to allow for the existence of rogue waves is likely to take many years.






</doc>
<doc id="3591456" url="https://en.wikipedia.org/wiki?curid=3591456" title="Interface (matter)">
Interface (matter)

In the physical sciences, an interface is the boundary between two spatial regions occupied by different matter, or by matter in different physical states. The interface between matter and air, or matter and vacuum, is called a surface, and studied in surface science. In thermal equilibrium, the regions in contact are called phases, and the interface is called a phase boundary. An example for an interface out of equilibrium is the grain boundary in polycrystalline matter.

The importance of the interface depends on the type of system: the bigger the quotient area/volume, the greater the effect the interface will have. Consequently, interfaces are very important in systems with large interface area-to-volume ratios, such as colloids.

Interfaces can be flat or curved. For example, oil droplets in a salad dressing are spherical but the interface between water and air in a glass of water is mostly flat.

Surface tension is the physical property which rules interface processes involving liquids. For a liquid film on flat surfaces, the liquid-vapor interface keeps flat to minimize interfacial area and system free energy. For a liquid film on rough surfaces, the surface tension tends to keep the meniscus flat, while the disjoining pressure makes the film conformal to the substrate. The equilibrium meniscus shape is a result of the competition between the capillary pressure and disjoining pressure.

Interfaces may cause various optical phenomena, such as refraction. Optical lenses serve as an example of a practical application of the interface between glass and air.

One topical interface system is the gas-liquid interface between aerosols and other atmospheric molecules.



</doc>
<doc id="50561501" url="https://en.wikipedia.org/wiki?curid=50561501" title="Transparent wood composites">
Transparent wood composites

Transparent wood composites are novel wood materials which have up to 90% transparency and higher mechanical properties than wood itself, made for the first time in 1992.

When these materials are commercially available, a significant benefit is expected due to their inherent biodegradable properties since it is wood. These materials are significantly more biodegradable than glass and plastics.. Transparent wood it is also shatterproof. On the other hand, concerns may be relevant due to the use of non-biodegradable plastics for long lasting purpose, such as in building.

A research group led by Professor Lars Berglund from Swedish KTH University along with a University of Maryland research group led by Professor Liangbing Hu have developed a method to remove the color and some chemicals from small blocks of wood, followed by adding polymers, such as Poly(methyl methacrylate) and epoxy, at the cellular level, thereby rendering them transparent.

As soon as released in between 2015 and 2016, see-through wood had a large press reaction, with articles in ScienceDaily, Wired, the Wall Street Journal, the New York Times, to name a few.

Actually those research groups rediscovered a work from Siegfried Fink, a German Researcher, from as early as 1992: with a process very similar to Berglund's and Hu's, the German Researcher turned wood transparent to reveal specific cavities of the wood structure for analytical purpose.

Transparent wood was made based on the idea of removing light-absorbing components (mainly lignin) followed by infiltration of a polymer with a refractive index matching the wood substrate .

One example is a three step process:

The first step consists of immersing the 4 or 5 inch block of wood in a solution of water, sodium hydroxide, and sodium sulfite at boiling temperature for two hours. This enables the lignin in the cell walls to be leached out. "Lignins are particularly important in the formation of cell walls, especially in wood and bark, because they lend rigidity and do not rot easily".

The second step of oxidation with hydrogen peroxide completes the leaching of the lignin.

The third step consists of immersing the material in epoxy and putting it under alternating vacuum and atmospheric pressure; this fills the wood's natural but now disused nutrient and hydrating microscopic channels. The epoxy-filled microscopic channels create a material that has transparent refractive properties.

The new material is rated stronger than plastic, but is only in the laboratory and experimental stage, and not yet ready for commercial use.

The length of the process is determined by the size and species of the wood.

Transparent wood could transform architecture by enabling novel structures such as load-bearing windows. Such elements could also yield improvements in energy efficiency over glass or other traditional materials.



</doc>
<doc id="50588414" url="https://en.wikipedia.org/wiki?curid=50588414" title="Ned Wingreen">
Ned Wingreen

Ned Wingreen is a theoretical physicist and the Howard A. Prior Professor of the Life Sciences at Princeton University. He is a member of the Department of Molecular Biology and of the Lewis-Sigler Institute for Integrative Genomics, where he is currently associate director. He is also associated faculty in the Department of Physics. Working with Yigal Meir, Wingreen formulated the Meir-Wingreen Formula which describes the electric current through an arbitrary mesoscopic system.

Wingreen received a B.S. in Physics from California Institute of Technology in 1984. Wingreen then received his Ph.D. in theoretical condensed matter physics from Cornell University in 1989 as a Hertz Fellow. His dissertation was titled "Resonant Tunneling with Electron-Phonon Interaction" and he was advised by John W. Wilkins. He did his postdoc in mesoscopic physics at MIT. There, along with Yigal Meir, he formulated the Meir-Wingreen Formula that describes the electric current through an arbitrary mesoscopic system.

In 1991 he moved to the NEC Research Institute in Princeton. At NEC, he continued to work in mesoscopic physics, but also started research in biophysics which grew into a general interest in problems at the interface of physics and biology. Wingreen joined Princeton University in 2004. Wingreen's current research focuses on modelling intracellular networks in bacteria and other micro-organisms, as well as studies of microbial communities. He is a fellow of the American Physical Society and the American Association for the Advancement of Science.

Academic:



</doc>
<doc id="50333728" url="https://en.wikipedia.org/wiki?curid=50333728" title="Diffuse field acoustic testing">
Diffuse field acoustic testing

Diffuse field acoustic testing is the testing of the mechanical resistance of a spacecraft to the acoustic pressures during launch.

In the aerospace industry, acoustic chambers are the main facilities for such tests. A chamber is a reverberant room that creates a diffuse sound field and is composed of an empty volume (from 1 m to 2900 m) and a multifrequency sound generation system.

Theoretically, diffuse field is defined as a Sound pressure field where there is no privileged direction of the energy. In other words, when sound pressure is the same everywhere in the room. This is obtained with large rooms with no absorbent materials on walls, ceiling or floor. Diffusion is enhanced in asymmetric rooms. To obtain such conditions, the room must be reverberant. The source's direct field must be negligible compared to the reverberant field, avoiding privileged propagation.

Reverberation is due to multiple reflections on walls with some delays that come back to the receptor. Summing up these contributions, a reverberant pressure field is created. The more reverberation, the more the field is diffused.

Two oft-used measures of reverberation time quantify this parameter, : formula_1 and formula_2. These values are the interval for the sound pressure level to the lower of 30 or 60 dBSPL. It can be obtained by measuring the sound pressure decrease after a sound impulse or by using approximate formulas such as Sabine's or Eyring's. In the case of a diffuse field (low absorption on the walls, and big volumes) Sabine's formula is used.

formula_3

Where formula_4 is the equivalent absorption area involving the surface formula_5 of the walls and their absorption coefficient formula_6.

Many theoretical ways to model sound propagation are used. One of these is the geometrical approach. This represents sound waves as a ray of energy propagating. When it meets an obstacle, this ray has two possible behaviors: It can be reflected following the normal of the plan, specular reflection. Alternatively it can be separated into many rays following a mathematical law (for example Lambert's law), diffused reflection.

Frequency is a main factor of a good diffused field. Some phenomena linked to frequency of the sound pressure field lead to poor homogeneity of a pressure field. The frequency response of a room is the amplification or reduction of some frequencies. It represents the repartition of pressure with respect to frequency. In low frequencies this can lead to mode apparition. These modes are due to standing waves that lead to maximum and minimum pressure according to the geometry of the room. To determine the frequency for which the pressure field can be considered diffused, Schroeder's frequency is commonly used. It is obtained considering the frequency from which the modal overlap exceeds formula_7. Below this frequency, the field is not diffuse and standing waves create pressure modes

formula_8

Where formula_2 is the reverberation time of the room and formula_10 its volume.

For example, in the case of a rectangular room, low frequency modes are determined relative to the room dimensions as

formula_11
Where formula_12, formula_13 and formula_14 are respectively the mode of the length formula_15, formula_16 and formula_17 of the room and formula_18 the celerity of sound in the working fluid.

Acoustic tests are mainly use for environmental tests on aircraft structures. Satellites are expensive products with high-engineering built-in components. To improve the resistance of a spacecraft during launch and during its orbital life, analysis is focused on tests in three categories : Thermal, Radio-frequencies and Vibrations. This last test area is focused on the mechanical stresses that the specimen will meet during its life, especially during launch.

Acoustics creates mechanical stresses during the first five seconds. Sound pressure levels can go up to 150 dBSPL. Acoustic tests are used to verify the mechanical resistance of the satellite and its elements to acoustic pressures generated.

Once the sound generation system is working, acceleration measurement is performed by accelerometers placed on the specimen.

To test a satellite, a sound generation system generates a broadband spectrum ([25 Hz-10000]Hz) simulating the maximum envelope of all launchers that the satellite may fly in. To qualify, three tests are realized with changing global gain compared to launcher spectrum:

Before testing the satellite, an empty room test is performed to check the chamber's signature.

This pressure field is generated by multifrequency sirens powered by nitrogen or compressed air modulators. This system can generate sound pressure levels up to 160 dBSPL. Each acoustic chamber has its own configurations, but each siren is centered on a frequency where sound pressure levels are the highest. In some cases these sirens can be completed with electroacoustic systems to generate and control midrange and high frequencies. Sirens generate low frequencies, but with high sound pressure levels distortion appears that leads to higher harmonics. Loudspeakers are used in some chambers to control these frequencies.

To produce exact levels, piloting microphones check sound pressure levels and apply a realtime gain correction to adjust the level.





</doc>
<doc id="50791779" url="https://en.wikipedia.org/wiki?curid=50791779" title="Energy well">
Energy well

In physics, an energy well describes a 'stable' equilibrium that is not at lowest possible energy.

In general, modern physics holds the view that the universe - and systems therein - spontaneously drives toward a state of lower energy, if possible. For example, a bowling ball pitched atop a smooth hump (which has potential energy in the presence of gravity), will tend to roll down to the lowest point it possibly can. Once there, this reduces the total potential energy of the system.

On the other hand, if the bowling ball is resting in a valley between two humps - no matter how big the drops outside the humps - it will stay there indefinitely. Even though the system could achieve a lower energy state, it cannot do so without external energy being applied: (locally) it is at its lowest energy state, and only a force from outside the system can 'push' it over one of the humps so a lower state can be achieved.

The concept of an energy well is a key part of teaching basic physics, especially quantum mechanics. Here, students often solve the one-dimensional Schrödinger Equation for an electron trapped in a potential well from which it has insufficient energy to escape. The solution to this problem is a series of sinusoidal waves of fractional integral wavelengths determined by the width of the well.


</doc>
<doc id="51215300" url="https://en.wikipedia.org/wiki?curid=51215300" title="Electron heat capacity">
Electron heat capacity

In solid state physics the electron heat capacity or electronic specific heat describes the contribution of electrons to the heat capacity. 
Heat is transported by phonons and by free electrons in solids. For pure metals, however, the electronic contributions dominate in the thermal conductivity. In impure metals, the electron mean free path is reduced by collisions with impurities, and the phonon contribution may be comparable with the electronic contribution.

Although the Drude model was fairly successful in describing the electron motion within metals, it has some erroneous aspects: it predicts the Hall coefficient with the wrong sign compared to experimental measurements, the assumed additional electronic heat capacity to the lattice heat capacity, namely formula_1 per electron at elevated temperatures, is also inconsistent with experimental values, since measurements of metals show no deviation from the Dulong-Petit law. The observed electronic contribution of electrons to the heat capacity is usually less than one percent of formula_2. This problem seemed insoluble prior to the development of quantum mechanics. This paradox was solved by Sommerfeld after the discovery of the Pauli exclusion principle, who recognised that the replacement of the Boltzmann distribution with the Fermi-Dirac distribution was required and incorporated it in the free electron model.

When a metallic system is heated from absolute zero, not every electron gains an energy formula_3 as equipartition dictates. Only those electrons in atomic orbitals within an energy range of formula_4 of the Fermi level are thermally excited. Electrons, in contrast to a classical gas, can only move into free states in their energetic neighbourhood.
The one-electron energy levels are specified by the wave vector formula_5 through the relation formula_6 with formula_7 the electron mass. This relation separates the occupied energy states from the unoccupied ones and corresponds to the spherical surface in k-space. As formula_8 the ground state distribution becomes:

formula_9
where

This implies that the ground state is the only occupied state for electrons in the limit formula_8, the formula_15 takes the Pauli exclusion principle into account. The internal energy formula_16 of a system within the free electron model is given by the sum over one-electron levels times the mean number of electrons in that level:

formula_17

Using the approximation that for a sum over a smooth function formula_18 over all allowed values of formula_5 for finite large system is given by:

formula_20
where

For the reduced internal energy formula_22 the expression for formula_16 can be rewritten as:

formula_24

and the expression for the electron density formula_25 can be written as:

formula_26

The integrals above can be evaluated using the fact that the dependence of the integrals on formula_27 can be changed to dependence on formula_28 through the relation for the electronic energy when described as free particles, formula_6, which yields for an arbitrary function formula_30:

formula_31

with formula_32
which is known as the density of levels or density of states per unit volume such that formula_33 is the total number of states between formula_28 and formula_35 . Using the expressions above the integrals can be rewritten as:

formula_36

These integrals can be evaluated for temperatures that are small compared to the Fermi temperature by applying the Sommerfeld expansion and using the approximation that formula_12 differs from formula_11 for formula_39 by terms of order formula_40. The expressions become:

formula_41

For the ground state configuration the first terms (the integrals) of the expressions above yield the internal energy and electron density of the ground state. The expression for the electron density reduces to formula_42. Substituting this into the expression for the internal energy, one finds the following expression:

formula_43

The contributions of electrons within the free electron model is given by:

formula_44, for free electrons : formula_45

Compared to the classical result formula_46 it can be concluded that this result is depressed by a factor of formula_47 which is at room temperature of order of magnitude formula_48. This explains the absence of an electronic contribution to the heat capacity as measured experimentally.

Note that in this derivation formula_11 is often denoted by formula_50 which is known as the Fermi energy. In this notation, the electron heat capacity becomes:

formula_51 and for free electrons : formula_52 using the definition for the Fermi energy with formula_53 the Fermi temperature.

For temperatures below both the Debye temperature formula_54 and the Fermi temperature formula_55 the heat capacity of metals can be written as a sum of electron and phonon contributions that are linear and cubic respectively: formula_56. The coefficient formula_57 can be calculated and determined experimentally. Some values are tabulated below:
The free electrons in a metal do not usually lead to a strong deviation from the Dulong–Petit law at high temperatures. Since formula_57 is linear in formula_59 and formula_60 is linear in formula_61, at low temperatures the lattice contribution vanishes faster than the electronic contribution and the latter can be measured. The deviation of the approximated and experimentally determined electronic contribution to the heat capacity of a metal is not too large. A few metals deviate significantly from this approximated prediction. Measurements indicate that these errors are associated with the electron mass being somehow changed in the metal, for the calculation of the electron heat capacity the effective mass of an electron should be considered instead. For Fe and Co the large deviations are attributed to the partially filled d-shells of these transition metals, whose d-bands lie at the Fermi energy.
The alkali metals are expected to have the best agreement with the free electron model since these metals only one s-electron outside a closed shell. However even sodium, which is considered to be the closest to a free electron metal, is determined to have a formula_57 more than 25 per cent higher than expected from the theory.

Certain effects influence the deviation from the approximation:

Superconductivity occurs in many metallic elements of the periodic system and also in alloys, intermetallic compounds, and doped semiconductors. This effect occurs upon cooling the material. The entropy decreases on cooling below the critical temperature formula_63 for superconductivity which indicates that the superconducting state is more ordered than the normal state. The entropy change is small, this must mean that only a very small fraction of electrons participate in the transition to the superconducting state but, the electronic contribution to the heat capacity changes drastically. There is a sharp jump of the heat capcacity at the critical temperature while for the temperatures above the critical temperature the heat capacity is linear with temperature.

The calculation of the electron heat capacity for super conductors can be done in the BCS theory. The entropy of a system of fermionic quasiparticles, in this case Cooper pairs, is:

formula_64

where formula_65 is the Fermi-Dirac distribution formula_66 with formula_67
and

The heat capacity is given by formula_72.
The last two terms can be calculated:

formula_73

Substituting this in the expression for the heat capacity and again applying that the sum over formula_27 in the reciprocal space can be replaced by an integral in formula_28 multipied by the density of states formula_76 this yields:

formula_77

To examine the typical behaviour of the electron heat capacity for species that can transition to the superconducting state, three regions must be defined:


For formula_78 it holds that formula_83 and the electron heat capacity becomes:

formula_84

This is just the result for a normal metal derived in the section above, as expected since a superconductor behaves as a normal conductor above the critical temperature.

For formula_80 the electron heat capacity for super conductors exhibits an exponential decay of the form:
formula_87

At the critical temperature the heat capacity is discontinuous. This discontinuity in the heat capacity indicates that the transition for a material from normal conducting to superconducting is a second order phase transition.

More recent research has found that since both the Fermi function formula_10 and the electronic density of states formula_90 vary with formula_91, a more exact expression for formula_92 is the following equation at given formula_91

formula_94.




</doc>
<doc id="51297098" url="https://en.wikipedia.org/wiki?curid=51297098" title="Farris effect (rheology)">
Farris effect (rheology)

In rheology, the Farris Effect describes the decrease of the viscosity of a suspension upon increasing the dispersity of the solid additive, at constant volume fraction of the solid additive. That is, that a broader particle size distribution yields a lower viscosity than a narrow particle size distribution, for the same concentration of particles. The phenomenon is names after Richard J. Farris, who modeled the effect. The effect is relevant whenever suspensions are flowing, particularly for suspensions with high loading fractions. Examples include hydraulic fracturing fluids, metal injection molding feedstocks, cosmetics, and various geological processes including sedimentation and lava flows.



</doc>
<doc id="51084847" url="https://en.wikipedia.org/wiki?curid=51084847" title="Dispersive medium">
Dispersive medium

A dispersive medium is a medium in which waves of different frequencies travel at different velocities. With electromagnetic radiation (e.g. light, radio waves), this occurs because the index of refraction of the medium is frequency dependent.


</doc>
<doc id="9079863" url="https://en.wikipedia.org/wiki?curid=9079863" title="Aerometer">
Aerometer

An aerometer is an instrument designed to measure the density (among other parameters) of the air and some gases.

The word aerometer (or Ärometer, from Ancient Greek ἀήρ -aer "air" and μέτρον -métron "measure, scale") refers to various types of devices for measuring or handling of gases. The instruments designated with this name can be used to find: the density, the flow, the amount or some other parameter of the air or a determined gas.

Another instrument called areometer (from Ancient Greek ἀραιός -araiós "lightness" and μέτρον -métron "measure, scale"), also known as hydrometer, used for measuring liquids density, is often confused with the term aerometer here defined.






[[Category:Physics]]
[[Category:Measuring instruments]]

</doc>
<doc id="21348562" url="https://en.wikipedia.org/wiki?curid=21348562" title="Homeokinetics">
Homeokinetics

Homeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy. Homeokinetics treats all complex systems on an equal footing, animate and inanimate, providing them with a common viewpoint. The complexity in studying how they work is reduced by the emergence of common languages in all complex systems.

Arthur Iberall, Warren McCulloch and Harry Soodak developed the concept of homeokinetics as a new branch of physics. It began through Iberall's biophysical research for the NASA exobiology program into the dynamics of mammalian physiological processes They were observing an area that physics has neglected, that of complex systems with their very long internal factory day delays. They were observing systems associated with nested hierarchy and with an extensive range of time scale processes. It was such connections, referred to as both up-down or in-out connections (as nested hierarchy) and side-side or flatland physics among atomistic-like components (as heterarchy) that became the hallmark of homeokinetic problems. By 1975, they began to put a formal catch-phrase name on those complex problems, associating them with nature, life, human, mind, and society. The major method of exposition that they began using was a combination of engineering physics and a more academic pure physics. In 1981, Iberall was invited to the Crump Institute for Medical Engineering of UCLA, where he further refined the key concepts of homeokinetics, developing a physical scientific foundation for complex systems.

A system is a collective of interacting ‘atomistic’-like entities. The word ‘atomism’ is used to stand both for the entity and the doctrine. As is known from ‘kinetic’ theory, in mobile or simple systems, the atomisms share their ‘energy’ in interactive collisions. That so-called ‘equipartitioning’ process takes place within a few collisions. Physically, if there is little or no interaction, the process is considered to be very weak. Physics deals basically with the forces of interaction—few in number—that influence the interactions. They all tend to emerge with considerable force at high ‘density’ of atomistic interaction. In complex systems, there is also a result of internal processes in the atomisms. They exhibit, in addition to the pair-by-pair interactions, internal actions such as vibrations, rotations, and association. If the energy and time involved internally creates a very large—in time—cycle of performance of their actions compared to their pair interactions, the collective system is complex. If you eat a cookie and you do not see the resulting action for hours, that is complex; if boy meets girl and they become ‘engaged’ for a protracted period, that is complex. What emerges from that physics is a broad host of changes in state and stability transitions in state. Viewing Aristotle as having defined a general basis for systems in their static-logical states and trying to identify a logic-metalogic for physics, e.g., metaphysics, then homeokinetics is viewed to be an attempt to define the dynamics of all those systems in the universe.

Ordinary physics is a flatland physics, a physics at some particular level. Examples include nuclear and atomic physics, biophysics, social physics, and stellar physics. Homeokinetic physics combines flatland physics with the study of the up down processes that binds the levels. Tools, such as mechanics, quantum field theory, and the laws of thermodynamics, provide key relationships for the binding of the levels, how they connect, and how the energy flows up and down. And whether the atomisms are atoms, molecules, cells, people, stars, galaxies, or universes, the same tools can be used to understand them. Homeokinetics treats all complex systems on an equal footing, animate and inanimate, providing them with a common viewpoint. The complexity in studying how they work is reduced by the emergence of common languages in all complex systems.

A homeokinetic approach to complex systems has been applied to ecological psychology, anthropology, geology,
bioenergetics, and political science.

It has also been applied to social physics where a homeokinetics analysis shows that one must account for flow variables such as the flow of energy, of materials, of action, reproduction rate, and value-in-exchange.



</doc>
<doc id="2664158" url="https://en.wikipedia.org/wiki?curid=2664158" title="Center of percussion">
Center of percussion

The center of percussion is the point on an extended massive object attached to a pivot where a perpendicular impact will produce no reactive shock at the pivot. Translational and rotational motions cancel at the pivot when an impulsive blow is struck at the center of percussion. The center of percussion is often discussed in the context of a bat, racquet, door, sword or other extended object held at one end. 

The same point is called the center of oscillation for the object suspended from the pivot as a pendulum, meaning that a simple pendulum with all its mass concentrated at that point will have the same period of oscillation as the compound pendulum. In sports, the center of percussion of a bat or racquet is related to the so-called "sweet spot", but the latter is also related to vibrational bending of the object.

Imagine a rigid beam suspended from a wire by a fixture that can slide freely along the wire at point P, as shown in the Figure. An impulsive blow is applied from the left. If it is below the center of mass (CM) it will cause the beam to rotate counterclockwise around the CM and also cause the CM to move to the right. The center of percussion (CP) is below the CM. If the blow falls above the CP, the rightward translational motion will be bigger than the leftward rotational motion at P, causing the net initial motion of the fixture to be rightward. If the blow falls below the CP the opposite will occur, rotational motion at P will be larger than translational motion and the fixture will move initially leftward. Only if the blow falls exactly on the CP will the two components of motion cancel out to produce zero net initial movement at point P. 

When the sliding fixture is replaced with a pivot that cannot move left or right, an impulsive blow anywhere but at the CP results in an initial reactive force at the pivot. 

For a free, rigid beam, an impulse formula_1 applied at right angle at a distance formula_2 from the center of mass (CM) will result in the CM changing velocity formula_3 according to the relation:

where formula_5 is the mass of the beam. Similarly, the torque about the CM will change the angular velocity formula_6 according to:

where formula_8 is the moment of inertia around the CM.

For any point P a distance formula_9 on the opposite side of the CM from the point of impact, the change in velocity of point P is

where formula_9 is the distance of P from the CM. Hence the acceleration at P due to the impulsive blow is:

When this acceleration is zero, formula_2 defines the center of percussion. Therefore, the CP distance, formula_2, from the CM, is given by

Note that P, the rotation axis, need not be at the end of the beam, but can be chosen at any distance formula_9. 

Length formula_17 also defines the center of oscillation of a physical pendulum, that is, the position of the mass of a simple pendulum that has the same period as the physical pendulum.

For the special case of a beam of uniform density of length formula_18, the moment of inertia around the CM is:

and for rotation about a pivot at the end, 

This leads to:

It follows that the CP is 2/3 of the length of the uniform beam formula_18 from the pivoted end. 

For example, a swinging door that is stopped by a doorstop placed 2/3 of the width of the door will do the job with minimal shaking of the door because the hinged end is subjected to no net reactive force. (This point is also the node in the second vibrational harmonic, which also minimizes vibration.)

The sweet spot on a baseball bat is generally defined as the point at which the impact "feels" best to the batter. The center of percussion defines a place where, if the bat strikes the ball and the batter's hands are at the pivot point, the batter feels no sudden reactive force. However, since a bat is not a rigid object the vibrations produced by the impact also play a role. Also, the pivot point of the swing may not be at the place where the batter's hands are placed. Research has shown that the dominant physical mechanism in determining where the sweet spot is arises from the location of nodes in the vibrational modes of the bat, not the location of the center of percussion.

The center of percussion concept can be applied to swords. Being flexible objects, the "sweet spot" for such cutting weapons depends not only on the center of percussion but also on the flexing and vibrational characteristics.


</doc>
<doc id="52588462" url="https://en.wikipedia.org/wiki?curid=52588462" title="Verwey transition">
Verwey transition

The Verwey transition is a low-temperature phase transition in the mineral magnetite near 125 kelvins associated with changes in its magnetic, electrical, and thermal properties. Upon warming through the Verwey transition temperature (), the magnetite crystal lattice changes from a monoclinic structure to the cubic inverse spinel structure that persists at room temperature. The phenomenon is named after Evert Verwey, a Dutch chemist who first recognized the connection between the structural transition and the changes in the physical properties of magnetite.

The Verwey transition is near in temperature, but distinct from, a magnetic isotropic point in magnetite, at which the first magnetocrystalline anisotropy constant changes sign from positive to negative. The temperature and physical expression of the Verwey transition are highly sensitive to the stress state of magnetite and the stoichiometry. Non-stoichiometry in the form of metal cation substitution or partial oxidation can lower the transition temperature or suppress it entirely.


</doc>
<doc id="48520204" url="https://en.wikipedia.org/wiki?curid=48520204" title="Computational anatomy">
Computational anatomy

Computational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability. It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.

The field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, machine learning, computational mechanics, computational science, biological imaging, neuroscience, physics, probability, and statistics; it also has strong connections with fluid mechanics and geometric mechanics. Additionally, it complements newer, interdisciplinary fields like bioinformatics and neuroinformatics in the sense that its interpretation uses metadata derived from the original sensor imaging modalities (of which Magnetic Resonance Imaging is one example). It focuses on the anatomical structures being imaged, rather than the medical imaging devices. It is similar in spirit to the history of Computational linguistics, a discipline that focuses on the linguistic structures rather than the sensor acting as the transmission and communication medium(s).

In computational anatomy, the diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow in formula_1. The flows between coordinates in Computational anatomy are constrained to be geodesic flows satisfying the principle of least action for the Kinetic energy of the flow. The kinetic energy is defined through a Sobolev smoothness norm with strictly more than two generalized, square-integrable derivatives for each component of the flow velocity, which guarantees that the flows in formula_2 are diffeomorphisms. 
It also implies that the diffeomorphic shape momentum taken pointwise satisfying the Euler-Lagrange equation for geodesics is determined by its neighbors through spatial derivatives on the velocity field. This separates the discipline from the case of incompressible fluids for which momentum is a pointwise function of velocity. Computational anatomy intersects the study of Riemannian manifolds and nonlinear global analysis, where groups of diffeomorphisms are the central focus. Emerging high-dimensional theories of shape are central to many studies in Computational anatomy, as are questions emerging from the fledgling field of shape statistics.
The metric structures in Computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology , the metric space study of coordinate systems via diffeomorphisms.

At Computational anatomy's heart is the comparison of shape by recognizing in one shape the other. This connects it to D'Arcy Wentworth Thompson's developments On Growth and Form which has led to scientific explanations of morphogenesis, the process by which patterns are formed in Biology. Albrecht Durer's Four Books on Human Proportion were arguably the earliest works on Computational anatomy. The efforts of Noam Chomsky in his pioneering of Computational Linguistics inspired the original formulation of Computational anatomy as a generative model of shape and form from exemplars acted upon via transformations.

Due to the availability of dense 3D measurements via technologies such as magnetic resonance imaging (MRI), Computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morphome scale in 3D. The spirit of this discipline shares strong overlap with areas such as computer vision and kinematics of rigid bodies, where objects are studied by analysing the groups responsible for the movement in question. Computational anatomy departs from computer vision with its focus on rigid motions, as the infinite-dimensional diffeomorphism group is central to the analysis of Biological shapes. It is a branch of the image analysis and pattern theory school at Brown University pioneered by Ulf Grenander. In Grenander's general Metric Pattern Theory, making spaces of patterns into a metric space is one of the fundamental operations since being able to cluster and recognize anatomical configurations often requires a metric of close and far between shapes. The diffeomorphometry metric of Computational anatomy measures how far two diffeomorphic changes of coordinates are from each other, which in turn induces a metric on the shapes and images indexed to them. The models of metric pattern theory, in particular group action on the orbit of shapes and forms is a central tool to the formal definitions in Computational anatomy.

Computational anatomy is the study of shape and form at the morphome or gross anatomy millimeter, or morphology scale, focusing on the study of sub-manifolds of formula_3 points, curves surfaces and subvolumes of human anatomy.
An early modern computational neuro-anatomist was David Van Essen performing some of the early physical unfoldings of the human brain based on printing of a human cortex and cutting. Jean Talairach's publication of Tailarach coordinates is an important milestone at the morphome scale demonstrating the fundamental basis of local coordinate systems in studying neuroanatomy and therefore the clear link to charts of differential geometry. Concurrently, virtual mapping in Computational anatomy across high resolution dense image coordinates was already happening in Ruzena Bajcy's and Fred Bookstein's earliest developments based on Computed axial tomography and Magnetic resonance imagery.
The earliest introduction of the use of flows of diffeomorphisms for transformation of coordinate systems in image analysis and medical imaging was by Christensen, Joshi, Miller, and Rabbitt.

The first formalization of Computational Anatomy as an orbit of exemplar templates under diffeomorphism group action was in the original lecture given by Grenander and Miller with that title in May 1997 at the 50th Anniversary of the Division of Applied Mathematics at Brown University, and subsequent publication. This was the basis for the strong departure from much of the previous work on advanced methods for spatial normalization and image registration which were historically built on notions of addition and basis expansion. The structure preserving transformations central to the modern field of Computational Anatomy, homeomorphisms and diffeomorphisms carry smooth submanifolds smoothly. They are generated via Lagrangian and Eulerian flows which satisfy a law of composition of functions forming the group property, but are not additive.

The original model of Computational anatomy was as the triple, formula_4 the group formula_5, the orbit of shapes and forms formula_6, and the probability laws formula_7 which encode the variations of the objects in the orbit. The template or collection of templates are elements in the orbit formula_8 of shapes.

The Lagrangian and Hamiltonian formulations of the equations of motion of Computational Anatomy took off post 1997 with several pivotal meetings including the 1997 Luminy meeting organized by the Azencott school at Ecole-Normale Cachan on the "Mathematics of Shape Recognition" and the 1998 Trimestre at Institute Henri Poincaré organized by David Mumford "Questions Mathématiques en Traitement du Signal et de l'Image" which catalyzed the Hopkins-Brown-ENS Cachan groups and subsequent developments and connections of Computational anatomy to developments in global analysis.

The developments in Computational Anatomy included the establishment of the Sobelev smoothness conditions on the diffeomorphometry metric to insure existence of solutions of variational problems in the space of diffeomorphisms, the derivation of the Euler-Lagrange equations characterizing geodesics through the group and associated conservation laws, the demonstration of the metric properties of the right invariant metric, the demonstration that the Euler-Lagrange equations have a well-posed initial value problem with unique solutions for all time, and with the first results on sectional curvatures for the diffeomorphometry metric in landmarked spaces. Following the Los Alamos meeting in 2002, Joshi's original large deformation singular "Landmark" solutions in Computational anatomy were connected to peaked "Solitons" or "Peakons" as solutions for the Camassa-Holm equation. Subsequently, connections were made between Computational anatomy's Euler-Lagrange equations for momentum densities for the right-invariant metric satisfying Sobolev smoothness to Vladimir Arnold's characterization of the Euler equation for incompressible flows as describing geodesics in the group of volume preserving diffeomorphisms. The first algorithms, generally termed LDDMM for large deformation diffeomorphic mapping for computing connections between landmarks in volumes and spherical manifolds, curves, currents and surfaces, volumes, tensors, varifolds, and time-series have followed.

These contributions of Computational anatomy to the global analysis associated to the infinite dimensional manifolds of subgroups of the diffeomorphism group is far from trivial. The original idea of doing differential geometry, curvature and geodesics on infinite dimensional manifolds goes back to Bernhard Riemann's Habilitation (Ueber die Hypothesen, welche der Geometrie zu Grunde liegen); the key modern book laying the foundations of such ideas in global analysis are from Michor.

The applications within Medical Imaging of Computational Anatomy continued to flourish after two organized meetings at the Institute for Pure and Applied Mathematics conferences at University of California, Los Angeles. Computational anatomy has been useful in creating accurate models of the atrophy of the human brain at the morphome scale, as well as Cardiac templates, as well as in modeling biological systems. Since the late 1990s, computational anatomy has become an important part of developing emerging technologies for the field of medical imaging. Digital atlases are a fundamental part of modern Medical-school education and in neuroimaging research at the morphome scale. Atlas based methods and virtual textbooks which accommodate variations as in deformable templates are at the center of many neuro-image analysis platforms including Freesurfer, FSL, MRIStudio, SPM. Diffeomorphic registration, introduced in the 90's, is now an important player with existing codes bases organized around ANTS,<ref name="stnava/ANTs"></ref> DARTEL, DEMONS, LDDMM,<ref name="NITRC: LDDMM: Tool/Resource Info"></ref> StationaryLDDMM, FastLDDMM, are examples of actively used computational codes for constructing correspondences between coordinate systems based on sparse features and dense images. Voxel-based morphometry (VBM) is an important technology built on many of these principles.

The model of human anatomy is a deformable template, an orbit of exemplars under group action. Deformable template models have been central to Grenander's Metric Pattern theory, accounting for typicality via templates, and accounting for variability via transformation of the template. An orbit under group action as the representation of the deformable template is a classic formulation from differential geometry. The space of shapes are denoted formula_9, with the group formula_10 with law of composition formula_11; the action of the group on shapes is denoted formula_12, where the action of the group formula_13 is defined to satisfy 

The orbit formula_15 of the template becomes the space of all shapes, formula_16, being homogenous under the action of the elements of formula_17.
The orbit model of computational anatomy is an abstract algebra - to be compared to linear algebra- since the groups act nonlinearly on the shapes. This is a generalization of the classical models of linear algebra, in which the set of finite dimensional formula_18 vectors are replaced by the finite-dimensional anatomical submanifolds (points, curves, surfaces and volumes) and images of them, and the formula_19 matrices of linear algebra are replaced by coordinate transformations based on linear and affine groups and the more general high-dimensional diffeomorphism groups.

The central objects are shapes or forms in Computational anatomy, one set of examples being the 0,1,2,3-dimensional submanifolds of formula_20, a second set of examples being images generated via medical imaging such as via magnetic resonance imaging (MRI) and functional magnetic resonance imaging. The 0-dimensional manifolds are landmarks or fiducial points; 1-dimensional manifolds are curves such as sulcul and gyral curves in the brain; 2-dimensional manifolds correspond to boundaries of substructures in anatomy such as the subcortical structures of the midbrain or the gyral surface of the neocortex; subvolumes correspond to subregions of the human body, the heart, the thalamus, the kidney.

The landmarks formula_21 are a collections of points with no other structure, delineating important fiducials within human shape and form (see associated landmarked image).
The sub-manifold shapes such as surfaces formula_22 are collections of points modeled as parametrized by a local chart or immersion formula_23, formula_24 (see Figure showing shapes as mesh surfaces).
The images such as MR images or DTI images formula_25, and are dense functions
formula_26 are scalars, vectors, and matrices (see Figure showing scalar image).

Groups and group actions are familiar to the Engineering community with the universal popularization and standardization of linear algebra as a basic model for analyzing signals and systems in mechanical engineering, electrical engineering and applied mathematics. In linear algebra the matrix groups (matrices with inverses) are the central structure, with group action defined by the usual definition of formula_27 as an formula_28 matrix, acting on formula_29 as formula_30 vectors; the orbit in linear-algebra is the set of formula_31-vectors given by formula_32, which is a group action of the matrices through the orbit of formula_33.

The central group in Computational anatomy defined on volumes in formula_1 are the diffeomorphisms formula_35 which are mappings with 3-components formula_36, law of composition of functions formula_37, with inverse formula_38.

Most popular are scalar images, formula_39, with action on the right via the inverse.

For sub-manifolds formula_22, parametrized by a chart or immersion formula_42, the diffeomorphic action the flow of the position

Several group actions in computational anatomy have been defined.

For the study of rigid body kinematics, the low-dimensional matrix Lie groups have been the central focus. The matrix groups are low-dimensional mappings, which are diffeomorphisms that provide one-to-one correspondences between coordinate systems, with a smooth inverse. The matrix group of rotations and scales can be generated via a closed form finite-dimensional matrices which are solution of simple ordinary differential equations with solutions given by the matrix exponential.

For the study of deformable shape in Computational anatomy, a more general diffeomorphism group has been the group of choice, which is the infinite dimensional analogue. The high-dimensional differeomorphism groups used in Computational Anatomy are generated via smooth flows formula_44 which satisfy the Lagrangian and Eulerian specification of the flow fields as first introduced in., satisfying the ordinary differential equation: 

with formula_45 the vector fields on formula_46 termed the Eulerian velocity of the particles at position formula_47 of the flow. The vector fields are functions in a function space, modelled as a smooth Hilbert space of high-dimension, with the Jacobian of the flow formula_48 a high-dimensional field in a function space as well, rather than a low-dimensional matrix as in the matrix groups. Flows were first introduced for large deformations in image matching; formula_49 is the instantaneous velocity of particle formula_50 at time formula_51 .

The inverse formula_52 required for the group is defined on the Eulerian vector-field with advective inverse flow

The group of diffeomorphisms is very big. To ensure smooth flows of diffeomorphisms avoiding shock-like solutions for the inverse, the vector fields must be at least 1-time continuously differentiable in space. For diffeomorphisms on formula_46, vector fields are modelled as elements of the Hilbert space formula_54 using the Sobolev embedding theorems so that each element has strictly greater than 2 generalized square-integrable spatial derivatives (thus formula_55 is sufficient), yielding 1-time continuously differentiable functions.

The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:
where
formula_56 
with the linear operator formula_57 mapping to the dual space formula_58, with the integral calculated by integration by parts when formula_59 is a generalized function in the dual space.
the images are denoted with the orbit as formula_60 and metric formula_61.

In classical mechanics the evolution of physical systems is described by solutions to the Euler–Lagrange equations associated to the Least-action principle of Hamilton. This is a standard way, for example of obtaining Newton's laws of motion of free particles. More generally, the Euler-Lagrange equations can be derived for systems of generalized coordinates. The Euler-Lagrange equation in Computational anatomy describes the geodesic shortest path flows between coordinate systems of the diffeomorphism metric. In Computational anatomy the generalized coordinates are the flow of the diffeomorphism and its Lagrangian velocity formula_62, the two related via the Eulerian velocity formula_63. 
Hamilton's principle for generating the Euler-Lagrange equation requires the action integral on the Lagrangian given by 
the Lagrangian is given by the kinetic energy:

In computational anatomy, formula_64 was first called the Eulerian or diffeomorphic shape momentum since when integrated against Eulerian velocity formula_65 gives energy density, and since there is a conservation of diffeomorphic shape momentum which holds. The operator formula_27 is the generalized moment of inertia or inertial operator.

Classical calculation of the Euler-Lagrange equation from Hamilton's principle requires the perturbation of the Lagrangian on the vector field in the kinetic energy with respect to first order perturbation of the flow. This requires adjustment by the Lie bracket of vector field, given by operator formula_67 which involves the Jacobian given by 
Defining the adjoint formula_69 then the first order variation gives the Eulerian shape momentum formula_59 satisfying the generalized equation:
meaning for all smooth formula_71

Computational anatomy is the study of the motions of submanifolds, points, curves, surfaces and volumes.
Momentum associated to points, curves and surfaces are all singular, implying the momentum is concentrated on subsets of formula_20 which are dimension formula_74 in Lebesgue measure. In such cases, the energy is still well defined formula_75 since although formula_76 is a generalized function, the vector fields are smooth and the Eulerian momentum is understood via its action on smooth functions. The perfect illustration of this is even when it is a superposition of delta-diracs, the velocity of the coordinates in the entire volume move smoothly.The Euler-Lagrange equation () on diffeomorphisms for generalized functions formula_77 was derived in. In Riemannian Metric and Lie-Bracket Interpretation of the Euler-Lagrange Equation on Geodesics derivations are provided in terms of the adjoint operator and the Lie bracket for the group of diffeomorphisms. It has come to be called EPDiff equation for diffeomorphisms connecting to the Euler-Poincare method having been studied in the context of the inertial operator formula_78 for incompressible, divergence free, fluids.

For the momentum density case formula_79, then Euler–Lagrange equation has a classical solution:The Euler-Lagrange equation on diffeomorphisms, classically defined for momentum densities first appeared in for medical image analysis.

In Medical imaging and Computational anatomy, positioning and coordinatizing shapes are fundamental operations; the system for positioning anatomical coordinates and shapes built on the metric and the Euler-Lagrange equation a geodesic positioning system as first explicated in Miller Trouve and Younes.
Solving the geodesic from the initial condition formula_80 is termed the Riemannian-exponential, a mapping formula_81 at identity to the group.

The Riemannian exponential satisfies formula_82 for initial condition formula_83, vector field dynamics formula_84,


Computing the flow formula_80 onto coordinates Riemannian logarithm, mapping formula_92 at identity from formula_93 to vector field formula_94;

formula_95

Extended to the entire group they become

formula_96 ; formula_97 .

These are inverses of each other for unique solutions of Logarithm; the first is called geodesic positioning, the latter geodesic coordinates (see Exponential map, Riemannian geometry for the finite dimensional version).The geodesic metric is a local flattening of the Riemannian coordinate system (see figure). 

In Computational anatomy the diffeomorphisms are used to push the coordinate systems, and the vector fields are used
as the control within the
anatomical orbit or morphological space. The model is that of a dynamical system, the flow of coordinates formula_98 and the control the vector field formula_99 related via formula_100 The Hamiltonian view

This function is the extended Hamiltonian. The Pontryagin maximum principle gives the optimizing vector field which determines the geodesic flow satisfying formula_105 as well as the reduced Hamiltonian 
The Lagrange multiplier in its action as a linear form has its own inner product of the canonical momentum acting on the velocity of the flow which is dependent on the shape, e.g. for landmarks a sum, for surfaces a surface integral, and. for volumes it is a volume integral with respect to formula_107 on formula_1. In all cases the Greens kernels carry weights which are the canonical momentum evolving according to an ordinary differential equation which corresponds to EL but is the geodesic reparameterization in canonical momentum. The optimizing vector field is given by
with dynamics of canonical momentum reparameterizing the vector field along the geodesic 

Whereas the vector fields are extended across the entire background space of formula_1, the geodesic flows associated to the submanifolds has Eulerian shape momentum which evolves as a generalized function formula_111 concentrated to the submanifolds. For landmarks the geodesics have Eulerian shape momentum which are a superposition of delta distributions travelling with the finite numbers of particles; the diffeomorphic flow of coordinates have velocities in the range of weighted Green's Kernels. For surfaces, the momentum is a surface integral of delta distributions travelling with the surface.

The geodesics connecting coordinate systems satisfying have stationarity of the Lagrangian. The Hamiltonian is given by the extremum along the path formula_112, formula_113, equalling the and is stationary along . Defining the geodesic velocity at the identity formula_114, then along the geodesic

In Computational anatomy the submanifolds are pointsets, curves, surfaces and subvolumes which are the basic primitives. The geodesic flows between the submanifolds determine the distance, and form the basic measuring and transporting tools of diffeomorphometry. At formula_120 the geodesic has vector field formula_123 determined by the conjugate momentum and the Green's kernel of the inertial operator defining the Eulerian momentum formula_124. The metric distance between coordinate systems connected via the geodesic determined by the induced distance between identity and group element:

Given the least-action there is a natural definition of momentum associated to generalized coordinates; the quantity acting against velocity gives energy. The field has studied two forms, the momentum associated to the Eulerian vector field termed Eulerian diffeomorphic shape momentum, and the momentum associated to the initial coordinates or canonical coordinates termed canonical diffeomorphic shape momentum. Each has a conservation law.The conservation of momentum goes hand in hand with the . In Computational anatomy, formula_64 is the Eulerian Momentum since when integrated against Eulerian velocity formula_65 gives energy density; operator formula_27 the generalized moment of inertia or inertial operator which acting on the Eulerian velocity gives momentum which is conserved along the geodesic: 

Conservation of Eulerian shape momentum was shown in and follows from ; conservation of canonical momentum was shown in
^T p_t</math>: 

LDDMM matching based on the entire tensor matrix
has group action becomes formula_130 transformed eigenvectors

The variational problem matching onto the principal eigenvector or the matrix is described
LDDMM Tensor Image Matching.

High angular resolution diffusion imaging (HARDI) addresses the well-known limitation of DTI, that is, DTI can only reveal one dominant fiber orientation at each location. HARDI measures diffusion along formula_132 uniformly distributed directions on the sphere and can characterize more complex fiber geometries. HARDI can be used to reconstruct an orientation distribution function (ODF) that characterizes the angular profile of the diffusion probability density function of water molecules. The ODF is a function defined on a unit sphere, formula_133.

Dense LDDMM ODF matching takes the HARDI data as ODF at each voxel and solves the LDDMM variational problem in the space of ODF. In the field of information geometry, the space of ODF forms a Riemannian manifold with the Fisher-Rao metric. For the purpose of LDDMM ODF mapping, the square-root representation is chosen because it is one of the most efficient representations found to date as the various Riemannian operations, such as geodesics, exponential maps, and logarithm maps, are available in closed form. In the following, denote square-root ODF (formula_134) as formula_135, where formula_135 is non-negative to ensure uniqueness and formula_137. The variational problem for matching assumes that two ODF volumes can be generated from one to another via flows of diffeomorphisms formula_138, which are solutions of ordinary differential equations 
formula_139 starting from
the identity map formula_140. Denote the action of the diffeomorphism on template as formula_141, formula_142, formula_143 are respectively the coordinates of the unit sphere, formula_144 and the image domain, with the target indexed similarly, formula_145,formula_142,formula_143.

The group action of the diffeomorphism on the template is given according to
where formula_149 is the Jacobian of the affined transformed ODF and is defined as
formula_150
This group action of diffeomorphisms on ODF reorients the ODF and reflects changes in both the magnitude of formula_151 and the sampling directions of formula_152 due to affine transformation. It guarantees that the volume fraction of fibers oriented toward a small patch must remain the same after the patch is transformed. 
The LDDMM variational problem is defined as

where the logarithm of formula_154 is defined as
where formula_156 is the normal dot product between points in the sphere under the formula_157 metric.

This LDDMM-ODF mapping algorithm has been widely used to study brain white matter degeneration in aging, Alzheimer's disease, and vascular dementia. The brain white matter atlas generated based on ODF is constructed via Bayesian estimation. Regression analysis on ODF is developed in the ODF manifold space in.

The principle mode of variation represented by the orbit model is change of coordinates. For setting in which pairs of images are not related by diffeomorphisms but have photometric variation or image variation not represented by the template, active appearance modelling has been introduced, originally by Edwards-Cootes-Taylor and in 3D medical imaging in.
In the context of Computational Anatomy in which metrics on the anatomical orbit has been studied, metamorphosis for modelling structures such as tumors and photometric changes which are not resident in the template was introduced in for Magnetic Resonance image models, with many subsequent developments extending the metamorphosis framework.

For image matching the image metamorphosis framework enlarges the action so that formula_158 with action formula_159. In this setting metamorphosis combines both the diffeomorphic coordinate system transformation of Computational Anatomy as well as the early morphing technologies which only faded or modified the photometric or image intensity alone.

Then the matching problem takes a form with equality boundary conditions:

Transforming coordinate systems based on Landmark point or fiducial marker features dates back to Bookstein's early work on small deformation spline methods for interpolating correspondences defined by fiducial points to the two-dimensional or three-dimensional background space in which the fiducials are defined. Large deformation landmark methods came on in the late 90's. The above Figure depicts a series of landmarks associated three brain structures, the amygdala, entorhinal cortex, and hippocampus.

Matching geometrical objects like unlabelled point distributions, curves or surfaces is another common problem in Computational Anatomy. Even in the discrete setting where these are commonly given as vertices with meshes, there are no predetermined correspondences between points as opposed to the situation of landmarks described above. From the theoretical point of view, while any submanifold formula_161 in formula_20, formula_163 can be parameterized in local charts formula_164, all reparametrizations of these charts give geometrically the same manifold. Therefore, early on in Computational anatomy, investigators have identified the necessity of parametrization invariant representations. One indispensable requirement is that the end-point matching term between two submanifolds is itself independent of their parametrizations. This can be achieved via concepts and methods borrowed from Geometric measure theory, in particular currents and varifolds which have been used extensively for curve and surface matching.

Denoted the landmarked shape formula_165 with endpoint formula_166, the variational problem becomes

</math>|}}The geodesic Eulerian momentum is a generalized function formula_167, supported on the landmarked set in the variational problem.The endpoint condition with conservation implies the initial momentum at the identity of the group:

The iterative algorithm 
for large deformation diffeomorphic metric mapping for landmarks is given.

Glaunes and co-workers first introduced diffeomorphic matching of pointsets in the general setting of matching distributions. As opposed to landmarks, this includes in particular the situation of weighted point clouds with no predefined correspondences and possibly different cardinalities. The template and target discrete point clouds are represented as two weighted sums of Diracs formula_169 and formula_170 living in the space of signed measures of formula_171. The space is equipped with a Hilbert metric obtained from a real positive kernel formula_172 on formula_171, giving the following norm:

The matching problem between a template and target point cloud may be then formulated using this kernel metric for the endpoint matching term:

where formula_176 is the distribution transported by the deformation.

In the one dimensional case, a curve in 3D can be represented by an embedding formula_177, and the group action of "Diff" becomes formula_178. However, the correspondence between curves and embeddings is not one to one as the any reparametrization formula_179, for formula_180 a diffeomorphism of the interval [0,1], represents geometrically the same curve. In order to preserve this invariance in the end-point matching term, several extensions of the previous 0-dimensional measure matching approach can be considered.

In the situation of oriented curves, currents give an efficient setting to construct invariant matching terms. In such representation, curves are interpreted as elements of a functional space dual to the space vector fields, and compared through kernel norms on these spaces. Matching of two curves formula_181 and formula_182 writes eventually as the variational problem

with the endpoint term formula_184 is obtained from the norm

the derivative formula_186 being the tangent vector to the curve and formula_187 a given matrix kernel of formula_20. Such expressions are invariant to any positive reparametrizations of formula_181 and formula_190, and thus still depend on the orientation of the two curves.

Varifold is an alternative to currents when orientation becomes an issue as for instance in situations involving multiple bundles of curves for which no "consistent" orientation may be defined. Varifolds directly extend 0-dimensional measures by adding an extra tangent space direction to the position of points, leading to represent curves as measures on the product of formula_20 and the Grassmannian of all straight lines in formula_20. The matching problem between two curves then consists in replacing the endpoint matching term by formula_193 with varifold norms of the form:

where formula_195 is the non-oriented line directed by tangent vector formula_186 and formula_197 two scalar kernels respectively on formula_2 and the Grassmannian. Due to the inherent non-oriented nature of the Grassmannian representation, such expressions are invariant to positive and negative reparametrizations.

Surface matching share many similarities with the case of curves. Surfaces in formula_20 are parametrized in local charts by embeddings formula_200, with all reparametrizations formula_201 with formula_202 a diffeomorphism of U being equivalent geometrically. Currents and varifolds can be also used to formalize surface matching.

Oriented surfaces can be represented as 2-currents which are dual to differential 2-forms. In formula_20, one can further identify 2-forms with vector fields through the standard wedge product of 3D vectors. In that setting, surface matching writes again:

with the endpoint term formula_184 given through the norm

with formula_207 the normal vector to the surface parametrized by formula_208.

This surface mapping algorithm has been validated for brain cortical surfaces against CARET and FreeSurfer. LDDMM mapping for multiscale surfaces is discussed in.

For non-orientable or non-oriented surfaces, the varifold framework is often more adequate. Identifying the parametric surface formula_208 with a varifold formula_210 in the space of measures on the product of formula_20 and the Grassmannian, one simply replaces the previous current metric formula_212 by:

where formula_214 is the (non-oriented) line directed by the normal vector to the surface.

There are many settings in which there are a series of measurements, a time-series to which the underlying
coordinate systems will be matched and flowed onto. This occurs for example
in the dynamic growth and atrophy models and motion tracking such as have been explored in
An observed time sequence is given and the goal is to infer the time flow of geometric change of coordinates carrying the exemplars or templars through the period of observations.

The generic time-series matching problem considers the series of times is formula_215. The flow optimizes at the series of costs formula_216 giving optimization problems of the form

There have been at least three solutions offered thus far, piecewise geodesic, principal geodesic and splines.

The random orbit model of Computational Anatomy first appeared in modelling the change in coordinates associated to the randomness of the group acting on the templates, which induces the randomness on the source of images in the anatomical orbit of shapes and forms and resulting observations through the medical imaging devices. Such a random orbit model in which randomness on the group induces randomness on the images was examined for the Special Euclidean Group for object recognition in.

Depicted in the figure is a depiction of the random orbits around each exemplar, formula_218, generated by randomizing the flow by generating the initial tangent space vector field at the identity formula_94, and then generating random object formula_220.

The random orbit model induces the prior on shapes and images formula_221 conditioned on a particular atlas formula_222. For this the generative model generates the mean field formula_223 as a random change in coordinates of the template according to formula_224, where the diffeomorphic change in coordinates is generated randomly via the geodesic flows. The prior on random transformations formula_225 on formula_226 is induced by the flow formula_227, with formula_228 constructed as a Gaussian random field prior formula_229. The density on the random observables at the output of the sensor formula_230 are given by
formula_231

Shown in the Figure on the right the cartoon orbit, are a random spray of the subcortical manifolds generated by randomizing the vector fields formula_118 supported over the submanifolds.

The central statistical model of Computational Anatomy in the context of medical imaging has been the source-channel model of Shannon theory; the source is the deformable template of images formula_233, the channel outputs are the imaging sensors with observables formula_234 (see Figure).

See The Bayesian model of computational anatomy for discussions (i) MAP estimation with multiple atlases, (ii)
MAP segmentation with multiple atlases, MAP estimation of templates from populations.

Shape in computational anatomy is a local theory, indexing shapes and structures to templates to which they are bijectively mapped. Statistical shape in Computational Anatomy is the empirical study of diffeomorphic correspondences between populations and common template coordinate systems. This is a strong departure from Procrustes Analyses and shape theories pioneered by David G. Kendall in that the central group of Kendall's theories are the finite-dimensional Lie groups, whereas the theories of shape in Computational Anatomy have focused on the diffeomorphism group, which to first order via the Jacobian can be thought of as a field–thus infinite dimensional–of low-dimensional Lie groups of scale and rotations. 

The random orbit model provides the natural setting to understand empirical shape and shape statistics within Computational anatomy since the non-linearity of the induced probability law on anatomical shapes and forms 
formula_6 is induced via the reduction to the vector fields formula_236 at the tangent space at the identity of the diffeomorphism group. The successive flow of the Euler equation induces the random space of shapes and forms formula_237.

Performing empirical statistics on this tangent space at the identity is the natural way for inducing probability laws on the statistics of shape. Since both the vector fields and the Eulerian momentum formula_119 are in a Hilbert space the natural model is one of a Gaussian random field, so that given test function formula_239, then the inner-products with the test functions are Gaussian distributed with mean and covariance.

This is depicted in the accompanying figure where sub-cortical brain structures are depicted in a two-dimensional coordinate system based on inner products of their initial vector fields that generate them from the template is shown in a 2-dimensional span of the Hilbert space.

The study of shape and statistics in populations are local theories, indexing shapes and structures to templates to which they are bijectively mapped. Statistical shape is then the study of diffeomorphic correspondences relative to the template. A core operation is the generation of templates from populations, estimating a shape that is matched to the population. There are several important methods for generating templates including methods based on Frechet averaging, and statistical approaches based on the expectation-maximization algorithm and the Bayes Random orbit models of Computational anatomy. Shown in the accompanying figure is a subcortical template reconstruction from the population of MRI subjects.

Software suites containing a variety of diffeomorphic mapping algorithms include the following:





</doc>
<doc id="52657328" url="https://en.wikipedia.org/wiki?curid=52657328" title="Bayesian model of computational anatomy">
Bayesian model of computational anatomy

Computational anatomy (CA) is a discipline within medical imaging focusing on the study of anatomical shape and form at the visible or gross anatomical scale of morphology. 
The field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, including medical imaging, neuroscience, physics, probability, and statistics. It focuses on the anatomical structures being imaged, rather than the medical imaging devices. 
The central focus of the sub-field of computational anatomy within medical imaging is mapping information across anatomical coordinate systems most often dense information measured within a magnetic resonance image (MRI). The introduction of flows into CA, which are akin to the equations of motion used in fluid dynamics, exploit the notion that dense coordinates in image analysis follow the Lagrangian and Eulerian equations of motion. In models based on Lagrangian and Eulerian flows of diffeomorphisms, the constraint is associated to topological properties, such as open sets being preserved, coordinates not crossing implying uniqueness and existence of the inverse mapping, and connected sets remaining connected. The use of diffeomorphic methods grew quickly to dominate the field of mapping methods post Christensen's
original paper, with fast and symmetric methods becoming available.

The central statistical model of Computational Anatomy in the context of medical imaging has been the source-channel model of Shannon theory; the source is the deformable template of images formula_1, the channel outputs are the imaging sensors with observables formula_2 (see Figure). The importance of the source-channel model is that the variation in the anatomical configuration are modelled separated from the sensor variations of the Medical imagery. The Bayes theory dictates that the model is characterized by the prior on the source, formula_3 on formula_4, and the conditional density on the observable

conditioned on formula_6.

In deformable template theory, the images are linked to the templates, with the deformations a group which acts on the template;
see group action in computational anatomy
For image action formula_7, then the prior on the group formula_8 induces the prior on images formula_9, written as densities the log-posterior takes the form

The random orbit model which follows specifies how to generate the group elements and therefore the random spray of objects which form the prior distribution.

The random orbit model of Computational Anatomy first appeared in modelling the change in coordinates associated to the randomness of the group acting on the templates, which induces the randomness on the source of images in the anatomical orbit of shapes and forms and resulting observations through the medical imaging devices. Such a random orbit model in which randomness on the group induces randomness on the images was examined for the Special Euclidean Group for object recognition in which the group element
formula_11 was the special Euclidean group in.

For the study of deformable shape in CA, the high-dimensional diffeomorphism groups used in computational anatomy are generated via smooth flows formula_12 which satisfy the Lagrangian and Eulerian specification of the flow fields satisfying the ordinary differential equation: 

with formula_13 the vector fields on formula_14 termed the Eulerian velocity of the particles at position formula_15 of the flow. The vector fields are functions in a function space, modelled as a smooth Hilbert space with the vector fields having 1-continuous derivative . For formula_16, the inverse of the flow is given by
and the formula_17 Jacobian matrix for flows in formula_18 given as formula_19

To ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_14 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_21 using the Sobolev embedding theorems so that each element formula_22 has 3-square-integrable derivatives. Thus formula_21 embed smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:
where formula_24 with formula_25 a linear operator formula_26 defining the norm of the RKHS. The integral is calculated by integration by parts when formula_27 is a generalized function in the dual space formula_28.

In the random orbit model of computational anatomy, the entire flow is reduced to the initial condition which forms the coordinates encoding the diffeomorphism. From the initial condition formula_29 then geodesic positioning with respect to the Riemannian metric of Computational anatomy solves for the flow of the Euler-Lagrange equation.
Solving the geodesic from the initial condition formula_29 is termed the Riemannian-exponential, a mapping formula_31 at identity to the group.

The Riemannian exponential satisfies formula_32 for initial condition formula_33, vector field dynamics formula_34,

It is extended to the entire group, formula_41
Depicted in the accompanying figure is a depiction of the random orbits around each exemplar, formula_42, generated by randomizing the flow by generating the initial tangent space vector field at the identity formula_43, and then generating random object formula_44.

Shown in the Figure on the right the cartoon orbit, are a random spray of the subcortical manifolds generated by randomizing the vector fields formula_45 supported over the submanifolds.The random orbit model induces the prior on shapes and images formula_46 conditioned on a particular atlas formula_47. For this the generative model generates the mean field formula_48 as a random change in coordinates of the template according to formula_49, where the diffeomorphic change in coordinates is generated randomly via the geodesic flows.

The random orbit model induces the prior on shapes and images formula_46 conditioned on a particular atlas formula_51. For this the generative model generates the mean field formula_52 as a random change in coordinates of the template according to formula_53, where the diffeomorphic change in coordinates is generated randomly via the geodesic flows. The prior on random transformations formula_54 on formula_55 is induced by the flow formula_56, with formula_57 constructed as a Gaussian random field prior formula_58. The density on the random observables at the output of the sensor formula_59 are given by

Maximum a posteriori estimation (MAP) estimation is central to modern statistical theory. Parameters of interest formula_61 take many forms including (i) disease type such as neurodegenerative or neurodevelopmental diseases, (ii) structure type such as cortical or subcorical structures in problems associated to segmentation of images, and (iii) template reconstruction from populations. Given the observed image formula_62, MAP estimation maximizes the posterior:

This requires computation of the conditional probabilities formula_64. The multiple atlas orbit model randomizes over the denumerable set of atlases formula_65. The model on images in the orbit take the form of a multi-modal mixture distribution

The conditional Gaussian model has been examined heavily for inexact matching in dense images and for alndmark matching.

Model formula_67 as a conditionally Gaussian random field conditioned, mean field, formula_68. For uniform variance the endpoint error terms plays the role of the log-conditional (only a function of the mean field) giving the endpoint term:
Model formula_69 as conditionally Gaussian with mean field formula_70, constant noise variance independent of landmarks. The log-conditional (only a function of the mean field) can be viewed as the endpoint term:

The random orbit model for multiple atlases models the orbit of shapes as the union over multiple anatomical orbits generated from the group action of diffeomorphisms, formula_72, with each atlas having a template and predefined segmentation field formula_73. incorporating the parcellation into anatomical structures of the coordinate of the MRI.. The pairs are indexed over the voxel lattice formula_74 with an MRI image and a dense labelling of every voxel coordinate.The anatomical labelling of parcellated structures are manual delineations by neuroanatomists.

The Bayes segmentation problem is given measurement formula_75 with mean field and parcellation formula_76, the anatomical labelling formula_77. mustg be estimated for the measured MRI image. The mean-field of the observable formula_62 image is modelled as a random deformation from one of the templates formula_79, which is also randomly selected, formula_80. The optimal diffeomorphism formula_81 is hidden and acts on the background space of coordinates of the randomly selected template image formula_82. Given a single atlas formula_83, the likelihood model for inference is determined by the joint probability formula_84; with multiple atlases, the fusion of the likelihood functions yields the multi-modal mixture model with the prior averaging over models.

The MAP estimator of segmentation formula_85 is the maximizer formula_86 given formula_62, which involves the mixture over all atlases.

The quantity formula_89 is computed via a fusion of likelihoods from multiple deformable atlases, with formula_90 being the prior probability that the observed image evolves from the specific template image formula_91.

The MAP segmentation can be iteratively solved via the expectation-maximization(EM) algorithm

Generating templates empirically from populations is a fundamental operation ubiquitous to the discipline.
Several methods based on Bayesian statistics have emerged for submanifolds and dense image volumes.
For the dense image volume case, given the observable formula_93 the problem is to estimate the template in the orbit of dense images formula_6. Ma's procedure takes an initial hypertemplate formula_95 as the starting point, and models the template in the orbit under the unknown to be estimated diffeomorphism formula_96, with the parameters to be estimated the log-coordinates formula_97 determining the geodesic mapping of the hyper-template formula_98.

In the Bayesian random orbit model of computational anatomy the observed MRI images formula_99 are modelled as a conditionally Gaussian random field with mean field formula_100, with formula_101 a random unknown transformation of the template. The MAP estimation problem is to estimate the unknown template formula_102 given the observed MRI images.

Ma's procedure for dense imagery takes an initial hypertemplate formula_95 as the starting point, and models the template in the orbit under the unknown to be estimated diffeomorphism formula_96. The observables are modelled as conditional random fields, formula_105 a random field with mean field formula_106. The unknown variable to be estimated explicitly by MAP is the mapping of the hyper-template formula_107, with the other mappings considered as nuisance or hidden variables which are integrated out via the Bayes procedure. This is accomplished using the expectation-maximization (EM) algorithm.

The orbit-model is exploited by associating the unknown to be estimated flows to their log-coordinates formula_108 via the Riemannian geodesic log and exponential for computational anatomy the initial vector field in the tangent space at the identity so that formula_109, with formula_110 the mapping of the hyper-template.
The MAP estimation problem becomes

The EM algorithm takes as complete data the vector-field coordinates parameterizing the mapping, formula_108 and compute iteratively the conditional-expectation


</doc>
<doc id="49342572" url="https://en.wikipedia.org/wiki?curid=49342572" title="Group actions in computational anatomy">
Group actions in computational anatomy

Group actions are central to Riemannian geometry and defining orbits (control theory). 
The orbits of computational anatomy consist of anatomical shapes and medical images; the anatomical shapes are submanifolds of differential geometry consisting of points, curves, surfaces and subvolumes.
This generalized the ideas of the more familiar orbits of linear algebra which are linear vector spaces. Medical images are scalar and tensor images from medical imaging. The group actions are used to define models of human shape which accommodate variation. These orbits are deformable templates as originally formulated more abstractly in pattern theory.

The central model of human anatomy in computational anatomy is a Groups and group action, a classic formulation from differential geometry. The orbit is called the space of shapes and forms. The space of shapes are denoted formula_1, with the group formula_2 with law of composition formula_3; the action of the group on shapes is denoted formula_4, where the action of the group formula_5 is defined to satisfy 

The orbit formula_7 of the template becomes the space of all shapes, formula_8.

The central group in CA defined on volumes in formula_9 are the diffeomorphism group formula_10 which are mappings with 3-components formula_11, law of composition of functions formula_12, with inverse formula_13.

For sub-manifolds formula_14, parametrized by a chart or immersion formula_15, the diffeomorphic action the flow of the position

Most popular are scalar images, formula_17, with action on the right via the inverse.

Many different imaging modalities are being used with various actions. For images such that formula_19 is a three-dimensional vector then

Cao et al.
examined actions for mapping MRI images measured via diffusion tensor imaging and represented via there principle eigenvector. 
For tensor fields a positively oriented orthonormal basis 
formula_22
of formula_23, termed frames, vector cross product denoted formula_24 then
The Fr\'enet frame of three orthonormal vectors, formula_26 deforms as a tangent, formula_27 deforms like
a normal to the plane generated by formula_28, and formula_27. H is uniquely constrained by the
basis being positive and orthonormal.

For formula_30 non-negative symmetric matrices, an action would become formula_31.

For mapping MRI DTI images (tensors), then eigenvalues are preserved with the diffeomorphism rotating eigenvectors and preserves the eigenvalues. 
Given eigenelements
formula_32, then the action becomes

Orientation distribution function (ODF) characterizes the angular profile of the diffusion probability density function of water molecules and can be reconstructed from High Angular Resolution Diffusion Imaging (HARDI). The ODF is a probability density function defined on a unit sphere, formula_35. In the field of information geometry, the space of ODF forms a Riemannian manifold with the Fisher-Rao metric. For the purpose of LDDMM ODF mapping, the square-root representation is chosen because it is one of the most efficient representations found to date as the various Riemannian operations, such as geodesics, exponential maps, and logarithm maps, are available in closed form. In the following, denote square-root ODF (formula_36) as formula_37, where formula_37 is non-negative to ensure uniqueness and formula_39.

Denote diffeomorphic transformation as formula_40. Group action of diffeomorphism on formula_37, formula_42, needs to guarantee the non-negativity and formula_43. Based on the derivation in, this group action is defined as

where formula_45 is the Jacobian of formula_46.


</doc>
<doc id="49418115" url="https://en.wikipedia.org/wiki?curid=49418115" title="Large deformation diffeomorphic metric mapping">
Large deformation diffeomorphic metric mapping

Large deformation diffeomorphic metric mapping (LDDMM) is a specific suite of algorithms used for diffeomorphic mapping and manipulating dense imagery based on diffeomorphic metric mapping within the academic discipline of Computational anatomy, to be distinguished from its precursor based on diffeomorphic mapping. The distinction between the two is that diffeomorphic metric maps satisfy the property that the length associated to their flow away from the identity induces a metric on the group of diffeomorphisms, which in turn induces a metric on the orbit of Shapes and Forms within the field of Computational Anatomy. The study of shapes and forms with the metric of diffeomorphic metric mapping is called .

A diffeomorphic mapping system is a system designed to map, manipulate, and transfer information which is stored in many types of spatially distributed medical imagery.

Diffeomorphic mapping is the underlying technology for mapping and analyzing information measured in human anatomical coordinate systems which have been measured via Medical imaging. Diffeomorphic mapping is a broad term that actually refers to a number of different algorithms, processes, and methods. It is attached to many operations and has many applications for analysis and visualization. Diffeomorphic mapping can be used to relate various sources of information which are indexed as a function of spatial position as the key index variable. Diffeomorphisms are by their Latin root structure preserving transformations, which are in turn differentiable and therefore smooth, allowing for the calculation of metric based quantities such as arc length and surface areas. Spatial location and extents in human anatomical coordinate systems can be recorded via a variety of Medical imaging modalities, generally termed multi-modal medical imagery, providing either scalar and or vector quantities at each spatial location. Examples are scalar T1 or T2 Magnetic resonance imagery, or as 3x3 diffusion tensor matrices Diffusion MRI and Diffusion-weighted imaging, to scalar densities associated to Computed Tomography (CT), or functional imagery such as temporal data of functional magnetic resonance imaging and scalar densities such as Positron emission tomography (PET).

Computational anatomy is a subdiscipline within the broader field of Neuroinformatics within Bioinformatics and Medical imaging. The first algorithm for dense image mapping via diffeomorphic metric mapping was Beg's LDDMM for volumes and Joshi's landmark matching for point sets with correspondence, with LDDMM algorithms now available for computing diffeomorphic metric maps between non-corresponding landmarks and landmark matching intrinsic to spherical manifolds, curves, currents and surfaces, tensors, varifolds, and time-series. The term LDDMM was first established as part of the National Institutes of Health supported Biomedical Informatics Research Network.

In a more general sense, diffeomorphic mapping is any solution that registers or builds correspondences between dense coordinate systems in Medical imaging by ensuring the solutions are diffeomorphic. There are now many codes organized around diffeomorphic registration including ANTS, DARTEL, DEMONS, StationaryLDDMM, FastLDDMM, as examples of actively used computational codes for constructing correspondences between coordinate systems based on dense images.

The distinction between diffeomorphic metric mapping forming the basis for LDDMM and the earliest methods of diffeomorphic mapping is the introduction of a Hamilton principle of least-action in which large deformations are selected of shortest length corresponding to geodesic flows. This important distinction arises from the original formulation of the Riemannian metric corresponding to the right-invariance. The lengths of these geodesics give the metric in the metric space structure of human anatomy. Non-geodeisc formulations of diffeomorphic mapping in general does not correspond to any metric formulation.

Diffeomorphic mapping 3-dimensional information across coordinate systems is central to high-resolution Medical imaging and the area of Neuroinformatics within the newly emerging field of bioinformatics. Diffeomorphic mapping 3-dimensional coordinate systems as measured via high resolution dense imagery has a long history in 3-D beginning with Computed Axial Tomography (CAT scanning) in the early 80's by the University of Pennsylvania group led by Ruzena Bajcsy, and subsequently the Ulf Grenander school at Brown University with the HAND experiments. In the 90's there were several solutions for image registration which were associated to linearizations of small deformation and non-linear elasticity.

The central focus of the sub-field of Computational anatomy (CA) within medical imaging is mapping information across anatomical coordinate systems at the 1 millimeter morphome scale. In CA mapping of dense information measured within Magnetic resonance image (MRI) based coordinate systems such as in the brain has been solved via inexact matching of 3D MR images one onto the other. The earliest introduction of the use of diffeomorphic mapping via large deformation flows of diffeomorphisms for transformation of coordinate systems in image analysis and medical imaging was by Christensen, Rabbitt and Miller and Trouve. The introduction of flows, which are akin to the equations of motion used in fluid dynamics, exploit the notion that dense coordinates in image analysis follow the Lagrangian and Eulerian equations of motion. This model becomes more appropriate for cross-sectional studies in which brains and or hearts are not necessarily deformations of one to the other. Methods based on linear or non-linear elasticity energetics which grows with distance from the identity mapping of the template, is not appropriate for cross-sectional study. Rather, in models based on Lagrangian and Eulerian flows of diffeomorphisms, the constraint is associated to topological properties, such as open sets being preserved, coordinates not crossing implying uniqueness and existence of the inverse mapping, and connected sets remaining connected. The use of diffeomorphic methods grew quickly to dominate the field of mapping methods post Christensen's original paper, with fast and symmetric methods becoming available.

Such methods are powerful in that they introduce notions of regularity of the solutions so that they can be differentiated and local inverses can be calculated. The disadvantages of these methods is that there was no associated global least-action property which could score the flows of minimum energy. This contrasts the geodesic motions which are central to the study of Rigid body kinematics and the many problems solved in Physics via Hamilton's principle of least action. In 1998, Dupuis, Grenander and Miller established the conditions for guaranteeing the existence of solutions for dense image matching in the space of flows of diffeomorphisms. These conditions require an action penalizing kinetic energy measured via the Sobolev norm on spatial derivatives of the flow of vector fields.

The Large Deformation Diffeomorphic Metric Mapping (LDDMM) code that Faisal Beg derived and implemented for his PhD at Johns Hopkins University developed the earliest algorithmic code which solved for flows with fixed points satisfying the necessary conditions for the dense image matching problem subject to least-action. Computational anatomy now has many existing codes organized around diffeomorphic registration including ANTS, DARTEL, DEMONS, LDDMM, StationaryLDDMM as examples of actively used computational codes for constructing correspondences between coordinate systems based on dense images.

These large deformation methods have been extended to landmarks without registration via measure matching, curves, surfaces, dense vector and tensor imagery, and varifolds removing orientation.

Deformable shape in Computational Anatomy (CA)is studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinates in Medical Imaging. In this setting, three dimensional medical images are modelled as a random deformation of some exemplar, termed the template formula_1, with the set of observed images element in the random orbit model of CA for images formula_2. The template is mapped onto the target by defining a variational problem in which the template is transformed via the diffeomorphism used as a change of coordinate to minimize a squared-error matching condition between the transformed template and the target.

The diffeomorphisms are generated via smooth flows formula_3 , with formula_4, satisfying the Lagrangian and Eulerian specification of the flow field associated to the ordinary differential equation,
with 
formula_6 the Eulerian vector fields determining the flow.
The vector fields are guaranteed to be 1-time continuously differentiable formula_7 by modelling them to be in a 
smooth Hilbert space formula_8 supporting 1-continuous derivative. The inverse formula_9 is defined by the Eulerian vector-field with flow given by

To ensure smooth flows of diffeomorphisms with inverse, the vector fields with components in formula_10 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_11 using the Sobolev embedding theorems so that each element formula_12 has 3-times square-integrable weak-derivatives. Thus formula_11 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm

In CA the space of vector fields formula_11 are modelled as a reproducing Kernel Hilbert space (RKHS) defined by a 1-1, differential operatorformula_15 determining the norm formula_16 
where the integral is calculated by integration by parts when formula_17 is a generalized function in the dual space formula_18. 
The differential operator is selected so that the Green's kernel, the inverse of the operator, is continuously differentiable in each variable implying that the vector fields support 1-continuous derivative; see for the necessary conditions on the norm for existence of solutions.

The original large deformation diffeomorphic metric mapping (LDDMM) algorithms of Beg, Miller, Trouve, Younes was derived taking variations with respect to the vector field parameterization of the group, since formula_19 are in a vector spaces. Beg solved the dense image matching minimizing the action integral of kinetic energy of diffeomorphic flow while
minimizing endpoint matching term according to
Update until convergence, formula_20 each iteration, with formula_21:
This implies that the fixed point at formula_22 satisfies 
which in turn implies it satisfies the Conservation equation given by the according to 

The landmark matching problem has a pointwise correspondence defining the endpoint condition with geodesics given by the following minimum:

Joshi originally defined the registered landmark matching probleme. Update until convergence, formula_20 each iteration, with formula_21:
This implies that the fixed point satisfy 
with

The Calculus of variations was used in Beg to derive the iterative algorithm as a solution which when it converges satisfies the necessary maximizer conditions given by the necessary conditions for a first order variation requiring the variation of the endpoint with respect to a first order variation of the vector field. The directional derivative calculates the Gateaux derivative as calculated in Beg's original paper and.

</math>

The LDDMM variational problem is defined as

Beg solved the early LDDMM algorithms by solving the variational matching taking variations with respect to the vector fields. Another solution by Vialard, reparameterizes the optimization problem in terms of the state formula_31, for image formula_32, with the dynamics equation controlling the state by the control given in terms of the advection equation according to formula_33. The endpoint matching term
formula_34 gives the variational problem:


</doc>
<doc id="49400436" url="https://en.wikipedia.org/wiki?curid=49400436" title="Riemannian metric and Lie bracket in computational anatomy">
Riemannian metric and Lie bracket in computational anatomy

Computational anatomy (CA) is the study of shape and form in medical imaging. The study of deformable shapes in computational anatomy rely on high-dimensional diffeomorphism groups formula_1 which generate orbits of the form formula_2. In CA, this orbit is in general considered a smooth Riemannian manifold
since at every point of the manifold formula_3 there is an inner product inducing the norm formula_4 on the tangent space
that varies smoothly from point to point in the manifold of shapes formula_3. This is generated by viewing the
group of diffeomorphisms formula_1 as a Riemannian manifold with formula_7, associated to the tangent space at formula_8 . This induces the norm and metric on the orbit formula_3 under the action from the group of diffeomorphisms.

The diffeomorphisms in computational anatomy are generated to satisfy the Lagrangian and Eulerian specification of the flow fields, formula_10, generated via the ordinary differential equation
with the Eulerian vector fields formula_11 in formula_12 for formula_13, with the inverse for the flow given by
and the formula_14 Jacobian matrix for flows in formula_15 given as formula_16

To ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_12 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_18 using the Sobolev embedding theorems so that each element formula_19 has 3-square-integrable derivatives thusly implies formula_18 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:

Shapes in Computational Anatomy (CA) are studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinate systems. In this setting, 3-dimensional medical images are modelled as diffemorphic transformations of some exemplar, termed the template formula_21, resulting in the observed images to be elements of the random orbit model of CA. For images these are defined as formula_22, with for charts representing sub-manifolds denoted as formula_23.

The orbit of shapes and forms in Computational Anatomy are generated by the group actionformula_24. This is made into a Riemannian orbit by introducing a metric associated to each point and associated tangent space. For this a metric is defined on the group which induces the metric on the orbit. Take as the metric for Computational anatomy at each element of the tangent space formula_25 in the group of diffeomorphisms 
with the vector fields modelled to be in a Hilbert space with the norm in the Hilbert space formula_18. We model formula_28 as a reproducing kernel Hilbert space (RKHS) defined by a 1-1, differential operatorformula_29. For formula_30 a distribution or generalized function, the linear form formula_31 determines the norm:and inner product for formula_32 according to 
where the integral is calculated by integration by parts for formula_34 a generalized function formula_35 the dual-space.
The differential operator is selected so that the Green's kernel associated to the inverse is sufficiently smooth so that the vector fields support 1-continuous derivative.

The metric on the group of diffeomorphisms is defined by the distance as defined on pairs of elements in the group of diffeomorphisms according to

This distance provides a right-invariant metric of diffeomorphometry, invariant to reparameterization of space since for all formula_1,

The Lie bracket gives the adjustment of the velocity term resulting from a perturbation of the motion in the setting of curved spaces. Using Hamilton's principle of least-action derives the optimizing flows as a critical point for the action integral of the integral of the kinetic energy. The Lie bracket for vector fields in Computational Anatomy was first introduced in Miller, Trouve and Younes. The derivation calculates the perturbation formula_38 on the vector fields
formula_39 in terms of the derivative in time of the group perturbation adjusted by the correction of the Lie bracket of vector fields in this function setting involving the Jacobian matrix, unlike the matrix group case:
Proof:
Proving Lie bracket of vector fields take a first order perturbation of the flow at point formula_1.

The Lie bracket gives the first order variation of the vector field with respect to first order variation of the flow.

The Euler–Lagrange equation can be used to calculate geodesic flows through the group which form the basis for the metric. The action integral for the Lagrangian of the kinetic energy for Hamilton's principle becomes 

The action integral in terms of the vector field corresponds to integrating the kinetic energy
The shortest paths geodesic connections in the orbit are defined via Hamilton's Principle of least action requires first order variations of the solutions in the orbits of Computational Anatomy which are based on computing critical points on the metric length or energy of the path.
The original derivation of the Euler equation associated to the geodesic flow of diffeomorphisms exploits the was a generalized function equation whenformula_43 is a distribution, or generalized function, take the first order variation of the action integral using the adjoint operator for the Lie bracket () gives for all smooth formula_44, 
Using the bracket formula_46 and formula_47 gives
meaning for all smooth formula_48
Equation () is the Euler-equation when diffeomorphic shape momentum is a generalized function.

This equation has been called EPDiff, Euler–Poincare equation for diffeomorphisms and has been studied in the context of fluid mechanics for incompressible fluids with formula_50 metric.

In the random orbit model of Computational anatomy, the entire flow is reduced to the initial condition which forms the coordinates encoding the diffeomorphism, as well as providing the means of positioning information in the orbit. This was first terms a geodesic positioning system in Miller, Trouve, and Younes. From the initial condition formula_51 then geodesic positioning with respect to the Riemannian metric of Computational anatomy solves for the flow of the Euler–Lagrange equation. Solving the geodesic from the initial condition formula_51 is termed the Riemannian-exponential, a mapping formula_53 at identity to the group.

The Riemannian exponential satisfies formula_54 for initial condition formula_55, vector field dynamics formula_56, 


It is
extended to the entire group,
formula_62.

Matching information across coordinate systems is central to computational anatomy. Adding a matching term formula_63 to the action integral of Equation ()
which represents the target endpoint 
The endpoint term adds a boundary condition for the Euler–Lagrange equation ()
which gives the Euler equation with boundary term. Taking the variation gives

Proof: The Proof via variation calculus uses the perturbations from above and classic calculus of variation arguments.
The earliest large deformation diffeomorphic metric mapping (LDDMM) algorithms solved matching problems associated to images and registered landmarks. are in a vector spaces. The image matching geodesic equation satisfies the classical dynamical equation with endpoint condition. The necessary conditions for the geodesic for image matching takes the form of the classic Equation () of Euler–Lagrange with boundary condition:


The registered landmark matching problem satisfies the dynamical equation for generalized functions with endpoint condition:
Proof:

The variation formula_70 requires variation of the inverse formula_71 generalizes the matrix perturbation of the inverse via formula_72 giving 
formula_73
giving



</doc>
<doc id="52748480" url="https://en.wikipedia.org/wiki?curid=52748480" title="Plexciton">
Plexciton

Plexcitons are polaritonic modes that result from coherently coupled plasmons and excitons. Plexcitons aid direct energy flows in exciton energy transfer (EET). Plexcitons travel for 20 μm, similar to the width of a human hair.

Plasmons are a quantity of collective electron oscillations. Excitons are excited electrons bound to the hole produced by their excitation.

Molecular crystal excitons were combined with the collective excitations within metals to create plexcitons. This allowed EET to reach distances of around 20,000 nanometers, an enormous increase over the some 10 nanometers possible previously. However, the transfer direction was uncontrolled.

Topological insulators (TI) act as insulators below their surface, but have conductive surfaces, constraining electrons to move only along that surface. Even materials with moderately flawed surfaces do not impede current flow. Topological plexcitons make use of the properties of TIs to achieve similar control over the direction of current flow.

Plexcitons were found to emerge from an organic molecular layer (excitons) and a metallic film (plasmons). Dirac cones appeared in the plexcitons' two-dimensional band-structure. An external magnetic field created a gap between the cones when the system was interfaced to a magneto-optical layer. The resulting energy gap became populated with topologically protected one-way modes, which traveled only at the system interface.

Plexcitons potentially offer an appealing platform for exploring exotic matter phases and for controlling nanoscale energy flows.


</doc>
<doc id="52907991" url="https://en.wikipedia.org/wiki?curid=52907991" title="Front (physics)">
Front (physics)

In physics, a front is a solution of an spatially extended system connecting two steady states. From dynamical systems point of view, a front correspond to a heteroclinic orbit of the system in the co-mobile frame (or proper frame).

The most simple example of front solution connecting a homogeneous stable state with a homogeneous unstable state can be shown in the one-dimensional Fisher-Kolmogorov equation: 

that describes a simple model for the density formula_2 of population. This equation has two steady states, formula_3, and formula_4. This solution corresponds to extinction and saturation of population. Observe that this model is spatially-extended, because it includes a diffusion term given by the second derivative. The state formula_3 is stable as a simple linear analysis can show and the state formula_4 is unstable. There exist a family of front solutions connecting formula_7 with formula_8, and such solution are propagative. Particularly, there exist one solution of the form formula_9, where formula_10 constants, depending only on the coefficient of diffusion and growth constant k.


</doc>
<doc id="1964288" url="https://en.wikipedia.org/wiki?curid=1964288" title="Jahn–Teller effect">
Jahn–Teller effect

The Jahn–Teller effect (JT effect or JTE) is an important mechanism of spontaneous symmetry breaking in molecular and solid-state systems which has far-reaching consequences for different fields, and it is related to a variety of applications in spectroscopy, stereochemistry and crystal chemistry, molecular and solid-state physics, and materials science. The effect is named for Hermann Arthur Jahn and Edward Teller, who first reported studies about it in 1937.

The Jahn–Teller effect, sometimes also known as Jahn–Teller distortion, describes the geometrical distortion of molecules and ions that is associated with certain electron configurations. The Jahn–Teller theorem essentially states that any non-linear molecule with a spatially degenerate electronic ground state will undergo a geometrical distortion that removes that degeneracy, because the distortion lowers the overall energy of the species. For a description of another type of geometrical distortion that occurs in crystals with substitutional impurities see article off-center ions.

The Jahn–Teller effect is most often encountered in octahedral complexes of the transition metals. The phenomenon is very common in six-coordinate copper(II) complexes. The "d" electronic configuration of this ion gives three electrons in the two degenerate "e" orbitals, leading to a doubly degenerate electronic ground state. Such complexes distort along one of the molecular fourfold axes (always labelled the "z" axis), which has the effect of removing the orbital and electronic degeneracies and lowering the overall energy. The distortion normally takes the form of elongating the bonds to the ligands lying along the "z" axis, but occasionally occurs as a shortening of these bonds instead (the Jahn–Teller theorem does not predict the direction of the distortion, only the presence of an unstable geometry). When such an elongation occurs, the effect is to lower the electrostatic repulsion between the electron-pair on the Lewis basic ligand and any electrons in orbitals with a "z" component, thus lowering the energy of the complex. The inversion centre is preserved after the distortion.

In octahedral complexes, the Jahn–Teller effect is most pronounced when an odd number of electrons occupy the "e" orbitals. This situation arises in complexes with the configurations "d", low-spin "d" or high-spin "d" complexes, all of which have doubly degenerate ground states. In such compounds the "e" orbitals involved in the degeneracy point directly at the ligands, so distortion can result in a large energetic stabilisation. Strictly speaking, the effect also occurs when there is a degeneracy due to the electrons in the "t" orbitals ("i.e." configurations such as "d" or "d", both of which are triply degenerate). In such cases, however, the effect is much less noticeable, because there is a much smaller lowering of repulsion on taking ligands further away from the "t" orbitals, which do not point "directly" at the ligands (see the table below). The same is true in tetrahedral complexes (e.g. manganate: distortion is very subtle because there is less stabilisation to be gained because the ligands are not pointing directly at the orbitals.

The expected effects for octahedral coordination are given in the following table:
w: weak Jahn–Teller effect ("t" orbitals unevenly occupied)

s: strong Jahn–Teller effect expected ("e" orbitals unevenly occupied)

blank: no Jahn–Teller effect expected.

The Jahn–Teller effect is manifested in the UV-VIS absorbance spectra of some compounds, where it often causes splitting of bands. It is readily apparent in the structures of many copper(II) complexes. Additional, detailed information about the anisotropy of such complexes and the nature of the ligand binding can be however obtained from the fine structure of the low-temperature electron spin resonance spectra.

The underlying cause of the Jahn–Teller effect is the presence of molecular orbitals that are both degenerate and open shell (i.e., incompletely occupied). This situation is not unique to coordination complexes and can be encountered in other areas of chemistry. In organic chemistry the phenomenon of antiaromaticity has the same cause and also often sees molecules distorting; as in the case of cyclobutadiene and cyclooctatetraene (COT).

The JT theorem can be stated in different forms, two of which are given here:

Alternatively and considerably shorter:

Spin-degeneracy was an exception in the original treatment and was later treated separately.

The formal mathematical proof of the Jahn–Teller theorem rests heavily on symmetry arguments, more specifically the theory of molecular point groups. The argument of Jahn and Teller assumes no details about the electronic structure of the system. Jahn and Teller made no statement about the strength of the effect, which may be so small that it is immeasurable. Indeed, for electrons in non-bonding or weakly bonding molecular orbitals, the effect is expected to be weak. However, in many situations the JT effect is important.

Interest in the JTE increased after its first experimental verification. Various model systems were developed probing the degree of degeneracy and the type of symmetry. These were solved partly analytically and partly numerically to obtain the shape of the pertinent potential energy surfaces (PES) and the energy levels for the nuclear motion on the JT-split PES. These energy levels are not vibrational energy levels in the traditional sense because of the intricate coupling to the electronic motion that occurs, and are better termed vibronic energy levels. The new field of ‘vibronic coupling’ or ‘vibronic coupling theory’ was born.

A further breakthrough occurred upon the advent of modern ("ab initio") electronic structure calculations whereby the relevant parameters characterising JT systems can be reliably determined from first principles. Thus one could go beyond studies of model systems that explore the effect of parameter variations on the PES and vibronic energy levels; one could also go on beyond fitting these parameters to experimental data without clear knowledge about the significance of the fit. Instead, well-founded theoretical investigations became possible which greatly improved the insight into the phenomena at hand and into the details of the underlying mechanisms.
While recognizing the JTE distortion as a concrete example of the general spontaneous symmetry breaking mechanism, the exact degeneracy of the involved electronic state was identified as a non-essential ingredient for this symmetry breaking in polyatomic systems. Even systems that in the undistorted symmetric configuration present electronic states which are near in energy but not precisely degenerate, can show a similar tendency to distort. The distortions of these systems can be treated within the related theory of the pseudo Jahn–Teller effect (in the literature often referred to as "second-order JTE"). This mechanism is associated to the vibronic couplings between adiabatic PES separated by nonzero energy gaps across the configuration space: its inclusion extends the applicability of JT-related models to symmetry breaking in a far broader range of molecular and solid-state systems.

"Chronology:"



A given JT problem will have a particular point group symmetry, such as T symmetry for magnetic impurity ions in semiconductors or I symmetry for the fullerene C. JT problems are conventionally classified using labels for the irreducible representations (irreps) that apply to the symmetry of the electronic and vibrational states. For example, E ⊗ e would refer to an electronic doublet state transforming as E coupled to a vibrational doublet state transforming as e.

In general, a vibrational mode transforming as Λ will couple to an electronic state transforming as Γ if the symmetric part of the Kronecker product [Γ ⊗ Γ] contains Λ, unless Γ is a double group representation when the antisymmetric part {Γ ⊗ Γ} is considered instead. Modes which do couple are said to be JT-active.

As an example, consider a doublet electronic state E in cubic symmetry. The symmetric part of E ⊗ E is A + E. Therefore, the state E will couple to vibrational modes formula_1 transforming as a and e. However, the a modes will result in the same energy shift to all states and therefore do not contribute to any JT splitting. They can therefore be neglected. The result is an E ⊗ e JT effect. This JT effect is experienced by triangular molecules X, tetrahedral molecules ML, and octahedral molecules ML when their electronic state has E symmetry.

Components of a given vibrational mode are also labelled according to their transformation properties. For example, the two components of an e mode are usually labelled formula_2 and formula_3, which in octahedral symmetry transform as formula_4 and formula_5 respectively.

Eigenvalues of the Hamiltonian of a polyatomic system define PESs as functions of normal modes formula_1 of the system (i.e. linear combinations of the nuclear displacements with specific symmetry properties . At the reference point of high symmetry, where the symmetry-induced degeneracy occurs, several of the eigenvalues coincide. By a detailed and laborious analysis, Jahn and Teller showed that – excepting linear molecules – there are always first-order terms in an expansion of the matrix elements of the Hamiltonian in terms of symmetry-lowering (in the language of group theory: non-totally symmetric) normal modes. These linear terms represent forces that distort the system along these coordinates and lift the degeneracy. The point of degeneracy can thus not be stationary, and the system distorts toward a stationary point of lower symmetry where stability can be attained.

Proof of the JT theorem follows from the theory of molecular symmetry (point group theory). A less rigorous but more intuitive explanation is given in section .

To arrive at a quantitative description of the JT effect, the forces appearing between the component wave functions are described by expanding the Hamiltonian in a power series in the formula_1. Owing to the very nature of the degeneracy, the Hamiltonian takes the form of a matrix referring to the degenerate wave function components. A matrix element between states formula_8 and formula_9 generally reads as:

The expansion can be truncated after terms linear in the formula_1, or extended to include terms quadratic (or higher) in the formula_1.

The adiabatic potential energy surfaces (APES) are then obtained as the eigenvalues of this matrix. In the original paper it is proven that there are always linear terms in the expansion. It follows that the degeneracy of the wave function cannot correspond to a stable structure.

In mathematical terms, the APESs characterising the JT distortion arise as the eigenvalues of the potential energy matrix (as described in ). Generally, the APESs take the characteristic appearance of a double cone, circular or elliptic, where the point of contact, i.e. degeneracy, denotes the high-symmetry configuration for which the JT theorem applies. For the above case of the linear E ⊗ e JT effect the situation is illustrated by the APES

displayed in the figure, with part cut away to reveal its shape, which is known as a Mexican Hat potential. Here, formula_14 is the frequency of the vibrational e mode, formula_15 is its mass and formula_16 is a measure of the strength of the JT coupling. 

The conical shape near the degeneracy at the origin makes it immediately clear that this point cannot be stationary, that is, the system is unstable against asymmetric distortions, which leads to a symmetry lowering. In this particular case there are infinitely many isoenergetic JT distortions. The formula_1 giving these distortions are arranged in a circle, as shown by the red curve in the figure. Quadratic coupling or cubic elastic terms lead to a warping along this "minimum energy path", replacing this infinite manifold by three equivalent potential minima and three equivalent saddle points. In other JT systems, linear coupling results in discrete minima.

The high symmetry of the double-cone topology of the linear E ⊗ e JT system directly reflects the high underlying symmetry. It is one of the earliest (if not the earliest) examples in the literature of a conical intersection of potential energy surfaces. Conical intersections have received wide attention in the literature starting in the 1990s and are now considered paradigms of nonadiabatic excited-state dynamics, with far-reaching consequences in molecular spectroscopy, photochemistry and photophysics. Some of these will be commented upon further below. In general, conical intersections are far less symmetric than depicted in the figure. They can be tilted and elliptical in shape etc., and also peaked and sloped intersections have been distinguished in the literature. Furthermore, for more than two degrees of freedom, they are not point-like structures but instead they are seams and complicated, curved hypersurfaces, also known as intersection space. The coordinate sub-space displayed in the figure is also known as a branching plane.

The characteristic shape of the JT-split APES has specific consequences for the nuclear dynamics, here considered in the fully quantum sense. For sufficiently strong JT coupling, the minimum points are sufficiently far (at least by a few vibrational energy quanta) below the JT intersection. Two different energy regimes are then to be distinguished, those of low and high energy.



As already stated above, the distinction of low and high energy regimes is valid only for sufficiently strong JT couplings, that is, when several or many vibrational energy quanta fit into the energy window between the conical intersection and the minimum of the lower JT-split APES. For the many cases of small to intermediate JT couplings this energy window and the corresponding adiabatic low-energy regime does not exist. Rather, the levels on both JT-split APES are intricately mixed for all energies and the nuclear motion always proceeds on both JT split APES simultaneously.

In 1965, Frank Ham proposed that the dynamic JTE could reduce the expected values of observables associated with the orbital wavefunctions due to the superposition of several electronic states in the total vibronic wavefunction. This effect leads, for example, to a partial quenching of the spin-orbit interaction and allowed the results of previous Electron Paramagnetic Resonance (EPR) experiments to be explained.

In general, the result of an orbital operator acting on vibronic states can be replaced by an effective orbital operator acting on purely electronic states. In first order, the effective orbital operator equals the actual orbital operator multiplied by a constant, whose value is less than one, known as a first-order (Ham) reduction factor. For example, within a triplet T electronic state, the spin-orbit coupling operator formula_18 can be replaced by formula_19, where formula_20 is a function of the strength of the JT coupling which varies from 1 in zero coupling to 0 in very strong coupling. Furthermore, when second-order perturbation corrections are included, additional terms are introduced involving additional numerical factors, known as second-order (Ham) reduction factors. These factors are zero when there is no JT coupling but can dominate over first-order terms in strong coupling, when the first-order effects have been significantly reduced.
Reduction factors are particularly useful for describing experimental results, such as EPR and optical spectra, of paramagnetic impurities in semiconducting, dielectric, diamagnetic and ferrimagnetic hosts.

For a long time, applications of JT theory consisted mainly in parameter studies (model studies) where the APES and dynamical properties of JT systems have been investigated as functions on the system parameters such as coupling constants etc. Fits of these parameters to experimental data were often doubtful and inconclusive. The situation changed in the 1980s when efficient ab initio methods were developed and computational resources became powerful enough to allow for a reliable determination of these parameters from first principles. Apart from wave function-based 
techniques (which are sometimes considered genuinely ab initio in the literature) the advent of density functional theory (DFT) opened up new avenues to treat larger systems including solids. This allowed details of JT systems to be characterised and experimental findings to be reliably interpreted. It lies at the heart of most developments addressed in Section .

Two different strategies are conceivable and have been used in the literature. One can


Naturally, the more accurate approach (2) may be limited to smaller systems, while the simpler approach (1) lends itself to studies of larger systems.

The JT distortion of small molecules (or molecular ions) is directly deduced from electronic structure calculations of their APES (through DFT and/or ab initio computations). These molecules / ions are often radicals, such as trimers of alkali atoms (Li and Na), that have unpaired spins and in particular in (but not restricted to) doublet states. Besides the JTE in E' and E" states, also the between an E state and a nearby A state may play a role. The JT distortion reduces the symmetry from D to C (see figure), and it depends on the details of the interactions whether the isosceles triangle has an acute or an obtuse-angled (such as Na) minimum energy structure. Natural extensions are systems like NO and NH where a JT distortion has been documented in the literature for ground or excited electronic states. 
A somewhat special role is played by tetrahedral systems like CH and P. Here threefold degenerate electronic states and vibrational modes come into play. Nevertheless, also twofold degeneracies continue to be important.

Among larger systems, a focus in the literature has been on benzene and its radical cation, as well as on their halo (especially fluoro) derivatives. Already in the early 1980s, a wealth of information emerged from the detailed analysis of experimental emission spectra of 1,3,5- trifluoro- and hexafluoro (and chloro) benzene radical cations. For the parent benzene cation one has to rely on photoelectron spectra with comparatively lower resolution because this species does not fluoresce (see also Section on ). Rather detailed ab initio calculations have been carried out which 
document the JT stabilization energies for the various (four) JT active modes and also quantify the moderate barriers for the JT pseudorotation.

Finally, a somewhat special role is played by systems with a fivefold symmetry axis like the cyclopentadienyl radical. Careful laser spectroscopic investigations have shed useful light on the JT interactions. In particular they reveal that the barrier to pseudorotation almost vanishes (the system is highly "fluxional") which can be attributed to the fact that the 2nd-order coupling terms vanish by symmetry and the leading higher-order terms are of 4th order.

The JTE is usually stronger where the electron density associated with the degenerate orbitals is more concentrated. This effect therefore plays a large role in determining the structure of transition metal complexes with active internal 3d orbitals. 

The most iconic and prominent of the JT systems in coordination chemistry is probably the case of Cu(II) octahedral complexes. While in perfectly equivalent coordination, like a CuF complex associated to a Cu(II) impurity in a cubic crystal like KMgF, perfect octahedral (O) symmetry is expected. In fact a lower tetragonal symmetry is usually found experimentally. The origin of this JTE distortion it revealed by examining the electronic configuration of the undistorted complex. For an octahedral geometry, the five 3d orbitals partition into t and e orbitals (see diagram). These orbitals are occupied by nine electrons corresponding to the formula_21 electronic configuration of Cu(II). Thus, the t shell is filled, and the e shell contains 3 electrons. Overall the unpaired electron produces a E state, which is Jahn–Teller active. The third electron can occupy either of the orbitals comprising the e shell: the mainly formula_4 orbital or the mainly formula_5 orbital. If the electron occupies the mainly formula_4 level, which antibonding orbital the final geometry of the complex would be elongated as the axial ligands will be pushed away to reduce the global energy of the system. On the other hand, if the electron went into the mainly formula_5 antibonding orbital the complex would distort into a compressed geometry. Experimentally elongated geometries are overwhelmingly observed and this fact has been attributed both to metal-ligand anharmonic interactions and 3d-4s hybridisations. Given that all the directions containing a fourfold axis are equivalent the distortion is equally likely to happen in any of these orientations. From the electronic point of view this means that the formula_4 and formula_5 orbitals, that are degenerate and free to hybridise in the octahedral geometry, will mix to produce appropriate equivalent orbitals in each direction like formula_28 or formula_29.

The JTE is not just restricted to Cu(II) octahedral complexes. There are many other configurations, involving changes both in the initial structure and electronic configuration of the metal that yield degenerate states and, thus, JTE. However, the amount of distortion and stabilisation energy of the effect is strongly dependent on the particular case. In octahedral Cu(II), the JTE is particularly strong because


In other configurations involving π or δ bonding, like for example when the degenerate state is associated to the t orbitals of an octahedral configuration, the distortion and stabilisation energies are usually much smaller and the possibility of not observing the distortion due to dynamic JT effects is much higher. Similarly for rare-earth ions where covalency is very small, the distortions associated to the JTE are usually very weak.

Importantly, the JTE is associated with strict degeneracy in the electronic subsystem and so it cannot appear in systems without this property. For example, the JTE is often associated to cases like quasi-octahedral CuXY complexes where the distances to X and Y ligands are clearly different. However, the intrinsic symmetry of these complexes is already tetragonal and no degenerate e orbital exists, having split into a (mainly formula_4) and b (mainly formula_5) orbitals due to the different electronic interactions with axial X ligands and equatorial Y ligands. In this and other similar cases some remaining vibronic effects related to the JTE are still present but are quenched with respect to the case with degeneracy due to the splitting of the orbitals.

From spectra with rotational resolution, moments of inertia and hence bond lengths and angles can be determined "directly" (at least in principle). From less well-resolved spectra one can still determine important quantities like JT stabilization energies and energy barriers (e.g. to pseudorotation). However, in the whole spectral intensity distribution formula_32 of an electronic transition more information is encoded. It has been used to decide on the presence (or absence) of the geometric phase which is accumulated during the pseudorotational motion around the JT (or other type of) conical intersection. Prominent examples of either type are the ground (X) or an excited (B) state of Na. The Fourier transform of formula_32, the so-called autocorrelation function formula_34 reflects the motion of the wavepacket after an optical (= vertical) transition to the APES of the final electronic state. Typically it will move on the timescale of a vibrational period which is (for small molecules) of the order of 5-50 fs, i.e. ultrafast. Besides a nearly periodic motion, mode-mode interactions with very irregular (also chaotic) behaviour and spreading of the wavepacket may also occur. Near a conical intersection this will be accompanied/complemented by nonradiative transitions (termed internal conversion) to other APESs occurring on the same ultrafast time scale.

For the JT case the situation is somewhat special, as compared to a general conical intersection, because the different JT potential sheets are symmetry-related to each other and have (exactly or nearly) the same energy minimum. The "transition" between them is thus more oscillatory than one would normally expect, and their time-averaged populations are close to 1/2. For a more typical scenario a more general conical intersection is "required".

The JT effect still comes into play, namely in combination with a different nearby, in general non-degenerate electronic state. The result is a pseudo Jahn–Teller effect, for example, of an E state interacting with an A state. This situation is common in JT systems, just as interactions between two nondegenerate electronic states are common for non-JT systems. Examples are excited electronic states of NH and the benzene radical cation. Here, crossings between the E and A state APESs amount to triple intersections, which are associated with very complex spectral features (dense line structures and diffuse spectral envelopes under low resolution). The population transfer between the states is also ultrafast, so fast that fluorescence (proceeding on a nanosecond time scale) cannot compete. This helps to understand why the benzene cation, like many other organic radical cation, does not fluoresce.

To be sure, photochemical reactivity emerges when the internal conversion makes the system explore the nuclear configuration space such that new chemical species are formed. There is a plethora of femtosecond pump-probe spectroscopic techniques to reveal details of these processes occurring, for example, in the process of vision.

As proposed originally by Landau 
free electrons in a solid, introduced for example by doping or irradiation, can interact with the vibrations of the lattice to form a localized quasi-particle known as a polaron. Strongly localized polarons (also called Holstein polarons) can condensate around high-symmetry sites of the lattice with electrons or holes occupying local degenerate orbitals that experience the JTE. These Jahn–Teller polarons break both translational and point group symmetries of the lattice where they are found and have been attributed important roles in effects like colossal magnetoresistance and superconductivity.

Paramagnetic impurities in semiconducting, dielectric, diamagnetic and ferrimagnetic hosts can all be described using a JT model. For example, these models were used extensively in the 1980s and 1990s to describe ions of Cr, V and Ti substituting for Ga in GaAs and GaP.

The fullerene C can form solid compounds with alkali metals known as fullerides. CsC can be superconducting at temperatures up to 38K under applied pressure, whereas compounds of the form AC are insulating (as reviewed by Gunnarsson ). JT effects both within the C molecules (intramolecular) and between C molecules (intermolecular) play a part in the mechanisms behind various observed properties in these systems. For example, they could mean that the Migdal-Eliashberg treatment of superconductivity breaks down. Also, the fullerides can form a so-called new state of matter known as a Jahn–Teller metal, where localised electrons coexist with metallicity and JT distortions on the C molecules persist 

The JTE is usually associated with degeneracies that are well localised in space, like those occurring in a small molecule or associated to an isolated transition metal complex. However, in many periodic high-symmetry solid-state systems, like perovskites, some crystalline sites allow for electronic degeneracy giving rise under adequate compositions to lattices of JT-active centers. This can produce a cooperative JTE, where global distortions of the crystal occur due to local degeneracies.

In order to determine the final electronic and geometric structure of a cooperative JT system, it is necessary to take into account both the local distortions and the interaction between the different sites, which will take such form necessary to minimise the global energy of the crystal.

While works on the cooperative JTE started in the late fifties 

, it was in 1960 that Kanamori published the first work on the cooperative JTE where many important elements present in the modern theory for this effect were introduced. This included the use of pseudospin notation to discuss orbital ordering, and discussions of the importance of the JTE to discuss magnetism, the competition of this effect with the spin-orbit coupling and the coupling of the distortions with the strain of the lattice. This point was later stressed in the review by Gehring and Gehring as being the key element to establish long-range order between the distortions in the lattice. An important part of the modern theory of the cooperative JTE, can lead to structural phase transitions.

It is important to note that many cooperative JT systems would be expected to be metals from band theory as, to produce them, a degenerate orbital has to be partially filled and the associated band would be metallic. However, under the perturbation of the symmetry-breaking distortion associated to the cooperative JTE, the degeneracies in the electronic structure are destroyed and the ground state of these systems is often found to be insulating (see e.g.). In many important cases like the parent compound for colossal magnetoresistance perovskites, LaMnO, an increase of temperature leads to disorder in the distortions which lowers the band splitting due to the cooperative JTE, thus triggering a metal-insulator transition.

In modern solid-state physics, it is common to classify systems according to the kind of degrees of freedom they have available, like electron (metals) or spin (magnetism). In crystals that can display the JTE, and before this effect is realised by symmetry-breaking distortions, it is found that there exists an orbital degree of freedom consisting of how electrons occupy the local degenerate orbitals. As initially discussed by Kugel and Khomskii, not all configurations are equivalent. The key is the relative orientation of these occupied orbital, in the same way that spin orientation is important in magnetic systems, and the ground state can only be realised for some particular orbital pattern. Both this pattern and the effect giving rise to this phenomenon is usually denominated orbital-ordering.

In order to predict the orbital-ordering pattern, Kugel and Khomskii used a particularisation of the Hubbard model. In particular they established how superexchange interactions, usually described by the Anderson–Kanamori–Goodenough rules, change in the presence of degenerate orbitals. Their model, using a pseudospin representation for the local orbitals, leads to a Heisenberg-like model in which the ground state is a combination of orbital and spin patterns. Using this model it can be shown, for example, that the origin of the unusual ground insulating ferromagnetic state of a solid like KCuF can be traced to its orbital ordering.

Even when starting from a relatively high-symmetry structure the combined effect of exchange interactions, spin-orbit coupling, orbital-ordering and crystal deformations activated by the JTE can lead to very low symmetry magnetic patterns with specific properties. For example, in CsCuCl an incommensurable helicoidal pattern appears both for the orbitals and the distortions along the formula_35-axis. Moreover, many of these compounds show complex phase diagrams when varying temperature or pressure.




</doc>
<doc id="102847" url="https://en.wikipedia.org/wiki?curid=102847" title="Solid-state physics">
Solid-state physics

Solid-state physics is the study of rigid matter, or solids, through methods such as quantum mechanics, crystallography, electromagnetism, and metallurgy. It is the largest branch of condensed matter physics. Solid-state physics studies how the large-scale properties of solid materials result from their atomic-scale properties. Thus, solid-state physics forms a theoretical basis of materials science. It also has direct applications, for example in the technology of transistors and semiconductors.

Solid materials are formed from densely packed atoms, which interact intensely. These interactions produce the mechanical (e.g. hardness and elasticity), thermal, electrical, magnetic and optical properties of solids. Depending on the material involved and the conditions in which it was formed, the atoms may be arranged in a regular, geometric pattern (crystalline solids, which include metals and ordinary water ice) or irregularly (an amorphous solid such as common window glass).

The bulk of solid-state physics, as a general theory, is focused on crystals. Primarily, this is because the periodicity of atoms in a crystal — its defining characteristic — facilitates mathematical modeling. Likewise, crystalline materials often have electrical, magnetic, optical, or mechanical properties that can be exploited for engineering purposes.

The forces between the atoms in a crystal can take a variety of forms. For example, in a crystal of sodium chloride (common salt), the crystal is made up of ionic sodium and chlorine, and held together with ionic bonds. In others, the atoms share electrons and form covalent bonds. In metals, electrons are shared amongst the whole crystal in metallic bonding. Finally, the noble gases do not undergo any of these types of bonding. In solid form, the noble gases are held together with van der Waals forces resulting from the polarisation of the electronic charge cloud on each atom. The differences between the types of solid result from the differences between their bonding.

The physical properties of solids have been common subjects of scientific inquiry for centuries, but a separate field going by the name of solid-state physics did not emerge until the 1940s, in particular with the establishment of the Division of Solid State Physics (DSSP) within the American Physical Society. The DSSP catered to industrial physicists, and solid-state physics became associated with the technological applications made possible by research on solids. By the early 1960s, the DSSP was the largest division of the American Physical Society.

Large communities of solid state physicists also emerged in Europe after World War II, in particular in England, Germany, and the Soviet Union. In the United States and Europe, solid state became a prominent field through its investigations into semiconductors, superconductivity, nuclear magnetic resonance, and diverse other phenomena. During the early Cold War, research in solid state physics was often not restricted to solids, which led some physicists in the 1970s and 1980s to found the field of condensed matter physics, which organized around common techniques used to investigate solids, liquids, plasmas, and other complex matter. Today, solid-state physics is broadly considered to be the subfield of condensed matter physics that focuses on the properties of solids with regular crystal lattices.

Many properties of materials are affected by their crystal structure. This structure can be investigated using a range of crystallographic techniques, including X-ray crystallography, neutron diffraction and electron diffraction.

The sizes of the individual crystals in a crystalline solid material vary depending on the material involved and the conditions when it was formed. Most crystalline materials encountered in everyday life are polycrystalline, with the individual crystals being microscopic in scale, but macroscopic single crystals can be produced either naturally (e.g. diamonds) or artificially.

Real crystals feature defects or irregularities in the ideal arrangements, and it is these defects that critically determine many of the electrical and mechanical properties of real materials.

Properties of materials such as electrical conduction and heat capacity are investigated by solid state physics. An early model of electrical conduction was the Drude model, which applied kinetic theory to the electrons in a solid. By assuming that the material contains immobile positive ions and an "electron gas" of classical, non-interacting electrons, the Drude model was able to explain electrical and thermal conductivity and the Hall effect in metals, although it greatly overestimated the electronic heat capacity.

Arnold Sommerfeld combined the classical Drude model with quantum mechanics in the free electron model (or Drude-Sommerfeld model). Here, the electrons are modelled as a Fermi gas, a gas of particles which obey the quantum mechanical Fermi–Dirac statistics. The free electron model gave improved predictions for the heat capacity of metals, however, it was unable to explain the existence of insulators.

The nearly free electron model is a modification of the free electron model which includes a weak periodic perturbation meant to model the interaction between the conduction electrons and the ions in a crystalline solid. By introducing the idea of electronic bands, the theory explains the existence of conductors, semiconductors and insulators.

The nearly free electron model rewrites the Schrödinger equation for the case of a periodic potential. The solutions in this case are known as Bloch states. Since Bloch's theorem applies only to periodic potentials, and since unceasing random movements of atoms in a crystal disrupt periodicity, this use of Bloch's theorem is only an approximation, but it has proven to be a tremendously valuable approximation, without which most solid-state physics analysis would be intractable. Deviations from periodicity are treated by quantum mechanical perturbation theory.

Research topics in solid state physics include:




</doc>
<doc id="51167783" url="https://en.wikipedia.org/wiki?curid=51167783" title="Silicon nanowire">
Silicon nanowire

Silicon nanowires, also referred to as SiNWs, are a type of semiconductor nanowire most often formed from a silicon precursor by etching of a solid or through catalyzed growth from a vapor or liquid phase. Such nanowires have promising applications in lithium ion batteries and sensors. Initial synthesis of SiNWs is often accompanied by thermal oxidation steps to yield structures of accurately tailored size and morphology.

SiNWs have unique properties that are not seen in bulk (three-dimensional) silicon materials. These properties arise from an unusual quasi one-dimensional electronic structure and are the subject of research across numerous disciplines and applications. The reason that SiNWs are considered as one of the most important one-dimensional materials is they could have a function as building blocks for nanoscale electronics assembled without the need for complex and costly fabrication facilities. SiNWs are frequently studied towards applications including photovoltaics, nanowire batteries, thermoelectrics and non-volatile memory.

Owing to their unique physical and chemical properties, silicon nanowires are a promising candidate for a wide range of applications that draw on their unique physico-chemical characteristics, which differ from those of bulk silicon material.

SiNWs exhibit charge trapping behavior which renders such systems of value in applications necessitating electron hole separation such as photovoltaics, and photocatalysts. Recent experiment on nanowire solar cells has led to a remarkable improvement of the power conversion efficiency of SiNW solar cells from <1% to >17% in the last few years.

Charge trapping behaviour and tuneable surface governed transport properties of SiNWs render this category of nanostructures of interest towards use as metal insulator semiconductors and field effect transistors, with further applications as nanoelectronic storage devices, in flash memory, logic devices as well as chemical and biological sensors.

The ability for lithium ions to intercalate into silicon structures renders various Si nanostructures of interest towards applications as anodes in Li-ion batteries (LiBs). SiNWs are of particular merit as such anodes as they exhibit the ability to undergo significant lithiation while maintaining structural integrity and electrical connectivity.

Several synthesis methods are known for SiNWs and these can be broadly divided into methods which start with bulk silicon and remove material to yield nanowires, also known as top-down synthesis, and methods which use a chemical or vapor precursor to build nanowires in a process generally considered to be bottom-up synthesis.

These methods use material removal techniques to produce nanostructures from a bulk precursor



Subsequent to physical or chemical processing, either top-down or bottom-up, to obtain initial silicon nanostructures, thermal oxidation steps are often applied in order to obtain materials with desired size and aspect ratio. Silicon nanowires exhibit a distinct and useful self-limiting oxidation behaviour whereby oxidation effectively ceases due to diffusion limitations, which can be modeled. This phenomenon allows accurate control of dimensions and aspect ratios in SiNWs and has been used to obtain high aspect ratio SiNWs with diameters below 5 nm. The self-limiting oxidation of SiNWs is of value towards lithium ion battery materials.

The orientation of SiNWs has profound influence on the strucutal and electronic properties of the systems. For this reason several procedures have been proposed for the alignment of nanowires in chosen orientations. This includes the use of electric fields in polar alignment, electrophoresis, mircofluidic methods and contact printing.

There is significant interest in SiNWs for their unique properties and the ability to control size and aspect ratio with great accuracy. As yet, limitations in large-scale fabrication impede the uptake of this material in the full range of investigated applications. Combined studies of synthesis methods, oxidation kinetics and properties of SiNW systems aim to overcome the present limitations and facilitate the implementation of SiNW systems, for example, high quality vapor-liquid-solid–grown SiNWs with smooth surfaces can be reversibly stretched with 10% or more elastic strain, approaching the theoretical elastic limit of silicon, which could open the doors for the emerging “elastic strain engineering” and flexible bio-/nano-electronics.


</doc>
<doc id="41232" url="https://en.wikipedia.org/wiki?curid=41232" title="Harmonic">
Harmonic

A harmonic is any member of the harmonic series. The term is employed in various disciplines, including music, physics, acoustics, electronic power transmission, radio technology, and other fields. It is typically applied to repeating signals, such as sinusoidal waves. A harmonic of such a wave is a wave with a frequency that is a positive integer multiple of the frequency of the original wave, known as the fundamental frequency. The original wave is also called the 1st harmonic, the following harmonics are known as higher harmonics. As all harmonics are periodic at the fundamental frequency, the sum of harmonics is also periodic at that frequency. For example, if the fundamental frequency is 50 Hz, a common AC power supply frequency, the frequencies of the first three higher harmonics are 100 Hz (2nd harmonic), 150 Hz (3rd harmonic), 200 Hz (4th harmonic) and any addition of waves with these frequencies is periodic at 50 Hz.

In music, harmonics are used on string instruments and wind instruments as a way of producing sound on the instrument, particularly to play higher notes and, with strings, obtain notes that have a unique sound quality or "tone colour". On strings, harmonics that are bowed have a "glassy", pure tone. On stringed instruments, harmonics are played by touching (but not fully pressing down the string) at an exact point on the string while sounding the string (plucking, bowing, etc.); this allows the harmonic to sound, a pitch which is always higher than the fundamental frequency of the string.

Harmonics may also be called "overtones", "partials" or "upper partials". The difference between "harmonic" and "overtone" is that the term "harmonic" includes all of the notes in a series, including the fundamental frequency (e.g., the open string of a guitar). The term "overtone" only includes the pitches above the fundamental. In some music contexts, the terms "harmonic", "overtone" and "partial" are used fairly interchangeably.

Most acoustic instruments emit complex tones containing many individual partials (component simple tones or sinusoidal waves), but the untrained human ear typically does not perceive those partials as separate phenomena. Rather, a musical note is perceived as one sound, the quality or timbre of that sound being a result of the relative strengths of the individual partials. Many acoustic oscillators, such as the human voice or a bowed violin string, produce complex tones that are more or less periodic, and thus are composed of partials that are near matches to integer multiples of the fundamental frequency and therefore resemble the ideal harmonics and are called "harmonic partials" or simply "harmonics" for convenience (although it's not strictly accurate to call a partial a harmonic, the first being real and the second being ideal).

Oscillators that produce harmonic partials behave somewhat like one-dimensional resonators, and are often long and thin, such as a guitar string or a column of air open at both ends (as with the modern orchestral transverse flute). Wind instruments whose air column is open at only one end, such as trumpets and clarinets, also produce partials resembling harmonics. However they only produce partials matching the odd harmonics, at least in theory. The reality of acoustic instruments is such that none of them behaves as perfectly as the somewhat simplified theoretical models would predict.

Partials whose frequencies are not integer multiples of the fundamental are referred to as "inharmonic partials". Some acoustic instruments emit a mix of harmonic and inharmonic partials but still produce an effect on the ear of having a definite fundamental pitch, such as pianos, strings plucked pizzicato, vibraphones, marimbas, and certain pure-sounding bells or chimes. Antique singing bowls are known for producing multiple harmonic partials or multiphonics.

An overtone is any partial higher than the lowest partial in a compound tone. The relative strengths and frequency relationships of the component partials determine the timbre of an instrument. The similarity between the terms overtone and partial sometimes leads to their being loosely used interchangeably in a musical context, but they are counted differently, leading to some possible confusion. In the special case of instrumental timbres whose component partials closely match a harmonic series (such as with most strings and winds) rather than being inharmonic partials (such as with most pitched percussion instruments), it is also convenient to call the component partials "harmonics" but not strictly correct (because harmonics are numbered the same even when missing, while partials and overtones are only counted when present). This chart demonstrates how the three types of names (partial, overtone, and harmonic) are counted (assuming that the harmonics are present):

In many musical instruments, it is possible to play the upper harmonics without the fundamental note being present. In a simple case (e.g., recorder) this has the effect of making the note go up in pitch by an octave, but in more complex cases many other pitch variations are obtained. In some cases it also changes the timbre of the note. This is part of the normal method of obtaining higher notes in wind instruments, where it is called "overblowing". The extended technique of playing multiphonics also produces harmonics. On string instruments it is possible to produce very pure sounding notes, called harmonics or "flageolets" by string players, which have an eerie quality, as well as being high in pitch. Harmonics may be used to check at a unison the tuning of strings that are not tuned to the unison. For example, lightly fingering the node found halfway down the highest string of a cello produces the same pitch as lightly fingering the node of the way down the second highest string. For the human voice see Overtone singing, which uses harmonics.

While it is true that electronically produced periodic tones (e.g. square waves or other non-sinusoidal waves) have "harmonics" that are whole number multiples of the fundamental frequency, practical instruments do not all have this characteristic. For example, higher "harmonics"' of piano notes are not true harmonics but are "overtones" and can be very sharp, i.e. a higher frequency than given by a pure harmonic series. This is especially true of instruments other than stringed or brass/woodwind ones, e.g., xylophone, drums, bells etc., where not all the overtones have a simple whole number ratio with the fundamental frequency. The fundamental frequency is the reciprocal of the period of the periodic phenomenon.

The following table displays the stop points on a stringed instrument, such as the guitar (guitar harmonics), at which gentle touching of a string will force it into a harmonic mode when vibrated. String harmonics (flageolet tones) are described as having a "flutelike, silvery quality" that can be highly effective as a special color or tone color (timbre) when used and heard in orchestration. It is unusual to encounter natural harmonics higher than the fifth partial on any stringed instrument except the double bass, on account of its much longer strings. Harmonics are widely used in plucked string instruments, such as acoustic guitar, electric guitar and electric bass. On an electric guitar played loudly through a guitar amplifier with distortion, harmonics are more sustained and can be used in guitar solos. In the heavy metal music lead guitar style known as shred guitar, harmonics, both natural and artificial, are widely used.

Although harmonics are most often used on open strings (natural harmonics), occasionally a score will call for an artificial harmonic, produced by playing an overtone on an already stopped string. As a performance technique, it is accomplished by using two fingers on the fingerboard, the first to shorten the string to the desired fundamental, with the second touching the node corresponding to the appropriate harmonic. On fretted instruments, such as an electric guitar, the performer can look at the frets to determine where to stop the string and where to touch the node. On unfretted instruments, such as the violin and related instruments, playing artificial harmonics is an advanced technique, as it requires the performer to find two precise locations on the same string.

Harmonics may be either used or considered as the basis of just intonation systems. Composer Arnold Dreyblatt is able to bring out different harmonics on the single string of his modified double bass by slightly altering his unique bowing technique halfway between hitting and bowing the strings. Composer Lawrence Ball uses harmonics to generate music electronically.




</doc>
<doc id="53790076" url="https://en.wikipedia.org/wiki?curid=53790076" title="Conformon">
Conformon

From a biological standpoint, the goal-directed molecular motions inside living cells are carried out by biopolymers acting like molecular machines (e.g. myosin, RNA/DNA polymerase, ion pumps, etc.). These molecular machines are driven by conformons, that is sequence-specific mechanical strains generated by free energy released in chemical reactions or stress induced destabilisations in supercoiled biopolymer chains. Therefore, conformons can be defined as packets of conformational energy generated from substrate binding or chemical reactions and confined within biopolymers.

On the other hand, from a physics standpoint, the conformon is a localization of elastic and electronic energy which may propagate in space with or without dissipation. The mechanism which involves dissipationless propagation is a form of molecular superconductivity. On quantum mechanical level both elastic/vibrational and electronic energy can be quantised, therefore the conformon carries a fixed portion of energy. This has led to the definition of quantum of conformation (shape).


</doc>
<doc id="53991267" url="https://en.wikipedia.org/wiki?curid=53991267" title="CCPForge">
CCPForge

The Collaborative Computational Projects (CCP) group was responsible for the development of CCPForge, which is a software development tool produced through collaborations by the CCP community. CCPs allow experts in computational research to come together and develop scientific software which can be applied to numerous research fields. It is used as a tool in many research and development areas, and hosts a variety of projects. Every CCP project is the result of years of valuable work by computational researchers.

It is advised for projects to have one application, this helps users to search a category and classification system so they can find the right project for their work. Furthermore, the project can be under up to three CCPs provided it is a collaboration. Each classification category will have sub-sections to filter the category further. CCPForge projects, such provide essential information which has been used in publications such as 'Recent developments in R-matrix applications to molecular processes' and 'Ab initio derivation of Hubbard models for cold atoms in optical lattices', in which codes from CCPQ were used.

The Joint Information Systems Committee (JISC) and EPSRC both fund the CCPForge project. The Scientific Computing Department (SCD) of the Science and Technology Facilities Council is responsible for the development and maintenance of CCPForge, and this is funded by a long term support grant from EPSRC.

<nowiki>*</nowiki> CCPQ was formed from CCP2 "Continuum States of Atoms and Molecules", incorporating aspects of CCP6 "Molecular Quantum Dynamics".


</doc>
<doc id="52160778" url="https://en.wikipedia.org/wiki?curid=52160778" title="Normal contact stiffness">
Normal contact stiffness

Normal contact stiffness is a physical quantity related to the generalized force displacement behavior of rough surfaces in contact with a rigid body or a second similar rough surface. As two solid bodies of the same material approach one another, they transition from conditions of non-contact to homogeneous bulk type behaviour. The varying values of stiffness and true contact area that is exhibited at an interface during this transition is dependent on conditions of applied pressure and is of notable importance for the study of systems involving the physical interactions of multiple bodies including granular matter, electrode contacts, and thermal contacts, where the interface-localized structures govern overall system performance.

The role of surface structure in normal contact mechanics, in terms of stiffness and true contact area is a frequently studied topic. Parameters of roughness, fractal dimension and asperity geometry are often discussed with reference to their significance on contact mechanics of surfaces.


</doc>
<doc id="2178434" url="https://en.wikipedia.org/wiki?curid=2178434" title="Straw tracker">
Straw tracker

A straw tracker is a type of particle detector which uses many straw chambers to track the path of a particle. 

The path of a particle is determined by the best fit to all the straws with hits. Since the time for a particular straw to produce a signal is proportional to the distance of the particle's closest approach to that chamber's wire, if a particle on a predictable path (e.g. a helix in a magnetic field) passes through many straws, the path of the particle can be determined more precisely than the size of any particular straw.



</doc>
<doc id="84400" url="https://en.wikipedia.org/wiki?curid=84400" title="Zero-point energy">
Zero-point energy

Zero-point energy (ZPE) is the difference between the lowest possible energy that a quantum mechanical system may have, and the classical minimum energy of the system. Unlike in classical mechanics, quantum systems constantly fluctuate in their lowest energy state due to the Heisenberg uncertainty principle. As well as atoms and molecules, the empty space of the vacuum has these properties. According to quantum field theory, the universe can be thought of not as isolated particles but continuous fluctuating fields: matter fields, whose quanta are fermions (i.e. leptons and quarks), and force fields, whose quanta are bosons (e.g. photons and gluons). All these fields have zero-point energy. These fluctuating zero-point fields lead to a kind of reintroduction of an aether in physics, since some systems can detect the existence of this energy. However this aether cannot be thought of as a physical medium if it is to be Lorentz invariant such that there is no contradiction with Einstein's theory of special relativity.

Physics currently lacks a full theoretical model for understanding zero-point energy; in particular the discrepancy between theorized and observed vacuum energy is a source of major contention. Physicists Richard Feynman and John Wheeler calculated the zero-point radiation of the vacuum to be an order of magnitude greater than nuclear energy, with a single light bulb containing enough energy to boil all the world's oceans. Yet according to Einstein's theory of general relativity any such energy would gravitate and the experimental evidence from both the expansion of the universe, dark energy and the Casimir effect show any such energy to be exceptionally weak. A popular proposal that attempts to address this issue is to say that the fermion field has a negative zero-point energy while the boson field has positive zero-point energy and thus these energies somehow cancel each other out. This idea would be true if supersymmetry were an exact symmetry of nature. However, the LHC at CERN has so far found no evidence to support supersymmetry. Moreover, it is known that if supersymmetry is valid at all, it is at most a broken symmetry, only true at very high energies, and no one has been able to show a theory where zero-point cancellations occur in the low energy universe we observe today. This discrepancy is known as the cosmological constant problem and it is one of the greatest unsolved mysteries in physics. Many physicists believe that "the vacuum holds the key to a full understanding of nature".

The term zero-point energy (ZPE) is a translation from the German Nullpunktsenergie.
The terms zero-point radiation or ground state energy are also sometimes used interchangeably. The term zero-point field (ZPF) can be used when referring to a specific vacuum field, for instance the QED vacuum which specifically deals with quantum electrodynamics (e.g. electromagnetic interactions between photons, electrons and the vacuum) or the QCD vacuum which deals with quantum chromodynamics (e.g. color charge interactions between quarks, gluons and the vacuum). A vacuum can be viewed not as empty space but as the combination of all zero-point fields. In quantum field theory this combination of fields is called the vacuum state, its associated zero-point energy is called the vacuum energy and the average energy value is called the vacuum expectation value (VEV) also called its condensate.

In classical mechanics all particles can be thought of as having some energy made up of their potential energy and kinetic energy. Temperature, for example, arises from the intensity of random particle motion caused by kinetic energy (known as brownian motion). As temperature is reduced to absolute zero, it might be thought that all motion ceases and particles come completely to rest. In fact, however, kinetic energy is retained by particles even at the lowest possible temperature. The random motion corresponding to this zero-point energy never vanishes as a consequence of the uncertainty principle of quantum mechanics.
The uncertainty principle states that no object can ever have precise values of position and velocity simultaneously. The total energy of a quantum mechanical object (potential and kinetic) is described by its Hamiltonian which also describes the system as a harmonic oscillator, or wave function, that fluctuates between various energy states (see wave-particle duality). All quantum mechanical systems undergo fluctuations even in their ground state, a consequence of their wave-like nature. The uncertainty principle requires every quantum mechanical system to have a fluctuating zero-point energy greater than the minimum of its classical potential well. This results in motion even at absolute zero. For example, liquid helium does not freeze under atmospheric pressure regardless of temperature due to its zero-point energy.

Given the equivalence of mass and energy expressed by Einstein's , any point in space that contains energy can be thought of as having mass to create particles. Virtual particles spontaneously flash into existence at every point in space due to the energy of quantum fluctuations caused by the uncertainty principle. Modern physics has developed quantum field theory (QFT) to understand the fundamental interactions between matter and forces, it treats every single point of space as a quantum harmonic oscillator. According to QFT the universe is made up of matter fields, whose quanta are fermions (i.e. leptons and quarks), and force fields, whose quanta are bosons (e.g. photons and gluons). All these fields have zero-point energy. Recent experiments advocate the idea that particles themselves can be thought of as excited states of the underlying quantum vacuum, and that all properties of matter are merely vacuum fluctuations arising from interactions of the zero-point field.

The idea that "empty" space can have an intrinsic energy associated to it, and that there is no such thing as a "true vacuum" is seemingly unintuitive. It is often argued that the entire universe is completely bathed in the zero-point radiation, and as such it can add only some constant amount to calculations. Physical measurements will therefore reveal only deviations from this value. For many practical calculations zero-point energy is dismissed by fiat in the mathematical model as a term that has no physical effect. Such treatment causes problems however, as in Einstein's theory of general relativity the absolute energy value of space is not an arbitrary constant and gives rise to the cosmological constant. For decades most physicists assumed that there was some undiscovered fundamental principle that will remove the infinite zero-point energy and make it completely vanish. If the vacuum has no intrinsic, absolute value of energy it will not gravitate. It was believed that as the universe expands from the aftermath of the big bang, the energy contained in any unit of empty space will decrease as the total energy spreads out to fill the volume of the universe; galaxies and all matter in the universe should begin to decelerate. This possibility was ruled out in 1998 by the discovery that the expansion of the universe is not slowing down but is in fact accelerating, meaning empty space does indeed have some intrinsic energy. The discovery of dark energy is best explained by zero-point energy, though it still remains a mystery as to why the value appears to be so small compared to huge value obtained through theory - the cosmological constant problem.

Many physical effects attributed to zero-point energy have been experimentally verified, such as spontaneous emission, Casimir force, Lamb shift, magnetic moment of the electron and Delbrück scattering, these effects are usually called "radiative corrections". In more complex nonlinear theories (e.g. QCD) zero-point energy can give rise to a variety of complex phenomena such as multiple stable states, symmetry breaking, chaos and emergence. Many physicists believe that "the vacuum holds the key to a full understanding of nature" and that studying it is critical in the search for the theory of everything. Active areas of research include the effects of virtual particles, quantum entanglement, the difference (if any) between inertial and gravitational mass, variation in the speed of light, a reason for the observed value of the cosmological constant and the nature of dark energy.

Zero-point energy evolved from historical ideas about the vacuum. To Aristotle the vacuum was , "the empty"; space independent of body. He believed this concept violated basic physical principles and asserted that the elements of fire, air, earth, and water were not made of atoms, but were continuous. To the atomists the concept of emptiness had absolute character: it was the distinction between existence and nonexistence. Debate about the characteristics of the vacuum were largely confined to the realm of philosophy, it was not until much later on with the beginning of the renaissance, that Otto von Guericke invented the first vacuum pump and the first testable scientific ideas began to emerge. It was thought that a totally empty volume of space could be created by simply removing all gases. This was the first generally accepted concept of the vacuum.

Late in the 19th century, however, it became apparent that the evacuated region still contained thermal radiation. The existence of the aether as a substitute for a true void was the most prevalent theory of the time. According to the successful electromagnetic aether theory based upon Maxwell's electrodynamics, this all-encompassing aether was endowed with energy and hence very different from nothingness. The fact that electromagnetic and gravitational phenomena were easily transmitted in empty space indicated that their associated aethers were part of the fabric of space itself. Maxwell himself noted that:

However, the results of the Michelson–Morley experiment in 1887 were the first strong evidence that the then-prevalent aether theories were seriously flawed, and initiated a line of research that eventually led to special relativity, which ruled out the idea of a stationary aether altogether. To scientists of the period, it seemed that a true vacuum in space might be completely eliminated by cooling thus eliminating all radiation or energy. From this idea evolved the second concept of achieving a real vacuum: cool it down to absolute zero temperature after evacuation. Absolute zero was technically impossible to achieve in the 19th century, so the debate remained unsolved.

In 1900, Max Planck derived the average energy of a single "energy radiator", e.g., a vibrating atomic unit, as a function of absolute temperature:

where is Planck's constant, is the frequency, is Boltzmann's constant, and is the absolute temperature. The zero-point energy makes no contribution to Planck's original law, as its existence was unknown to Planck in 1900.

The concept of zero-point energy was developed by Max Planck in Germany in 1911 as a corrective term added to a zero-grounded formula developed in his original quantum theory in 1900.

In 1912, Max Planck published the first journal article to describe the discontinuous emission of radiation, based on the discrete quanta of energy. In Planck's "second quantum theory" resonators absorbed energy continuously, but emitted energy in discrete energy quanta only when they reached the boundaries of finite cells in phase space, where their energies became integer multiples of . This theory led Planck to his new radiation law, but in this version energy resonators possessed a zero-point energy, the smallest average energy a resonator could take on. Planck's radiation equation contained a residual energy factor, one , as an additional term dependent on the frequency , which was greater than zero (where is Planck's constant). It is therefore widely agreed that "Planck's equation marked the birth of the concept of zero-point energy." In a series of papers from 1911 to 1913, Planck found that the average energy of an oscillator to be:

Soon, the idea of zero-point energy attracted the attention of Albert Einstein and his assistant Otto Stern. In 1913 they published a paper that attempted to prove the existence of zero-point energy by calculating the specific heat of hydrogen gas and compared it with the experimental data. However, after assuming they had succeeded, they retracted support for the idea shortly after publication because they found Planck's second theory may not apply to their example. In a letter to Paul Ehrenfest of the same year Einstein declared zero-point energy “dead as a doornail” Zero-point energy was also invoked by Peter Debye, who noted that zero-point energy of the atoms of a crystal lattice would cause a reduction in the intensity of the diffracted radiation in X-ray diffraction even as the temperature approached absolute zero. In 1916 Walther Nernst proposed that empty space was filled with zero-point electromagnetic radiation. With the development of general relativity Einstein found the energy density of the vacuum to contribute towards a cosmological constant in order to obtain static solutions to his field equations; the idea that empty space, or the vacuum, could have some intrinsic energy associated to it had returned, with Einstein stating in 1920:

In 1913 Niels Bohr had proposed what is now called the Bohr model of the atom, but despite this it remained a mystery as to why electrons do not fall into their nuclei. According to classical ideas, the fact that an accelerating charge loses energy by radiating implied that an electron should spiral into the nucleus and that atoms should not be stable. This problem of classical mechanics was nicely summarized by James Hopwood Jeans in 1915: "There would be a very real difficulty in supposing that the (force) law held down to the zero values of . For the forces between two charges at zero distance would be infinite; we should have charges of opposite sign continually rushing together and, when once together, no force would tend to shrink into nothing or to diminish indefinitely in size" This resolution to this puzzle came in 1926 with Schrodinger's famous equation. This equation explained the new, non-classical, fact that as an electron moves close to a nucleus its kinetic energy necessarily increases in such a way that the minimum total energy (kinetic plus potential) occurs at some positive separation rather than at zero separation; in other words, that zero-point energy is essential for atomic stability.

In 1926 Pascual Jordan published the first attempt to quantize the electromagnetic field. In a joint paper with Max Born and Werner Heisenberg he considered the field inside a cavity as a superposition of quantum harmonic oscillators. In his calculation he found that in addition to the "thermal energy" of the oscillators there also had to exist infinite zero-point energy term. He was able to obtain the same fluctuation formula that Einstein had obtained in 1909. However, Jordan did not think that his infinite zero-point energy term was "real", writing to Einstein that "it is just a quantity of the calculation having no direct physical meaning" Jordan found a way to get rid of the infinite term, publishing a joint work with Pauli in 1928, performing what has been called "the first infinite subtraction, or renormalisation, in quantum field theory"
Building on the work of Heisenberg and others Paul Dirac's theory of emission and absorption (1927) was the first application of the quantum theory of radiation. Dirac's work was seen as crucially important to the emerging field of quantum mechanics; it dealt directly with the process in which "particles" are actually created: spontaneous emission. Dirac described the quantization of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. The theory showed that spontaneous emission depends upon the zero-point energy fluctuations of the electromagnetic field in order to get started. In a process in which a photon is annihilated (absorbed), the photon can be thought of as making a transition into the vacuum state. Similarly, when a photon is created (emitted), it is occasionally useful to imagine that the photon has made a transition out of the vacuum state. In the words of Dirac:

Contemporary physicists, when asked to give a physical explanation for spontaneous emission, generally invoke the zero-point energy of the electromagnetic field. This view was popularized by Victor Weisskopf who in 1935 wrote:

This view was also later supported by (1948), who argued that spontaneous emission "can be thought of as forced emission taking place under the action of the fluctuating field." This new theory, which Dirac coined quantum electrodynamics (QED) predicted a fluctuating zero-point or "vacuum" field existing even in the absence of sources.

Throughout the 1940s improvements in microwave technology made it possible to take more precise measurements of the shift of the levels of a hydrogen atom, now known as the Lamb shift, and measurement of the magnetic moment of the electron. Discrepancies between these experiments and Dirac's theory led to the idea of incorporating renormalisation into QED to deal with zero-point infinities. Renormalization was originally developed by Hans Kramers and also Victor Weisskopf(1936), and first successfully applied to calculate a finite value for the Lamb shift by Hans Bethe (1947). As per spontaneous emission, these effects can in part be understood with interactions with the zero-point field. But in light of renormalisation being able to remove some zero-point infinities from calculations, not all physicists were comfortable attributing zero-point energy any physical meaning, viewing it instead as a mathematical artifact that might one day be fully eliminated. In Wolfgang Pauli's 1945 Nobel lecture he made clear his opposition to the idea of zero-point energy stating "It is clear that this zero-point energy has no physical reality".
In 1948 Hendrik Casimir showed that one consequence of the zero-point field is an attractive force between two uncharged, perfectly conducting parallel plates, the so-called Casimir effect. At the time, Casimir was studying the properties of "colloidal solutions". These are viscous materials, such as paint and mayonnaise, that contain micron-sized particles in a liquid matrix. The properties of such solutions are determined by van der Waals forces – long-range, attractive forces that exist between neutral atoms and molecules. One of Casimir's colleagues, Theo Overbeek, realized that the theory that was used at the time to explain van der Waals forces, which had been developed by Fritz London in 1930, did not properly explain the experimental measurements on colloids. Overbeek therefore asked Casimir to investigate the problem. Working with Dirk Polder, Casimir discovered that the interaction between two neutral molecules could be correctly described only if the fact that light travels at a finite speed was taken into account. Soon afterwards after a conversation with Bohr about zero-point energy, Casimir noticed that this result could be interpreted in terms of vacuum fluctuations. He then asked himself what would happen if there were two mirrors – rather than two molecules – facing each other in a vacuum. It was this work that led to his famous prediction of an attractive force between reflecting plates. The work by Casimir and Polder opened up the way to a unified theory of van der Waals and Casimir forces and a smooth continuum between the two phenomena. This was done by Lifshitz (1956) in the case of plane parallel dielectric plates. The generic name for both van der Waals and Casimir forces is dispersion forces, because both of them are caused by dispersions of the operator of the dipole moment. The role of relativistic forces becomes dominant at orders of a hundred nanometers.

In 1951 Herbert Callen and Theodore Welton proved the quantum fluctuation-dissipation theorem (FDT) which was originally formulated in classical form by Nyquist (1928) as an explanation for observed Johnson noise in electric circuits. Fluctuation-dissipation theorem showed that when something dissipates energy, in an effectively irreversible way, a connected heat bath must also fluctuate. The fluctuations and the dissipation go hand in hand; it is impossible to have one without the other. The implication of FDT being that the vacuum could be treated as a heat bath coupled to a dissipative force and as such energy could, in part, be extracted from the vacuum for potentially useful work. FDT has been shown to be true experimentally under certain quantum, non-classical, conditions.

In 1963 the Jaynes–Cummings model was developed describing the system of a two-level atom interacting with a quantized field mode (i.e. the vacuum) within an optical cavity. It gave nonintuitive predictions such as that an atom's spontaneous emission could be driven by field of effectively constant frequency (Rabi frequency). In the 1970s experiments were being performed to test aspects of quantum optics and showed that the rate of spontaneous emission of an atom could be controlled using reflecting surfaces. These results were at first regarded with suspicion in some quarters: it was argued that no modification of a spontaneous emission rate would be possible, after all, how can the emission of a photon be affected by an atom's environment when the atom can only "see" its environment by emitting a photon in the first place? These experiments gave rise to cavity quantum electrodynamics (CQED), the study of effects of mirrors and cavities on radiative corrections. Spontaneous emission can be suppressed (or "inhibited") or amplified. Amplification was first predicted by Purcell in 1946 (the Purcell effect) and has been experimentally verified. This phenomenon can be understood, partly, in terms of the action of the vacuum field on the atom.

Zero-point energy is fundamentally related to the Heisenberg uncertainty principle. Roughly speaking, the uncertainty principle states that complementary variables (such as a particle's position and momentum, or a field's value and derivative at a point in space) cannot simultaneously be specified precisely by any given quantum state. In particular, there cannot exist a state in which the system simply sits motionless at the bottom of its potential well: for, then, its position and momentum would both be completely determined to arbitrarily great precision. Therefore, instead, the lowest-energy state (the ground state) of the system must have a distribution in position and momentum that satisfies the uncertainty principle−−which implies its energy must be greater than the minimum of the potential well.

Near the bottom of a potential well, the Hamiltonian of a general system (the quantum-mechanical operator giving its energy) can be approximated as a quantum harmonic oscillator,
where is the minimum of the classical potential well.

The uncertainty principle tells us that
making the expectation values of the kinetic and potential terms above satisfy

The expectation value of the energy must therefore be at least
where is the angular frequency at which the system oscillates.

A more thorough treatment, showing that the energy of the ground state actually saturates this bound and is exactly , requires solving for the ground state of the system.

The idea of a quantum harmonic oscillator and its associated energy can apply to either an atom or subatomic particle. In ordinary atomic physics, the zero-point energy is the energy associated with the ground state of the system. The professional physics literature tends to measure frequency, as denoted by above, using angular frequency, denoted with and defined by . This leads to a convention of writing Planck's constant with a bar through its top () to denote the quantity . In these terms, the most famous such example of zero-point energy is the above associated with the ground state of the quantum harmonic oscillator. In quantum mechanical terms, the zero-point energy is the expectation value of the Hamiltonian of the system in the ground state.

If more than one ground state exists, they are said to be degenerate. Many systems have degenerate ground states. Degeneracy occurs whenever there exists a unitary operator which acts non-trivially on a ground state and commutes with the Hamiltonian of the system.

According to the third law of thermodynamics, a system at absolute zero temperature exists in its ground state; thus, its entropy is determined by the degeneracy of the ground state. Many systems, such as a perfect crystal lattice, have a unique ground state and therefore have zero entropy at absolute zero. It is also possible for the highest excited state to have absolute zero temperature for systems that exhibit negative temperature.

The wave function of the ground state of a particle in a one-dimensional well is a half-period sine wave which goes to zero at the two edges of the well. The energy of the particle is given by:

where is the Planck constant, is the mass of the particle, is the energy state ( corresponds to the ground-state energy), and is the width of the well.

In quantum field theory (QFT), the fabric of "empty" space is visualized as consisting of fields, with the field at every point in space and time being a quantum harmonic oscillator, with neighboring oscillators interacting with each other. According to QFT the universe is made up of matter fields whose quanta are fermions (e.g. electrons and quarks) and force fields, whose quanta are bosons (i.e. photons and gluons). All these fields have zero-point energy. A related term is "zero-point field" (ZPF), which is the lowest energy state of a particular field. The vacuum can be viewed not as empty space, but as the combination of all zero-point fields.

In QFT this combination of fields is called the vacuum state, its associated zero-point energy is called the vacuum energy and the average expectation value of the Hamiltonian is called the vacuum expectation value (also called condensate or simply VEV). The QED vacuum is a part of the vacuum state which specifically deals with quantum electrodynamics (e.g. electromagnetic interactions between photons, electrons and the vacuum) and the QCD vacuum deals with quantum chromodynamics (e.g. color charge interactions between quarks, gluons and the vacuum). Recent experiments advocate the idea that particles themselves can be thought of as excited states of the underlying quantum vacuum, and that all properties of matter are merely vacuum fluctuations arising from interactions with the zero-point field.

Each point in space makes a contribution of , resulting in a calculation of infinite zero-point energy in any finite volume; this is one reason renormalization is needed to make sense of quantum field theories. In cosmology, the vacuum energy is one possible explanation for the cosmological constant and the source of dark energy. 
Scientists are not in agreement about how much energy is contained in the vacuum. Quantum mechanics requires the energy to be large as Paul Dirac claimed it is, like a sea of energy. Other scientists specializing in General Relativity require the energy to be small enough for curvature of space to agree with observed astronomy. The Heisenberg uncertainty principle allows the energy to be as large as needed to promote quantum actions for a brief moment of time, even if the average energy is small enough to satisfy relativity and flat space. To cope with disagreements, the vacuum energy is described as a virtual energy potential of positive and negative energy.

In quantum perturbation theory, it is sometimes said that the contribution of one-loop and multi-loop Feynman diagrams to elementary particle propagators are the contribution of vacuum fluctuations, or the zero-point energy to the particle masses.

The oldest and best known quantized force field is the electromagnetic field. Maxwell's equations have been superseded by quantum electrodynamics (QED). By considering the zero-point energy that arises from QED it is possible to gain a characteristic understanding of zero-point energy that arises not just through electromagnetic interactions but in all quantum field theories.

In the quantum theory of the electromagnetic field, classical wave amplitudes and are replaced by operators and that satisfy:

The classical quantity appearing in the classical expression for the energy of a field mode is replaced in quantum theory by the photon number operator . The fact that:

implies that quantum theory does not allow states of the radiation field for which the photon number and a field amplitude can be precisely defined, i.e., we cannot have simultaneous eigenstates for and . The reconciliation of wave and particle attributes of the field is accomplished via the association of a probability amplitude with a classical mode pattern. The calculation of field modes is entirely classical problem, while the quantum properties of the field are carried by the mode "amplitudes" and associated with these classical modes.

The zero-point energy of the field arises formally from the non-commutativity of and . This is true for any harmonic oscillator: the zero-point energy appears when we write the Hamiltonian:

It is often argued that the entire universe is completed bathed in the zero-point electromagnetic field, and as such it can add only some constant amount to expectation values. Physical measurements will therefore reveal only deviations from the vacuum state. Thus the zero-point energy can be dropped from the Hamiltonian by redefining the zero of energy, or by arguing that it is a constant and therefore has no effect on Heisenberg equations of motion. Thus we can choose to declare by fiat that the ground state has zero energy and a field Hamiltonian, for example, can be replaced by:

without affecting any physical predictions of the theory. The new Hamiltonian is said to be normally ordered (or Wick ordered) and is denoted by a double-dot symbol. The normally ordered Hamiltonian is denoted , i.e.:

In other words, within the normal ordering symbol we can commute and . Since zero-point energy is intimately connected to the non-commutativity of and , the normal ordering procedure eliminates any contribution from the zero-point field. This is especially reasonable in the case of the field Hamiltonian, since the zero-point term merely adds a constant energy which can be eliminated by a simple redefinition for the zero of energy. Moreover, this constant energy in the Hamiltonian obviously commutes with and and so cannot have any effect on the quantum dynamics described by the Heisenberg equations of motion.

However, things are not quite that simple. The zero-point energy cannot be eliminated by dropping its energy from the Hamiltonian: When we do this and solve the Heisenberg equation for a field operator, we must include the vacuum field, which is the homogeneous part of the solution for the field operator. In fact we can show that the vacuum field is essential for the preservation of the commutators and the formal consistent of QED. When we calculate the field energy we obtain not only a contribution from particles and forces that may be present but also a contribution from the vacuum field itself i.e. the zero-point field energy. In other words, the zero-point energy reappears even though we may have deleted it from the Hamiltonian.

From Maxwell's equations, the electromagnetic energy of a "free" field i.e. one with no sources, is described by:

We introduce the "mode function" that satisfies the Helmholtz equation:

where and assume it is normalized such that:

We wish to "quantize" the electromagnetic energy of free space for a multimode field. The field intensity of free space should be independent of position such that should be independent of for each mode of the field. The mode function satisfying these conditions is:

where in order to have the transversality condition satisfied for the Coulomb gauge in which we are working.

To achieve the desired normalization we pretend space is divided into cubes of volume and impose on the field the periodic boundary condition:

or equivalently

where can assume any integer value. This allows us to consider the field in any one of the imaginary cubes and to define the mode function:

which satisfies the Helmholtz equation, transversality, and the "box normalization":

where is chosen to be a unit vector which specifies the polarization of the field mode. The condition means that there are two independent choices of , which we call and where and . Thus we define the mode functions:

in terms of which the vector potential becomes:

or:

where and , are photon annihilation and creation operators for the mode with wave vector and polarization . This gives the vector potential for a plane wave mode of the field. The condition for shows that there are infinitely many such modes. The linearity of Maxwell's equations allows us to write:

for the total vector potential in free space. Using the fact that:

we find the field Hamiltonian is:

This is the Hamiltonian for an infinite number of uncoupled harmonic oscillators. Thus different modes of the field are independent and satisfy the commutation relations:

Clearly the least eigenvalue for is:

This state describes the zero-point energy of the vacuum. It appears that this sum is divergent – in fact highly divergent, as putting in the density factor

shows. The summation becomes approximately the integral:

for high values of . It diverges proportional to for large .

There are two separate questions to consider. First, is the divergence a real one such that the zero-point energy really is infinite? If we consider the volume is contained by perfectly conducting walls, very high frequencies can only be contained by taking more and more perfect conduction. No actual method of containing the high frequencies is possible. Such modes will not be stationary in our box and thus not countable in the stationary energy content. So from this physical point of view the above sum should only extend to those frequencies which are countable; a cut-off energy is thus eminently reasonable. However, on the scale of a "universe" questions of general relativity must be included. Suppose even the boxes could be reproduced, fit together and closed nicely by curving spacetime. Then exact conditions for running waves may be possible. However the very high frequency quanta will still not be contained. As per John Wheeler's "geons" these will leak out of the system. So again a cut-off is permissible, almost necessary. The question here becomes one of consistency since the very high energy quanta will act as a mass source and start curving the geometry.

This leads to the second question. Divergent or not, finite or infinite, is the zero-point energy of any physical significance? The ignoring of the whole zero-point energy is often encouraged for all practical calculations. The reason for this is that energies are not typically defined by an arbitrary data point, but rather changes in data points, so adding or subtracting a constant (even if infinite) should to be allowed. However this is not the whole story, in reality energy is not so arbitrarily defined: in general relativity the seat of the curvature of spacetime is the energy content and there the absolute amount of energy has real physical meaning. There is no such thing as an arbitrary additive constant with density of field energy. Energy density curves space, and an increase in energy density produces an increase of curvature. Furthermore, the zero-point energy density has other physical consequences e.g. the Casimir effect, contribution to the Lamb shift, or anomalous magnetic moment of the electron, it is clear it is not just a mathematical constant or artifact that can be cancelled out.

The vacuum state of the "free" electromagnetic field (that with no sources) is defined as the ground state in which for all modes . The vacuum state, like all stationary states of the field, is an eigenstate of the Hamiltonian but not the electric and magnetic field operators. In the vacuum state, therefore, the electric and magnetic fields do not have definite values. We can imagine them to be fluctuating about their mean value of zero.

In a process in which a photon is annihilated (absorbed), we can think of the photon as making a transition into the vacuum state. Similarly, when a photon is created (emitted), it is occasionally useful to imagine that the photon has made a transition out of the vacuum state. An atom, for instance, can be considered to be "dressed" by emission and reabsorption of "virtual photons" from the vacuum. The vacuum state energy described by is infinite. We can make the replacement:

the zero-point energy density is:

or in other words the spectral energy density of the vacuum field:

The zero-point energy density in the frequency range from to is therefore:

This can be large even in relatively narrow "low frequency" regions of the spectrum. In the optical region from 400 to 700 nm, for instance, the above equation yields around 220 erg/cm.

We showed in the above section that the zero-point energy can be eliminated from the Hamiltonian by the normal ordering prescription. However, this elimination does not mean that the vacuum field has been rendered unimportant or without physical consequences. To illustrate this point we consider a linear dipole oscillator in the vacuum. The Hamiltonian for the oscillator plus the field with which it interacts is:

This has the same form as the corresponding classical Hamiltonian and the Heisenberg equations of motion for the oscillator and the field are formally the same as their classical counterparts. For instance the Heisenberg equations for the coordinate and the canonical momentum of the oscillator are:

or:

since the rate of change of the vector potential in the frame of the moving charge is given by the convective derivative

For nonrelativistic motion we may neglect the magnetic force and replace the expression for by:

Above we have made the electric dipole approximation in which the spatial dependence of the field is neglected. The Heisenberg equation for is found similarly from the Hamiltonian to be:

In the electric dipole approximation.

In deriving these equations for , , and we have used the fact that equal-time particle and field operators commute. This follows from the assumption that particle and field operators commute at some time (say, ) when the matter-field interpretation is presumed to begin, together with the fact that a Heisenberg-picture operator evolves in time as , where is the time evolution operator satisfying

Alternatively, we can argue that these operators must commute if we are to obtain the correct equations of motion from the Hamiltonian, just as the corresponding Poisson brackets in classical theory must vanish in order to generate the correct Hamilton equations. The formal solution of the field equation is:

and therefore the equation for may be written:

where:

and:

It can be shown that in the radiation reaction field, if the mass is regarded as the "observed" mass then we can take:

The total field acting on the dipole has two parts, and . is the free or zero-point field acting on the dipole. It is the homogeneous solution of the Maxwell equation for the field acting on the dipole, i.e., the solution, at the position of the dipole, of the wave equation

satisfied by the field in the (source free) vacuum. For this reason is often referred to as the "vacuum field", although it is of course a Heisenberg-picture operator acting on whatever state of the field happens to be appropriate at . is the source field, the field generated by the dipole and acting on the dipole.

Using the above equation for we obtain an equation for the Heisenberg-picture operator formula_48 that is formally the same as the classical equation for a linear dipole oscillator:

where . in this instance we have considered a dipole in the vacuum, without any "external" field acting on it. the role of the external field in the above equation is played by the vacuum electric field acting on the dipole.

Classically, a dipole in the vacuum is not acted upon by any "external" field: if there are no sources other than the dipole itself, then the only field acting on the dipole is its own radiation reaction field. In quantum theory however there is always an "external" field, namely the source-free or vacuum field .

According to our earlier equation for the free field is the only field in existence at as the time at which the interaction between the dipole and the field is "switched on". The state vector of the dipole-field system at is therefore of the form

where is the vacuum state of the field and is the initial state of the dipole oscillator. The expectation value of the free field is therefore at all times equal to zero:

since . however, the energy density associated with the free field is infinite:

The important point of this is that the zero-point field energy does not affect the Heisenberg equation for since it is a c-number or constant (i.e. an ordinary number rather than an operator) and commutes with . We can therefore drop the zero-point field energy from the Hamiltonian, as is usually done. But the zero-point field re-emerges as the homogeneous solution for the field equation. A charged particle in the vacuum will therefore always see a zero-point field of infinite density. This is the origin of one of the infinities of quantum electrodynamics, and it cannot be eliminated by the trivial expedient dropping of the term in the field Hamiltonian.

The free field is in fact necessary for the formal consistency of the theory. In particular, it is necessary for the preservation of the commutation relations, which is required by the unitary of time evolution in quantum theory:

We can calculate from the formal solution of the operator equation of motion

Using the fact that

and that equal-time particle and field operators commute, we obtain:

For the dipole oscillator under consideration it can be assumed that the radiative damping rate is small compared with the natural oscillation frequency, i.e., . Then the integrand above is sharply peaked at and:

the necessity of the vacuum field can also be appreciated by making the small damping approximation in

and

Without the free field in this equation the operator would be exponentially dampened, and commutators like would approach zero for . With the vacuum field included, however, the commutator is at all times, as required by unitarity, and as we have just shown. A similar result is easily worked out for the case of a free particle instead of a dipole oscillator.

What we have here is an example of a "fluctuation-dissipation elation". Generally speaking if a system is coupled to a bath that can take energy from the system in an effectively irreversible way, then the bath must also cause fluctuations. The fluctuations and the dissipation go hand in hand we cannot have one without the other. In the current example the coupling of a dipole oscillator to the electromagnetic field has a dissipative component, in the form of the zero-point (vacuum) field; given the existence of radiation reaction, the vacuum field must also exist in order to preserve the canonical commutation rule and all it entails.

The spectral density of the vacuum field is fixed by the form of the radiation reaction field, or vice versa: because the radiation reaction field varies with the third derivative of , the spectral energy density of the vacuum field must be proportional to the third power of in order for to hold. In the case of a dissipative force proportional to , by contrast, the fluctuation force must be proportional to formula_60 in order to maintain the canonical commutation relation. This relation between the form of the dissipation and the spectral density of the fluctuation is the essence of the fluctuation-dissipation theorem.

The fact that the canonical commutation relation for a harmonic oscillator coupled to the vacuum field is preserved implies that the zero-point energy of the oscillator is preserved. it is easy to show that after a few damping times the zero-point motion of the oscillator is in fact sustained by the driving zero-point field.

The QCD vacuum is the vacuum state of quantum chromodynamics (QCD). It is an example of a "non-perturbative" vacuum state, characterized by a non-vanishing condensates such as the gluon condensate and the quark condensate in the complete theory which includes quarks. The presence of these condensates characterizes the confined phase of quark matter. In technical terms, gluons are vector gauge bosons that mediate strong interactions of quarks in quantum chromodynamics (QCD). Gluons themselves carry the color charge of the strong interaction. This is unlike the photon, which mediates the electromagnetic interaction but lacks an electric charge. Gluons therefore participate in the strong interaction in addition to mediating it, making QCD significantly harder to analyze than QED (quantum electrodynamics) as it deals with nonlinear equations to characterize such interactions.

The Standard Model hypothesises a field called the Higgs field (symbol: ), which has the unusual property of a non-zero amplitude in its ground state (zero-point) energy after renormalization; i.e., a non-zero vacuum expectation value. It can have this effect because of its unusual "Mexican hat" shaped potential whose lowest "point" is not at its "centre". Below a certain extremely high energy level the existence of this non-zero vacuum expectation spontaneously breaks electroweak gauge symmetry which in turn gives rise to the Higgs mechanism and triggers the acquisition of mass by those particles interacting with the field. The Higgs mechanism occurs whenever a charged field has a vacuum expectation value. This effect occurs because scalar field components of the Higgs field are "absorbed" by the massive bosons as degrees of freedom, and couple to the fermions via Yukawa coupling, thereby producing the expected mass terms. The expectation value of in the ground state (the vacuum expectation value or VEV) is then , where . The measured value of this parameter is approximately . It has units of mass, and is the only free parameter of the Standard Model that is not a dimensionless number.

The Higgs mechanism is a type of superconductivity which occurs in the vacuum. It occurs when all of space is filled with a sea of particles which are charged and thus the field has a nonzero vacuum expectation value. Interaction with the vacuum energy filling the space prevents certain forces from propagating over long distances (as it does in a superconducting medium; e.g., in the Ginzburg–Landau theory).

Zero-point energy has many observed physical consequences. It is important to note that zero-point energy is not merely an artefact of mathematical formalism that can, for instance, be dropped from a Hamiltonian by redefining the zero of energy, or by arguing that it is a constant and therefore has no effect on Heisenberg equations of motion without latter consequence. Indeed, such treatment could create a problem at a deeper, as of yet undiscovered, theory. For instance, in general relativity the zero of energy (i.e. the energy density of the vacuum) contributes to a cosmological constant of the type introduced by Einstein in order to obtain static solutions to his field equations. The zero-point energy density of the vacuum, due to all quantum fields, is extremely large, even when we cut off the largest allowable frequencies based on plausible physical arguments. It implies a cosmological constant larger than the limits imposed by observation by about 120 orders of magnitude. This "cosmological constant problem" remains one of the greatest unsolved mysteries of physics.

A phenomenon that is commonly presented as evidence for the existence of zero-point energy in vacuum is the Casimir effect, proposed in 1948 by Dutch physicist Hendrik Casimir, who considered the quantized electromagnetic field between a pair of grounded, neutral metal plates. The vacuum energy contains contributions from all wavelengths, except those excluded by the spacing between plates. As the plates draw together, more wavelengths are excluded and the vacuum energy decreases. The decrease in energy means there must be a force doing work on the plates as they move.

Early experimental tests from the 1950s onwards gave positive results showing the force was real, but other external factors could not be ruled out as the primary cause, with the range of experimental error sometimes being nearly 100%. That changed in 1997 with Lamoreaux conclusively showing that the Casimir force was real. Results have been repeatedly replicated since then.

In 2009 Munday et al. published experimental proof that (as predicted in 1961) the Casimir force could also be repulsive as well as being attractive. Repulsive Casimir forces could allow quantum levitation of objects in a fluid and lead to a new class of switchable nanoscale devices with ultra-low static friction

An interesting hypothetical side effect of the Casimir effect is the Scharnhorst effect, a hypothetical phenomenon in which light signals travel slightly faster than between two closely spaced conducting plates.

The quantum fluctuations of the electromagnetic field have important physical consequences. In addition to the Casimir effect, they also lead to a splitting between the two energy levels and (in term symbol notation) of the hydrogen atom which was not predicted by the Dirac equation, according to which these states should have the same energy. Charged particles can interact with the fluctuations of the quantized vacuum field, leading to slight shifts in energy, this effect is called the Lamb shift. The shift of about is roughly of the difference between the energies of the 1s and 2s levels, and amounts to 1,058 MHz in frequency units. A small part of this shift (27 MHz ≈ 3%) arises not from fluctuations of the electromagnetic field, but from fluctuations of the electron–positron field. The creation of (virtual) electron–positron pairs has the effect of screening the Coulomb field and acts as a vacuum dielectric constant. This effect is much more important in muonic atoms.

Taking (Planck's constant divided by ), (the speed of light), and (the electromagnetic coupling constant i.e. a measure of the strength of the electromagnetic force (where is the absolute value of the electronic charge and formula_61 is the vacuum permittivity)) we can form a dimensionless quantity called the fine-structure constant:

The fine-structure constant is the coupling constant of quantum electrodynamics (QED) determining the strength of the interaction between electrons and photons. It turns out that the fine structure constant is not really a constant at all owing to the zero-point energy fluctuations of the electron-positron field. The quantum fluctuations caused by zero-point energy have the effect of screening electric charges: owing to (virtual) electron-positron pair production, the charge of the particle measured far from the particle is far smaller than the charge measured when close to it.

The Heisenberg inequality where , and , are the standard deviations of position and momentum states that:

It means that a short distance implies large momentum and therefore high energy i.e. particles of high energy must be used to explore short distances. QED concludes that the fine structure constant is an increasing function of energy. It has been shown that at energies of the order of the Z boson rest energy, 90 GeV, that:

rather than the low-energy . The renormalization procedure of eliminating zero-point energy infinities allows the choice of an arbitrary energy (or distance) scale for defining . All in all, depends on the energy scale characteristic of the process under study, and also on details of the renormalization procedure. The energy dependence of has been observed for several years now in precision experiment in high-energy physics.

In the presence of strong electrostatic fields it is predicted that virtual particles become separated from the vacuum state and form real matter. The fact that electromagnetic radiation can be transformed into matter and vice versa leads to fundamentally new features in quantum electrodynamics. One of the most important consequences is that, even in the vacuum, the Maxwell equations have to be exchanged by more complicated formulas. In general, it will be not possible to separate processes in the vacuum from the processes involving matter since electromagnetic fields can create matter if the field fluctuations are strong enough. This leads to highly complex nonlinear interaction - gravity will have an effect on the light at the same time the light has an effect on gravity. These effects were first predicted by Werner Heisenberg and Hans Heinrich Euler in 1936 and independently the same year by Victor Weisskopf who stated: "The physical properties of the vacuum originate in the “zero-point energy” of matter, which also depends on absent particles through the external field strengths and therefore contributes an additional term to the purely Maxwellian field energy". Thus strong magnetic fields vary the energy contained in the vacuum. The scale above which the electromagnetic field is expected to become nonlinear is known as the Schwinger limit. At this point the vacuum has all the properties of a birefringent medium, thus in principle a rotation of the polarization frame (the Faraday effect) can be observed in empty space.
Both Einstein's theory of special and general relativity state that light should pass freely through a vacuum without being altered, a principle known as Lorentz invariance. Yet, in theory, large nonlinear self-interaction of light due to quantum fluctuations should lead to this principle being measurably violated if the interactions are strong enough. Nearly all theories of quantum gravity predict that that Lorentz invariance is not an exact symmetry of nature. It is predicted the speed at which light travels through the vacuum depends on its direction, polarization and the local strength of the magnetic field. There have been a number of inconclusive results which claim to show evidence of a Lorentz violation by finding a rotation of the polarization plane of light coming from distant galaxies. The first concrete evidence for vacuum birefringence was published in 2017 when a team of astronomers looked at the light coming from the star RX J1856.5-3754, the closest discovered neutron star to Earth.

Roberto Mignani at the National Institute for Astrophysics in Milan who led the team of astronomers has commented that "“When Einstein came up with the theory of general relativity 100 years ago, he had no idea that it would be used for navigational systems. The consequences of this discovery probably will also have to be realised on a longer timescale.” The team found that visible light from the star had undergone linear polarisation of around 16%. If the birefringence had been caused by light passing through interstellar gas or plasma, the effect should have been no more than 1%. Definitive proof would require repeating the observation at other wavelengths and on other neutron stars. At X-ray wavelengths the polarization from the quantum fluctuations should be near 100%. Although no telescope currently exists that can make such measurements, there are several proposed X-ray telescopes that may soon be able to verify the result conclusively such as China's Hard X-ray Modulation Telescope (HXMT) and NASA's Imaging X-ray Polarimetry Explorer (IXPE).

In the late 1990s it was discovered that very distant supernova were dimmer than expected suggesting that the universe's expansion was accelerating rather than slowing down. This revived discussion that Einstein's cosmological constant, long disregarded by physicists as being equal to zero, was in fact some small positive value. This would indicate empty space exerted some form of negative pressure or energy.

There is no natural candidate for what might cause what has been called dark energy but the current best guess is that it is the zero-point energy of the vacuum. One difficulty with this assumption is that the zero-point energy of the vacuum is absurdly large compared to the observed cosmological constant. In general relativity, mass and energy are equivalent; both produce a gravitational field and therefore the theorized vacuum energy of quantum field theory should have led the universe ripping itself to pieces. This obviously has not happened and this issue, called the cosmological constant problem, is one of the greatest unsolved mysteries in physics.

The European Space Agency is building the Euclid telescope. Due to launch in 2020 it will map galaxies up to 10 billion light years away. By seeing how dark energy influences their arrangement and shape, the mission will allow scientists to see if the strength of dark energy has changed. If dark energy is found to vary throughout time it would indicate it is due to quintessence, where observed acceleration is due to the energy of a scalar field, rather than the cosmological constant. No evidence of quintessence is yet available, but it has not been ruled out either. It generally predicts a slightly slower acceleration of the expansion of the universe than the cosmological constant. Some scientists think that the best evidence for quintessence would come from violations of Einstein's equivalence principle and variation of the fundamental constants in space or time. Scalar fields are predicted by the "Standard Model of particle physics" and string theory, but an analogous problem to the cosmological constant problem (or the problem of constructing models of cosmological inflation) occurs: renormalization theory predicts that scalar fields should acquire large masses again due to zero-point energy.

Cosmic inflation is a faster-than-light expansion of space just after the Big Bang. It explains the origin of the large-scale structure of the cosmos. It is believed quantum vacuum fluctuations caused by zero-point energy arising in the microscopic inflationary period, later became magnified to a cosmic size, becoming the gravitational seeds for galaxies and structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the Universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the Universe is flat, and why no magnetic monopoles have been observed.

The mechanism for inflation is unclear, it is similar in effect to dark energy but is a far more energetic and short lived process. As with dark energy the best explanation is some form of vacuum energy arising from quantum fluctuations. It may be that inflation caused baryogenesis, the hypothetical physical processes that produced an asymmetry (imbalance) between baryons and antibaryons produced in the very early universe, but this is far from certain.

There has been a long debate over the question of whether zero-point fluctuations of quantized vacuum fields are “real” i.e. do they have physical effects that cannot be interpreted by an equally valid alternative theory? Schwinger, in particular, attempted to formulate QED without reference to zero-point fluctuations via his "source theory". From such an approach it is possible to derive the Casimir Effect without reference to a fluctuating field. Such a derivation was first given by Schwinger (1975) for a scalar field, and then generalized to the electromagnetic case by Schwinger, DeRaad, and Milton (1978). in which they state "the vacuum is regarded as truly a state with all physical properties equal to zero". More recently Jaffe (2005) has highlighted a similar approach in deriving the Casimir effect stating "the concept of zero-point fluctuations is a heuristic and calculational aid in the description of the Casimir effect, but not a necessity in QED."

Nevertheless, as Jaffe himself notes in his paper, "no one has shown that source theory or another S-matrix based approach can provide a complete description of QED to all orders." Furthermore, Milonni has shown the necessity of the vacuum field for the formal consistency of QED. In QCD, color confinement has led physicists to abandon the source theory or S-matrix based approach for the strong interactions. The Higgs mechanism, Hawking Radiation and the Unruh effect are also theorized to be dependent on zero-point vacuum fluctuations, the field contribution being an inseparable parts of these theories. Jaffe continues "Even if one could argue away zero-point contributions to the quantum vacuum energy, the problem of spontaneous symmetry breaking remains: condensates [ground state vacua] that carry energy appear at many energy scales in the Standard Model. So there is good reason to be skeptical of attempts to avoid the standard formulation of quantum field theory and the zero-point energies it brings with it." It is difficult to judge the physical reality of infinite zero-point energies that are inherent in field theories, but modern physics does not know any better way to construct gauge-invariant, renormalizable theories than with zero-point energy and they would seem to be a necessity for any attempt at a unified theory.

The mathematical models used in classical electromagnetism, quantum electrodynamics (QED) and the standard model all view the electromagnetic vacuum as a linear system with no overall observable consequence (e.g. in the case of the Casimir effect, Lamb shift, and so on) these phenomena can be explained by alternative mechanisms other than action of the vacuum by arbitrary changes to the normal ordering of field operators. See alternative theories section). This is a consequence of viewing electromagnetism as a U(1) gauge theory, which topologically does not allow the complex interaction of a field with and on itself. In higher symmetry groups and in reality, the vacuum is not a calm, randomly fluctuating, largely immaterial and passive substance, but at times can be viewed as a turbulent virtual plasma that can have complex vortices (i.e. solitons vis-à-vis particles), entangled states and a rich nonlinear structure. There are many observed nonlinear physical electromagnetic phenomena such as Aharonov–Bohm (AB) and Altshuler–Aronov–Spivak (AAS) effects, Berry, Aharonov–Anandan, Pancharatnam and Chiao–Wu phase rotation effects, Josephson effect,

What are called Maxwell's equations today, are in fact a simplified version of the original equations reformulated by Heaviside, FitzGerald, Lodge and Hertz. The original equations used Hamilton's more expressive quaternion notation, a kind of Clifford algebra, which fully subsumes the standard Maxwell vectorial equations largely used today. In the late 1880s there was a debate over the relative merits of vector analysis and quaternions. According to Heaviside the electromagnetic potential field was purely metaphysical, an arbitrary mathematical fiction, that needed to be "murdered". It was concluded that there was no need for the greater physical insights provided by the quaternions if the theory was purely local in nature. Local vector analysis has become the dominant way of using Maxwell's equations ever since. However, this strictly vectorial approach has led to a restrictive topological understanding in some areas of electromagnetism, for example, a full understanding of the energy transfer dynamics in Tesla's oscillator-shuttle-circuit can only be achieved in quaternionic algebra or higher SU(2) symmetries. It has often been argued that quaternions are not compatible with special relativity, but multiple papers have shown ways of incorporating relativity.

A good example of nonlinear electromagnetics is in high energy dense plasmas, where vortical phenomena occur which seemingly violate the second law of thermodynamics by increasing the energy gradient within the electromagnetic field and violate Maxwell's laws by creating ion currents which capture and concentrate their own and surrounding magnetic fields. In particular Lorentz force law, which elaborates Maxwell's equations is violated by these force free vortices. These apparent violations are due to the fact that the traditional conservation laws in classical and quantum electrodynamics (QED) only display linear U(1) symmetry (in particular, by the extended Noether theorem, conservation laws such as the laws of thermodynamics need not always apply to dissipative systems, which are expressed in gauges of higher symmetry). The second law of thermodynamics states that in a closed linear system entropy flow can only be positive (or exactly zero at the end of a cycle). However, negative entropy (i.e. increased order, structure or self-organisation) can spontaneously appear in an open nonlinear thermodynamic system that is far from equilibrium, so long as this emergent order accelerates the overall flow of entropy in the total system. The 1977 Nobel Prize in Chemistry was awarded to thermodynamicist Ilya Prigogine for his theory of dissipative systems that described this notion. Prigogine described the principle as "order through fluctuations" or "order out of chaos". It has been argued by some that all emergent order in the universe from galaxies, solar systems, planets, weather, complex chemistry, evolutionary biology to even consciousness, technology and civilizations are themselves examples of thermodynamic dissipative systems; nature having naturally selected these structures to accelerate entropy flow within the universe to an ever-increasing degree. For example, it has been estimated that human body is 10,000 times more effective at dissipating energy per unit of mass than the sun.

One may query what this has to do with zero-point energy. Given the complex and adaptive behaviour that arises from nonlinear systems considerable attention in recent years has gone into studying a new class of phase transitions which occur at absolute zero temperature. These are quantum phase transitions which are driven by EM field fluctuations as a consequence of zero-point energy. A good example of a spontaneous phase transition that are attributed to zero-point fluctuations can be found in superconductors. Superconductivity is one of the best known empirically quantified macroscopic electromagnetic phenomena whose basis is recognised to be quantum mechanical in origin. The behaviour of the electric and magnetic fields under superconductivity is governed by the London equations. However, it has been questioned in a series of journal articles whether the quantum mechanically canonised London equations can be given a purely classical derivation. Bostick, for instance, has claimed to show that the London equations do indeed have a classical origin that applies to superconductors and to some collisionless plasmas as well. In particular it has been asserted that the Beltrami vortices in the plasma focus display the same paired flux-tube morphology as Type II superconductors. Others have also pointed out this connection, Fröhlich has shown that the hydrodynamic equations of compressible fluids, together with the London equations, lead to a macroscopic parameter (formula_65 = electric charge density / mass density), without involving either quantum phase factors or Planck's constant. In essence, it has been asserted that Beltrami plasma vortex structures are able to at least simulate the morphology of Type I and Type II superconductors. This occurs because the "organised" dissipative energy of the vortex configuration comprising the ions and electrons far exceeds the "disorganised" dissipative random thermal energy. The transition from disorganised fluctuations to organised helical structures is a phase transition involving a change in the condensate's energy (i.e. the ground state or zero-point energy) but "without any associated rise in temperature". This is an example of zero-point energy having multiple stable states (see Quantum phase transition, Quantum critical point, Topological degeneracy, Topological order) and where the overall system structure is independent of a reductionist or deterministic view, that "classical" macroscopic order can also causally affect quantum phenomena. Furthermore, the pair production of Beltrami vortices has been compared to the morphology of pair production of virtual particles in the vacuum.

The idea that the vacuum energy can have multiple stable energy states is a leading hypothesis for the cause of cosmic inflation. In fact, it has been argued that these early vacuum fluctuations led to the expansion of the universe and in turn have guaranteed the non-equilibrium conditions necessary to drive order from chaos, as without such expansion the universe would have reached thermal equilibrium and no complexity could have existed. With the continued accelerated expansion of the universe, the cosmos generates an energy gradient that increases the "free energy" (i.e. the available, usable or potential energy for useful work) which the universe is able to utilize to create ever more complex forms of order. The only reason Earth's environment does not decay into an equilibrium state is that it receives a daily dose of sunshine and that, in turn, is due to the sun "polluting" interstellar space with decreasing entropy. The sun's fusion power is only possible due to the gravitational disequilibrium of matter that arose from cosmic expansion. In this essence, the vacuum energy can be viewed as the key cause of the negative entropy (i.e. structure) throughout the universe. That humanity might alter the morphology of the vacuum energy to create an energy gradient for useful work is the subject of much controversy.

Physicists overwhelmingly reject any possibility that the zero-point energy field can be exploited to obtain useful energy (work) or uncompensated momentum; such efforts are seen as tantamount to perpetual motion machines.

Nevertheless, the allure of free energy has motivated such research, usually falling in the category of fringe science. As long ago as 1889 (before quantum theory or discovery of the zero point energy) Nikola Tesla proposed that useful energy could be obtained from free space, or what was assumed at that time to be an all-pervasive aether. Others have since claimed to exploit zero-point or vacuum energy with a large amount of pseudoscientific literature causing ridicule around the subject. Despite rejection by the scientific community, harnessing zero-point energy remains an interest of research by non-scientific entities, particularly in the US where it has attracted the attention of major aerospace/defence contractors and the U.S. Department of Defence as well as in China, Germany, Russia and Brazil.

A common assumption is that the Casimir force is of little practical use; the argument is made that the only way to actually gain energy from the two plates is to allow them to come together (getting them apart again would then require more energy), and therefore it is a one-use-only tiny force in nature. In 1984 Robert Forward published work showing how a "vacuum-fluctuation battery" could be constructed. The battery can be recharged by making the electrical forces slightly stronger than the Casimir force to reexpand the plates.

In 1995 and 1998 Maclay et al. published the first models of a microelectromechanical system (MEMS) with Casimir forces. While not exploiting the Casimir force for useful work, the papers drew attention from the MEMS community due to the revelation that Casimir effect needs to be considered as a vital factor in the future design of MEMS. In particular, Casimir effect might be the critical factor in the stiction failure of MEMS.

In 1999 Pinto, a former scientist at NASA's Jet Propulsion Laboratory at Caltech in Pasadena, published in "Physical Review" his thought experiment (Gedankenexperiment) for a "Casimir engine". The paper showed that continuous positive net exchange of energy from the Casimir effect was possible, even stating in the abstract "In the event of no other alternative explanations, one should conclude that major technological advances in the area of endless, by-product free-energy production could be achieved." In 2001 Capasso et al. showed how the force can be used to control the mechanical motion of a MEMS device, The researchers suspended a polysilicon plate from a torsional rod – a twisting horizontal bar just a few microns in diameter. When they brought a metallized sphere close up to the plate, the attractive Casimir force between the two objects made the plate rotate. They also studied the dynamical behaviour of the MEMS device by making the plate oscillate. The Casimir force reduced the rate of oscillation and led to nonlinear phenomena, such as hysteresis and bistability in the frequency response of the oscillator. According to the team, the system’s behaviour agreed well with theoretical calculations.

Despite this and several similar peer reviewed papers, there is not a consensus as to whether such devices can produce a continuous output of work. Garret Moddel at University of Colorado has highlighted that he believes such devices hinge on the assumption that the Casimir force is a nonconservative force, he argues that there is sufficient evidence (e.g. analysis by Scandurra (2001)) to say that the Casimir effect is a conservative force and therefore even though such an engine can exploit the Casimir force for useful work it cannot produce more output energy than has been input into the system.

In 2008 DARPA solicited research proposals in the area of Casimir Effect Enhancement (CEE). The goal of the program is to develop new methods to control and manipulate attractive and repulsive forces at surfaces based on engineering of the Casimir Force.

A 2008 patent by Haisch and Moddel details a device that is able to extract power from zero-point fluctuations using a gas that circulates through a Casimir cavity. As gas atoms circulate around the system they enter the cavity. Upon entering the electrons spin down to release energy via electromagnetic radiation. This radiation is then extracted by an absorber. On exiting the cavity the ambient vacuum fluctuations (i.e. the zero-point field) impart energy on the electrons to return the orbitals to previous energy levels, as predicted by Senitzky (1960). The gas then goes through a pump and flows through the system again. A published test of this concept by Moddel was performed in 2012 and seemed to give excess energy that could not be attributed to another source. However it has not been conclusively shown to be from zero-point energy and the theory requires further investigation.

In 1951 Callen and Welton proved the quantum fluctuation-dissipation theorem (FDT) which was originally formulated in classical form by Nyquist (1928) as an explanation for observed Johnson noise in electric circuits. Fluctuation-dissipation theorem showed that when something dissipates energy, in an effectively irreversible way, a connected heat bath must also fluctuate. The fluctuations and the dissipation go hand in hand; it is impossible to have one without the other. The implication of FDT being that the vacuum could be treated as a heat bath coupled to a dissipative force and as such energy could, in part, be extracted from the vacuum for potentially useful work. Such a theory has met with resistance: Macdonald (1962) and Harris (1971) claimed that extracting power from the zero-point energy to be impossible, so FDT could not be true. Grau and Kleen (1982) and Kleen (1986), argued that the Johnson noise of a resistor connected to an antenna must satisfy Planck's thermal radiation formula, thus the noise must be zero at zero temperature and FDT must be invalid. Kiss (1988) pointed out that the existence of the zero-point term may indicate that there is a renormalization problem—i.e., a mathematical artifact—producing an unphysical term that is not actually present in measurements (in analogy with renormalization problems of ground states in quantum electrodynamics). Later, Abbott et al. (1996) arrived at a different but unclear conclusion that "zero-point energy is infinite thus it should be renormalized but not the ‘zero-point fluctuations’". Despite such criticism, FDT has been shown to be true experimentally under certain quantum, non-classical conditions. Zero-point fluctuations can, and do, contribute towards systems which dissipate energy. A paper by Armen Allahverdyan and Theo Nieuwenhuizen in 2000 showed the feasibility of extracting zero-point energy for useful work from a single bath, without contradicting the laws of thermodynamics, by exploiting certain quantum mechanical properties.

There have been a growing number of papers showing that in some instances the classical laws of thermodynamics, such as limits on the Carnot efficiency, can be violated by exploiting negative entropy of quantum fluctuations.

Despite efforts to reconcile quantum mechanics and thermodynamics over the years, their compatibility is still an open fundamental problem. The full extent that quantum properties can alter classical thermodynamic bounds is unknown

The use of zero-point energy for space travel is highly speculative. A complete quantum theory of gravitation (that would deal with the role of quantum phenomena like zero-point energy) does not yet exist. Speculative papers explaining a relationship between zero-point energy and gravitational shielding effects have been proposed, but the interaction (if any) is not yet fully understood. Most serious scientific research in this area depends on the theorized anti-gravitational properties of antimatter (currently being tested at the alpha experiment at CERN) and/or the effects of non-Newtonian forces such as the gravitomagnetic field under specific quantum conditions. According to the general theory of relativity, rotating matter can generate a new force of nature, known as the gravitomagnetic interaction, whose intensity is proportional to the rate of spin. In certain conditions the gravitomagnetic field can be repulsive. In neutrons stars for example it can produce a gravitational analogue of the Meissner effect, but the force produced in such an example is theorized to be exceedingly weak.

In 1963 Robert Forward, a physicist and aerospace engineer at Hughes Research Laboratories, published a paper showing how within the framework of general relativity "anti-gravitational" effects might be achieved. Since all atoms have spin, gravitational permeability may be able to differ from material to material. A strong toroidal gravitational field that acts against the force of gravity could be generated by materials that have nonlinear properties that enhance time-varying gravitational fields. Such an effect would be analogous to the nonlinear electromagnetic permeability of iron making it an effective core (i.e. the doughnut of iron) in a transformer, whose properties are dependent on magnetic permeability. In 1966 Dewitt was first to identify the significance of gravitational effects in superconductors. Dewitt demonstrated that a magnetic-type gravitational field must result in the presence of fluxoid quantization. In 1983, Dewitt's work was substantially expanded by Ross.

From 1971 to 1974 Henry William Wallace, a scientist at GE Aerospace was issued with three patents. Wallace used Dewitt's theory to develop an experimental apparatus for generating and detecting a secondary gravitational field, which he named the kinemassic field (now better known as the gravitomagnetic field). In his three patents, Wallace describes three different methods used for detection of the gravitomagnetic field – change in the motion of a body on a pivot, detection of a transverse voltage in a semiconductor crystal, and a change in the specific heat of a crystal material having spin-aligned nuclei. There are no publicly available independent tests verifying Wallace's devices. Such an effect if any would be small. Referring to Wallace's patents, a New Scientist article in 1980 stated "Although the Wallace patents were initially ignored as cranky, observers believe that his invention is now under serious but secret investigation by the military authorities in the USA. The military may now regret that the patents have already been granted and so are available for anyone to read." A further reference to Wallace's patents occur in an electric propulsion study prepared for the Astronautics Laboratory at Edwards Air Force Base which states: "The patents are written in a very believable style which include part numbers, sources for some components, and diagrams of data. Attempts were made to contact Wallace using patent addresses and other sources but he was not located nor is there a trace of what became of his work. The concept can be somewhat justified on general relativistic grounds since rotating frames of time varying fields are expected to emit gravitational waves."

In 1986 the U.S. Air Force's then Rocket Propulsion Laboratory (RPL) at Edwards Air Force Base solicited "Non Conventional Propulsion Concepts" under a small business research and innovation program. One of the six areas of interest was "Esoteric energy sources for propulsion, including the quantum dynamic energy of vacuum space..." In the same year BAE Systems launched "Project Greenglow" to provide a "focus for research into novel propulsion systems and the means to power them"

In 1988 Kip Thorne et al. published work showing how traversable wormholes can exist in spacetime only if they are threaded by quantum fields generated by some form of exotic matter that has negative energy. In 1993 Scharnhorst and Barton showed that the speed of a photon will be increased if it travels between two Casimir plates, an example of negative energy. In the most general sense, the exotic matter needed to create wormholes would share the repulsive properties of the inflationary energy, dark energy or zero-point radiation of the vacuum. Building on the work of Thorne, in 1994 Miguel Alcubierre proposed a method for changing the geometry of space by creating a wave that would cause the fabric of space ahead of a spacecraft to contract and the space behind it to expand (see Alcubierre drive). The ship would then ride this wave inside a region of flat space, known as a "warp bubble" and would not move within this bubble but instead be carried along as the region itself moves due to the actions of the drive.

In 1992 Evgeny Podkletnov published a heavily debated journal article claiming a specific type of rotating superconductor could shield gravitational force. Independently of this, from 1991 to 1993 Ning Li and Douglas Torr published a number of articles about gravitational effects in superconductors. One finding they derived is the source of gravitomagnetic flux in a type II superconductor material is due to spin alignment of the lattice ions. Quoting from their third paper: "It is shown that the coherent alignment of lattice ion spins will generate a detectable gravitomagnetic field, and in the presence of a time-dependent applied magnetic vector potential field, a detectable gravitoelectric field." The claimed size of the generated force has been disputed by some but defended by others. In 1997 Li published a paper attempting to replicate Podkletnov's results and showed the effect was very small, if it existed at all. Li is reported to have left the University of Alabama in 1999 to found the company "AC Gravity LLC". AC Gravity was awarded a U.S. DOD grant for $448,970 in 2001 to continue anti-gravity research. The grant period ended in 2002 but no results from this research were ever made public.

In 2002 Phantom Works, Boeing's advanced research and development facility in Seattle, approached Evgeny Podkletnov directly. Phantom Works was blocked by Russian technology transfer controls. At this time Lieutenant General George Muellner, the outgoing head of the Boeing Phantom Works, confirmed that attempts by Boeing to work with Podkletnov had been blocked by Moscow, also commenting that "The physical principles – and Podkletnov's device is not the only one – appear to be valid... There is basic science there. They're not breaking the laws of physics. The issue is whether the science can be engineered into something workable"

Froning and Roach (2002) put forward a paper that builds on the work of Puthoff, Haisch and Alcubierre. They used fluid dynamic simulations to model the interaction of a vehicle (like that proposed by Alcubierre) with the zero-point field. Vacuum field perturbations are simulated by fluid field perturbations and the aerodynamic resistance of viscous drag exerted on the interior of the vehicle is compared to the Lorentz force exerted by the zero-point field (a Casimir-like force is exerted on the exterior by unbalanced zero-point radiation pressures). They find that the optimized negative energy required for an Alcubierre drive is where it is a saucer-shaped vehicle with toroidal electromagnetic fields. The EM fields distort the vacuum field perturbations surrounding the craft sufficiently to affect the permeability and permittivity of space.

In 2014 NASA's Eagleworks Laboratories announced that they had successfully validated the use of a Quantum Vacuum Plasma Thruster which makes use of the Casimir effect for propulsion. In 2016 a scientific paper by the team of NASA scientists passed peer review for the first time. The paper suggests that the zero-point field acts as pilot-wave and that the thrust may be due to particles pushing off the quantum vacuum. While peer review doesn’t guarantee that a finding or observation is valid, it does indicate that independent scientists looked over the experimental setup, results, and interpretation and that they could not find any obvious errors in the methodology and that they found the results reasonable. In the paper, the authors identify and discuss nine potential sources of experimental errors, including rogue air currents, leaky electromagnetic radiation, and magnetic interactions. Not all of them could be completely ruled out, and further peer reviewed experimentation is needed in order to rule these potential errors out.




</doc>
<doc id="53759905" url="https://en.wikipedia.org/wiki?curid=53759905" title="Williams spray equation">
Williams spray equation

In combustion, the Williams spray equation, also known as the Williams–Boltzmann equation, describes the statistical evolution of sprays contained in another fluid, analogous to the Boltzmann equation for the molecules, named after Forman A. Williams, who derived the equation in 1958.

The sprays are assumed to be spherical with radius formula_1, even though the assumption is valid for solid particles(liquid droplets) when their shape has no consequence on the combustion. For liquid droplets to be nearly spherical, the spray has to be dilute(total volume occupied by the sprays is much less than the volume of the gas) and the Weber number formula_2, where formula_3 is the gas density, formula_4 is the spray droplet velocity, formula_5 is the gas velocity and formula_6 is the surface tension of the liquid spray, should be formula_7.

The equation is described by a number density function formula_8, which represents the probable number of spray particles (droplets) of chemical species formula_9 (of formula_10 total species), that one can find with radii between formula_1 and formula_12, located in the spatial range between formula_13 and formula_14, traveling with a velocity in between formula_4 and formula_16, having the temperature in between formula_17 and formula_18 at time formula_19. Then the spray equation for the evolution of this density function is given by

where

This model for the rocket motor was developed by Probert, Williams and Tanasawa. It is reasonable to neglect formula_31, for distances not very close to the spray atomizer, where major portion of combustion occurs. Consider a one-dimensional liquid-propellent rocket motor situated at formula_32, where fuel is sprayed. Neglecting formula_33(density function is defined without the temperature so accordingly dimensions of formula_34 changes) and due to the fact that the mean flow is parallel to formula_35 axis, the steady spray equation reduces to

where formula_37 is the velocity in formula_35 direction. Integrating with respect to the velocity results

The contribution from the last term (spray acceleration term) becomes zero (using Divergence theorem) since formula_40 when formula_41 is very large, which is typically the case in rocket motors. The drop size rate formula_42 is well modeled using vaporization mechanisms as

where formula_44 is independent of formula_1, but can depend on the surrounding gas. Defining the number of droplets per unit volume per unit radius and average quantities averaged over velocities,

the equation becomes

If further assumed that formula_48 is independent of formula_1, and with a transformed coordinate

formula_50

If the combustion chamber has varying cross-section area formula_51, a known function for formula_52 and with area formula_53 at the spraying location, then the solution is given by

where formula_55 are the number distribution and mean velocity at formula_32 respectively.



</doc>
<doc id="54477794" url="https://en.wikipedia.org/wiki?curid=54477794" title="Resistive plate chamber">
Resistive plate chamber

A Resistive plate chamber (RPC) is a particle detector widely used in high energy physics. They are used for detecting muons in most of the modern experiments including ATLAS, CMS, and BES III.


</doc>
<doc id="39098" url="https://en.wikipedia.org/wiki?curid=39098" title="Physical law">
Physical law

A physical law or a law of physics is a statement "inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present." Physical laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. The production of a summary description of our environment in the form of such laws is a fundamental aim of science. These terms are not used the same way by all authors.

The distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from "physis", the Greek word (translated into Latin as "natura") for "nature".

Several general properties of physical laws have been identified. Physical laws are:


Some of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his "Philosophiae Naturalis Principia Mathematica", and in Albert Einstein's theory of relativity. Other examples of laws of nature include Boyle's law of gases, conservation laws, the four laws of thermodynamics, etc.

Many scientific laws are couched in mathematical terms (e.g. Newton's Second law "F" = , or the uncertainty principle, or the principle of least action, or causality). While these scientific laws explain what our senses perceive, they are still empirical, and so are not "mathematical" laws. (Mathematical laws can be proved purely by mathematics and not by scientific experiment.)

Other laws reflect mathematical symmetries found in Nature (say, Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, Lorentz transformations reflect rotational symmetry of spacetime). Laws are constantly being checked experimentally to higher and higher degrees of precision. This is one of the main goals of science. Just because laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed.

Well-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies can be said to generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations (see below), to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. Thus, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.

Many fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different than any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. The rotational symmetry between time and space coordinate axes (when one is taken as imaginary, another as real) results in Lorentz transformations which in turn result in special relativity theory. Symmetry between inertial and gravitational mass results in general relativity.

The inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.

One strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.

Some laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to Quantum Electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.

According to a positivist view, when compared to pre-modern accounts of causality, laws of nature fill the role played by divine causality on the one hand, and accounts such as Plato's theory of forms on the other.

The observation that there are underlying regularities in nature dates from prehistoric times, since the recognition of cause-and-effect relationships is an implicit recognition that there are laws of nature. The recognition of such regularities as independent scientific laws "per se", though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as meteorological, astronomical and biological phenomena—to the actions of various gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.

In Europe, systematic theorizing about nature ("physis") began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.The formula "law of nature" first appears as "a live metaphor" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.

For the Romans . . . the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's "Natural Questions", and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.

The precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and development of advanced forms of mathematics. During this period, natural philosophers such as Isaac Newton were influenced by a religious view which held that God had instituted absolute, universal and immutable physical laws. In chapter 7 of "The World", René Descartes described "nature" as matter itself, unchanging as created by God, thus changes in parts "are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'." The modern scientific method which took shape at this time (with Francis Bacon and Galileo) aimed at total separation of science from theology, with minimal speculation about metaphysics and ethics. Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period (by Grotius, Spinoza, and Hobbes, to name a few).

Some mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.

Examples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as "laws". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.





</doc>
<doc id="784621" url="https://en.wikipedia.org/wiki?curid=784621" title="Many-body theory">
Many-body theory

Many-body theory (or many-body physics) is an area of physics which provides the framework for understanding the collective behavior of large numbers of interacting particles, often on the order of Avogadro's number. In general terms, many-body theory deals with effects that manifest themselves only in systems containing large numbers of constituents. While the underlying physical laws that govern the motion of each individual particle may (or may not) be simple, the study of the collection of particles can be extremely complex. In some cases emergent phenomena may arise which bear little resemblance to the underlying elementary laws.

Many-body theory plays a central role in condensed matter physics.



</doc>
<doc id="55121627" url="https://en.wikipedia.org/wiki?curid=55121627" title="Parallel force system">
Parallel force system

In mechanical engineering, a parallel force system is a situation in which two forces of equal magnitude act in the same direction within the same plane, with the counter force in the middle. An example of this is a see saw. The children are applying the two forces at the ends, and the fulcrum in the middle gives the counter force to maintain the see saw in neutral position. Another example are the major vertical forces on an airplane in flight (see image at right).



</doc>
<doc id="51860534" url="https://en.wikipedia.org/wiki?curid=51860534" title="Infinite derivative gravity">
Infinite derivative gravity

Infinite derivative gravity is a theory of gravity which attempts to remove cosmological and black hole singularities by adding extra terms to the Einstein–Hilbert action, which weaken gravity at short distances.

In 1987, Krasnikov considered an infinite set of higher derivative terms acting on the curvature terms and showed that by choosing the coefficients wisely, the propagator would be ghost-free and exponentially suppressed in the ultraviolet regime. Tomboulis (1997) later extended this work. By looking at an equivalent scalar-tensor theory, Biswas, Mazumdar and Siegel (2005) looked at bouncing FRW solutions. In 2011, Biswas, Gerwick, Koivisto and Mazumdar demonstrated that the most general infinite derivative action in 4 dimensions, around constant curvature backgrounds, parity invariant and torsion free, can be expressed by:
where the formula_2 are functions of the D'Alembert operator formula_3 and a mass scale formula_4, formula_5 is the Ricci scalar, formula_6 is the Ricci tensor and formula_7 is the Weyl tensor. In order to avoid ghosts, the propagator (which is a combination of the formula_8s) must be the exponential of an entire function. A lower bound was obtained on the mass scale of IDG using experimental data on the strength of gravity at short distances, as well as by using data on inflation and on the bending of light around the Sun. The GHY boundary terms were found using the ADM 3+1 spacetime decomposition. One can show that the entropy for this theory is finite in various contexts.

The effect of IDG on black holes and the propagator was examined by Modesto. Modesto further looked at the renormalisability of the theory, as well as showing that it could generate "super-accelerated" bouncing solutions instead of a big bang singularity. Calcagni and Nardelli investigated the effect of IDG on the diffusion equation. IDG modifies the way gravitational waves are produced and how they propagate through space. The amount of power radiated away through gravitational waves by binary systems is reduced, although this effect is far smaller than the current observational precision.

This action can produce a bouncing cosmology, by taking a flat FRW metric with a scale factor formula_9 or formula_10, thus avoiding the cosmological singularity problem. The propagator around a flat space background was obtained in 2013.

This action avoids a curvature singularity for a small perturbation to a flat background near the origin, while recovering the formula_11 fall of the GR potential at large distances. This is done using the linearised equations of motion which is a valid approximation because if the perturbation is small enough and the mass scale formula_4 is large enough, then the perturbation will always be small enough that quadratic terms can be neglected. It also avoids the Hawking-Penrose singularity in this context.

It was shown that in non-local gravity, Schwarzschild singularities are stable to small perturbations. Further stability analysis of black holes was carried out by Myung and Park.

The equations of motion for this action are

formula_13

where


</doc>
<doc id="55503653" url="https://en.wikipedia.org/wiki?curid=55503653" title="Camelback potential">
Camelback potential

A camelback potential is potential energy curve that looks like a normal distribution with a distinct dip where the peak would be, so named because it resembles the humps on a camel's back. The term was applied to a configuration of a superconducting quantum interference device in 2009, and to an arrangement of magnets in 2014.

The latter system consists of two parallel diametric cylindrical magnets, that is, magnets that are magnetized perpendicular to their axis, with the north and south poles located on the curved surface as opposed to either end. When a diamagnetic rod (usually graphite) is placed between the magnets, it will remain in place and move back and forth in harmonic motion when disturbed. This arrangement, also known as a "PDL trap" for "parallel dipole line", was the subject of the 2017 International Physics Olympiad.

In the magnetic system, the camelback potential effect only occurs when the length of the diamagnetic rod is between two critical lengths. Below the minimum length, the magnet is hypothesized to align with magnetic field lines, hence not maintaining its orientation and touching the magnet. The maximum length is limited by the distance between the peaks of the camelback humps; thus, a rod longer than that will be unstable and fall out of the trap. Both the radius and the length of the rod determine the damping of the system. The damping is primarily due to Stokes drag, as damping is non-observable under vacuum.

Possible practical uses of the concept include being a platform for custom-designed 1D potentials, a highly sensitive force-distance transducer or a trap for semiconductor nanowires.


</doc>
<doc id="3339367" url="https://en.wikipedia.org/wiki?curid=3339367" title="Elitzur–Vaidman bomb tester">
Elitzur–Vaidman bomb tester

The Elitzur–Vaidman bomb-tester is a quantum mechanics thought experiment that uses interaction-free measurements to verify that a bomb is functional without having to detonate it. It was conceived in 1993 by Avshalom Elitzur and Lev Vaidman. Since their publication, real-world experiments have confirmed that their theoretical method works as predicted.

The bomb tester takes advantage of two characteristics of elementary particles, such as photons or electrons: nonlocality and wave-particle duality. By placing the particle in a quantum superposition, the experiment can verify that the bomb works without ever triggering its detonation, although there is a 50% chance that the bomb will explode in the effort.

The bomb test is an interaction-free measurement. The idea of getting information about an object without interacting with it is not a new one. For example, there are two boxes, one of which contains something, the other of which contains nothing. If you open one box and see nothing, you know that the other contains something, without ever opening it.

This experiment has its roots in the double-slit experiment and other, more complex concepts it inspired, including Schrodinger's cat, and Wheeler's delayed choice experiment. The behavior of elementary particles is very different from what we experience in our macroscopic world. They can behave like a wave or like a particle (see wave–particle duality). When they are in a wave state, they are in what is called a "superposition". In this state, some properties of the particle, for example, its location, are not definite. While in a superposition, any and all possibilities are equally real. So, if it can exist in more than one location, it "does" exist in them all. The particle's wave can later be "collapsed" by observing it, at which time its location once again becomes definite. Information can then be gleaned not only about the actual state of the particle, but also other states, or locations in which it existed before the collapse. This is possible even though the particle was never factually in those states or locations.

Consider a collection of light-sensitive bombs, of which some are duds. When their triggers detect any light, even a single photon, the light is absorbed and the bomb explodes. The triggers on the dud bombs have no sensor, so the photon can't be absorbed. Thus, the dud bomb will not detect the photon and will not detonate. Is it possible to determine which bombs are functional and which are duds without detonating all of the live ones?


A superposition in the bomb tester is created with an angled half-silvered mirror, which allows a photon to either pass through it, or be reflected off it at a 90-degree angle (see figure 3). There is equal probability it will do either. The photon enters a superposition, in which it does both. The single particle both passes through, and is reflected off the half-silvered mirror. From that moment on, the single photon exists in two different locations.

Along both the upper and lower path, the particle will encounter an ordinary mirror, positioned to redirect the two routes toward one another. They then intersect at a second half-silvered mirror. On the other side, a pair of detectors are placed such that the photon can be detected by either detector, but never by both. It is also possible that it will not be detected by either. Based on this outcome, with a live bomb, there is a 50% chance it will explode, a 25% chance it will be identified as good without exploding and a 25% chance there will be no result.

A light-sensitive bomb is placed along the lower path. If the bomb is good, when a photon arrives, it will explode and both will be destroyed. If it's a dud, the photon will pass by unaffected (see figure 4). To understand how this experiment works, it is important to know that the bomb is a kind of observer and that this encounter is a kind of observation. It can therefore collapse the photon's superposition, in which the photon is travelling along both the upper and lower paths. When it reaches the live bomb, or the detectors, however, it can only have been on one or the other. But, like the radioactive material in the box with Schrödinger's famous cat, upon its encounter with the half-silvered mirror at the beginning of the experiment, the photon, paradoxically does and does not interact with the bomb. According to the authors, the bomb both explodes and doesn't explode. This is only in the case of a live bomb, however. In any event, once observed by the detectors, it will have only traveled one of the paths.

When two waves collide, the process by which they affect each other is called interference. They can either strengthen each other by "constructive interference", or weaken each other by "destructive interference". This is true whether the wave is in water, or a single photon in a superposition. So even though there is only one photon in the experiment, because of its encounter with the half-silvered mirror, it acts like two. When "it" or "they" are reflected off the ordinary mirrors, it will interfere with itself as if it were two different photons. "But that's only true if the bomb is a dud." A live bomb will absorb the photon when it explodes and there will be no opportunity for the photon to interfere with itself.

When it reaches the second half-silvered mirror, if the photon in the experiment is behaving like a particle (in other words, if it is not in a superposition), then it has a fifty-fifty chance it will pass through or be reflected and be detected by one or the other detector. "But that's only possible if the bomb is live." If the bomb "observed" the photon, it detonated and destroyed the photon on the lower path, therefore only the photon that takes the upper path will be detected, either at Detector C or Detector D.

Detector D is the key to confirming that the bomb is live.

The two detectors and the second half-silvered mirror are precisely aligned with one another. Detector C is positioned to detect the particle if the bomb is a dud and the particle traveled both paths in its superposition and then constructively interfered with itself. Detector D is positioned to detect the photon only in the event of destructive interference—an impossibility (see figure 6). In other words, if the photon is in a superposition at the time it arrives at the second half-silvered mirror, it will always arrive at detector C and never at detector D.

If the bomb is live, there is a 50/50 chance that the photon took upper path. If it "factually" did so, then it "counter-factually" took the lower path (see figure 7). That counter-factual event destroyed that photon and left only the photon on the upper to arrive at the second half-silvered mirror. At which point it will, again, have a 50/50 chance of passing through it or being reflected off it, and, subsequently, it will be detected at either of the two detectors with the same probability. This is what makes it possible for the experiment to verify the bomb is live without actually blowing it up.

With a dud, the photon will always arrive at Detector C.<br>
With a live bomb, there can be three possible outcomes:


These correspond with the following conditions of the bomb being tested:

1. No photon was detected: The bomb exploded and destroyed the photon before it could be detected. This is because the photon in fact took the lower path and triggered the bomb, destroying itself in the process. There is a 50% chance that this will be the outcome if the bomb is live.<br>
2. The photon was detected at C: This will always be the outcome if a bomb is a dud, however, there is a 25% chance that this will be the outcome if the bomb is live. If the bomb is a dud, this is because the photon remained in its superposition until it reached the second half-silvered mirror and constructively interfered with itself. If the bomb is live, this is because the photon in fact took the upper path and reflected off the second half-silvered mirror.<br>
3. The photon was detected at D: The bomb is live but unexploded. That's because the photon in fact took the upper path and passed through the second half-silvered mirror, something possible only because there was no photon from the lower path with which it could interfere. "This is the only way that a photon can ever be detected at D." If this is the outcome, the experiment has successfully verified that the bomb is live despite the fact that the photon never "factually" encountered the bomb itself. There is a 25% chance that this will be the outcome if the bomb is live.

If the result is 2, the experiment is repeated. If the photon continues to be observed at C and the bomb doesn't explode, it can eventually be concluded that the bomb is a dud.

With this process 25% of live bombs can be identified without being detonated, 50% will be detonated and 25% remain uncertain. By repeating the process with the uncertain ones, the ratio of identified non-detonated live bombs approaches 33% of the initial population of bombs. See the "Experiments" section below for a modified experiment that can identify the live bombs with a yield rate approaching 100%.

The authors point out that the ability to obtain information about the bomb's functionality without ever "touching" it, appears to be a paradox. That, they claim, is based on the assumption that there is only a single "real" result. But according to the many-worlds interpretation, each possible state of a particle's superposition is real. Therefore, the particle does actually interact with the bomb and it does explode, just not in our "world".

In 1994, Anton Zeilinger, Paul Kwiat, Harald Weinfurter, and Thomas Herzog actually performed an equivalent of the above experiment, proving interaction-free measurements are indeed possible.

In 1996, Kwiat "et al." devised a method, using a sequence of polarising devices, that efficiently increases the yield rate to a level arbitrarily close to one. The key idea is to split a fraction of the photon beam into a large number of beams of very small amplitude and reflect all of them off the mirror, recombining them with the original beam afterwards.
It can also be argued that this revised construction is simply equivalent to a resonant cavity and the result looks much less shocking in this language, see Watanabe and Inoue (2000).

In 2016, Carsten Robens, Wolfgang Alt, Clive Emary, Dieter Meschede, and Andrea Alberti demonstrated that the Elitzur–Vaidman bomb testing experiment can be recast in a rigorous test of the macro-realistic worldview based on the violation of the Leggett–Garg inequality using ideal negative measurements. In their experiment they perform the “bomb test” with a single atom trapped in a polarization-synthesized optical lattice. This optical lattice enables interaction-free measurements by entangling the spin and position of atoms.





</doc>
<doc id="55823152" url="https://en.wikipedia.org/wiki?curid=55823152" title="Diffeomorphometry">
Diffeomorphometry

Diffeomorphometry is the metric study of imagery, shape and form in the discipline of computational anatomy (CA) in medical imaging. The study of images in computational anatomy rely on high-dimensional diffeomorphism groups formula_1 which generate orbits of the form formula_2, in which images formula_3 can be dense scalar magnetic resonance or computed axial tomography images. For deformable shapes these are the collection of manifolds formula_4, points, curves and surfaces. The diffeomorphisms move the images and shapes through the orbit according to formula_5 which are defined as the group actions of computational anatomy.

The orbit of shapes and forms is made into a metric space by inducing a metric on the group of diffeomorphisms. The study of metrics on groups of diffeomorphisms and the study of metrics between manifolds and surfaces has been an area of significant investigation. In Computational anatomy, the diffeomorphometry metric measures how close and far two shapes or images are from each other. Informally, the metric is constructed by defining a flow of diffemorphisms formula_6 which connect the group elements from one to another, so for formula_7 then formula_8. The metric between two coordinate systems or diffeomorphisms is then the shortest length or geodesic flow connecting them. The metric on the space associated to the geodesics is given byformula_9. The metrics on the orbits formula_10 are inherited from the metric induced on the diffeomorphism group.

The group formula_1 is thusly made into a smooth Riemannian manifold with Riemannian metric formula_12 associated to the tangent spaces at all formula_13. The Riemannian metric satisfies at every point of the manifold formula_14 there is an inner product inducing the norm on the tangent space formula_15 that varies smoothly across formula_16.

Oftentimes, the familiar Euclidean metric is not directly applicable because the patterns of shapes and images don't form a vector space. In the Riemannian orbit model of Computational anatomy, diffeomorphisms acting on the forms formula_17 don't act linearly. There are many ways to define metrics, and for the sets associated to shapes the Hausdorff metric is another. The method used to induce the Riemannian metric is to induce the metric on the orbit of shapes by defining it in terms of the metric length between diffeomorphic coordinate system transformations of the flows. Measuring the lengths of the geodesic flow between coordinates systems in the orbit of shapes is called diffeomorphometry.

The diffeomorphisms in computational anatomy are generated to satisfy the Lagrangian and Eulerian specification of the flow fields, formula_18, generated via the ordinary differential equation

with the Eulerian vector fields formula_19 in formula_20 for formula_21. The inverse for the flow is given by
formula_22
and the formula_23 Jacobian matrix for flows in formula_24 given as formula_25

To ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_20 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_27 using the Sobolev embedding theorems so that each element formula_28 has 3-square-integrable derivatives thusly implies formula_27 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:

Shapes in Computational Anatomy (CA) are studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinate systems. In this setting, 3-dimensional medical images are modelled as diffemorphic transformations of some exemplar, termed the template formula_30, resulting in the observed images to be elements of the random orbit model of CA. For images these are defined as formula_31, with for charts representing sub-manifolds denoted as formula_32.

The orbit of shapes and forms in Computational Anatomy are generated by the group action formula_33 , formula_34. These are made into a Riemannian orbits by introducing a metric associated to each point and associated tangent space. For this a metric is defined on the group which induces the metric on the orbit. Take as the metric for Computational anatomy at each element of the tangent space formula_35 in the group of diffeomorphisms

with the vector fields modelled to be in a Hilbert space with the norm in the Hilbert space formula_27. We model formula_38 as a reproducing kernel Hilbert space (RKHS) defined by a 1-1, differential operator formula_39, where formula_40 is the dual-space. In general, formula_41 is a generalized function or distribution, the linear form associated to the inner-product and norm for generalized functions are interpreted by integration by parts according to for formula_42,

When formula_44, a vector density, formula_45

The differential operator is selected so that the Green's kernel associated to the inverse is sufficiently smooth so that the vector fields support 1-continuous derivative. The Sobolev embedding theorem arguments were made in demonstrating that 1-continuous derivative is required for smooth flows. The Green's operator generated from the Green's function(scalar case) associated to the differential operator smooths.

For proper choice of formula_46 then formula_47 is an RKHS with the operator formula_48. The Green's kernels associated to the differential operator smooths since for controlling enough derivatives in the square-integral sense the kernel formula_49 is continuously differentiable in both variables implying

(I,J)=\inf_{\phi \in \operatorname{Diff}_V: \phi \cdot I = J } d_{\operatorname{Diff}_V}(id,\phi) \ ;

The distance on shapes and forms, formula_51,

For calculating the metric, the geodesics are a dynamical system, the flow of coordinates formula_52 and the control the vector field formula_53 related via formula_54 The Hamiltonian view

The Pontryagin maximum principle gives the Hamiltonian formula_59
The optimizing vector field formula_60 with dynamics formula_61. Along the geodesic the Hamiltonian is constant:
formula_62. The metric distance between coordinate systems connected via the geodesic determined by the induced distance between identity and group element:

For landmarks, formula_64, the Hamiltonian momentum

with Hamiltonian dynamics taking the form

with
The metric between landmarks formula_68

The dynamics associated to these geodesics is shown in the accompanying figure.

For surfaces, the Hamiltonian momentum is defined across the surface has Hamiltonian

and dynamics

For volumes the Hamiltonian


</doc>
<doc id="31969872" url="https://en.wikipedia.org/wiki?curid=31969872" title="Droplet vaporization">
Droplet vaporization

The vaporizing droplet (droplet vaporization) problem is a challenging issue in fluid dynamics. It is part of many engineering situations involving the transport and computation of sprays: fuel injection, spray painting, aerosol spray, flashing releases… In most of these engineering situations there is a relative motion between the droplet and the surrounding gas. The gas flow over the droplet has many features of the gas flow over a rigid sphere: pressure gradient, viscous boundary layer, wake. In addition to these common flow features one can also mention the internal liquid circulation phenomenon driven by surface-shear forces and the boundary layer blowing effect.

One of the key parameter which characterizes the gas flow over the droplet is the droplet Reynolds number based on the relative velocity, droplet diameter and gas phase properties. The features of the gas flow have a critical impact on the exchanges of mass, momentum and energy between the gas and the liquid phases and thus, they have to be properly accounted for in any vaporizing droplet model.

As a first step it is worth investigating the simple case where there is no relative motion between the droplet and the surrounding gas. It will provide some useful insights on the physics involved in the vaporizing droplet problem. In a second step models used in engineering situations where a relative motion between the droplet and the surrounding exists are presented.

In this section we assume that there is no relative motion between the droplet and the gas, formula_1, and that the temperature inside the droplet is uniform (models that account for the non-uniformity of the droplet temperature are presented in the next section). The time evolution of the droplet radius, formula_2, and droplet temperature, formula_3, can be computed by solving the following set of ordinary differential equations.::

where:

The heat flux entering the droplet can be expressed as:

where:

Analytical expressions for the droplet vaporization rate, formula_7, and for the heat flux formula_11 are now derived. A single, pure, component droplet is considered and the gas phase is assumed to behave as an ideal gas. A spherically symmetric field exists for the gas field surrounding the droplet. Analytical expressions for formula_7 and formula_11 are found by considering heat and mass transfer processes in the gas film surrounding the droplet. The droplet vaporizes and creates a radial flow field in the gas film. The vapor from the droplet convects and diffuses away from the droplet surface. Heat conducts radially against the convection toward the droplet interface. This process is called Stefan convection or Stefan flow.
The gas phase conservation equations for mass, fuel-vapor mass fraction and energy are written in a spherical coordinate system:

where:

It is assumed that the gas phase heat and mass transfer processes are quasi-steady and that the thermo-physical properties might be considered as constant. The assumption of quasi-steadiness of the gas phase finds its limitation in situations in which the gas film surrounding the droplet is in a near-critical state or in a situation in which the gas field is submitted to an acoustic field. The assumption of constant thermo-physical properties is found to be satisfying provided that the properties are evaluated at some reference conditions 

where:
The "1/3" averaging rule, formula_37, is often recommended in the literature

The conservation equation of mass simplifies to:

Combining the conservation equations for mass and fuel vapor mass fraction the following differential equation for the fuel vapor mass fraction formula_39 is obtained:

Integrating this equation between formula_21 and the ambient gas phase region formula_42 and applying the boundary condition at formula_43 gives the expression for the droplet vaporization rate:

and

where:

Phase equilibrium is assumed at the droplet surface and the mole fraction of fuel vapor at the droplet surface is obtained via the use of the Clapeyron's equation.

An analytical expression for the heat flux formula_11 is now derived. After some manipulations the conservation equation of energy writes:

where:

Applying the boundary condition at the droplet surface and using the relation formula_50 we have:

where:

Integrating this equation from formula_21 to the ambient gas phase conditions (formula_54) gives the variation of the gas film temperature (formula_26) as a function of the radial distance:

The above equation provides a second expression for the droplet vaporization rate:

and

where:

Finally combining the new expression for the droplet vaporization rate and the expression for the variation of the gas film temperature the following equation is obtained for formula_11:

Two different expressions for the droplet vaporization rate formula_7 have been derived. Hence, a relation exists between the Spalding mass transfer number and the Spalding heat transfer number and writes:

where:

The droplet vaporization rate can be expressed as a function of the Sherwood number. The Sherwood number describes the non-dimensional mass transfer rate to the droplet and is defined as:

Thus, the expression for the droplet vaporization rate can be re-written as:

Similarly, the conductive heat transfer from the gas to the droplet can be expressed as a function of the Nusselt number. The Nusselt number describes a non-dimensional heat transfer rate to the droplet and is defined as:

and then:

In the limit where formula_70 we have formula_71 which corresponds to the classical heated sphere result.

The relative motion between a droplet and the gas results in an increase of the heat and mass transfer rates in the gas film surrounding the droplet. A convective boundary layer and a wake can surround the droplet. Furthermore, the shear force on the liquid surface causes an internal circulation that enhances the heating of the liquid. As a consequence, the vaporization rate increases with the droplet Reynolds number. Many different models exist for the single convective droplet vaporization case. Vaporizing droplet models can be seen to belong to six different classes:


The main difference between all these models is the treatment of the heating of the liquid phase which is usually the rate controlling phenomenon in droplet vaporization. The first three models do not consider internal liquid circulation. The effective conductivity model (4) and the vortex model of droplet heating (5) account for internal circulation and internal convective heating. The direct resolution of the Navier-Stokes equations provide, in principle, exact solutions both for the gas phase and the liquid phase.

Model (1) is a simplification of model (2) which is in turn a simplification of model (3). The spherically symmetric transient droplet heating model (3) solves the equation for heat diffusion through the liquid phase. A droplet heating time τ can be defined as the time required for a thermal diffusion wave to penetrate from the droplet surface to its center. The droplet heating time is compared to the droplet lifetime, τ. If the droplet heating time is short compared to the droplet lifetime we can assume that the temperature field inside the droplet is uniform and model (2) is obtained. In the infinite liquid conductivity model (2) the temperature of the droplet is uniform but varies with time. It is possible to go one step further and find the conditions for which we can neglect the temporal variation of the droplet temperature. The liquid temperature varies in time until the wet-bulb temperature is reached. If the wet-bulb temperature is reached in a time of the same order of magnitude than the droplet heating time then the liquid temperature can be considered to be constant with regards to time. Model (1), the d2-law, is obtained.

The infinite liquid conductivity model is widely used in industrial spray calculations: for its balance between computational costs and accuracy. To account for the convective effects which enhanced the heat and mass transfer rates around the droplet, a correction is applied to the spherically symmetric expressions of the Sherwood and Nusselt numbers 

Abramzon and Sirignano suggest the following formulation for the modified Sherwood and Nusselt numbers:

where:


and the well known Frossling correlations (or Ranz-Marshall correlations ) can be used to express "Nu" and "Sh":

where


The expressions above show that the heat and mass transfer rates increase with increasing Reynolds number.


</doc>
<doc id="56310192" url="https://en.wikipedia.org/wiki?curid=56310192" title="Self-propulsion">
Self-propulsion

Self-Propulsion is the autonomous displacement of nano-, micro- and macroscopic natural and artificial objects, containing their own means of motion. Self-propulsion is driven mainly by interfacial phenomena. Various mechanisms of self-propelling have been introduced and investigated, which exploited phoretic effects, gradient surfaces, breaking the wetting symmetry of a droplet on a surface, the Leidenfrost effect, the self-generated hydrodynamic and chemical fields originating from the geometrical confinements, and soluto- and thermo-capillary Marangoni flows. Self-propelled system demonstrate a potential as micro-fluidics devices and micro-mixers. Self-propelled liquid marbles have been demonstrated.


</doc>
<doc id="50311973" url="https://en.wikipedia.org/wiki?curid=50311973" title="Microfluidic cell culture">
Microfluidic cell culture

Microfluidic cell culture integrates knowledge from biology, biochemistry, engineering, and physics to develop devices and techniques for culturing, maintaining, analyzing, and experimenting with cells at the microscale. It merges microfluidics, a set of technologies used for the manipulation of small fluid volumes (μL, nL, pL) within artificially fabricated microsystems, and cell culture, which involves the maintenance and growth of cells in a controlled laboratory environment. Microfluidics has been used for cell biology studies as the dimensions of the microfluidic channels are well suited for the physical scale of cells. For example, eukaryotic cells have linear dimensions between 10-100 μm which falls within the range of microfluidic dimensions. A key component of microfluidic cell culture is being able to mimic the cell microenvironment which includes soluble factors that regulate cell structure, function, behavior, and growth. Another important component for the devices is the ability to produce stable gradients that are present "in vivo" as these gradients play a significant role in understanding chemotactic, durotactic, and haptotactic effects on cells.

Some considerations for microfluidic devices relating to cell culture include:
Fabrication material is crucial as not all polymers are biocompatible, with some materials such as PDMS causing undesirable adsorption or absorption of small molecules. Additionally, uncured PDMS oligomers can leach into the cell culture media, which can harm the microenvironment. As an alternative to commonly used PDMS, there have been advances in the use of thermoplastics (e.g., polystyrene) as a replacement material.

Spatial organization of cells in microscale devices largely depends on the culture region geometry for cells to perform functions "in vivo". For example, long, narrow channels may be desired to culture neurons. The perfusion system chosen might also affect the geometry chosen. For example, in a system that incorporates syringe pumps, channels for perfusion inlet, perfusion outlet, waste, and cell loading would need to be added for the cell culture maintenance. Perfusion in microfluidic cell culture is important to enable long culture periods on-chip and cell differentiation.

Other critical aspects for controlling the microenvironment include: cell seeding density, reduction of air bubbles as they can rupture cell membranes, evaporation of media due to an insufficiently humid environment, and cell culture maintenance (i.e. regular, timely media changes).

Some of the major advantages of microfluidic cell culture include reduced sample volumes (especially important when using primary cells, which are often limited) and the flexibility to customize and study multiple microenvironments within the same device. A reduced cell population can also be used in a microscale system (e.g., a few hundred cells) in comparison to macroscale culture systems (which often require 10 – 10 cells); this can make studying certain cell-cell interactions more accessible. These reduced cell numbers make studying non-dividing or slow dividing cells (e.g., stem cells) easier than traditional culture methods (e.g., flasks, petri dishes, or well plates) due to the smaller sample volumes. Given the small dimensions in microfluidics, laminar flow can be achieved, allowing manipulations with the culture system to be done easily without affecting other culture chambers. Laminar flow is also useful as is it mimics "in vivo" fluid dynamics more accurately, often making microscale culture more relevant than traditional culture methods.

Two-dimensional (2D) cell culture is cell culture that takes place on a flat surface, e.g. the bottom of a well-plate, and is known as the conventional method. While these platforms are useful for growing and passaging cells to be used in subsequent experiments, they are not ideal environments to monitor cell responses to stimuli as cells cannot freely move or perform functions as observed "in vivo" that are dependent on cell-extracellular matrix material interactions.

Three-dimensional (3D) cell culture is cell culture that takes place in a biologically relevant matrix, usually this involves cells being embedded in a hydrogel containing extracellular molecules (e.g., collagen). By adding an additional dimension, more advanced cell architectures can be achieved, and cell behavior is more representative of "in vivo" dynamics; cells can engage in enhanced communication with neighboring cells and cell-extracellular matrix interactions can be modeled. These simplified 3D cell culture models can be combined in a manner that recapitulates tissue- and organ-level functions in devices known as organ-on-a-chip. In these devices, chambers or collagen layers containing different cell types can interact with one another for multiple days while various channels deliver nutrients to the cells. An advantage of these devices is that tissue function can be characterized and observed under controlled conditions (e.g., effect of shear stress on cells, effect of cyclic strain or other forces) to better understand the overall function of the organ. While these 3D models ofter better model organ function on a cellular level compared with 2D models, there are still challenges. Some of the challenges include: imaging of the cells, control of gradients in static models (i.e., without a perfusion system), and difficulty recreating vasculature. Despite these challenges, 3D models are still used as tools for studying and testing drug responses in pharmacological studies. In recent years, there are microfluidic devices reproducing the complex "in vivo" microvascular network. The device is able to create a physiologically realistic 3D environment, which is desirable as a tool for drug screening, drug delivery, cell-cell interactions, tumor metastasis etc.


</doc>
<doc id="24489" url="https://en.wikipedia.org/wiki?curid=24489" title="Outline of physics">
Outline of physics

The following outline is provided as an overview of and topical guide to physics:

Physics – natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.

Physics can be described as all of the following:



History of physics – history of the physical science that studies matter and its motion through space-time, and related concepts such as energy and force

Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:

Gravity, light, physical system, physical observation, physical quantity, physical state, physical unit, physical theory, physical experiment

Theoretical concepts
Mass–energy equivalence, particle, physical field, physical interaction, physical law, fundamental force, physical constant, wave

Physics
This is a list of the primary theories in physics, major subtopics, and concepts.

Index of physics articles





</doc>
<doc id="57243717" url="https://en.wikipedia.org/wiki?curid=57243717" title="Nonlinear frictiophoresis">
Nonlinear frictiophoresis

Nonlinear frictiophoresis is the unidirectional drift of a particle in a medium caused by periodic driving force with zero mean. The effect is possible due to nonlinear dependence of the friction-drag force on the particle's velocity. It was discovered theoretically.,
and is mainly known as nonlinear electrofrictiophoresis
At first glance, a periodic driving force with zero mean is able to entrain a particle into an oscillating movement without unidirectional drift, because integral momentum provided to the particle by the force is zero. The possibility of unidirectional drift can be recognized if one takes into account that the particle itself loses momentum through transferring it further to the medium it moves in/at. If the friction is nonlinear, then it may so happen that the momentum loss during movement in one direction does not equal to that in the opposite direction and this causes unidirectional drift. For this to happen, the driving force time-dependence must be more complicated than it is in a single sinusoidal harmonic.

The simplest case of friction-velocity dependence law is the
Stokes's one:
where formula_2 is the friction/drag force applied to a particle moving with velocity formula_3 in a medium. The friction-velocity law (1) is observed for a slowly moving spherical particle in a Newtonian fluid. It is linear, see Fig. 1, and is not suitable for nonlinear frictiophoresis to take place. The characteristic property of the law (1) is that any, even a very small driving force is able to get particle moving. This is not the case for such media as Bingham plastic. For those media, it is necessary to apply some threshold force, formula_4, to get the particle moving. This kind of friction-velocity (dry friction) law has a jump discontinuity at formula_5:

It is nonlinear, see Fig. 2, and is used in this example.

Let formula_7 denote the period of driving force. Chose a time
value formula_8
such that formula_9
and two force values, formula_10, formula_11
such that the following relations are
satisfied:
The periodic driving force formula_14
used in this example is as follows:
It is clear that, due to (3), formula_14 has zero mean:

See also Fig. 3.

For the sake of simplicity, we consider here the physical situation when inertia may be neglected. The latter can be achieved if
particle's mass is small, velocity is low and friction is high. This conditions have to ensure that formula_18,
where formula_19 is the relaxation time. In this situation, the particle driven with force (4) immediately starts moving with constant velocity 
formula_20 during interval
formula_21
and will immediately
stop moving during interval formula_22, see Fig. 4.

This results in the
positive mean velocity of unidirectional drift:

Analysis of possibility to get a nonzero drift by periodic force with
zero integral has been made in
The dimensionless
equation of motion for a particle driven by periodic force
formula_14, formula_25, formula_26
is as follows:
where the friction/drag force
formula_28 satisfies
the following:
It is proven in 
that any solution to (5)
settles down onto periodic regime
formula_30, formula_31, which has nonzero mean:
almost certainly, provided formula_14
is not antiperiodic.

For formula_34, two cases of formula_14 have been considered explicitly:

1. Saw-shaped driving force, see Fig. 5:

In this case, found in 
first order in formula_37 approximation to formula_30,
formula_39, has the following mean value:
This estimate is made expecting formula_41.

2. Two harmonics driving force,
In this case, the first order in formula_37 approximation
has the following mean value:
This value is maximized in formula_45, formula_46, keeping formula_47 constant. Interesting that the drift value depends on formula_45 and changes its direction twice as formula_45 spans over the interval formula_50. Another type of analysis, based on symmetry breaking suggests as well that a zero mean driving force is able to generate a directed drift.

 In applications, the nature of force formula_14 in (5), is usually electric, similar to forces acting during standard electrophoresis. The only differences are that the force is periodic and without constant component.
For the effect to show up, the dependence of friction/drag force on velocity must be nonlinear. This is the case for numerous substances known as non-Newtonian fluids. Among these are gels, and dilatant fluids, pseudoplastic fluids, liquid crystals.

Dedicated experiments
have determined formula_52 for a standard DNA ladder up to 1500 bp long in 1.5% agarose gel. The dependence found, see Fig. 6, supports the possibility of nonlinear frictiophoresis in such a system. Based on data in Fig. 6, an optimal time course for driving electric field with zero mean, formula_53, has been found in, which ensures maximal drift for 1500 b.p. long fragment,
see Fig. 7.
The effect of unidirectional drift caused by periodic force with zero integral value has a peculiar dependence on the time course of the force applied. See the previous section for examples. This offers a new dimension to a set of separation problems.

In the DNA fragments separation,
zero mean periodic electric field is used in zero-integrated-field electrophoresis (ZIFE),
where the field time dependence similar to that shown in Fig. 3 is used.
This allows to separate long fragments in agarose gel,
nonseparable by standard constant field electrophoresis.
The long DNA geometry and its manner of movement in a gel,
known as reptation
do not allow to apply directly the consideration based on Eq. (5), above.

It was observed,

that under certain physical conditions
the mechanism described in Mathematical analysis section, above,
can be used for separation with respect to specific mass,
like particles made of isotopes of the same material.

The idea of organizing directed drift with zero mean periodic drive
have obtained further development for other configurations
and other physical mechanism of nonlinearity.

An electric dipole
rotating freely around formula_54-axis
in a medium with nonlinear friction can be manipulated by
applying electromagnetic wave polarized circularly along formula_55
and composed of two harmonics. The equation of motion
for this system is as follows:
where formula_57 is the torque acting on the dipole due to
circular wave: 
where formula_59 is the dipole moment component orthogonal
to formula_54-axis
and formula_61 defines the dipole direction in the formula_62 plane. By choosing proper
phase shift formula_45 in (6) it is possible to
orient the dipole in any desired direction, formula_64.
The direction formula_64 is
attained due to angular directed drift, which becomes zero when
formula_66.
A small detuning between the first and second harmonic in (6) results in continuous rotational drift.

If a particle undergoes a directed drift while moving freely in accordance
with Eq. (5), then it drifts similarly if a shallow enough
potential field formula_67 is imposed. Equation of motion in that case is:
where formula_69 is the force due to potential field. The drift continues until a steep enough region in the course of formula_67 is met, which is able to stop the drift. This kind of behavior, as rigorous mathematical analysis shows,
results in modification of formula_67 by adding a linear in formula_72 term. This may change the formula_67 qualitatively, by, e.g. changing the number of equilibrium points, see Fig. 8. The effect may be essential during high frequency
electric field acting on biopolymers. 
For electrophoresis of colloid particles under a small strength electric field, the force formula_14
in the right-hand side of Eq. (5) is linearly proportional to the
strength formula_53 of the electric field applied. For a high strength,
the linearity is broken due to nonlinear polarization.
As a result, the force may depend nonlinearly on the applied field:
In the last expression, even if
the applied field, formula_53 has zero mean, the applied force formula_14 may happen to have a constant component that can cause a directed drift.

As above, for this to happen, formula_53 must have more than a single sinusoidal harmonic.
This same effect for a liquid in a tube may serve in
electroosmotic pump
driven with zero mean electric field.


</doc>
<doc id="6756239" url="https://en.wikipedia.org/wiki?curid=6756239" title="Landolt–Börnstein">
Landolt–Börnstein

Landolt–Börnstein is the largest collection of critically evaluated property data in materials science and the closely related fields of chemistry, physics and engineering published by Springer Nature.
On July 28, 1882, Dr. Hans Heinrich Landolt and Dr. Richard Börnstein, both professors at the "Landwirtschaftliche Hochschule" (Agricultural College) at Berlin, signed a contract with the publisher Ferdinand Springer on the publication of a collection of tables with physical-chemical data. The title of this book "Physikalisch-chemische Tabellen" (Physical-Chemical Tables) published in 1883 was soon forgotten. Owing to its success the data collection has been known for more than a hundred years by each scientist only as "The Landolt-Börnstein".

1250 copies of the 1st Edition were printed and sold. In 1894, the 2nd Edition was published, in 1905 the 3rd Edition, in 1912 the 4th Edition, and finally in 1923 the 5th Edition. Supplementary volumes of the latter were printed until as late as 1936. New Editions saw changes in large expansion of volumes, number of authors, updated structure, additional tables and coverage of new areas of physics and chemistry.

The 5th Edition was eventually published in 1923, consisting of two volumes and comprising a total of 1,695 pages. Sixty three authors had contributed to it. The growth that had already been noticed in previous editions, continued. It was clear, that "another edition in approximately 10 years" was no solution. A complete conceptual change of the Landolt-Börnstein had thus become necessary. For the meantime supplementary volumes in two-year intervals should be provided to fill in the blanks and add the latest data. The first supplementary volume of the 5th Edition was published in 1927, the second in 1931 and the third in 1935/36. The latter consisted of three sub-volumes with a total of 3,039 pages and contributions from 82 authors.

The 6th Edition (1950) was published in line with the revised general frame. The basic idea was to have four volumes instead of one, each of which was to cover different fields of the Landolt-Börnstein under different editors. Each volume was given a detailed table of contents. Two major restrictions were also imposed. The author of a contribution was asked to choose a "Bestwert" (optimum value) from the mass of statements of an experimental value in the publications of different authors, or derive a "wahrscheinlichster Wert” (most possible value). The other change of importance was that not only diagrams became as important as tables, but that text also became necessary to explain the presented data.

The new concept of the 6th Edition – splitting of the edition in four volumes with a yet unknown number of sub-volumes – was a slight progress, but eventually, it has not proven to be of much use. Instead of publishing new editions of the data collection one after the other, the "New Series" concept was intended to publish independent volumes that were each to cover a certain closed theme. It was flexible enough to follow the changes in the scientific interest and abandon the previous structuring of physics and chemistry in favor of new themes. Thus, new subjects such as solid-state physics could easily be comprised in groups of volumes. This concept has been adhered to up to now.

The New Series represents over 520 books published between 1961 and 2018 and includes more than 220,000 pages covering mechanical, optical, acoustical, thermal, spectroscopic, electrical and magnetic properties among others. The New Series offers critically evaluated data by over 1,000 expert authors and editors in materials science. 

Landolt–Börnstein books have gone through various digitization initiatives, from CD-ROM to FTP and PDF formats. Landolt–Börnstein books content is now available on SpringerMaterials, a comprehensive database for identifying material properties which covers data from materials science, physics, physical and inorganic chemistry, engineering and other related fields. SpringerMaterials offers advanced materials science-specific search functionality that allows easy retrieval of accurate results. Recently introduced SpringerMaterials Interactive is a set of advanced functionalities for visualizing and analyzing materials property data. Examples of these functionalities include interactive graphs, dynamic data tables, and side-by- side comparisons of materials/properties.




</doc>
<doc id="57507652" url="https://en.wikipedia.org/wiki?curid=57507652" title="Gravitationally-interacting massive particles">
Gravitationally-interacting massive particles

Gravitationally-interacting massive particles (GIMPs) are a set of particles theorised to explain the dark matter in our universe, as opposed to an alternative theory based on weakly-interacting massive particles (WIMPs). Dark matter was postulated by F. Zwicky in 1933 who noticed the failure of the velocity curves of stars to decrease when plotted as functions of their distance from the center of galaxies. Since Einstein's work, our universe is described by four-dimensional spacetime whose metric is calculable by the Einstein field equations:
Here is the Ricci curvature tensor, the scalar curvature, the metric tensor, Newton's gravitational constant, the speed of light in vacuum, and the stress–energy tensor. The constant is the so-called cosmological constant.

While WIMPs would be elementary particles described by the standard model that can in principle be studied by experimentalists in laboratories such as CERN, the proposed particles called GIMPs would follow the Vacuum Solutions of Einstein's equation. They are just singular structures of spacetime in a geometry whose average forms the dark energy that Einstein expressed in his cosmological constant. The identification of "dark matter" with GIMPs proposed makes dark matter a form of dark energy filled with singularities, i.e., an entangled dark energy. This would roughly confirm Einstein's hope in 1919 that all particles in the universe would follow the traceless version of his equation. If we identify all matter as the sum of dark energy plus dark matter in the form of GIMPs, his expectation would turn out to have been "almost" right. Matter would play a similar role as the point charges in the homogeneous Maxwell equation formula_1 in which delta functions are ignored. The sum of dark matter plus dark energy makes up 76% of all matter, which is sufficient to allow computer simulations to produce a good impression of the behavior of all matter.


</doc>
<doc id="57642278" url="https://en.wikipedia.org/wiki?curid=57642278" title="RAON">
RAON

RAON is a South Korean particle physics laboratory within the Rare Isotope Science Project (RISP) that is being constructed in the outskirts of Daejeon neighboring Sejong, South Korea by the Institute for Basic Science (IBS). It is expected to be finished by 2021.

The name Rare isotope Accelerator complex for ON-line experiment or RAON, was selected through a contest open to the public in 2012. RAON comes from the Korean word meaning "happy" or "joyful". Among 639 entries, the winning name was actually Raonhaje (라온하제) meaning "happy tomorrow" but was shortened for easier pronunciation. RAON is also the name of their chemical element mascot with atomic number 41 and niobium written on the stomach.

RAON is a heavy ion particle accelerator that will include both ISOL (Isotope Separation On-Line) and IF (In-flight Fragmentation) methods, and aims to be the first to use both. The superconducting linear accelerator will have a maximum beam power of 400 kW, and projectile fragmentation will be powered by a 200 MeV/u uranium beam in the IF system. The ISOL system will have a H- cyclotron of 70 kW.

Due to the complexity of the project, RAON's researchers are working in collaboration with a number of other accelerator research groups, including CERN, Fermilab, TRIUMF, and Riken.

The cost is estimated at 1.4523 trillion KRW (roughly 1.4 billion USD) in which 460.2 billion KRW is for device construction, 635 billion KRW for facility construction, and 357.1 billion KRW for land purchase. The size of the site is 652,066 m with a total floor area of 130,144 m. In additional to the primary accelerator site under construction in Shindong (신동), RISP has the ISOL Off-line Test Facility in Yuseong-gu, Superconducting Radio Frequency test facility in KAIST's Munji Campus, and the Accelerator and ICT Building in Korea University Sejong Campus.

Research areas in the field of nuclear science include the study of the origin of elements and evolution of stars, nuclear force and structure, nuclear reactions, and nuclear science theory. 

The group aims to develop an ultra-sensitive device for measuring the physical properties of muons, and study the properties of new materials, including semiconductors, nano-magnetic materials, high-temperature superconductors, and topological insulators. 

In these fields, they aim to precisely measure rare isotope mass and develop atomic manipulation technology, develop micro-measurement technology for atomic structures, and find the precise measurements of basic physical constants. 

Research the application of rare isotopes in cancer treatment.





</doc>
<doc id="53031776" url="https://en.wikipedia.org/wiki?curid=53031776" title="Stochastic thermodynamics">
Stochastic thermodynamics

Stochastic thermodynamics is an emergent field of research in statistical mechanics that uses stochastic variables to better understand the non-equilibrium dynamics present in microscopic systems such as colloidal particles, biopolymers (e.g. DNA, RNA, and proteins), enzymes, molecular motors and many other types of systems.

When a microscopic machine (e.g. a MEM) performs useful work it generates heat and entropy as a byproduct of the process, however it is also predicted that this machine will operate in "reverse" or "backwards" over appreciable short periods. That is, heat energy from the surroundings will be converted into useful work. For larger engines, this would be described as a violation of the second law of thermodynamics, as entropy is consumed rather than generated. Loschmidt's paradox states that in a time reversible system, for every trajectory there exists a time-reversed anti-trajectory. As the entropy production of a trajectory and its equal anti-trajectory are of identical magnitude but opposite sign, then, so the argument goes, one cannot prove that entropy production is positive.

For a long time, exact results in thermodynamics were only possible in linear systems capable of reaching equilibrium, leaving other questions like the Loschmidt paradox unsolved. During the last few decades fresh approaches have revealed general laws applicable to non-equilibrium system which are described by nonlinear equations, pushing the range of exact thermodynamic statements beyond the realm of traditional linear solutions. These exact results are particularly relevant for small systems where appreciable (typically non-Gaussian) fluctuations occur. Thanks to stochastic thermodynamics it is now possible to accurately predict distribution functions of thermodynamic quantities relating to exchanged heat, applied work or entropy production for these systems.

The mathematical resolution to Loschmidt's paradox is called the (steady state) fluctuation theorem (FT), which is a generalisation of the second law of thermodynamics. The FT shows that as a system gets larger or the trajectory duration becomes longer, entropy-consuming trajectories become more unlikely, and the expected second law behaviour is recovered. The FT was first put forward by and much of the work done in developing and extending the theorem was accomplished by theoreticians and mathematicians interested in nonequilibrium statistical mechanics.

The first observation and experimental proof of Evan's fluctuation theorem (FT) was performed by 

A recent review states that "proved a remarkable relation which allows to express the free energy difference between two equilibrium systems by a nonlinear average over the work required to drive the system in a non-equilibrium process from one state to the other. By comparing probability distributions for the work spent in the original process with the time-reversed one, Crooks found a “refinement” of the Jarzynski relation (JR), now called the Crooks fluctuation theorem. Both, this relation and another refinement of the JR, the Hummer-Szabo relation became particularly useful for determining free energy differences and landscapes of biomolecules. These relations are the most prominent ones within a class of exact results (some of which found even earlier and then rediscovered) valid for non-equilibrium systems driven by time-dependent forces. A close analogy to the JR, which relates different equilibrium states, is the Hatano-Sasa relation that applies to transitions between two different non-equilibrium steady states". 

This is shown to be a special case of a more general relation.

Classical thermodynamics, at its heart, deals with general laws governing the transformations of a system, in particular, those involving the exchange of heat, work and matter with an environment. As a central result, total entropy production is identified that in any such process can never decrease, leading, inter alia, to fundamental limits on the efficiency of heat engines and refrigerators.

The thermodynamic characterisation of systems in equilibrium got its microscopic justification from equilibrium statistical mechanics which states that for a system in contact with a heat bath the probability to find it in any specific microstate is given by the Boltzmann factor. For small deviations from equilibrium, linear response theory allows to express transport properties caused by small external fields through equilibrium correlation functions. On a more phenomenological level, linear irreversible thermodynamics provides a relation between such transport coefficients and entropy production in terms of forces and fluxes. Beyond this linear response regime, for a long time, no universal exact results were available.

During the last 20 years fresh approaches have revealed general laws applicable to non-equilibrium system thus pushing the range of validity of exact thermodynamic statements beyond the realm of linear response deep into the genuine non-equilibrium region. These exact results, which become particularly relevant for small systems with appreciable (typically non-Gaussian) fluctuations, generically refer to distribution functions of thermodynamic quantities like exchanged heat, applied work or entropy production.

Stochastic thermodynamics combines the stochastic energetics introduced by with the idea that entropy can consistently be assigned to a single fluctuating trajectory.

Stochastic thermodynamics can be applied to driven (i.e. open) quantum systems whenever the effects of quantum coherence can be ignored. The dynamics of an open quantum system is then equivalent to a classical stochastic one. However, this is sometimes at the cost of requiring unrealistic measurements at the beginning and end of a process.

Understanding non-equilibrium quantum thermodynamics more broadly is an important and active area of research. The efficiency of some computing and information theory tasks can be greatly enhanced when using quantum correlated states; quantum correlations can be used not as a valuable resource in quantum computation, but also in the realm of quantum thermodynamics. New types of quantum devices in non-equilibrium states function very differently to their classical counterparts. For example, it has been theoretically shown that non-equilibrium quantum ratchet systems function far more efficiently then that predicted by classical thermodynamics. It has also been shown that quantum coherence can be used to enhance the efficiency of systems beyond the classical Carnot limit. This is because it could be possible to extract work, in the form of photons, from a single heat bath. Quantum coherence can be used in effect to play the role of Maxwell's demon though it should be emphasized that the broader information theory based interpretation of the second law of thermodynamics is not violated.

Quantum versions of stochastic thermodynamics have been studied for some time and the past few years have seen a surge of interest in this topic. Quantum mechanics involves profound issues around the interpretation of reality (e.g. the Copenhagen interpretation, many-worlds, de Broglie-Bohm theory etc are all competing interpretations that try to explain the unintuitive results of quantum theory) . It is hoped that by trying to specify the quantum-mechanical definition of work, dealing with open quantum systems, analyzing exactly solvable models, or proposing and performing experiments to test non-equilibrium predictions, important insights into the interpretation of quantum mechanics and the true nature of reality will be gained.

Applications of non-equilibrium work relations, like the Jarzynski equality, have recently been proposed for the purposes of detectiing quantum entanglement and to improving optimization problems (minimize or maximize a function of multivariables called the cost function) via quantum annealing .

Until recently thermodynamics has only considered systems coupled to a thermal bath and, therefore, satisfying Boltzmann statistics. However, systems satisfying these conditions do not include many systems that are far from equilibrium such as living matter, for which fluctuations are expected to be non-Gaussian.

Active particle systems are able to take energy from their environment and drive themselves far from equilibrium. An important example of active matter is constituted by objects capable of self propulsion. Thanks to this property, they feature a series of novel behaviours that are not attainable by matter at thermal equilibrium, including, for example, swarming and the emergence of other collective properties. A passive particle is considered in an active bath when it is in an environment where a wealth of active particles are present. These particles will exert nonthermal forces on the passive object so that it will experience non-thermal fluctuations and will behave widely different from a passive Brownian particle in a thermal bath. The presence of an active bath can significantly influence the microscopic thermodynamics of a particle. Experiments have suggested that the Jarzynski equality does not hold in some cases due to the presence of non-Boltzmann statistics in active baths. This observation points towards a new direction in the study of non-equilibrium statistical physics and stochastic thermodynamics, where also the environment itself is far from equilibrium.

Active baths are a question of particular importance in biochemistry. For example, biomolecules within cells are coupled with an active bath due to the presence of molecular motors within the cytoplasm, which leads to striking and largely not yet understood phenomena such as the emergence of anomalous diffusion (Barkai et al., 2012). Also, protein folding might be facilitated by the presence of active fluctuations (Harder et al., 2014b) and active matter dynamics could play a central role in several biological functions (Mallory et al., 2015; Shin et al., 2015; Suzuki et al., 2015). It is an open question to what degree stochastic thermodynamics can be applied to systems coupled to active baths.


</doc>
<doc id="57780625" url="https://en.wikipedia.org/wiki?curid=57780625" title="Diffraction-limited storage ring">
Diffraction-limited storage ring

Diffraction-limited storage rings (DLSR), or ultra-low emittance storage rings, are synchrotron light sources where the emittance of the electron-beam in the storage ring is smaller or comparable to the emittance of the x-ray photon beam they produce at the end of their insertion devices.
These facilities operate in the soft to hard x-ray range (100eV—100keV) with extremely high brilliance (in the order of 10—10 photons/s/m/mrad/0.1%BW)

Together with x-ray Free-electron laser, they constitute the fourth generation of light sources, characterized by a relatively high coherent flux (in the order of 10—10photons/s/0.1%BW for DLSR) and enable extended physical and chemical characterizations at the nano-scale.






</doc>
<doc id="52837586" url="https://en.wikipedia.org/wiki?curid=52837586" title="Pseudo Jahn–Teller effect">
Pseudo Jahn–Teller effect

The pseudo Jahn–Teller effect (PJTE), occasionally also known as second-order JTE, is a direct extension of the Jahn–Teller effect (JTE) where spontaneous symmetry breaking in polyatomic systems (molecules and solids) occurs even in nondegenerate electronic states under the influence of sufficiently low-lying excited states of appropriate symmetry.
"The pseudo Jahn–Teller effect is the only source of instability and distortions of high-symmetry configurations of polyatomic systems in nondegenerate states, and it contributes significantly to the instability in degenerate states".

In the first publication in 1957 on the (what is now called) pseudo Jahn–Teller effect (PJTE), Öpik and Pryce showed that a small splitting of the degenerate electronic term does not necessarily remove the instability and distortion of the polyatomic system induced by the Jahn–Teller effect (JTE), provided the splitting is sufficiently small (the two split states remain “pseudodegenerate”), and the vibronic coupling between them is strong enough. From another perspective, the idea of vibronic admixture of different electronic terms by low-symmetry vibrations was introduced in 1933 by Herzberg and Teller to explore forbidden electronic transitions, and extended in the late 1950s by Murrell and Pople and by Liehr. The role of excited states in softening the ground state with respect to distortions in benzene was demonstrated qualitatively by Longuet-Higgins and Salem by analyzing the π electron levels in the Hückel approximation, while a general second-order perturbation formula for such vibronic softening was derived by Bader in 1960. In 1961 Fulton and Gouterman presented a symmetry analysis of the two-level case in dimers and introduced the term "pseudo Jahn–Teller effect". The first application of the PJTE to solving a major solid-state structural problem with regard to the origin of ferroelectricity was published in 1966 by Bersuker, and the first book on the JTE covering the PJTE was published in 1972 by Englman. The second-order perturbation approach was employed by Pearson in 1975 to predict instabilities and distortions in molecular systems; he called it "second-order JTE" (SOJTE). The first explanation of PJT origin of puckering distortion as due to the vibronic coupling to the excited state was given for the NH radical by Borden, Davidson, and Feller in 1980 (they called it "piramidalization"). Methods of numerical calculation of the PJT vibronic coupling effect with applications to spectroscopic problems were developed in the early 1980s

The equilibrium geometry of any polyatomic system in nondegenerate states is defined as corresponding to the point of the minimum of the adiabatic potential energy surface (APES), where its first derivatives are zero and the second derivatives are positive. Denote the energy of the system as a function of normal displacements Q by E(Q). At the point of minimum (Q=0) of the APES the curvature K of E(Q) in the Q direction,

is positive, K > 0. Very often the geometry of the system at this point of equilibrium on the APES does not coincide with the highest possible (or even with any high) symmetry expected from general symmetry considerations. For instance, linear molecules are bent at equilibrium, planar molecules are puckered, octahedral complexes are elongated, or compressed, or tilted, cubic crystals are tetragonally polarized (or have several structural phases), etc. The PJTE is the general driving force of all these distortions if they occur in the nondegenerate electronic states of the high-symmetry (reference) geometry.

K=<ψ|(dH/dQ)|ψ> (3)

K=-Σ|<ψ|(dH/dQ)|ψ>|/[E-E] (4)
where ψ are the wavefunctions of the excited states, and the K expression, obtained as a second order perturbation correction, is always negative, K<0. Therefore, if K>0, the K contribution is the only source of instability. The matrix elements in Eq. (4) are off-diagonal vibronic coupling constants,

They measure the mixing of the ground and excited states under the nuclear displacements Q, and therefore K is termed the vibronic contribution. Together with the K value and the energy gap 2∆=E-E between the mixing states, F are the main parameters of the PJTE (see below). 
In a series of papers beginning in 1980 (see references in ) it was proved that for any polyatomic system in the high-symmetry configuration

K>0, (6)

and hence the vibronic contribution is the only source of instability of any polyatomic system in nondegenerate states.
If K >0 for the high-symmetry configuration of any polyatomic system, then a negative curvature, K =(K + K)< 0, can be achieved only due to the negative vibronic coupling component K, and only if |K|> K. It follows that any distortion of the high-symmetry configuration is due to, and only to the mixing of its ground state with excited electronic states by the distortive nuclear displacements realized via the vibronic coupling in Eq. (5). The latter softens the system with respect to certain nuclear displacements (K<0), and if this softening is larger than the original (nonvibronic) hardness K in this direction, the system becomes unstable with respect to the distortions under consideration, leading to its equilibrium geometry of lower symmetry, or to dissociation. 
There are many cases when neither the ground state is degenerate, nor is there a significant vibronic coupling to the lowest excited states to realize PJTE instability of the high-symmetry configuration of the system, and still there is a ground state equilibrium configuration with lower symmetry. In such cases the symmetry breaking is produced by a hidden PJTE (similar to a hidden JTE); it takes place due to a strong PJTE mixing of two excited states, one of which crosses the ground state to create a new (lower) minimum of the APES with a distorted configuration.

The use of the second order perturbation correction, Eq. (4), for the calculation of the K value in the case of PJTE instability is incorrect because in this case |K|>K, meaning the first perturbation correction is larger than the main term, and hence the criterion of applicability of the perturbation theory in its simplest form does not hold. In this case, we should consider the contribution of the lowest excited states (that make the total curvature negative) in a pseudodegenerate problem of perturbation theory. For the simplest case when only one excited state creates the main instability of the ground state, we can treat the problem via a pseudodegenerate two-level problem, including the contribution of the higher, weaker-influencing states as a second order correction.
In the PJTE two-level problem we have two electronic states of the high-symmetry configuration, ground β and excited γ, separated by an energy interval of 2Δ, that become mixed under nuclear displacements of certain symmetry Q=Q; the denotations α, β, and γ indicate, respectively, the irreducible representations to which the symmetry coordinate and the two states belong. In essence, this is the original formulation of the PJTE. Assuming that the excited state is sufficiently close to the ground one, the vibronic coupling between them should be treated as a perturbation problem for two near-degenerate states. With both interacting states non-degenerate the vibronic coupling constant F in Eq.(5) (omitting indices)is non-zero for only one coordinate Q=Q with α=β×γ. This gives us directly the symmetry of the direction of softening and possible distortion of the ground state. Assuming that the primary force constants K in the two states are the same (for different K see [1]), we get a 2×2 secular equation with the following solution for the energies ε of the two states interacting under the linear vibronic coupling (the energy is read off the middle of the 2Δ interval between the initial levels):

ε= (1/2)Q±[Δ+FQ] (7)

It is seen from this expressions that, on taking into account the vibronic coupling, F≠0, the two APES curves change in different ways: in the upper sheet the curvature (the coefficient at Q in the expansion on Q) increases, whereas in the lower one it decreases. But until (F/K)<Δ the minima of both states correspond to the point Q = 0, as in the absence of vibronic mixing. However, if

the curvature of the lower curve of the APES becomes negative, and the system is unstable with respect to the Q displacements (Fig. 1). The minima points on the APES in this case are given by

±Q=[F/K-Δ/F] (9)

From these expressions and Fig. 1 it is seen that while the ground state is softened (destabilized) by the PJTE, the excited state is hardened (stabilized), and this effect is the larger, the smaller Δ and the larger F. It takes place in any polyatomic system and influences many molecular properties, including the existence of stable excited states of molecular systems that are unstable in the ground state (e.g., excited states of intermediates of chemical reactions); in general, even in the absence of instability the PJTE softens the ground state and increases the vibrational frequencies in the excited state.

The two branches of the APES for the case of strong PJTE resulting in the instability of the ground state (when the condition of instability (11) holds) are illustrated in Fig. 1b in comparison with the case when the two states have the same energy (Fig. 1a), i. e. when they are degenerate and the Jahn–Teller effect (JTE) takes place. We see that the two cases, degenerate and nondegenerate but close-in-energy (pseudodegenerate) are similar in generating two minima with distorted configurations, but there are important differences: while in the JTE there is a crossing of the two terms at the point of degeneracy (leading to conical intersections in more complicated cases), in the nondegenerate case with strong vibronic coupling there is an “avoided crossing” or “pseudo crossing”. Even a more important difference between the two vibronic coupling effects emerges from the fact that the two interacting states in the JTE are components of the same symmetry type, whereas in the PJTE each of the two states may have any symmetry. For this reason the possible kinds of distortion is very limited in the JTE, and unlimited in the PJTE. It is also noticeable that while the systems with JTE are limited by the condition of electron degeneracy, the applicability of the PJTE has no a priori limitations, as it includes also the cases of degeneracy. Even when the PJT coupling is weak and the inequality (11) does not hold, the PJTE is still significant in softening (lowering the corresponding vibrational frequency) of the ground state and increasing it in the excited state. When considering the PJTE in an excited state, all the higher in energy states destabilize it, while the lower ones stabilize it.
For a better understanding it is important to follow up on how the PJTE is related to intramolecular interactions. In other words, what is the physical driving force of the PJTE distortions (transformations) in terms of well-known electronic structure and bonding? The driving force of the PJTE is added (improved) covalence: the PJTE distortion takes place when it results in energy gain due to better covalence bonding between the atoms in the distorted configuration. Indeed, in the starting high-symmetry configuration the wavefunctions of the electronic states, ground and excited, are orthogonal by definition. By distortion their orthogonality is violated, and a nonzero overlap between them occurs. If for two near-neighbor atoms the ground state wavefunction pertains (mainly) to one of them, while the excited state wavefunction belongs (mainly) to the other one, the overlap by distortion adds covalency to the bonding between them, facilitating the distortion (Fig. 2).

Applications of the PJTE to solving chemical, physical, biological, and materials science problems are innumerable; as stated above, the PJTE is the only source of instability and distortions in high-symmetry configurations of molecular systems and solids with nondegenerate states, hence any problem steaming from such instability can be treated by the PJTE. Below are some illustrative examples.

PJTE versus Renner–Teller effect in bending distortions. Linear molecules are exceptions from the JTE, and for a long time it was assumed that their bending distortions in degenerate states (observed in many molecules) is produced by the Renner–Teller effect (RTE) (the splitting of the generate state by the quadratic terms of the vibronic coupling). However, recently it was proved (see in the review ) that the RTE, by splitting the degenerate electronic state, just softens the lower branch of the APES, but this lowering of the energy is not enough to overcome the rigidity of the linear configuration and to produce bending distortions. It follows that the bending distortion of linear molecular systems is due to, and only to the PJTE that mixes the electronic state under consideration with higher in energy (excited) states. This statement is enhanced by the fact that many linear molecules in nondegenerate states (and hence with no RTE) are, too, bent in the equilibrium configuration. The physical reason for the difference between the PJTE and the RTE in influencing the degenerate term is that while in the former case the vibronic coupling with the excited state produces additional covalent bonding that makes the distorted configuration preferable (see above, section 2.3), the RTE has no such influence; the splitting of the degenerate term in the RTE takes place just because the charge distribution in the two states becomes nonequivalent under the bending distortion.

Peierls distortions in linear chains. In linear molecules with three or more atoms there may be PJTE distortions that do not violate the linearity but change the interatomic distances. For instance, as a result of the PJTE a centrosymmetric linear system may become non-centrosymmetric in the equilibrium configurations, as, for example, in the BNB molecule (see in ). An interesting extension of such distortions in sufficiently long (infinite) linear chains was first considered by Peierls. In this case the electronic states, combinations of atomic states, are in fact band states, and it was shown that if the chain is composed by atoms with unpaired electrons, the valence band is only half filled, and the PJTE interaction between the occupied and unoccupied band states leads to the doubling of the period of the linear chain (see also in the books ).
Broken cylindrical symmetry. It was shown also that the PJTE not only produces the bending instability of linear molecules, but if the mixing electronic states involve a Δ state (a state with a nonzero momentum with respect to the axis of the molecule, its projection quantum number being Λ=2), the APES, simultaneously with the bending, becomes warped along the coordinate of rotations around the molecular axis, thus violating both the linear and cylindrical symmetry. It happens because the PJTE, by mixing the wavefunctions of the two interacting states, transfers the high momentum of the electrons from states with Λ=2 to states with lower momentum, and this may alter significantly their expected rovibronic spectra.

PJTE and combined PJTE plus JTE effects in molecular structures. There is a practically unlimited number of molecular systems for which the origin of their structural properties was revealed and/or rationalized based on the PJTE, or a combination of the PJTE and JTE. The latter stems from the fact that in any system with a JTE in the ground state the presence of a PJT active excited state is not excluded, and vice versa, the active excited state for the PJTE of the ground one may be degenerate, and hence JT active. Examples are shown, e.g., in Refs., including molecular systems like Na, CH, CX (X= H, F, Cl, Br), CO, SiR (with R as large ligands), planar cyclic CH, all kind of coordination systems of transition metals, mixed-valence compounds, biological systems, origin of conformations, geometry of ligands’ coordination, etc., etc. In fact it is difficult to find a molecular system for which the PJTE implications are a priori excluded, which is understandable in view of the mentioned above unique role of the PJTE in such instabilities.
Hidden PJTE, spin crossover, and magnetic-dielectric bistability. As mentioned above, there are molecular systems in which the ground state in the high-symmetry configuration is neither degenerate to trigger the JTE, nor does it interact with the low-lying excited states to produce the PJTE (e.g., because of their different spin multiplicity). In these situations the instability is produced by a strong PJTE in the excited states; this is termed “hidden PJTE” in the sense that its origin is not seen explicitly as a PJTE in the ground state. An interesting typical situation of hidden PJTE emerges in molecular and solid state systems with valence half-filed closed shells electronic configurations e and t. For instance, in the e case the ground state in the high-symmetry equilibrium geometry is an orbital non-degenerate triplet A, while the nearby low-lying two excited electronic states are close-in-energy singlets E and A; due to the strong PJT interaction between the latter, the lower component of E crosses the triplet state to produce a global minimum with lower symmetry. Fig. 3 illustrates the hidden PJTE in the CuF molecule, showing also the singlet-triplet spin crossover and the resulting two coexisting configurations of the molecule: high-symmetry (undistorted) spin-triplet state with a nonzero magnetic moment, and a lower in energy dipolar-distorted singlet state with zero magnetic moment. Such magnetic-dielectric bistability is inherent to a whole class of molecular systems and solids.

Puckering in planar molecules and graphene-like 2D and quasi 2D systems. Special attention has been paid recently to two-dimensional (2D) systems in view of a variety of their planar-surface-specific physical and chemical properties and possible graphene-like applications in electronics. Similar-to-graphene properties are sought for in silicene, phosphorene, boron nitride, zinc oxide, gallium nitride, as well as in other attractive subjects like 2D transition metal dichalcogenids and oxides, and there is a number of other organic and inorganic 2D and quasi-2D compounds with expected similar properties. One of the main important features of these systems is their planarity or quasi-planarity, but many of the quasi-2D compounds are subject to out-of-plane deviations known as puckering (buckling). It was shown, as expected, that the instability and distortions of the planar configuration (as in any other systems in nondegenerate state) is due to the PJTE. Detailed exploration of the PJTE in such systems allows one to reveal the excited states that are responsible for the puckering, and suggest possible external influence that restores their planarity, including oxidation, reduction, substitutions, or coordination to other species.

Cooperative PJTE in BaTiO-type crystals and ferroelectricity. In crystals with PJTE centers the interaction between the local distortions may lead to their ordering to produce a phase transition to a regular crystal phase with lower symmetry. Such cooperative PJTE is quite similar to the cooperative JTE; it was shown in one of the first applications of the PJTE to solid state systems that in case of ABO crystals with perovskite structure the local dipolar PJTE distortions at the transition metal B center and their cooperative interactions leads to ferroelectric phase transitions. Provided the criterion of PJTE is obeyed, each [BO] center has an APES with eight equivalent minima along the trigonal axes, six orthorhombic, and (higher) twelve tetragonal saddle-points between them. With temperature, the gradually reached transitions between the minima via the different kind of saddle-points explains the origin of all the four phases (three ferroelectric and one paraelectric) in perovskites of the type BaTiO and their properties. The predicted by the theory trigonal displacement of the Ti ion in all four phases, the fully disordered PJTE distortions in the paraelectric phase, and their partially disordered state in two other phases was confirmed by a variety of experimental investigations (see in ).

Multiferroicity and magnetic-ferroelectric crossover. The PJTE theory of ferroelectricity in ABO3 crystals was expanded to show that, dependent of the number of electrons in the d shell of the transition metal ion B and their low spin or high spin arrangement (which controls the symmetry and spin multiplicity of the ground and PJTE active excited states of the [BO] center), their ferroelectricity may coexist with a magnetic moment (multiferroicity). Moreover, in combination with the temperature dependent spin crossover phenomenon (which changes the spin multiplicity), this kind of multiferroicity may lead to a novel effect of magnetic-ferroelectric crossover.

Solid state magnetic-dielectric bistability. Similar to the mentioned above molecular bistability induced by the hidden PJTE, a magnetic-dielectric bistability due to two coexisting equilibrium configurations with corresponding properties may take place also in crystals with transition metal centers, subject to the electronic configuration with half-filled e or t shells. As in molecular systems, the latter produce a hidden PJTE and local bistability which, distinguished from the molecular case, are enhanced by the cooperative interactions, thus acquiring larger lifetimes. This crystal bistability was proved by calculations for LiCuO and NaCuO crystals, in which the Cu ion has the electronic e(d) configuration (similar to the CuF molecule).
Giant enhancement of observable properties in interaction with external perturbations. In a recent development of the PJTE applications in materials science it was shown that in crystals with PJTE centers, in which the local distortion are not ordered (before the phase transition to the cooperative phases), the effect of interaction with external perturbations acquires an orientational contribution which enhances the observable properties by several orders of magnitude. This was demonstrated on the properties of crystals like paraelectric BaTiO in interaction with electric fields (in permittivity and electrostriction), or under a strain gradient (flexoelectricity). These giant enhancement effects occur due to the dynamic nature of the PJTE local dipolar distortions (their tunneling between the equivalent minima); the independently rotating dipole moments on each center become oriented (frozen) along the external perturbation resulting in an orientational polarization which is not there in the absence of the PJTE


</doc>
<doc id="5435566" url="https://en.wikipedia.org/wiki?curid=5435566" title="Action-angle coordinates">
Action-angle coordinates

In classical mechanics, action-angle coordinates are a set of canonical coordinates useful in solving many integrable systems. The method of action-angles is useful for obtaining the frequencies of oscillatory or rotational motion without solving the equations of motion. Action-angle coordinates are chiefly used when the Hamilton–Jacobi equations are completely separable. (Hence, the Hamiltonian does not depend explicitly on time, i.e., the energy is conserved.) Action-angle variables define an invariant torus, so called because holding the action constant defines the surface of a torus, while the angle variables parameterize the coordinates on the torus.

The Bohr–Sommerfeld quantization conditions, used to develop quantum mechanics before the advent of wave mechanics, state that the action must be an integral multiple of Planck's constant; similarly, Einstein's insight into EBK quantization and the difficulty of quantizing non-integrable systems was expressed in terms of the invariant tori of action-angle coordinates.

Action-angle coordinates are also useful in perturbation theory of Hamiltonian mechanics, especially in determining adiabatic invariants. One of the earliest results from chaos theory, for the non-linear perturbations of dynamical systems with a small number of degrees of freedom is the KAM theorem, which states that the invariant tori are stable under small perturbations.

The use of action-angle variables was central to the solution of the Toda lattice, and to the definition of Lax pairs, or more generally, the idea of the isospectral evolution of a system.

Action angles result from a type-2 canonical transformation where the generating function is Hamilton's characteristic function formula_1 ("not" Hamilton's principal function formula_2). Since the original Hamiltonian does not depend on time explicitly, the new Hamiltonian formula_3 is merely the old Hamiltonian formula_4 expressed in terms of the new canonical coordinates, which we denote as formula_5 (the action angles, which are the generalized coordinates) and their new generalized momenta formula_6. We will not need to solve here for the generating function formula_7 itself; instead, we will use it merely as a vehicle for relating the new and old canonical coordinates.

Rather than defining the action angles formula_5 directly, we define instead their generalized momenta, which resemble the classical action for each original generalized coordinate

where the integration path is implicitly given by the constant energy function formula_10. Since the actual motion is not involved in this integration, these generalized momenta formula_11 are constants of the motion, implying that the transformed Hamiltonian formula_12 does not depend on the conjugate generalized coordinates formula_13

where the formula_13 are given by the typical equation for a type-2 canonical transformation

Hence, the new Hamiltonian formula_17 depends only on the new generalized momenta formula_6.

The dynamics of the action angles is given by Hamilton's equations

The right-hand side is a constant of the motion (since all the formula_20's are). Hence, the solution is given by

where formula_22 is a constant of integration. In particular, if the original generalized coordinate undergoes an oscillation or rotation of period formula_23, the corresponding action angle formula_13 changes by formula_25.

These formula_26 are the frequencies of oscillation/rotation for the original generalized coordinates formula_27. To show this, we integrate the net change in the action angle formula_13 over exactly one complete variation (i.e., oscillation or rotation) of its generalized coordinates formula_27

Setting the two expressions for formula_31 equal, we obtain the desired equation

The action angles formula_5 are an independent set of generalized coordinates. Thus, in the general case, each original generalized coordinate formula_34 can be expressed as a Fourier series in "all" the action angles

where formula_36 is the Fourier series coefficient. In most practical cases, however, an original generalized coordinate formula_27 will be expressible as a Fourier series in only its own action angles formula_13

The general procedure has three steps:


In some cases, the frequencies of two different generalized coordinates are identical, i.e., formula_42 for formula_43. In such cases, the motion is called degenerate.

Degenerate motion signals that there are additional general conserved quantities; for example, the frequencies of the Kepler problem are degenerate, corresponding to the conservation of the Laplace–Runge–Lenz vector.

Degenerate motion also signals that the Hamilton–Jacobi equations are completely separable in more than one coordinate system; for example, the Kepler problem is completely separable in both spherical coordinates and parabolic coordinates.




</doc>
<doc id="58103913" url="https://en.wikipedia.org/wiki?curid=58103913" title="Spectroscopy of multiply ionized atoms">
Spectroscopy of multiply ionized atoms

This branch of spectroscopy deals with radiation related to atoms that are stripped of several electrons (multiply ionized atoms (MIA), multiply charged ions, highly charged ions). These are observed in very hot plasmas (laboratory or astrophysical) or in accelerator experiments (beam-foil, electron beam ion trap (EBIT)). The lowest exited electron shells of such ions decay into stable ground states producing photons in VUV, EUV and soft X-ray spectral regions (so-called resonance transitions).

After Newton's. discovery of spectral structure of white light (17th century) and subsequent studies of the nature of light (Hooke, Huygens, Young) J. Fraunhofer observed and measured dark lines in the Sun's spectrum (they bear now his name although several of them were observed earlier by Wollaston). It may be the first example of fundamental research in spectroscopy.

Later Bunsen and Kirchhoff found that Fraunhofer lines correspond to emission spectral lines observed in laboratory light sources, and so they laid way for spectrochemical analysis in laboratory and astrophysics.

In the 19th century new developments such as the discovery of photography, Rowland's invention of the concave diffraction grating, and Schumann's works on discovery of vacuum ultraviolet (fluorite for prisms and lenses, low-gelatin photographic plates and absorption of UV in air below 185 nm) made advance to shorter wavelengths very fast. At the same time Dewar observed series in alkali spectra, Hartley found constant wave-number differences, Balmer discovered a relation connecting wavelengths in the visible hydrogen spectrum, and finally Rydberg derived a formula for wave-numbers of spectral series.

The first decade of the 20th century brought the basics of quantum theory (Plank, Einstein) and interpretation of spectral series of hydrogen by Lyman in VUV and by Paschen in infrared. Ritz formulated the combination principle.

In 1913 Bohr formulated his quantum mechanical model of atom. This stimulated empirical term analysis (see references in, page 83).

Between 1920 and 1930 fundamental concepts of quantum mechanics were developed by Pauli, Heisenberg, Schrödinger, and Dirac. Understanding of the spin and exclusion principle allowed conceiving how electron shells of atoms are filled with the increasing atomic number.

Further progress in studies of atomic structure was in tight connection with the advance to shorter wavelength in EUV region. Millikan, Sawyer, Bowen used electric discharges in vacuum to observe some emission spectral lines down to 13 nm they prescribed to stripped atoms. In 1927 Osgood and Hoag reported on grazing incidence concave grating spectrographs and photographed lines down to 4.4 nm (K of carbon). Dauvillier used a fatty acid crystal of large crystal grating space to extend soft x-ray spectra up to 12.1 nm, and the gap was closed. In the same period Manne Siegbahn constructed a very sophisticated grazing incidence spectrograph that enabled Ericson and Edlén to obtain spectra of vacuum spark with high quality and to reliably identify lines of multiply ionized atoms up to O VI, with five stripped electrons. Grotrian developed his graphic presentation of energy structure of the atoms. Russel and Saunders proposed their coupling scheme for the spin-orbit interaction and their generally recognized notation for spectral terms.

Theoretical quantum-mechanical calculations become rather accurate to describe the energy structure of some simple electronic configurations. The results of theoretical developments were summarized by Condon and Shortley in 1935.

Edlén thoroughly analyzed spectra of MIA for many chemical elements and derived regularities in energy structures of MIA for many isoelectronic sequences (ions with the same number of electrons, but different nuclear charges). Spectra of rather high ionization stages (e.g. Cu XIX) were observed.

The most exciting event was in 1942, when Edlén proved the identification of some solar coronal lines on the basis of his precise analyses of spectra of MIA. This implied that the solar corona has a temperature of a million degrees, and strongly advanced understanding of solar and stellar physics.

After the WW II experiments on balloons and rockets were started to observe the VUV radiation of the Sun. (See X-ray astronomy). More intense research continued since 1960 including spectrometers on satellites.

In the same period the laboratory spectroscopy of MIA becomes relevant as a diagnostic tool for hot plasmas of thermonuclear devices (see Nuclear fusion) which begun with building Stellarator in 1951 by Spitzer, and continued with tokamaks, z-pinches and the laser produced plasmas. Progress in ion accelerators stimulated beam-foil spectroscopy as a means to measure lifetimes of exited states of MIA. Many various data on highly exited energy levels, autoionization and inner-core ionization states were obtained. 

Simultaneously theoretical and computational approaches provided data necessary for identification of new spectra and interpretation of observed line intensities. New laboratory and theoretical data become very useful for spectral observation in space. It was a real upheaval of works on MIA in USA, England, France, Italy, Israel, Sweden, Russia and other countries

A new page in the spectroscopy of MIA may be dated as 1986 with development of EBIT (Levine and Marrs, LLNL) due to a favorable composition of modern high technologies such as cryogenics, ultra-high vacuum, superconducting magnets, powerful electron beams and semiconductor detectors. Very quickly EBIT sources were created in many countries (see NIST summary for many details as well as reviews).

A wide field of spectroscopic research with EBIT is enabled including achievement of highest grades of ionization (U), wavelength measurement, hyperfine structure of energy levels, quantum electrodynamic studies, ionization cross-sections (CS) measurements, electron-impact excitation CS, X-ray polarization, relative line intensities, dielectronic recombination CS, magnetic octupole decay, lifetimes of forbidden transitions, charge-exchange recombination, etc.


</doc>
<doc id="59478233" url="https://en.wikipedia.org/wiki?curid=59478233" title="Perpendicular paramagnetic bond">
Perpendicular paramagnetic bond

A perpendicular paramagnetic bond is a type of chemical bond (in contrast to covalent or ionic bonds) that does not exist under normal, atmospheric conditions. Such a phenomenon was first hypothesized through simulation to exist in the atmospheres of white dwarf stars whose magnetic fields, on the order of 10 teslas, allow such interactions to exist. Normally, at such intense temperatures as those near a white dwarf, more common molecular bonds cannot form and existing ones decompose.


</doc>
<doc id="8929369" url="https://en.wikipedia.org/wiki?curid=8929369" title="Vaiśeṣika Sūtra">
Vaiśeṣika Sūtra

Vaiśeṣika Sūtra, (Sanskrit: वैशेषिक सूत्र), also called Kanada sutra, is an ancient Sanskrit text at the foundation of the Vaisheshika school of Hindu philosophy. The sutra was authored by the Hindu sage Kanada, also known as Kashyapa. According to some scholars, he flourished before the advent of Buddhism because the "Vaiśeṣika Sūtra" makes no mention of Buddhism or Buddhist doctrines; however, the details of Kanada's life are uncertain, and the "Vaiśeṣika Sūtra" was likely compiled sometime between 6th and 2nd century BCE, and finalized in the currently existing version before the start of the common era.

A number of scholars have commented on it since the beginning of common era; the earliest commentary known is the "Padartha Dharma Sangraha" of Prashastapada. Another important secondary work on "Vaiśeṣika Sūtra" is Maticandra's "Dasha padartha sastra" which exists both in Sanskrit and its Chinese translation in 648 CE by Yuanzhuang.

The "Vaiśeṣika Sūtra" is written in aphoristic sutras style, and presents its theories on the creation and existence of the universe using naturalistic atomism, applying logic and realism, and is one of the earliest known systematic realist ontology in human history. The text discusses motions of different kind and laws that govern it, the meaning of dharma, a theory of epistemology, the basis of Atman (self, soul), and the nature of yoga and moksha. The explicit mention of motion as the cause of all phenomena in the world and several propositions about it make it one of the earliest texts on physics.

The name "Vaiśeṣika Sūtra" (Sanskrit: वैशेषिक सूत्र) is derived from "viśeṣa", विशेष, which means "particularity", that is to be contrasted from "universality". The classes particularity and universality belong to different categories of experience.

Till the 1950s, only one manuscript of "Vaiseshika sutra" was known and this manuscript was part of a bhasya by the 15th century Sankaramisra. Scholars had doubted its authenticity, given the inconsistencies in this manuscript and the quotes in other Hindu, Jaina and Buddhist literature claiming to be from the "Vaisheshika Sutra". In the 1950s and early 1960s, new manuscripts of "Vaiśeṣika Sūtra" were discovered in distant parts of India, which were later identified as this Sutra. These newer manuscripts are quite different, more consistent with the historical literature, and suggests that, like other major texts and scriptures of Hinduism, "Vaiśeṣika Sūtra" too suffered interpolations, errors in transmission and distortion over time. A critical edition of the "Vaiśeṣika Sūtra" is now available.

The "Vaisheshika Sutras" mention the doctrines of competing schools of Indian philosophy such as Samkhya and Mimamsa, but make no mention of Buddhism, which has led scholars in more recent publications to posit estimates of 6th to 2nd century BCE.

The critical edition studies of "Vaisheshika Sutras" manuscripts discovered after 1950, suggest that the text attributed to Kanada existed in a finalized form sometime between 200 BCE and the start of the common era, with the possibility that its key doctrines are much older. Multiple Hindu texts dated to the 1st and 2nd century CE, such as the "Mahavibhasa" and "Jnanaprasthana" from the Kushan Empire, quote and comment on Kanada's doctrines. Although the "Vaisheshika Sutras" makes no mention of the doctrines of Jainism and Buddhism, their ancient texts mention "Vaisheshika Sutras" doctrines and use its terminology, particularly Buddhism's Sarvastivada tradition, as well as the works of Nagarjuna.

Physics is central to Kaṇāda’s assertion that all that is knowable is based on motion. His ascribing centrality to physics in the understanding of the universe also follows from his invariance principles. For example, he says that the atom must be spherical since it should be the same in all dimensions. He asserts that all substances are composed of atoms, two of which have mass and two are massless.
The philosophy in "Vaiseshika sutra" is atomistic pluralism, states Jayatilleke. Its ideas are known for its contributions to "inductive inference", and often coupled with the "deductive logic" of the sister school of Hinduism called the Nyaya. James Thrower and others call Vaiśeṣika philosophy to be naturalism, one that rejects the supernatural.

The text states:

Several traits of substances (dravya) are given as colour, taste, smell, touch, number, size, the separate, coupling and uncoupling, priority and posterity, comprehension, pleasure and pain, attraction and revulsion, and wishes. Like many foundational texts of classical schools of Hindu philosophy, God is not mentioned in the sutra, and the text is non-theistic.

The critical edition of the "Vaisheshika Sutras" are divided into ten chapters, each subdivided into two sections called āhnikas:




</doc>
<doc id="59833659" url="https://en.wikipedia.org/wiki?curid=59833659" title="Gapped Hamiltonian">
Gapped Hamiltonian

In many-body physics, most commonly within condensed-matter physics, a gapped Hamiltonian is a Hamiltonian for an infinitely large many-body system where there is a finite energy gap separating the (possibly degenerate) ground space from the first excited states. A Hamiltonian that is not gapped is called gapless.

The property of being gapped or gapless is formally defined through a sequence of Hamiltonians on finite lattices in the thermodynamic limit.

An example is the BCS Hamiltonian in the theory of superconductivity.

In quantum field theory, a continuum limit of many-body physics, a gapped Hamiltonian induces a mass gap.


</doc>
